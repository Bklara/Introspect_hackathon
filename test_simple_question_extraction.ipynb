{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from slack_data_loader import SlackLoader\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_dump = './opendatascience Slack export May 20 2017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exporter = SlackLoader(path_to_dump, only_channels=('deep_learning',),\n",
    "                           start_date=datetime.datetime(2017, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7540 messages\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded {} messages\".format(len(exporter.messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel_attrs = ['id', 'name', 'created', 'creator', 'is_archived', 'is_general', 'pins', 'topic']\n",
    "\n",
    "def channels_to_df(channels):\n",
    "    full_list = []\n",
    "    for ch_id, ch_dict in channels.items():\n",
    "        new_channel_dict = {}\n",
    "        for k in channel_attrs:\n",
    "            new_channel_dict[k] = ch_dict.get(k, None)\n",
    "        new_channel_dict['num_members'] = len(ch_dict['members'])\n",
    "        new_channel_dict['purpose'] = ch_dict['purpose']['value']\n",
    "        full_list.append(new_channel_dict)\n",
    "    return pd.DataFrame(full_list).set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created</th>\n",
       "      <th>creator</th>\n",
       "      <th>is_archived</th>\n",
       "      <th>is_general</th>\n",
       "      <th>name</th>\n",
       "      <th>num_members</th>\n",
       "      <th>pins</th>\n",
       "      <th>purpose</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C2A4GEL6M</th>\n",
       "      <td>1473445368</td>\n",
       "      <td>U04ELQZAU</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>alexyashadasha</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>{'value': '', 'creator': '', 'last_set': '0'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1P8YT7C7</th>\n",
       "      <td>1467817046</td>\n",
       "      <td>U04URBM8V</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>bayesian</td>\n",
       "      <td>307</td>\n",
       "      <td>[{'id': '1467888432.000030', 'type': 'C', 'use...</td>\n",
       "      <td>Church of Bayes: Discussing Bayesian statistic...</td>\n",
       "      <td>{'value': ':bayes:', 'creator': 'U04ELQZAU', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C0804BS5Q</th>\n",
       "      <td>1437511383</td>\n",
       "      <td>U049NHC4X</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>big_data</td>\n",
       "      <td>1301</td>\n",
       "      <td>[{'id': '1485303977.000947', 'type': 'C', 'use...</td>\n",
       "      <td>Hadoop, Spark и прочее\\r\\n\\r\\nПолезные материа...</td>\n",
       "      <td>{'value': 'Big Pain in the ...', 'creator': 'U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C0MQQT6E6</th>\n",
       "      <td>1455738772</td>\n",
       "      <td>U070Y25AS</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>bioinformatics</td>\n",
       "      <td>125</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>{'value': ':bioscience:', 'creator': 'U04ELQZA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C115898GZ</th>\n",
       "      <td>1460749144</td>\n",
       "      <td>U04422XJL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>blackoxchallenge</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>{'value': '', 'creator': '', 'last_set': '0'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              created    creator  is_archived  is_general              name  \\\n",
       "id                                                                            \n",
       "C2A4GEL6M  1473445368  U04ELQZAU         True       False    alexyashadasha   \n",
       "C1P8YT7C7  1467817046  U04URBM8V        False       False          bayesian   \n",
       "C0804BS5Q  1437511383  U049NHC4X        False       False          big_data   \n",
       "C0MQQT6E6  1455738772  U070Y25AS        False       False    bioinformatics   \n",
       "C115898GZ  1460749144  U04422XJL         True       False  blackoxchallenge   \n",
       "\n",
       "           num_members                                               pins  \\\n",
       "id                                                                          \n",
       "C2A4GEL6M            0                                               None   \n",
       "C1P8YT7C7          307  [{'id': '1467888432.000030', 'type': 'C', 'use...   \n",
       "C0804BS5Q         1301  [{'id': '1485303977.000947', 'type': 'C', 'use...   \n",
       "C0MQQT6E6          125                                               None   \n",
       "C115898GZ            0                                               None   \n",
       "\n",
       "                                                     purpose  \\\n",
       "id                                                             \n",
       "C2A4GEL6M                                                      \n",
       "C1P8YT7C7  Church of Bayes: Discussing Bayesian statistic...   \n",
       "C0804BS5Q  Hadoop, Spark и прочее\\r\\n\\r\\nПолезные материа...   \n",
       "C0MQQT6E6                                                      \n",
       "C115898GZ                                                      \n",
       "\n",
       "                                                       topic  \n",
       "id                                                            \n",
       "C2A4GEL6M      {'value': '', 'creator': '', 'last_set': '0'}  \n",
       "C1P8YT7C7  {'value': ':bayes:', 'creator': 'U04ELQZAU', '...  \n",
       "C0804BS5Q  {'value': 'Big Pain in the ...', 'creator': 'U...  \n",
       "C0MQQT6E6  {'value': ':bioscience:', 'creator': 'U04ELQZA...  \n",
       "C115898GZ      {'value': '', 'creator': '', 'last_set': '0'}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = channels_to_df(exporter.channels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qwords = (\"как\", \"какой\", \"зачем\", \"почему\", \"когда\", \"кто\", \"где\", \"когда\", \"куда\", \"чот\")\n",
    "splitter = re.compile(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\")\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemm(st):\n",
    "    if st == '':\n",
    "        return ''\n",
    "    else:\n",
    "        return morph.parse(st)[0].normal_form\n",
    "\n",
    "def is_question(d):\n",
    "    x = d.lower()\n",
    "    snt = x.split()\n",
    "    num_words = len(snt)\n",
    "    snt = [lemm(w) for w in snt]\n",
    "    #print((num_words > 4) and  any(w in qwords for w in snt), [w in qwords for w in snt])\n",
    "    return (num_words > 4) and any(w in qwords for w in snt)\n",
    "\n",
    "def contains_sentance_with_questions(d):\n",
    "    x = d['text'].lower()\n",
    "    sents = splitter.split(x)\n",
    "    #print(any(map(is_question, sents)))\n",
    "    return any(map(is_question, sents))\n",
    "\n",
    "questions = list(filter(contains_sentance_with_questions, exporter.messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "questions = list(filter(contains_sentance_with_questions, exporter.messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1255 questions\n"
     ]
    }
   ],
   "source": [
    "print(\"found {} questions\".format(len(questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Variational Inference используется для нахождения апостериорного распределения. А зачем нам нужно апостериорное распределение? Используется ли как-то VI в нейросетях (может это имеет отношение ко второму вопросу)?\n",
      "----------------------------------------\n",
      "2. В вероятностном подходе по машинному обучению задача обучения (MAP) ставится как W = argmax(p(D|W) * p(W)), где p(W) - априорное распределение весов, D - обучающая выборка, p(D|W) = p(x_1|W)p(x_2|W)...p(x_N|W), p(x|W) - вероятность появления x, т.е. p(x|W) - не произвольная функция, а положительная и суммируется к единице, Если f(x, W) - нейросеть, то p(x|W)=f(x,W)/integral(f(x, W))dx. Но по факту при обучении никакого интеграла не вычисляется. Как это происходит и где подробнее об этом почитать?\n",
      "----------------------------------------\n",
      "<@U0WFHMB4L>: точечная оценка ничего не говорит о том, насколько модель уверена в своём ответе. А вот в байесовском (см. <#C1P8YT7C7|bayesian>) подходе к выводу ищется как раз распределение или какое-то к нему приближение. Кроме того, это самое апостеорное полезно иметь для оптимального (с учётом неопредённости) моделирования новых данных. Однако, это всё непростые вещи. т.к. в процессе возникают неберущиеся интегралы или экспоненциального размера суммы\n",
      "----------------------------------------\n",
      "Касаемо второго: надо смотреть на конкретную модель, но\n",
      "1. В случае стандартой supervised задачи у нас есть набор пар (xᵢ, yᵢ) и моделируем мы `yᵢ` при условии наблюдения `xᵢ`, т.е. ищем `argmax p(W | D) = argmax ∏ᵢ p(yᵢ | xᵢ, W) p(W | xᵢ)`, где `p(W | xᵢ) = p(W)` поскольку нейросеть не зависит от генеративной модели `x`'ов, а только от `y|x`. То есть нам нужно брать интеграл не по `x, y`, а только по `y`, что мы и делаем, ставя в выходной слой softmax, sigmoid или L1 / L2.\n",
      "2. В случае генеративных unsupervised задач вроде вариационного автокодировщика, у нас есть 2 сети: `enc(x)`, генерирующая код `z` (который является случайной величиной, зависящей от `x`) и `dec(z)` (тоже случая величина, зависящая от `z`), генерирующая восстановленную картинку. Для время обучения мы не можем точно посчитать интеграл в знаменателе `p(z|x)` (распределение для `enc(x)`), поэтому и пользумся вариационными методами для его приближения с помощью какого-то \"берущегося\" `q(z|x)`\n",
      "----------------------------------------\n",
      "То есть, как только у нас есть какое-то наблюдение `x`, нам уже неважно, откуда оно пришло, \"мы\" в его формировании не участвовали, поэтому и подстраиваться под него не надо (что совсем не так для `y`)\n",
      "----------------------------------------\n",
      "Понятно, спасибо. А где можно подробно почитать на тему вероятностного подхода для нейросетей?\n",
      "----------------------------------------\n",
      ":wat:, `p(y|x)` параметризовано W, ещё как зависит\n",
      "----------------------------------------\n",
      "2) когда мы вдруг начинаем хотеть интерпретируемую модель и начинаем оптимизировать в пространстве объектов, то хотя бы без простейшего априорного распределения на X получается лажа\n",
      "----------------------------------------\n",
      "Пойнт в том, что в седловой точке есть направления, которые меняются часто когда ты по ней туда-сюда мотаешься\n",
      "----------------------------------------\n",
      "А есть которые маленькие, но в постоянную сторону - это то, где у седловой точки вторая производная максимум, а не минимум\n",
      "----------------------------------------\n",
      "В статье дается другое интуитивное объяснение зачем усиливать сигнал по направлениям, где градиент маленький\n",
      "----------------------------------------\n",
      "Вот наверное где я это прочитал\n",
      "----------------------------------------\n",
      "Мне ещё интересно, что там происходит в зависимости от соотношения между знаками собственных чисел гессиана. Кажется, что чем больше доля положительных, тем меньше должен быть loss. И, быть может, наши нейросети не сходятся к локальным минимумам, а застревают в сёдлах, где всего одно направление ведёт к уменьшению ошибки\n",
      "----------------------------------------\n",
      "там был простенький пейпер на несколько страниц где стажер лекуна подробно смотрит на собственные числа гессиана сетки решающей простую задачу (вроде мнист)\n",
      "----------------------------------------\n",
      "Сейчас по-моему проблемы не выполнить оптимизацию точно, а как бы не переобучиться\n",
      "----------------------------------------\n",
      "Есть даже очень популярные статьи, где советуют нормальный шум к градиентам добавлять\n",
      "----------------------------------------\n",
      "Аригато в хату! У меня нубский вопрос про свертки, подскажите плиз. Вот мы берем сеть типа Ленет. Первая свертка - 6 фильтров в виде матриц 5х5, получаем 6 feature maps. А вторая свертка записыватся как 50 фильтров, тоже 5х5.  Сам вопрос: означает ли это, что каждый фильтр \"смотрит\" на все 6 feature maps, и происходит поэлементное умножение и суммирование 5х5*6 значений? И так получаем 50 feature maps.\n",
      "----------------------------------------\n",
      "И этот прикол  с использованием только части слоев, как у Лекуна, теперь не актуален и не  особо используется?\n",
      "----------------------------------------\n",
      "<@U0C1BGRB2>  <@U07V1URT9> спасибо, буду посмотреть, когда дорасту до нужного уровня понимания \n",
      "----------------------------------------\n",
      "Я думаю надо тупо FCN с Resnet как бейзлайн пробовать\n",
      "----------------------------------------\n",
      "Обычный segmentation я делать вроде как умею. Но вот к спутниковым снимкам у меня он прикручивается пока так себе. Что FCN, что Unet.\n",
      "----------------------------------------\n",
      "Спасибо. Вот что было на последних конференциях я как раз и не в крусе.\n",
      "----------------------------------------\n",
      "Спасибо.\n",
      "\n",
      "А теперь более специфичный вопрос: \"Какие статьи/блоги/книжки можно почитать про то, как нейронки и прочий CV прикручивается к обработке картографических снимков?\"\n",
      "----------------------------------------\n",
      "звучит как отличный вариант :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "а как ты сопоставляешь стадии обучения?\n",
      "----------------------------------------\n",
      "Я обычно учу на фиксированном lr и когда лосс начинает расти/калбаситься на месте, то понижаю lr в 10 раз с лучшей эпохи.\n",
      "----------------------------------------\n",
      "&gt; Сложно сказать сколько сетка раз выдела весь датасет.\n",
      "А как ты минибатчи формируешь? Так-то одна эпоха = один проход по датасету\n",
      "----------------------------------------\n",
      "Поэтому после того как доучу нужно будет прогнать бенчмарки в которых сетка будет использоваться и уже делать выводы.\n",
      "----------------------------------------\n",
      "Типа, что за архитектура примерно, какая скорость, как реализовано\n",
      "----------------------------------------\n",
      "Это из-за того, что в сети ~300 очень мелких слоев и затраты на запуск куда ядер очень большие)\n",
      "----------------------------------------\n",
      "<@U3MH9AU1Z> не, никто не говорит, что неправда, просто реально неожиданно. выкладывайте на гитхаб, будьте как большие ребята )))\n",
      "----------------------------------------\n",
      "причины я описал выше, куда хороша когда мало тяжких слоев, а в случае 500 но очень малых скорость самих сверток уже нивелируется накладными расходами связанными с памятью и тп\n",
      "----------------------------------------\n",
      "тренирую Keras модель и получаю довольно разные результаты, уже зафиксировал seed где смог, отключил shuffle а всё равно результаты немного разняться… как добиться более стабильной воспроизводимости?\n",
      "----------------------------------------\n",
      "<https://opendatascience.slack.com/archives/deep_learning/p1483712859001836>\n",
      "вот для этого seed какой нибудь хотелось бы иметь… чтобы не плавало\n",
      "----------------------------------------\n",
      "привет!\n",
      "вопрос по тензорфлоу в связке с керасом. у меня есть модель которая натрейнена керасом - мне надо ее вызвать с помощью тензор флоу.\n",
      "\n",
      "для этого я делаю \n",
      "import tensorflow as tf\n",
      "sess = tf.Session()\n",
      "\n",
      "from keras import backend as K\n",
      "K.set_learning_phase( 0 )\n",
      "K.set_session(sess)\n",
      "\n",
      "далее делаю у \n",
      "model = keras.models.load_model(...)\n",
      "\n",
      "вся проблема когда я делаю sess.run(model.output, {'input_1:0', img})\n",
      "если сделать вот так, то ругается на не инициализированные переменные\n",
      "если до этого сделать sess.run(tf.initialize_all_variables()), то все веса перетираются на кашу и выдает неправильные результат\n",
      "----------------------------------------\n",
      "самое обидное, что я делал ровно такое месяц назад, но забыл как =/\n",
      "----------------------------------------\n",
      "Я читаю диссертацию Minh'a и он там долго рассказывает, как они прикрутили CRF к loss function в задаче сегментации, чтобы усугубить тот факт, что близко расположенные пиксели сильно скоррелироаны.\n",
      "\n",
      "Его диссертация 2013 года. \n",
      "\n",
      "Вопрос: народ все еще занимается такой подкруткой или используются другие методы, или же сети стали насатолько умные, что это уже и не надо?\n",
      "----------------------------------------\n",
      "<@U34Q3KU8H> когда я смотрел эту статью, выигрыш был не адский\n",
      "----------------------------------------\n",
      "Ну, каждый решает какой input range у них сам\n",
      "----------------------------------------\n",
      "Если используешь pretrained, то надо чтобы был такой же, как у тех, кто тренировал\n",
      "----------------------------------------\n",
      "С pretrained понятно, что надо все так же как и у авторов.\n",
      "\n",
      "А вот mean когда вычитают - это же constant factor, который все-равно будет сдвинут во время batchNorm. так уж велика разница в скорости сходимости?\n",
      "----------------------------------------\n",
      "Как правильно использовать BatchNormalization в keras, в том смысле, что какие параметры втыкать в mode и axis?\n",
      "\n",
      "batch normalization втыкается после convolution2d\n",
      "\n",
      "Я правильно понимаю, что если на вход сети идет (num_batch, num_channels, X, Y)\n",
      "\n",
      "то надо\n",
      "\n",
      "`BatchNormalization(mode=0, axis=1)`\n",
      "\n",
      "?\n",
      "----------------------------------------\n",
      "Всем привет\n",
      "\n",
      "Я хотел уточнить.\n",
      "Я пробую ковырять керас и просто беру модели из других скриптов as is.\n",
      "Например, squezenet отсюда:\n",
      "<https://github.com/yhenon/pysqueezenet/blob/master/squeezenet.py>\n",
      "\n",
      "Проблема в том, что когда я использую функцию модели она ломается compile time\n",
      "на добавлении второго слоя \"ValueError: Negative dimension size caused by subtracting 3 from 1”\n",
      "\n",
      "Учитывая, что поведение идентично для двух разных моделей, я думаю что проблема на моей стороне, но я, честно говоря, теряюсь и не знаю даже куда смотреть. Подскажите куда смотреть\n",
      "\n",
      "Бекенд — TensorFlow\n",
      "----------------------------------------\n",
      "Всем привет. Можете подсказать про правильное использование batch norm. Я правильно понимаю, что после того как я закончил обучение, то мне надо пересчитать для всех слоев для которых которых я использовал batch norm средние и стандартные отклонения по всему тренировочному набору? Просто, наприер, если я в лазанье ставлю `deterministic=False`, то для нормализации будет использоваться накопленная moving average статистика, что кажется не совсем правильно, и я не нашел какого-то удобного способа, чтобы заменять эту статистику на статистику по всему тренировочнумо сету.  Насколько критично пересчитывать нормализацию по всему трейнинг сету?\n",
      "----------------------------------------\n",
      "господа, а как так происходит что когда тормозишь цикл обучения, nvidia-smi показывает в перед отключением 75 градусов, а на следующую секунду уже 68, не может же так сразу на 7 градусов холоднее стать?\n",
      "----------------------------------------\n",
      "Точнее, скорость теплообмена выше там, где большой градиент температуры :)\n",
      "----------------------------------------\n",
      "А есть какая-то интуиция какую долю должна регуляризационная часть составлять в лоссе при обучении (от ошибки примеров)? 100%, 10%, 1%?\n",
      "----------------------------------------\n",
      "Чат привет.\n",
      "Есть сверточная сеть для классификации картинок с последним softmax.\n",
      "Чат, посоветуй, как лучше заменить softmax на one-class классификацию для каждого класса?\n",
      "----------------------------------------\n",
      "ну да норм, недавно была статья где еще такие картинки после обучения прогоняли через “выправлятор”, кторый из говнокартинок делал нормас картинки\n",
      "----------------------------------------\n",
      "Народ, seq2seq для NTM кто нибудь юзал? а то на гитхабе из коробки ни у кого нет.\n",
      "----------------------------------------\n",
      "подскажите как я могу в tflearn загружать трейнсет батчами и отдавать на обучение. \n",
      "я взял пример с генератором: <https://github.com/pannous/tensorflow-speech-recognition/blob/master/speaker_classifier_tflearn.py>\n",
      "там так:\n",
      "`batch=data.wave_batch_generator(batch_size=1000, source=data.Source.DIGIT_WAVES, target=data.Target.speaker)`\n",
      "`X,Y=next(batch)`\n",
      "и потом\n",
      "`model.fit(X, Y, n_epoch=100, show_metric=True, snapshot_step=100)`\n",
      "----------------------------------------\n",
      "кто ж теану то возьмет под свой контроль\n",
      "----------------------------------------\n",
      "а почему теану не удобно?\n",
      "----------------------------------------\n",
      "но закрытые так что всем пох сколько и каких\n",
      "----------------------------------------\n",
      "ну кароч у них есть свой внутренний чисто для дл и как минимум один\n",
      "----------------------------------------\n",
      "Я имею в виду не зачем запустил, а что конкретно, какой из ганов\n",
      "----------------------------------------\n",
      "А какая интуиция говорит, что л2 регуляризация не нужна когда есть батчнорм? Мне совсем неочевидно <@U06J1LG1M>\n",
      "----------------------------------------\n",
      "где-то еще статья проскакивала, что в дип лернинге про регуляризацию надо по-другому думать, не так как в “классических” моделях: <https://arxiv.org/abs/1611.03530>\n",
      "----------------------------------------\n",
      "Я на днях как раз думал, что L2 регуляризация весов бессмысленна, если потом к слою применяется BN\n",
      "----------------------------------------\n",
      "Кстати, понятно, почему BN - это хорошо. Кто может поделиться опытом / литературой на тему, когда  BN - это плохо и когда его втыкать не надо?\n",
      "----------------------------------------\n",
      "А по опыту, примерно, где граница между совсем маленький vs можно применять BN\n",
      "----------------------------------------\n",
      "Почему вокруг тф больше хайпа, чем вокруг теано?\n",
      "----------------------------------------\n",
      "почему не для продакшена тогда?\n",
      "----------------------------------------\n",
      "под \"продакшеном\" ты имеешь ввиду когда нужно гонять inference на натренированной модели, более-менее в реалтайме?\n",
      "----------------------------------------\n",
      "А какое железо нужно чтобы такое генерить за адекватное время?\n",
      "----------------------------------------\n",
      "но у меня есть ощущение что процентов 80 времени сеть тусовалась в аттракторе где то\n",
      "----------------------------------------\n",
      "пока то что больше всего опечалило это то что если фиксировать все кроме одной размерности и ее изменять, то картинка особо не меняется, плавный переход от одной формы к другой это какая то нелинейная траектория в скрытом пространстве, которая по идее должна быть disentangled в процессе обучения\n",
      "----------------------------------------\n",
      "почему должна быть disentangled ?\n",
      "----------------------------------------\n",
      "вроде как вся суть representation learning это disentanglement степеней свободы в латентные факторы, ну т.е. ожидается что то типа как конструктор рожи, типа <https://www.youtube.com/watch?v=7OjAVDeBbLs>\n",
      "----------------------------------------\n",
      "а пока видно каждая размерность добавляет какой то слой по всему изображения, который похож на эннную компоненту ПЦА, далеко не первую\n",
      "----------------------------------------\n",
      "кароче человеческому глазу он кажется как шум\n",
      "----------------------------------------\n",
      "вот есть два стула, модели в смысле, у одной одни нейрон отвечает за открытость рта, а у другой открытость рта это такая траектория когда некоторая нелинейная функция f(x1, x2, …, х10) от значений 10 нейронов удовлетворяет каким то ограничениям\n",
      "\n",
      "какую ты предпочтешь? наверное первую\n",
      "\n",
      "это как мне видется понятие развязывания размерностей, как бы во второй модели относительно пространства пикселей очень даже развязана фича открытость рта, но у первой еще более\n",
      "----------------------------------------\n",
      "ну это так чисто мое видение, а каких то определений я не видел\n",
      "----------------------------------------\n",
      "лемпицкий вроде упоминал про всякие адовые ганы где конкретная координата в эмбеддинге отвечает например за поворот головы\n",
      "----------------------------------------\n",
      "да я согласен с примером про улыбку, где то есть граница разумного, что например открытость глаза описывается одномерной фичой, а улыбка нет\n",
      "----------------------------------------\n",
      "например про эмоции, <http://cbcsl.ece.ohio-state.edu/cvpr16.pdf> тут вот ссылка есть на какую то статью, где физиолог описал весь спектр человеческих эмоций всего 50-100 факторами лица, которые можно обозревать\n",
      "----------------------------------------\n",
      "не могу достучаться до судоера на сервере, нет пермиссий писать что-то в директорию с кудой\n",
      "----------------------------------------\n",
      "поставь куду в хомяка и пропиши в PATH и LD_LIBRARY_PATH\n",
      "----------------------------------------\n",
      "я же так себе сейчас еще и тф накатить смогу, с 8 то кудой\n",
      "----------------------------------------\n",
      "Обучаю LSTM-сеточку.\n",
      "Последовательности разной длины, для каждой надо предсказать класс (0 или 1)\n",
      "Написал все это на керасе, объединил объекты в батчи в зависимости от длины последовательности. Получилось 44 батча с разными длинами.\n",
      "Далее задаю число эпох (например 1000)\n",
      "Внутри каждой эпохи обучаюсь на всех батчах в случайном порядке, в конце эпохи валидируюсь.\n",
      "Первый вопрос - правильно ли я выстроил подход к обучению?\n",
      "Второй вопрос - почему-то у меня лосс скачет и не устаканивается. Ведет себя практически как случайный шум в окрестности некой величины (валидационное значение - pr_auc тоже скачет)\n",
      "----------------------------------------\n",
      "Запедить - тогда в память не влезет. Последовательности есть длины 2, а есть длины 150\n",
      "loss - binary_crossentropy\n",
      "optimizer - Adam(lr=0.01)\n",
      "Какой вообще оптимайзер брать?\n",
      "----------------------------------------\n",
      "<https://github.com/dmlc/mxnet/blob/master/tools/im2rec.py#L132> - единственное место, где используется этот параметр\n",
      "----------------------------------------\n",
      "Спасибо, но все равно непонятно, как же png прочитать. В руководстве <https://github.com/dmlc/mxnet/tree/master/example/image-classification> точно заявлена поддержка этого формата.\n",
      "----------------------------------------\n",
      "`img = cv2.imread(fullpath, args.color)` на 109 строке как бы намекает, что без pass_through прочитается всё, что умеет читать OpenCV. А умение читать PNG зависит от того, собирался ли OpenCV с его поддержкой.\n",
      "----------------------------------------\n",
      "Вопрос следующий. В новой версии кафе теперь как то через одно место инпуты на входы подаются. Вопрос: как? ) В документации найти не могу как получить доступ к блобам уровней.\n",
      "----------------------------------------\n",
      "<@U30Q72KLJ> , как для Net получить доступ к layers, ведь layers   const vector&lt; shared_ptr&lt; Layer&lt; Dtype &gt; &gt; &gt; &amp; \tlayers () const\n",
      "----------------------------------------\n",
      "А расскажите мне немного про современные возможности генеративных моделей, какие рубежы взяты?\n",
      "----------------------------------------\n",
      "Какого примерно размера выборку нужно иметь чтобы натренировать модель генерировать подобные?\n",
      "----------------------------------------\n",
      "<@U053R9RS6> \n",
      "&gt;А что является входом для сгенерированных картинок?\n",
      "вообще не только шум, модель с гумом она же самая бесполезная, тк не дает возможности вывести фичи из образа; есть VAE и AAE которые позволяют делать вывод фичей и манипулировать представлением; вае и аае это модели как две стороны одной монеты\n",
      "\n",
      "&gt;А на счет размера выборки?\n",
      "ага нужно много данных\n",
      "\n",
      "&gt;Какого примерно размера выборку нужно иметь чтобы натренировать модель генерировать подобные?\n",
      "в публикациях особо не выбирают датасеты, все предопределенно системой, так что трудно судить о размерах когда все юзают одно и то же; так что только пробовать самим и делиться тут результатами -)\n",
      "\n",
      "&gt;А там не работает дообучение какое-нибудь?\n",
      "и дообучение норм и предобучение, например в какой то статье они сначала обучают денойзинг автоенкодер, а потом уже ган; в некоторых статьях энкодер от обученного гана забирают как начальную инициализацию для сети классификации и потом уже супервайзд лернинг на метки\n",
      "\n",
      "<@U0AD1L5NC> \n",
      "&gt;Я слышал, что получается тренировать обратную функцию для GAN’ов\n",
      "а что за обратная функция для ганов? если ты про то что бы из образа генерить фичо то это ААЕ\n",
      "----------------------------------------\n",
      "ну это наркомания какая то\n",
      "----------------------------------------\n",
      "господа, так скажите свое мнение по этому поводу\n",
      "```\n",
      "Instead, for our final models, we provide noise only in the form of dropout, applied on several layers of our generator at both training and test time. Convolution-BatchNormDropout-ReLUlayer with a dropout rate of 50%.: <https://arxiv.org/pdf/1611.07004v1.pdf>\n",
      "```\n",
      "как думаете им фортануло или реально дропаут после бн имеет смысл? я не то что бы вижу какие то препятствия, просто не стандартное решение для текущего тренда\n",
      "----------------------------------------\n",
      "а там написано как он их делал?\n",
      "----------------------------------------\n",
      "чот не пойму, деконвы там что ли как то приво сделаны, отжирает памяти больше чем ожидается\n",
      "----------------------------------------\n",
      "ужасно интересно, велик ли выигрыш с mxnet в производительности, на каких задачах и в каких \"окружениях\"\n",
      "держите в курсе, пожалуйста : )\n",
      "----------------------------------------\n",
      "&gt; держите в курсе, пожалуйста : )\n",
      "как только руки дойдут сделать честное сравнение отпишусь. Пока выглядит быстрее caffe\n",
      "----------------------------------------\n",
      "Просто из интереса, а вообще в каких случаях может быть такое, чтобы лосс вёл себя как шум (при нормальных данных и коде без косяков)? Кроме, очевидно, гигантского lr\n",
      "----------------------------------------\n",
      "а что значит вести себя как шум?\n",
      "----------------------------------------\n",
      "Значит, график выглядит как шум, случайный шум вокруг точки. Как писали выше\n",
      "----------------------------------------\n",
      "Мб, когда есть какая-то симметрия в данных что-то такое наблюдается или ещё есть какие-то хитрые случаи\n",
      "----------------------------------------\n",
      "Все таки по совету чята пробую перейти от батчей разной длины к зеро-паддингу последовательностей, чтобы была стохастичнойсть\n",
      "Такая проблема - входные данные у меня имеют следующую форму `(batch_size,max_seq_len,vector_size)` (из названий думаю понятно что это за параметры).\n",
      "Соответственно это сделано при помощи `pad_sequences`\n",
      "Но согласно документации(и у меня возникает ошибка) `Embedding` ждет следующее:\n",
      "`2D tensor with shape: (nb_samples, sequence_length).`\n",
      "А куда собственно запихивать размерность вектора?\n",
      "----------------------------------------\n",
      "<@U0XF4GAM8> а почему на входе на embdding третья размерность vector_size - что это такое?\n",
      "----------------------------------------\n",
      "а, а тогда зачем embedding? он нужен чтобы сделать как раз векторы из последовательности значений categorical фичи (слов например)\n",
      "----------------------------------------\n",
      "я не помню как иначе из коробки в керасе делать правильную маску \n",
      "----------------------------------------\n",
      "убедись только что у тебя в реальных данных не может быть ситуации, когда все значения 0\n",
      "----------------------------------------\n",
      "У меня вопрос.\n",
      "\n",
      "Допустим, я хочу в качестве фана выложить демку модели на heroku.\n",
      "Никаких требования по perfomance нет, как и по красивости кода / интерфейса.\n",
      "Как бы вы подошли к этому вопросу?\n",
      "\n",
      "В моем воображении это такая html страница где есть поле аплоад с пост запросом. Это все внутри передается в питоновскую функцию, которая внутри вызывает модель и генерит результат. Потом это все выплевывается с каким-то форматированием обратно.\n",
      "\n",
      "Где тут можно срезать углы? Где, наоборот, высокий шанс закопаться в ненужную реализацию? Есть ли что-то специфичное для моделек DL / изображений на входе?\n",
      "----------------------------------------\n",
      "были проблемы когда хостил бота на тф \n",
      "----------------------------------------\n",
      "1. Хостить на хероку кажется будет проблемой (там инстанс гаснет без запросов, верно?)\n",
      "2. Модельку можно завернуть в прожку, которая в вечном цикле спрашивает новые таски какую-нибудь очередь сообщений.\n",
      "3. Запросы с вебки падают в очередь, как будет готовы -- показываются.\n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> помню,мы с тобой уже обсуждали это, как проще всего хостить без тф-сервинг, никак не могу найти поиском, не помнишь в каком канале это было?\n",
      "----------------------------------------\n",
      "<@U14GG4E69>: а есть ли какие-нибудь инсайды, почему <https://github.com/NVlabs/GA3C> до сих пор не выложили?\n",
      "----------------------------------------\n",
      "Как это в веб-демку завернуть, можно посмотреть в стандартных примерах для caffe\n",
      "----------------------------------------\n",
      "какие то у меня подозрения есть, но я туповат лезть в исходники теаны -)\n",
      "----------------------------------------\n",
      "а расскажи как конкретно ты деплоил\n",
      "----------------------------------------\n",
      "ага, спасибо, керас+тф+фласк - как раз мой случай\n",
      "----------------------------------------\n",
      "Мне два дня представители Google на конференции расписывали какой крутой TF и я решил попробовать...\n",
      "----------------------------------------\n",
      "Вообще tf если использовать tensorflow slim дает почти такой же уровень абстракции как keras\n",
      "----------------------------------------\n",
      "А может быть вот что еще. Theano выбирает какой метод конволюции работает лучше всего - его и использует. У TF такая фича есть?\n",
      "----------------------------------------\n",
      "Откатился обратно на Theano. Пока мой вывод такой - на single GPU, как backend к Keras, TensorFlow - слабоват.\n",
      "----------------------------------------\n",
      "Где можно почитать поизучать про архитектуры рекуррентных сетей ? Например мне не очень понятно когда стоит использовать один слой lstm, когда несколько , что ещё и когда стоит добавлять между ними. Да и во всех примерах что я нахожу модель как правило состоит из одного lstm слоя.\n",
      "----------------------------------------\n",
      "А подскажите мне, пожалуйста, батчнорм, когда его есть смысл добавлять, а когда нет? После (между) каких слоев? Какая в этом есть логика?\n",
      "----------------------------------------\n",
      "ща все добавляют после сумматора перед нелинейностью, я даже где то читал обоснование, но забыл )\n",
      "----------------------------------------\n",
      "Вроде не ухудшилось как минимум\n",
      "----------------------------------------\n",
      "Может keras просто выкидывает слои там где они смысла не имеют?\n",
      "----------------------------------------\n",
      "Сделай как тут и все\n",
      "----------------------------------------\n",
      "всмысле у меня нет ответа на вопрос почему \"Пуллинг и дропаут не добавляют нелинейностей” = “батчнорм не нужен\"\n",
      "----------------------------------------\n",
      "Тут, вроде, было про то, куда вставлять бн\n",
      "----------------------------------------\n",
      "если по статье бн то вроде да, они там на сигмоиде рассматривают и выглядит логично, типа сдвинем туда где сигмоид на линию похож; имхо как то не вяжется это с релу\n",
      "----------------------------------------\n",
      "А как использовать batch normalization в shared сети? У меня есть CNN (inception v3), обучаю через triplet loss. Получается, что каждую итерацию через CNN проходит 3 батча. keras говорит, что в таком случае нельзя посчитать усредненную статистику (то что называется mode=0), а можно использовать только mode=2, в котором статистика в предикте тоже считается по батчу. Как можно это обойти? Хотелось бы, чтоб при предикте уже усредненная статистика использовалась.\n",
      "----------------------------------------\n",
      "друзья! ни у кого не было такой ситуации, что оптимизатор theano при компиляции вычислительного графа некоторые из операций оставлял на хосте и не выполнял на gpu?\n",
      "----------------------------------------\n",
      "не было (вроде) такого, но не удивлюсь если какие то экзотические оставляет\n",
      "----------------------------------------\n",
      "ой, а как это посмотреть??\n",
      "----------------------------------------\n",
      "и где выводиться будет это?\n",
      "----------------------------------------\n",
      "А подскажите какую модель взять которая есть pre-trained на imagenet для keras + tensorflow? Ограничения ~300МБ на память и на 100 мб для файла с весами\n",
      "InceptionV3 не пролез по обоим критериям\n",
      "----------------------------------------\n",
      "спасибо, сейчас попробую, как гитхаб поднимется\n",
      "----------------------------------------\n",
      "<@U0K4S432S>: так а какое отличие в коде вызывает эффект?\n",
      "----------------------------------------\n",
      "Я не совсем понимаю как влияет батч\n",
      "----------------------------------------\n",
      "Не знаю какая там доля модели, но как минимум все что больше 500МБ\n",
      "----------------------------------------\n",
      "а как же, разве получится не подгружая в память?\n",
      "----------------------------------------\n",
      "Как вариант конвертировать все в tf.float16, и посмотреть на граф, нет ли там лишнего чего-нибудь.\n",
      "----------------------------------------\n",
      "Я вот тут посмотрел, что при файн-тюнинге сначала тюнят пару верхних слоев, а оптом уже все остальное\n",
      "А не пробовал ли кто-то обобщать, например тренировать верхний слой одну эпоху, потом два верхних еще эпоху, ну и так далее\n",
      "Чисто интуитивно кажется если первое имеет смысл, то и второе. Но наверное это мерять надо, может кто видел статьи?\n",
      "----------------------------------------\n",
      "начал делать файн тюн, а картинки препроцессены не так как в оригинальной сети\n",
      "----------------------------------------\n",
      "Еще один вопрос, а на какой задачке поупражняться в DL подходе для текстов?\n",
      "\n",
      "Хочется, чтобы:\n",
      "Постановка задачи была понятной обычному человеку\n",
      "Задача решалась хорошо\n",
      "Был датасет в открытом доступе подходящий для обучения модели\n",
      "Не нужно кластера видеокарт, чтобы обучать модели\n",
      "----------------------------------------\n",
      "А какой там сейчас уровень качества, навскидку?\n",
      "----------------------------------------\n",
      "как идея — взять дамп сообщений реддита, например (они ежемесячно дампы выкладывают)\n",
      "попредсказывать рейтинг сообщения, или поста, или любую метрику\n",
      "----------------------------------------\n",
      "Но лучше дождись комментарии от людей которые этим по настоящему занимаются, а не неделю как\n",
      "----------------------------------------\n",
      "вроде толи год, толи 2 назад на физтехе слили базу фоток студентов на пропуска и сделали божественный сервис, где нужно было рандомные фотки девиц от 1 до 5 рейтить, божественный датасет получился бы, но я не знаю где людей искать, у которых он мог бы сохраниться, шифруются\n",
      "----------------------------------------\n",
      "&gt; слили базу фоток студентов на пропуска и сделали божественный сервис, где нужно было рандомные фотки девиц от 1 до 5 рейтить\n",
      "Я такой фильм видел :not-sure-if:\n",
      "----------------------------------------\n",
      "а вот на баду - еще как будет\n",
      "----------------------------------------\n",
      "<@U04725QK7>: для лазаньи есть U-Net где то в ее пулреквестах, или в issues, он не предобученный, надо тренить, но рабочий, я проверял \n",
      "----------------------------------------\n",
      "Я хотел вот это <https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf> на лазанье реализовать, но хз как пойдет.\n",
      "----------------------------------------\n",
      "А какая доля в твоем общем датасете от этого датасета?\n",
      "----------------------------------------\n",
      "я планировал использовать размеченные вручную как финальный тест\n",
      "----------------------------------------\n",
      "Кто-нибудь подскажет, как у Keras'овского pretrained resnet50 выходной слой поменять?\n",
      "\n",
      "Я делаю вот так:\n",
      "```\n",
      "model = ResNet50(weights='imagenet')\n",
      "model.layers.pop()\n",
      "model.outputs = [model.layers[-1].output]\n",
      "model.layers[-1].outbound_nodes = []\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "    \n",
      "```\n",
      "Вылетает со словами:\n",
      "\n",
      "```\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "AttributeError: 'Model' object has no attribute 'add'\n",
      "\n",
      "```\n",
      "----------------------------------------\n",
      "я такое делал только со своими моделями, где я точно знал что на входе происходит \n",
      "----------------------------------------\n",
      "Если что, я находил рецепт как уже для готовой модели оторвать верхний слой\n",
      "----------------------------------------\n",
      "Я еще хотел спросить. Вот сделали артистик фильтры, а насколько сложно сделать что-то управляемое.\n",
      "Я вот, например, имею базу оригиналов и отфильтрованных картинок. На выходе я хочу сделать свой фильтр так, чтобы он был с одной кнопкой “фильтр”. Насколько это сейчас реально? Если да, то какую базу пар картинок для этого надо примерно по объему? 100? 1000? 10000?\n",
      "----------------------------------------\n",
      "Но исследовано не настолько хорошо, как style transfer\n",
      "----------------------------------------\n",
      "как я понял он хочет image2image из обычных картинок в фильтрованные\n",
      "----------------------------------------\n",
      "прикольная идея, кстати, мне тоже подобное когда-то в голову приходило. только я не могу вдуплить, в каких случаях оно понадобится :grimacing:\n",
      "----------------------------------------\n",
      "Картинку я взял из блога Сергея Доли, где он описывал шаги по достижению желаемого в Lightroom: <http://sergeydolya.livejournal.com/920762.html> (там три части).\n",
      "----------------------------------------\n",
      "Гипотеза: идея в том, как при помощи DL из исходной картинки получить картинку с “эталонными” характеристиками (яркость/контраст/прочее) с сохранением составляющих (трава должна остаться травой, деревья - деревьями и т.п.) так, чтобы они между собой органично сочетались. Т.о. задача сводится не к банальной попиксельной цветокоррекции, а к декомпозиции на отдельные элементы, их правке и дальнейшей композиции. При этом понятиям “эталон” и “органично сочетаться” вполне можно обучить на примерах.\n",
      "----------------------------------------\n",
      "Всем привет. Можете подсказать по Keras, могу ли я там как-то learning rate поменять после того как вызвал `model.compile`? видел там есть арумент `decay` когда создается optimizer, но можно ли как-то напрямую задать?\n",
      "----------------------------------------\n",
      " Задача распознавания лиц из заданной базы (работники предприятия). Как обезопаситься от того, что левые люди, которых нет в базе, эмбедятся рядом с настоящими работниками и распознаются как работники предприятия? <@U041P485A> ?\n",
      "----------------------------------------\n",
      "классификация тупо, дообучать каждый раз как нового петровича на ферму завезли, так себе звучит \n",
      "----------------------------------------\n",
      "В style transfer сетях обычно используют  фичи VGG, а кто нибудь видел сравнение с фичами взятыми у ResNet или других архитектур?\n",
      "----------------------------------------\n",
      "еще вопрос - есть ли какая то стратегия куда ставить батч норм?  Я имею в виду не до/после нелинейности, а в целом - после каждого слоя например, или там между основными блоками какими-то?\n",
      "----------------------------------------\n",
      "смысл то выровнить covariance shift, так что тут скорее нужно думать где не нужно его юзать, чем где нужно, напрмиер в сетях-генераторах картинок нет смысла юзать в последнем слое, тк нет смысла там что то выравнивать, наоборот нужно дать пикселям найти какой то свой центр\n",
      "----------------------------------------\n",
      "Подскажите плс, зачем в U-net копируют свертки с этапа \"сжатия\" в соответствующий этап апсемплинга? Такое вроде во всех архитектурах для сегментации делают. Почему нельзя просто делать последовательный апсемплинг и деконволюцию без копирования более ранних этапов?\n",
      "----------------------------------------\n",
      "Вопрос: а как дебажить нейро-сетки ? А то при обучении лосс падал, а при предиктах получаются подозрительно похожие значение(такое ощущение, что сеть какое-то дефолт значение запомнила и входы очень опосредственно влияют) .\n",
      "----------------------------------------\n",
      "<@U04725QK7> это позволяет всякие детали высокого разрешения которые есть на картинке передать дальше, без нужды как то их сжимать\n",
      "----------------------------------------\n",
      "У меня созрел вопрос:\n",
      "В документации <https://keras.io/applications/> есть пример \"Fine-tune InceptionV3 on a new set of classes”\n",
      "Там упущено как именно мы подготавливаем генератор для обучения\n",
      "А если посмотреть пример \"Classify ImageNet classes with ResNet50\":\n",
      "Интересующая нас часть:\n",
      "img_path = 'elephant.jpg'\n",
      "img = image.load_img(img_path, target_size=(224, 224))\n",
      "x = image.img_to_array(img)\n",
      "x = np.expand_dims(x, axis=0)\n",
      "x = preprocess_input(x)\n",
      "\n",
      "preds = model.predict(x)\n",
      "Видно, что тут явно вставлен preprocess_input помимо ресайзинга.\n",
      "\n",
      "Вопрос: правильно ли я понимаю, что в генератор для файн тюнинга нужно воткнуть подобный препроцессинг?\n",
      "from keras.applications.inception_v3 import preprocess_input\n",
      "----------------------------------------\n",
      "а где 1080 в облаке можно взять?\n",
      "----------------------------------------\n",
      "ну тип зачем тф-контриб если в гугле есть специальный чувак, уже курирующий отличный фронтенд \n",
      "----------------------------------------\n",
      "спасибо,  как раз то что хотел узнать и по поводу чего сомневался!\n",
      "----------------------------------------\n",
      "так явная симметрия это как раз криптота\n",
      "----------------------------------------\n",
      "А почему всё сразу в цвете генерится? Можно же в grayscale генерить, да и пямяти раза в три меньше будет бессмысленно гоняться.\n",
      "----------------------------------------\n",
      "а подскажите, кто в чем красивые картинки с сетками рисует для презентации? да, помню что такое обсуждение было, сам использовал один раз <http://draw.io|draw.io> для этого, в целом сьедобно, но не очень красивенько \n",
      "----------------------------------------\n",
      "а какой кейс ты решаешь?\n",
      "----------------------------------------\n",
      "<@U07V1URT9> расскажи ему как псин и машины в легких искать :more-layers: \n",
      "----------------------------------------\n",
      "Подскажите, используют ли метод сопряженных градиентов для оптимизации. Почему все уперлись в SGD? \n",
      "Вот тут <http://ai.stanford.edu/~ang/papers/icml11-OptimizationForDeepLearning.pdf> коллеги утверждают, что сопряженные градиенты вполне себе\n",
      "----------------------------------------\n",
      "Хотя мне и не ясно до конца как это происходит\n",
      "----------------------------------------\n",
      "Ну вот я как раз обернул генератор\n",
      "----------------------------------------\n",
      "Теперь когда я делаю model.save()\n",
      "А потом сразу же keras.models.load_model\n",
      "----------------------------------------\n",
      "Не знаю почему ошибка выглядит именно так, но появилась после обрезания картинки, кажется больше я ничего не менял\n",
      "----------------------------------------\n",
      "2. Квантизация весов. Позволяет кардинально уменьшить размер, но с потерей качества. В runtime'е, всё равно, требует столько же памяти. Как эту операцию правильно делать я не знаю :confused:\n",
      "----------------------------------------\n",
      "Чувак, который указан как контакт, чтобы инвайт в слак получить, из FAIR\n",
      "----------------------------------------\n",
      "Вопрос по бенчмарку. Никто не пробовал запускать cifar10 из mxnet с resnet? Мне интересно знать, какая скорость является приемлемой. Размер батча 500.\n",
      "----------------------------------------\n",
      "А вообще, есть какое нибудь сравнение разных фреймворков на разном железе с каким нибудь набором архитектур сетей? Вроде в этом чате кто что то такое писал...\n",
      "----------------------------------------\n",
      "думаем вот кто авторам напишет предложение выступить\n",
      "----------------------------------------\n",
      "как насчёт real-time мониторинга на основе картинки с трансляции? :smile:\n",
      "----------------------------------------\n",
      "Теперь представим ситуацию где это приложение нужно, ребенку 3 года, родители увидели этот апп в сторе и такие модет проверим нашего, чот он странный, проверяют и им говорят что сорям но ваш ребенок болен; это же фантастическая ситуация\n",
      "----------------------------------------\n",
      "на самом деле, если эту тему развить, и анализировать краткосрочную динамику лица (часто дергается глаз и т.д.), то может быть хорошим дополнением к теме с последнего NIPS, где ребята анализировали речь и классифицировали её в смысле психического нездоровья. deep learning sensor fusion :science:\n",
      "----------------------------------------\n",
      "но не такие глупые кейзы как в посте\n",
      "----------------------------------------\n",
      "^ это, кстати, вроде как IBM. слайд №17 (<http://www.slideshare.net/SessionsEvents/irina-rish-research-staff-ibm-tj-watson-research-center-at-mlconf-nyc>)\n",
      "----------------------------------------\n",
      "тип с какими быками спаривали на самом деле мамок самых годных коров\n",
      "----------------------------------------\n",
      "Мне кажется, что детектирование течки у коров и момента, когда куре пора отрубить голову, с помощью ML/DL это интересно.\n",
      "----------------------------------------\n",
      "<@U1CF22N7J> где можно оставить предзаявку? :smile:\n",
      "----------------------------------------\n",
      "А может кто-нибудь по лазанье одну штуку подсказать? <https://github.com/FabianIsensee/NeuralNetworks/blob/master/NeuralNetworks/UNet.py> Здесь функция build_unet() возвращает OrderedDict. Как из него получить параметры для передачи в целевую функцию? Тупо из последнего слоя get_all_params?\n",
      "----------------------------------------\n",
      "какие архитектуры принято использовать, если нужно на входе из (n_cols, n_rows, n_channels) получить (n_cols, n_rows, 1)? \n",
      "это не каггловское распознавание дорог, если что :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Ох. Как ужасно, когда треды используют чтобы отвечать на сообщение\n",
      "----------------------------------------\n",
      "А тут нужно чтобы все кто пишут в канале так делали\n",
      "----------------------------------------\n",
      "<@U04ELQZAU> а в ШАДе какое расписание?\n",
      "----------------------------------------\n",
      "Всем привет\n",
      "\n",
      "Мы вот с <@U1BAKQH2M> только что обсуждали и не поняли зачем DropOut после Embedding слоя в примере для текстовой модели?\n",
      "<https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py>\n",
      "\n",
      "Может кто подскажет? <@U0H7VBQQ1> может ты знаешь?\n",
      "----------------------------------------\n",
      "вот как на картинке показано\n",
      "----------------------------------------\n",
      "а если я впилил вместо не очень большой не ризидуал сети большую и резидуал, и утилизация гпу спустилась с 90-100 до 70-98%, это же проблемы фреймворка поди? типа как то хреново съоптимизировал?\n",
      "----------------------------------------\n",
      "кароч одно наблюдение про ганы может кому пригодиться, много пишут кого тренировать дольше генератор или дискриминатор, гудфелоу высказал предполодение что вроде как пох и можно одинаковое количество раз; я тут пробрвал стратегию такую, что считаю EWMA тупо по аккураси с порогом 0,5, и как только достигает некоторого порога, например 0,95, то передавать ход второму игроку; ну и кароче как только я сделал генератор и дискриминатор одинаковыми по сложности (оба глубокие и резидуал), то при такой стратегии они обучаются одинаковое количество итераций (ну почти)\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC>  эх, поздно заметил твой ответ. А какую метрику использовал для сравнения картинок и выход из какого слоя использовал?\n",
      "----------------------------------------\n",
      "ну типа того, визуальная оценка -) вроде как лучше метода еще нет\n",
      "----------------------------------------\n",
      "хм, пробовал, но дало плохой результат. А выход какого слоя использовал? Как-то сжимал или преобразовывал выход слоя?\n",
      "----------------------------------------\n",
      "а какая архитектура сети была? использую vgg-19\n",
      "----------------------------------------\n",
      "в общем идет третья неделя как я тут кручу верчу только еще обычный ган\n",
      "----------------------------------------\n",
      "Я вот сейчас не могу понять зачем им метки\n",
      "----------------------------------------\n",
      "У меня в целом цели те же, но мне нравится когда сначала деньги а потом стулья\n",
      "----------------------------------------\n",
      "как минимум стоит прочитать ган, дцган, сондишн ган, аае и инфоган, а и еще гудфеловский туториал\n",
      "----------------------------------------\n",
      "Пока что я попробую просто воткнуть как есть\n",
      "----------------------------------------\n",
      "<https://github.com/zhangqianhui/AdversarialNetsPapers> вот еще список всего что как то с ганом связано\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> а ты понимаешь как именно используются метки для гана?\n",
      "----------------------------------------\n",
      "Я конкретно про вот эту реализацию:\n",
      "<https://github.com/yunjey/dtn-tensorflow>\n",
      "\n",
      "Как я понимаю это используется в “pre-train” фазе\n",
      "----------------------------------------\n",
      "как я понял, там речь о том, что стандартный ган это бинарная классификация на фейк и не фейк, а они делают дискриминатор мультиклассовым (точнее три класса), чот в четвертом разделе не упоминается про претрейн ничо\n",
      "----------------------------------------\n",
      "четвертый раздел в статье как раз об архитектуре модели\n",
      "----------------------------------------\n",
      "Всмысле когда появляется второй класс\n",
      "----------------------------------------\n",
      "там как раз про то как с помощью ААЕ генерить семплы из класса (это позаимствовано из условного гана)\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> а какие интересные эксперименты получились с ганами? тоже хочу погрузиться и думаю с чего начать\n",
      "----------------------------------------\n",
      "Ну то есть там где на входе именно лицо, получается еще куда ни шло\n",
      "----------------------------------------\n",
      "А подскажите картичные датасеты размера порядка 10^5+ картинок с разметкой классификации с не очень большим разнообразием (например, как разные лица, разные породы котов, etc.)\n",
      "----------------------------------------\n",
      "Ну требования, как ни странно: метки и размер :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Пробую гонять fully connected сетку на данных. Какие есть эвристики для выбора размера dense слоев и их количества ?\n",
      "----------------------------------------\n",
      "<https://github.com/alno/kaggle-allstate-claims-severity/blob/master/train.py>\n",
      "Тут еще можно заценить какую сеть использовал alno\n",
      "----------------------------------------\n",
      "а еще <@U0JJ69UB1> проинтерпретировал смешивание предсказаний от одной сетки на этапах обучения как какую-то умную баесовщину.\n",
      "----------------------------------------\n",
      "вроде как достаточно часто их так используют\n",
      "----------------------------------------\n",
      "если я правильно понял, то когда мы обучаем с минибатчами с помощью SGD и сохраняем веса с определенной периодичностью,  то \"стохастичность\" от минибатчей как бы воспроизводит MCMC семплинг для весов\n",
      "----------------------------------------\n",
      "<https://opendatascience.slack.com/archives/deep_learning/p1485183859003935> а почему увеличить а не уменьшить?\n",
      "----------------------------------------\n",
      "а имеет смысл выправлять решение с тем же самым адам/рмспроп, но более низким lr? просто кажется, что имеет смылс использовать ту статистику которую они накопили, но я не уверен, так как не ставил эксперименты, может кто-то пробовал?\n",
      "----------------------------------------\n",
      "рмспроп вроде с адаптивными весами по каждой координате, нет? как раз избавляясь от этой статистики вносишь дополнительную регуляризацию? вроде ты всякими рмспропами можешь загнать лр перед каким-нибудь весом в нулище и он у тебя особо обновляться не будет\n",
      "----------------------------------------\n",
      "но я не уверен что помню как работает рмспроп\n",
      "----------------------------------------\n",
      "Зависит от того, для каких нужд и какие требования. Что-то на коленке можно за пару вечеров собрать\n",
      "----------------------------------------\n",
      "<@U04422XJL> С датасетом напряг, как всегда, в пару вечеров не уложишься\n",
      "----------------------------------------\n",
      "Еще дурацкий вопрос: как размечают фотки? Вот ту же рыбу с кэггла? Должен же быть удобный инструмент?\n",
      "----------------------------------------\n",
      "меняешь какую то мелочь и оп работает\n",
      "----------------------------------------\n",
      "Пример того как (не) получается\n",
      "----------------------------------------\n",
      "Самое грустное, что получается довольно “однообразно”, как будто есть несколько классов куда картинки переводятся\n",
      "----------------------------------------\n",
      "Да, пока выглядит как gan'ы под бутиратом\n",
      "----------------------------------------\n",
      "Мне неясно зачем они претрейн такой долгий делают, у них очень быстро метрика на тесте приходит в некоторый потолок\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> Расскажешь как правильно должно выглядеть поведение ошибок в примере?\n",
      "----------------------------------------\n",
      "я уменьшаю комплексити дискриминатора когда виду что тот переобучается\n",
      "----------------------------------------\n",
      "&gt;Д = source, Г = target в их именовании?\n",
      "у них есть f = энкодер, а далее как в гане есть Gенератор и Dискриминатор\n",
      "----------------------------------------\n",
      "стадия обучения Д: в Д приходит пачка из реальных картинок и не реальных, его задача отличить какая фейк а какая нет, ну и есть аккураси\n",
      "стадия обучения Г: в Д приходит куча фейков, но оптимизируется же Г, то и аккураси это сколько из всей пачки фейков было распознанно дискриминатором как реальные\n",
      "----------------------------------------\n",
      "Возможно, это как раз отражает факт что одна из сторон “побеждает\"\n",
      "----------------------------------------\n",
      "ну и да ждать долго нада, вот это меня как то печалит, в супервайзд лернинге часто можно по динамики ошибки остановить обучение если видишь аномалию\n",
      "----------------------------------------\n",
      "видно что часто они становятся внезапно хуже чем были, потом становятся еще лучше чем были до того как стали хуже\n",
      "----------------------------------------\n",
      "я вот как раз щас балуюсь с различными, пока лучше всего отрадатывает обычный дискриминатор с пятью слоями сверток 3х3 (это как в статье DCGAN)\n",
      "----------------------------------------\n",
      "карточка вообще отображается как девайс от невидии?\n",
      "----------------------------------------\n",
      "Кажется, там было что-то странное, когда мне вывалился в лог ошибки вывалился сорс чуть ли не от nvcc\n",
      "----------------------------------------\n",
      "Как бы, карточки не видно, оторвалась по ходу дела. nvidia-docker от этого не поможет\n",
      "----------------------------------------\n",
      "Если я правильно понял консолидированное мнение, то если lspci не показывает то это не базовая хрень и как минимум проблема не только в дровах\n",
      "----------------------------------------\n",
      "так как в биос залезть я не могу, то придется пока статьи почитать :disappointed:\n",
      "----------------------------------------\n",
      "Там где дисперсия упала, это lr на порядок уменьшил\n",
      "----------------------------------------\n",
      "потому мне кажется нада еще и аккураси выводить, что бы хоть какая то интерпретация была\n",
      "----------------------------------------\n",
      "Немало, это на какой видеокарте?\n",
      "----------------------------------------\n",
      "вот получается что иногда шарахает так что количество батчей Г или Д больше другого в одну итерацию, но в целом получается примерно одинаково, как и говорил Гудфелоу в туториале, что его мнение что нужно одинаково\n",
      "----------------------------------------\n",
      "а как без паддинга разные картинки пихать?\n",
      "----------------------------------------\n",
      "а, MxM. я распарсил как MxN и не думал, что соотношение сохраняется\n",
      "----------------------------------------\n",
      "Я пока не гуглил, но какой в этом смысл?\n",
      "----------------------------------------\n",
      "<@U053R9RS6> Стой, а какая изначальная задача ?\n",
      "----------------------------------------\n",
      "как минимум, новомодный генератор юзерпиков для github'а уже есть :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "просто интересно какого рода там получается изображение. хотя бы для baseline :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "<@U053R9RS6> я когда-то пиксель-арт руками делал,  как ресайз + выкрутку контраста в Фотошопе, это какой-нибудь конволюцией наверно легче сделать, чем надеяться на Ган.\n",
      "----------------------------------------\n",
      "кароче пока вывод такой, что не все сети одинаково полезны, вот резнет Д например легко выучиватеся отличать что угодно, но от него какие то накуренные градиенты в Г приходят и он учится какой то дичи, хотя если на тот же Г повесить Д похожуй на мелкую vgg, то сразу все ок\n",
      "----------------------------------------\n",
      "все картинки получаются какими структурированными по сетке, какие то мазки появляются\n",
      "----------------------------------------\n",
      "хз, но помню когда резнет в стайл трансфер пихали тоже фигня угловато-гранулированная получалась - это неспроста наверное\n",
      "----------------------------------------\n",
      "это вроде как сегментация на лес/не лес\n",
      "----------------------------------------\n",
      "Я щас может тупой вопрос задам: а почему CNN на RGB обучают, это ж так себе цветовое представление ? Какой-нить HSV ж полегче будет, грубоговоря смена тона +-  10% в 1 канале, а в ргб это все 3 переменные в разные стороны.\n",
      "----------------------------------------\n",
      "&gt;Я щас может тупой вопрос задам: а почему CNN на RGB обучают, \n",
      "как мне кажется идея дл (одна из) как раз в том что он сам должен все выучить, так что подсовывать ему то что нам кажется лучше не самая хорошая идея, модели самое виднее что лучше\n",
      "----------------------------------------\n",
      "Ну, кстати, когда в сети используются сепарейбл свертки уже не так очевидно\n",
      "----------------------------------------\n",
      "Версус как всегда подоспел с актуальной ссылкой\n",
      "----------------------------------------\n",
      "`cleanliness of the data is more important then the size` тут, скорее всего, имеется ввиду ситуация, когда новые данные не улучшают качество. В общем случае чем больше данных -- тем лучше.\n",
      "----------------------------------------\n",
      "Сейчас они используют keras как бэкенд\n",
      "----------------------------------------\n",
      "<@U053R9RS6> еще я хотел спросить, у меня такой артифакт, что модель в любой итерации дает в основном изображения одной гаммы. Это очень странно учитывая, что гамма постоянно меняется. \n",
      "это проблема в том, как говорят в статьях, что ган переобучается вокруг одной моды распределения, и пока общего решения нету, но есть 100500 хаков в статьях, у кого то что то получается\n",
      "----------------------------------------\n",
      "Коллеги приветствую!\n",
      "Подскажите решения для задачи \"Time series Analysis\". Time данные идут по нескольким каналам.\n",
      "Нужно быстро прогнать csv файл, посмотреть что на выходе.\n",
      "У нас несколько каналов, и один дискретный.\n",
      "Когда на дискретном канале 0 становится 1, то нужно анализировать другие каналы (момент перехода) и прогнозировать переход.\n",
      "Фактически прогнозировать переход 0 в 1\n",
      "Имеем наборы выборок в виде десятков - сотен csv файлов\n",
      "Столбцы\n",
      "- время ежесекундно\n",
      "- несколько каналов - столбцов, цифры разные\n",
      "- дискретный столбец\n",
      "----------------------------------------\n",
      "```\n",
      "gen = ImageDataGenerator(\n",
      "   ...)\n",
      "g1 = gen.flow_from_directory(train_data_dir, ..., shuffle=True, seed=0)\n",
      "train_conv_feats = base_model2.predict_generator(g1,  2 * nb_train_samples)\n",
      "```\n",
      "вопрос по Kerasу как получить список соответствующих таргетов в данном случае? или список индексов который использовался для выборки примеров и последующей аугментации?\n",
      "`g1.filenames` или `g1.classes` не подходят (даже по размерности она у них в 2 раза меньше)\n",
      "----------------------------------------\n",
      "Гайз, вопрос по :tensorflow: :\n",
      "Как закинуть созданный граф на девайз, а не создавать граф на девайте?\n",
      "\n",
      "Работает:\n",
      "```\n",
      "with graph.as_default():\n",
      "\twith tf.device(device):\n",
      "```\n",
      "\n",
      "Не работает:\n",
      "```\n",
      "with tf.device(device):\n",
      "\twith graph.as_default():\n",
      "```\n",
      "\n",
      "Хочу именно граф на девайз закинуть. freeze не предлагать :slightly_smiling_face::upside_down_face:\n",
      "----------------------------------------\n",
      "Кто-нибудь сталкивался в своей практике с расходящимся градиентом (топовый фреймворк, стандартные слои, популярный датасет, адекватные на первый взгляд параметры) (пример обсуждения проблемы <http://stackoverflow.com/questions/38157657/salvaging-diverged-neural-networks>)? Какие могут быть причины, исключая кривые руки, и что делать в этом случае (пример инструмента для решения проблемы <http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization>)?\n",
      "----------------------------------------\n",
      "Упрощать задачу до тех пор, пока не заработает, последовательно увеличивать сложность и смотреть, что все как надо.\n",
      "----------------------------------------\n",
      "Ну вот и хотелось бы почитать, какие могут быть причины. Сам пока мало что придумал кроме кривого кода, битых пикселей и неадекватной lr\n",
      "----------------------------------------\n",
      "О, как раз к боулу :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Не знаю в какой канал правильнее написать. Напишу сюда.\n",
      "Работал ли кто-нибудь с сервера Microsoft Azure для обучения сетей на GPU? Есть ли какие особенности? Как впечатления в целом?\n",
      "----------------------------------------\n",
      "как думаете что еще за 5 кусков?\n",
      "----------------------------------------\n",
      "Не уверен что это что-то значит, но сами картинки тоже вроде как ничего не значат\n",
      "----------------------------------------\n",
      "Вот здесь есть код как меняют размер входа у модели\n",
      "----------------------------------------\n",
      "Небольшой оффтоп - как посчитать, сколько мне надо будет видеопамяти для батча размера Х?\n",
      "----------------------------------------\n",
      "Разные фреймворки по разному используют память. Я до сих пор не понял какая зависимость даже в mxnet\n",
      "----------------------------------------\n",
      "Кто мне подскажет, где найти код callback  в  keras, который экспеоненциальное усреднение весов делает между эпохами? (Polyak Averaging)\n",
      "----------------------------------------\n",
      "ternaus: Можно None подставлять вместо N (если tensorflow как backend используется).\n",
      "----------------------------------------\n",
      "<@U053R9RS6> а какого размера был трейнсет?\n",
      "----------------------------------------\n",
      "Просто тут мне кажется есть одна тонкость, с тем, что распределение каждого из столбцов свое, так как является усреднением двух равномерно распределенных векторов шума\n",
      "----------------------------------------\n",
      "Было бы что в опенсурс. \n",
      "Код? Его навалом, я там ничего не придумал. Например, <@U06J1LG1M> выкладывал свой \n",
      "Модель? Это еще далековато от того чтобы использовать как есть\n",
      "Датасет? Не уверен, что там с правами :pirate:\n",
      "----------------------------------------\n",
      "так какого размера датасет был?\n",
      "----------------------------------------\n",
      "господа, курящие теану/лазану\n",
      "есть такой файлик <https://drive.google.com/file/d/0B4bl7YMqDnViMld3V1Jzckx1VFU/view?usp=sharing>\n",
      "это типа упрощенный conditional GAN, там есть два графа, генератор и дискриминатор, в ячейке 10 объявляется fake_prob, который говорит генератору типа сгенери картинку, и передай ее в качестве одного из входов в дискриминатор (x_img_sym)\n",
      "но как видите чот в 12ой ячейке вылазит ошибка\n",
      "(причем если делать не conditional gan, т.е. просто выпилить все упоминания y_condition_sym то будет все ок работать)\n",
      "может есть соображения на этот счет? (я подозреваю что скорее всего я чтото не так делаю, но хз что, тк если бы был баг в теане то я бы нашел его уже в сети)\n",
      "----------------------------------------\n",
      "а если сделать как стактрейс говорит?\n",
      "&gt;To make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function\n",
      "----------------------------------------\n",
      "может кто вдруг захочет воспроизвести то вот ноут а не хтмлка <https://drive.google.com/file/d/0B4bl7YMqDnViN0NaT01pWkJJVFE/view?usp=sharing>\n",
      "----------------------------------------\n",
      "да как то ничего -) этип то ГАН и бесполезен, что спровоцировало появление ААЕ\n",
      "----------------------------------------\n",
      "а так то, если совсем коротко, то статьи три\n",
      "GAN - базовая идея, но десполезная\n",
      "AAE - сделали полезную\n",
      "InfoGAN - показали как можно поизвразаться\n",
      "----------------------------------------\n",
      "Привет! Во время профайлинга обнаружил что в tf изменение формы тензора занимает какое то абсурдно долгое время, что прям пользоваться нельзя: \n",
      "Пример: \n",
      "`import numpy as np`\n",
      "`import tensorflow as tf`\n",
      "`x = tf.Variable(np.zeros((100, 1)))`\n",
      "`with tf.Session() as sess:`\n",
      "`   %timeit sess.run(tf.squeeze(x), feed_dict={x: a})`\n",
      "`   %timeit sess.run(tf.reshape(x, (100,)), feed_dict={x: a})`\n",
      "`   %timeit sess.run(x, feed_dict={x: a})`\n",
      "Результат: \n",
      "`10 loops, best of 3: 14.5 ms per loop`\n",
      "`10 loops, best of 3: 25.5 ms per loop`\n",
      "`10000 loops, best of 3: 69 µs per loop`\n",
      "----------------------------------------\n",
      "Кто нибудь сталкивался, в чем тут дело? tf 0.12, cpu\n",
      "----------------------------------------\n",
      "Просто определить эту операцию как доп. переменную. Даже не предполагал что такой эффект будет.\n",
      "----------------------------------------\n",
      "Уже час безуспешно пытаюсь заставить работать U-net, поэтому такой вопрос: если U-net предсказывает класс конкретно для каждого пикселя, то каждый пиксель исходного изображения отображается в вектор Kx1 (где K- число классов). Соответственно выход у слоя с софтмаксом будет размера Kx(MxM) - где M - число пикселей по одной стороне. В случае бинарной классификации я должен подать как target не одно изображение с 0 и 1 а склеить в тензор исходную бинарную маску и \"обратную\" бинарную маску (где 1 заменены на 0 и наоборот) ?\n",
      "----------------------------------------\n",
      "И какой входной размер картинки?\n",
      "----------------------------------------\n",
      "<@U07V1URT9> Я пока на спутниках 224x224 гоняю, но там надо разбираться что хорошо и что плохо и как поступать с угасанием receptive field к краям.\n",
      "----------------------------------------\n",
      "Слабо представляю как связано количество каналов на входе сетки и фрагментарность маски на выходе.\n",
      "----------------------------------------\n",
      "А еще, <@U34Q3KU8H> <@U04725QK7> вы как решили поделить локальную валидацию? На каких id тренируетесь, на каких валидируетесь?\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> Попробовал твой дискриминатор впилить вместо своего, но чего-то странное получается. А какой там принцип построения когда меняешь разрешение входа?\n",
      "----------------------------------------\n",
      "&gt;А какой там принцип построения когда меняешь разрешение входа?\n",
      "а какой из? там несколько\n",
      "----------------------------------------\n",
      "&gt;Conditional GAN еще похоже полезное развитие\n",
      "хз, не слышал о полезном применении; он же тоже делает из шума образы, а это бесполезно; ну т.е. они к шуму подмешивают какие то контроллирующие параметры, но это не меняет принципиально ничего; главное же уметь из образов доставать фичи\n",
      "----------------------------------------\n",
      "как перевести gated units, gated functions и прочие упоминания слова gate, когда речь об LSTM? Ворота? Гугло-переводчик говорит про какое-то стробирование.\n",
      "----------------------------------------\n",
      "возьми из старого коммита, ну или проследи по истории зачем удалили <https://github.com/fchollet/keras/blob/ce814302acee2c474a88b2f4bfcdc92ff866f94f/keras/engine/training.py#L241>\n",
      "----------------------------------------\n",
      "кароч запустил Conditional GAN, пока пои ставки что ничо не измениться и рожи будут генериться не особо лучше -) как то все это на шаманство похоже уж слишком\n",
      "----------------------------------------\n",
      "Привет. Кто нибудь занимался таким упражнением: взять модель кераса (.h5), тренированную с бекендом theano, и сделать аналогичную (те же веса, структура, все) в keras но с backend'ом tensorflow? Сеть сверточная. \n",
      "Кажется, что небольшая засада должна быть с порядком осей в сверточных слоях, а все остальное должно быть такое же, но сам еще не успел это проделать.\n",
      "----------------------------------------\n",
      "вроде как в бэкенде кераса, когда я там последний раз ковырялся, обёрнуты все операции проверками на порядок axis в тензорах, поэтому проблем возникнуть не должно при смене бэкенда\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> а как картинки с conditional GAN?\n",
      "----------------------------------------\n",
      "Ну вот накопишь ты скилл, а завтра придумают как задачу свести к минимизации одной функции ошибки. И то, что ты принимал за скилл, окажется бесполезным опытом\n",
      "----------------------------------------\n",
      "В виртуалку ты не прокинешь куду\n",
      "----------------------------------------\n",
      "я просто помню, как пробовала поставить где-то месяца 1,5 назад, и куча проблем возникало\n",
      "----------------------------------------\n",
      "вроде все устанавливалось, но когда вызывала в питоновском ноубуке, все висло\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC> а можешь подсказать, где можно посмотреть для совсем нубов?\n",
      "----------------------------------------\n",
      "<@U14GG4E69> как мининмум ААЕ полезен примерно так же как любой другой автоэнкодер, но потенциально еще дает всякие штуки по управлению процессом генерации, типа такого <https://www.youtube.com/watch?v=pqkpIfu36Os;> а с точки зрения теории очень забавно то как оно все связанно VAE-GAN-RL\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> А как GAN связаны с RL?\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> полностью на убунту я пока морально не готова переходить. При установке новых программ мне нравится со всем соглашаться в окошке и не думать о тайнах мира и 100500 особенностей терминала. Хотя практически все DS, с кем я говорила, ругали Винду и утверждали, что она не подходит для анализа данных.\n",
      "----------------------------------------\n",
      "ты не поверишь но я тоже тайный поклонник винды, си шарп дучший язык, вижуал студия лучшая иде (я потому саблаймом пользуюсь, ибо посе студии все иде кажутся говном), а винда самая няшная ось; но боль установки дс энваермента (еще и не все установишь как нужно) пересиливает мои нежные чувства к мс\n",
      "----------------------------------------\n",
      "Из праздного любопытства  -- кто нибудь пробовал запускать tf/keras/etc на Windows пользуясь докером? На убунте просто очень удобно, если есть какой то замороченный пакет (типа RLLab) с мутными зависямостями, и по идее уже есть даже nvidia-docker, т.е. можно использовать ГПУ. А про Windows не знаю, насколько это возможно вообще.\n",
      "----------------------------------------\n",
      "<@U053R9RS6> чот у тебя второй раз странные проблемы с гпу, ты сервак свой юзаешь или амазон или еще что?\n",
      "----------------------------------------\n",
      "<@U053R9RS6> Какие GPU и сколько их?\n",
      "----------------------------------------\n",
      "Насчёт GAN - то как генерировали изображения в google deep dream, подавая в модель шум и итерационно усиливая изображения которые сеть в этом шуме видела, в результате генерируя эти известные изображения замков и др - это GAN? Или нет? Или GAN позволяет делать тоже самое, но круче потому что там forward и backward шаги выделены в отдельные сети?\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> У меня последнее разочарование, что генератор зашел в некоторое состояние где все генерируемые картинки стали идентичны. У тебя такое было?\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> А еще я понял что лучший вариант у меня получался когда у меня батчнорм был в режиме когда он и на предикт тоже “работает”. В результате получалось очень странные артефакты зависимости сгенерированного от размера батча\n",
      "----------------------------------------\n",
      "Дропаут выглядит как очень хорошая идея, надо попробовать\n",
      "----------------------------------------\n",
      "вообще любой шум с такой системе на пользу, вроде как\n",
      "----------------------------------------\n",
      "о это прям как в РЛ\n",
      "----------------------------------------\n",
      "Это как раз и привело к “статическому” генератору, но зато генератор перестал побеждаться дискриминатором\n",
      "----------------------------------------\n",
      "Я сейчас попробую оставить BatchNorm как был и внедрить вот это усреднение по последним эпохам. Интересно посмотреть изолировано как это влияет. В моем понимании это должно делать дискриминатор более медленным и более общим\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> А как иначе то померять, еще и может оказаться что если то и другое перезапустить пару раз — результаты очень сильно изменятся\n",
      "----------------------------------------\n",
      "Доля сгенерированных картинок которые были распознаны как настоящие (после обучения генератора)\n",
      "----------------------------------------\n",
      "Всмысле на каких классах предобучать?\n",
      "----------------------------------------\n",
      "тоже самое как в той ссылке что ты кидал раньше, там сначала энкодер предобучается, а тут будет дискриминатор\n",
      "----------------------------------------\n",
      "а так получается что дискриминатор быстро выучивает какую то килер фичу как отличить фейк от реала (например какой нибудь бред типа сркости) и передает это генератору, тот быстро фиксит яркость, на следующем ходу тот изменяет порог яркости и тд, кароче пока дело дойдет до серьезных фичей проходят сутки\n",
      "----------------------------------------\n",
      "mephistopheies: а как думаешь не может быть что это наоборот помешает? Ведь с предобучением он начнёт учиться различать 6 меток друг от друга, начнёт этой логикой заполнять параметры. А ведь задача отличить любую из этих меток от другой картинки которой вообще в них нету - это немножко другая задача вообще?\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> я как раз сейчас читаю статью которую ты присылал с воркшопа NIPs, как раз  говорят что качество улучшается несравнимо\n",
      "----------------------------------------\n",
      "не исключено, я все равно отрежу от головы несколько слоев (как обычно при трансфере), при такой эвристике должны остаться более общие фичи не привязанные к классам (т.е. как раз фичи дескрипторов лица), а специфичные для датасета, т.е. классы, должны отвязаться\n",
      "----------------------------------------\n",
      "просто если они в одном графе то как то не понятно как разные алгоритмы применять\n",
      "----------------------------------------\n",
      "ага это видел, но я юзаю статистику, чот субъективно лучше\n",
      "----------------------------------------\n",
      "а ну да точно, кароче на теане тоже можно разные алгоритмы, чот я туплю\n",
      "----------------------------------------\n",
      "ну да, вопрос только как это записать\n",
      "----------------------------------------\n",
      "Кто еще сомневается на счет участия в кэггле со спутниками, самое время начать!\n",
      "Я запилил кернел с полным пайплайном от картонк до csv файла:\n",
      "<https://www.kaggle.com/drn01z3/dstl-satellite-imagery-feature-detection/end-to-end-baseline-with-u-net-keras/discussion>\n",
      "И уже набралась целая банда кэгглеров 2 из которых в топ10 и участвуют в обсуждении в <#C043ZEF6K|kaggle_crackers> \n",
      "Один из них даже посвящает в свою личную жизнь (отгадайте с одного раза - кто он?)\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> попробовал внедрить как можно больше советов из хаков по гану — пока выглядит многообещающе\n",
      "----------------------------------------\n",
      "Как раз дропаут из генератора выпилил\n",
      "----------------------------------------\n",
      "А, еще сделал прям как в оригинале количество слоев в генераторе. Раньше у меня был базовые слой 6 на 6 и соответственно три слоя деконволюшена, теперь 3 на 3 и 4 слоя (как в оригинале)\n",
      "----------------------------------------\n",
      "On an unrelated note, а у кого на чем крутится deep learning в продакшене?\n",
      "----------------------------------------\n",
      "А расскажите мне пожалуйста, а в чем прикол caffe? Просто потому что он старый и те кто давно начинали делать выбирали что было тогда? Или там и сейчас есть за что его выбрать?\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC> Я тут просто веду переговоры в стартап где у них своя ветка каффе и вот до конца не понимал, почему именно он. Но если быстро то ясно — им на мобильном надо гонять\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC> а почему ты говоришь что быстрый? Я посмотрел рандомные бенчмарки и он вроде где-то быстрее, а где-то медленнее\n",
      "----------------------------------------\n",
      "Я подамаю как к этому подступиться, чтобы это не надо было делать руками\n",
      "----------------------------------------\n",
      "Задачи в самом деле разные, так как важно не только чтобы дискриминатор был достаточно хорош, но и чтобы он правильно “делился” градиентами на генератор\n",
      "----------------------------------------\n",
      "&gt;В GAN это ещё просто не успели встроить, или будет очень медленно работать?\n",
      "я пробовал большие сети в дискриминатор вставлять, как то не очень работает\n",
      "----------------------------------------\n",
      "Решил таки причесать и выложить свой вариант кода для dcgan под keras, как будет готово — кину ссылку.\n",
      "Владелец сайта откуда я брал спрайты дал разрешение выложить датасет, так что можно будет сразу запускать!\n",
      "----------------------------------------\n",
      "В данный момент мой некий код ни с какой другой нелинейностью кроме нее не работает - так она замечательна\n",
      "----------------------------------------\n",
      "<@U13E1AWCX> - Elu - наше все, давно ее втыкаю где надо и не надо.\n",
      "----------------------------------------\n",
      "Сопоставимо, но, наверно, чуть лучше, во всяком случае мне аргумент про то, что батч более центрирован, и что веса не вымораживаются, как с relu нравится. Честно, я не замерял.\n",
      "----------------------------------------\n",
      "Если кто работает с ELU - попробуйте ещё ARFA (Adaptive Rational Fraction Activation):\n",
      "это что-то типа HLU <http://ieeexplore.ieee.org/document/7727220/> только обучаемое, и со скейлингом правой части \n",
      "формула:\n",
      "f(x) =  { kx, x&gt;0\n",
      "             { ax/(b-x) , x&lt;=0 ;  a,b &gt; 0\n",
      "a - необучаемый параметр (по умолчанию a=1)\n",
      "b - обучаемый параметр (как в PReLU <https://arxiv.org/abs/1502.01852> или PELU <https://arxiv.org/abs/1605.09332>)\n",
      "k - необучаемый параметр, в качестве стартового значения берём единицу, а затем инициализируем с помощью LSUV <https://arxiv.org/abs/1511.06422> (как и остальные веса сети). Вроде как даёт эффект, похожий на то, что описано в п. 3.3 в <https://arxiv.org/abs/1602.07261>\n",
      "параметры b и k - поканальные (channel-wise)\n",
      "в отличие от ELU (или PELU) при вычислении не используется операция взятия экспоненты, так что работает быстрее, хотя обладает в целом теми же свойствами. В отличие от HLU - обучаемая (и даёт более высокую точность). Вообще в моих экспериментах (пока только на CIFAR-100 и на нашей базе в задаче распознавания лиц) было улучшение по точности классификации и скорости обучения и по сравнению с ELU/PELU и по сравнению с HLU. Особенно помогал скейлинг правой части с помощью параметра k. В общем, у кого есть время и желание - попробуйте, может оно и в ваших задачах будет работать :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "<@U06K9ELB1> А ты не желаешь показать как все это действо прикручивается к задаче про спутники?\n",
      "----------------------------------------\n",
      "Я подумаю.\n",
      "Да, вот ещё вариант ARFA, скрещенный с RReLU <https://arxiv.org/abs/1505.00853> (тут уже я экспериментов не проводил, так что пока только в качестве идеи). В RReLU в отличие от PReLU для указания угла наклона левой части функции вместо обучаемого параметра используется случайное значение в заранее определённых границах. Добавляемая таким образом случайность (по словам авторов статьи) помогает бороться с переобучением сети.( Авторы этой статьи кстати заняли второе место в первом Data Science Bowl на kaggle). В тестовом режиме используется середина диапазона (для детерминистического результата).\n",
      "\n",
      "В случае ARFA такую случайность также можно добавить, причём в разные места (можно поэкспериментировать). Можно, например, сделать параметр b не обучаемым, а рандомным, в некотором диапазоне значений (по аналогии с RReLU вместо PReLU)\n",
      "Но в этом случае мы теряем способность к обучению параметра b, которая нам может пригодиться. Для того, чтобы этого избежать, мы можем сделать следующее:\n",
      "- на каждой итерации обучения случайным образом определять текущий \"режим работы\" активационной функции - либо она у нас сейчас обучаемая либо рандомизированная (можно с разными вероятностями - например, 0.8 и 0.2). Если режим работы - обучаемая функция, то наша функция - это стандартная ARFA (и мы с ней работаем как указано выше, параметр b - обучается), а если режим работы - рандомизированная функция, то мы используем следующую формулу:\n",
      "\n",
      "f(x) =  { kx, x&gt;0\n",
      "             { ax/((b + r*b) -x) , x&lt;=0 ;  a,b &gt; 0\n",
      "r - случайное число в неком диапазоне, например, от -0.5 до 0.5, а b - это текущее значение обучаемого параметра b. Получается что мы добавляем некоторую случайность относительно (\"вокруг\") текущего положения параметра b. В этой фазе параметр b не обучается.\n",
      "\n",
      "В тестовом режиме используем просто значение параметра b. С точки зрения имплементации режим работы лучше выбирать сразу для всего слоя, а не для отдельных каналов. Ещё можно диапазон значений r постепенно уменьшать к концу обучения (как иногда делают с dropout)\n",
      "\n",
      "В итоге получается, что функция является рандомизированной (т.е. по идее как и RReLU лучше борется с переобучением сети), при этом оставаясь обучаемой (как PReLU).\n",
      "\n",
      "Но это неточно (я пока нормальные эксперименты не проводил).\n",
      "----------------------------------------\n",
      "Тоже кто хочет - попробуйте :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Ребят, как обстоят дела со скейлингом изображений? Можно ли юзать это для продакш решений? В публикациях, которые встречались, все очень круто, но хочется узнать в реальном мире как \n",
      "----------------------------------------\n",
      "он как то в мейле рассказывал про их решения\n",
      "----------------------------------------\n",
      "Осталось узнать, что за тулза у нвидиа, и кто ее в мейле показывал :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "А upscaling это что? Это когда берется картинка небольшого размера на вход а на выходе она же в высоком разрешении? Читал я про это где-то и картинки были красивые, но еще тогда терзали меня сомнения что это будет работать на всех картинках, а не только на тех которые сиииииильно похожи на train set.\n",
      "----------------------------------------\n",
      "Да, увеличение разрешения\n",
      "Я почему апскейлинг должен работать только если похоже на трейн сет? Почему та же логика не работает для классификации, к примеру?\n",
      "\n",
      "Я понимаю логику, что информацию нельзя “вернуть”, но ведь в реальной жизни у нас картинки очень даже предсказуемы в локальности. То есть конечно можно \"сбрить\" какие-то детали, но в среднем нет причин этому происходить\n",
      "----------------------------------------\n",
      "&gt;А upscaling это что? Это когда берется картинка небольшого размера на вход а на выходе она же в высоком разрешении\n",
      "це апскейлинг, а если то же самое только хорошего качества, то это super resolution\n",
      "----------------------------------------\n",
      "тут есть любители ганов - в статье про SRGAN как раз  очень красивые картинки с суперрезолюшном\n",
      "----------------------------------------\n",
      "Где вообще можно норм архитектуру для такой сети глянуть?\n",
      "----------------------------------------\n",
      "спасибо!\n",
      "Если кто-то запустит, пришлите комментарии какие были сложности / что дописать в ридми\n",
      "----------------------------------------\n",
      "<@U2TP5JELS> добавил описание к датасету и в двух словах зачем нужен DCGAN\n",
      "----------------------------------------\n",
      "Кто-нибудь пробовал использовать нестандартную функцию ошибки в задаче классификации для оптимизации F1-score? Уже не первый раз сталкиваюсь с ситуацией, когда ошибка (кросс-энтропия) на валидационной выборке растёт, а с ней и целевая метрика (F1-score) до какого момента, после чего болтается около одного значения при продолжающемся росте ошибке, т.е. явном переобучении.\n",
      "----------------------------------------\n",
      "Привет. \n",
      "Интересно узнать, как вы пользуетесь сервисами Amazon (или чего-то другого) для обучения нейросеток. Желательно какой-нибудь актуальный гайд с самыми дешевыми способами. \n",
      "Цель - поучиться, то есть времени на машине тратить буду много и амазоновские предложения кажутся пока дороговатыми. Возможно, я просто не знаю как ими правильно пользоваться.\n",
      "----------------------------------------\n",
      "зачем тебе дожидаться, если ты хочешь просто поучиться?\n",
      "----------------------------------------\n",
      "<@U1G303UTW> Какие ami можешь посоветовать?\n",
      "----------------------------------------\n",
      "но сейчас цены вроде как стабильны, то есть умирать не должен\n",
      "----------------------------------------\n",
      "<@U2TP5JELS> меньше компрессия, поэтому больше файл. Почему он занимает 5Гб я не понимаю, должен быть 1Гб\n",
      "----------------------------------------\n",
      "а керас и теано у тебя какой версии ?\n",
      "----------------------------------------\n",
      "в 3-ем питоне, как минимум\n",
      "----------------------------------------\n",
      "<http://blog.stratospark.com/multiprocess-image-augmentation-keras.html>\n",
      "Длиннопост о том, как парень добавил асинхронное батчевание в Керас. \n",
      "А что, его там правда не было?\n",
      "----------------------------------------\n",
      "Отличная книга, где всё разжёвывается так что дальше некуда- Tensorflow for machine intelligence, читая вместе с документацией\n",
      "----------------------------------------\n",
      "По поводу генератора керас с несколькими воркерами, то у меня вылетало с ошибкой, когда я в `model.fit_generator` ставил  `nb_worker&gt;1`, помогло решение отсюда <https://github.com/fchollet/keras/issues/1638>\n",
      "----------------------------------------\n",
      "Привет. Вопрос к знатокам TF. Запускаю distributed код, и наблюдаю такую магию: на компе где устанавлена версия с CUDA, все работает нормально. На версии TF с CPU скрипт виснет на этапе открытия сессии. Везде стоит tf 0.12.1, Mint/Ubuntu x64. Минимальный код для воспроизведения ошибки:\n",
      "<https://gist.github.com/dd210/2a5b93ef91b7f2e45b448ded38fbfab0>\n",
      "Кто нибудь сталкивался? У меня кончились все идеи.\n",
      "----------------------------------------\n",
      "ага, похоже ошибка в том что передается что-то не то, как rocknrollnerd писал. я pandas не так хорошо знаю(\n",
      "----------------------------------------\n",
      "Выглядит как иллюстрации к закону о домашнем насилии \n",
      "----------------------------------------\n",
      "<@U053R9RS6> девиантарт ок, но что если пихнуть в него немного гитхабовщины и помимо артов добавить возможность еще и код размещать?\n",
      "то есть маркетплейс исключительно под нейроарт (без конкуренции с людьми, как на девиантарте было бы).\n",
      "\n",
      "(полагаю, это слишком уж :more-layers: идея, но чо бы нет)\n",
      "----------------------------------------\n",
      "Какой objective лучше использовать при бинарной классификации с несбалансированными классами? вообще интересует максимизация pr auc на маленьком классе, есть какой-то лосс который может помочь быстрее это выучить?\n",
      "----------------------------------------\n",
      "У меня наверное глупый вопрос, но я не могу понять что за softmax function? Для чего она нужна? Вроде бы уже везде прочитал, понимаю как работает, а практический смысл ускользает\n",
      "----------------------------------------\n",
      "<@U14BPHDK6> не совсем понятно, можешь более специфицировать, какие например функции ты там видишь? Возможность продавать код/генераторы арта? Вместо просто картинок? Или совместить с печатанием их? А то гитхабовщина - она уже есть - на гитхабе, люди публикуют картинки, снизу ссылки на их гитхаб, и так уже всё работает.\n",
      "----------------------------------------\n",
      "А так идея интересная. Лично мне кажется что нейроарт - очень большая но нераскрытая пока область. И не очень понятно как её раскрывать.\n",
      "----------------------------------------\n",
      "Но какой за этим математический смысл?\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC> если взять категориальное распределение и записать его, как пример из экспоненциального семейства, то софтмакс там вылезает сам собой. Лично я это проделывал только для Бернули, но википедия, говорит, что для N классов это тоже работает <https://en.wikipedia.org/wiki/Generalized_linear_model>\n",
      "----------------------------------------\n",
      "А можно чуть подробнее? Какое изначальное допущение на модель, что является результатом?\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC> можно еще не через GLM, а через вывод линейного и квадратичного дискриминанта, тут <https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb> в самом конце есть раздел “Logistic regression”, там как раз вывод верез предположение о нормальности фичей при заданном классе p(x | c = i) ~ N(m_i, s_i); если проделать тоже самое при количестве классов больеш двух, то получится несколько log odds, и при том жепредположении о фичах будет софтмакс\n",
      "----------------------------------------\n",
      "вообще куда более интересная связь софтмакса с распределением Гиббса, никакой математики, просто если трактовать классы как состояния динамической системы, это у Хинтона в статьях про машины Больцмана описано\n",
      "----------------------------------------\n",
      "так сходу не вспомню в какой, вроде как это было и в обычно машине больцмана и в ограниченной\n",
      "\n",
      "точно помню что про эту связь упоминается в 11 главе третьего издания Саймона Хайкина neurl networks and learning machines\n",
      "----------------------------------------\n",
      "Что-то я не понял, а почему это у tf нечитаемые исходники?\n",
      "----------------------------------------\n",
      "Привет! Есть здесь кто нибудь кто работал с distributed tensorflow или пробовал разобраться? Очень хотелось бы обсудить некоторые вопросы с тем, кто уже на эти грабли понаступал. С самыми простыми случаями я вроде разобрался, но все равно есть моменты, где это все выглядит как натуральные танцы с бубном. И работает 50/50 :thinking_face:\n",
      "----------------------------------------\n",
      "тоже закину удочку, но больше про теорию. в голове какое-то время крутится дурацкая мысль про то, как бы скрестить adversarial loss и anything2vec; может, кто-нибудь читал про такое и подкинет статью? (ну или сразу скажет, что это невозможно и не подкинет)\n",
      "\n",
      "суть примерно такова: когда мы делаем негативный сэмплинг (в w2v и в восстановлениях всяких дырявых матриц, например), у нас обычно есть какое-то статическое распределение, откуда мы сэмплим - либо униформное, либо по частотам встречаемости слов в корпусе. печалит как раз тот факт, что оно статическое - мы не можем сэмплить с контекстом, мы вынуждены надеяться, что из этого распределения можно достать более-менее все репрезентативные примеры (а если оно сильно скошено, то это далеко не факт), плюс были всякие пруфы про то, что градиентный спуск в таких ситуациях быстро останавливается (<http://webia.lip6.fr/~gallinar/gallinari/uploads/Teaching/WSDM2014-rendle.pdf>), и т.д.\n",
      "\n",
      "кажется, было бы круто, если бы негативный сэмплер мог на ходу учиться искать новые коварные негативные примеры, неожиданно подсовывая их основной модели. получается такой adversarial-сэмплер - он обучается понижать метрику основной модели, может принимать на вход контекст (например, профиль пользователя) и апдейтится на ходу.\n",
      "\n",
      "проблема в том, что если я правильно понимаю, такой сэмплер нельзя обучать градиентным спуском. где-то на выходе у него должен быть условный софтмакс и argmax по нему, чтобы выбрать наиболее подходящий негативный сэмпл, а эта операция недиффиренцируемая. или еще какой-то полуоформленный альтернативный способ подразумевает искать минимум в пространстве оценок, выставляемых основной моделью, но для этого нужно будет  поставить оценки всем словам из словаря (заново в каждой эпохе), чего делать бы не хотелось.\n",
      "\n",
      "может, кто-нибудь встречал что-то подобное?\n",
      "----------------------------------------\n",
      "пока без контекста, верно. контекст можно ввести в виде дополнительного входа сэмплеру (правда, в случае word2vec я навскидку слабо представляю, какой в нем смысл. в условных рекомендациях, как в статье по ссылке, это может быть искомый юзер, а модель будет учиться pairwise-ранкингу)\n",
      "----------------------------------------\n",
      "(это я пытаюсь вслух рассуждать). пока для меня немедленным образом не очевидно, как такую метрику сделать\n",
      "----------------------------------------\n",
      "Но понятное дело подразумевается что веса примеров соответствуют распределению того как “в приложении”\n",
      "----------------------------------------\n",
      "Мне сложно просто обсуждать, так как я плохо понимаю на самом деле как устроен w2v\n",
      "----------------------------------------\n",
      "я тоже с ним близко не работал, мне проще оперировать всякими bayesian personalized ranking)\n",
      "энивей, спасибо, что прошелся пошагово - по крайней мере, не стало очевидно, что я где-то сильно затупил)\n",
      "\n",
      "напрашиваются, конечно, всякие неградиентные оценки типа REINFORCE (сделать из негативного сэмплера, условно говоря, reinforcement-агента, у которого количество возможных действий - это как раз количество слов в словаре и есть). но как-то это выглядит совсем грустно.\n",
      "----------------------------------------\n",
      "как вариант просто начать с того чтобы запилить эвристику которая была бы лучше случайной\n",
      "----------------------------------------\n",
      "Если как-то научиться оценивать как хорошо “добавило” каждое слово, то можно попробовать обучить что-то supervised\n",
      "----------------------------------------\n",
      "<@U4051KF2R>: а на какой loss обучать семплер-то планируется? И на каких данных? Что помешает семплеру выучить те же вектора внутри, и всегда заваливать эмбеддинги?\n",
      "----------------------------------------\n",
      "а почему он должен подавать семантически близкие слова?\n",
      "давай возьмем ситуацию с контекстом; кажется, она в какой-то степени репрезетативная. обучаем мы w2v, а в качестве контекста передаем жанр текста - техническая проза, публицистика или сказки для детей. твой умный сэмплер должен понять, что в физическом учебнике слова \"body\" и \"political\" не должны стоять вместе (и одно может идти негативным примером к другому), а в статье про про политоту это возможно, а вот \"body\" + \"solid\" - уже вряд ли.\n",
      "----------------------------------------\n",
      "И мне не ясно, как контекст \"medicine\" (что это вообще?) скажет семплеру о том, что \"body\" и \"torso\" близки, если они не встречаются в совпадающих контекстах\n",
      "----------------------------------------\n",
      "это не должно случаться часто, потому что сэмплер все еще ограничен своими окнами из скип-грамм - он может выбирать только из тех мест, где искомое слово не стоит рядом. у него все еще есть шанс наткнуться на семантически соседское слово, но основная масса соседских слов все-таки будет стоять рядом со словом X\n",
      "----------------------------------------\n",
      "С точки зрения того, как ты хочешь делать adversarial sampling, семплеру *выгодно* найти синоним и подать его\n",
      "----------------------------------------\n",
      "Я бы все-таки вместо попытки научить семплер давать сложные, простые, какие-то еще примеры, пробовал обучать его давать примеры которые улучшают модель. Вопрос только как это мерять\n",
      "----------------------------------------\n",
      "с применением мелков и школьной доски картинку можно представить примерно так - у нас есть большое пространство предметов, которое нельзя обойти все, и нам надо его как-то поменять так, чтобы близкие предметы лежали рядом, а далекие - на расстоянии\n",
      "\n",
      "модель \"улучшается\", когда все становится чуть более правильно разложено. соответственно если у нас _уже_ много предметов правильно рассортированы, улучшить мы ее можем только найдя следующую ошибку, неправильно расположенную пару предметов. вопрос в том, как ее быстро найти) \n",
      "\n",
      "<@U04ELQZAU> кстати, возможно, это запоздалый контрпойнт про синоним, который выгодно найти - если сэмплер действительно найдет его и w2v растащит два семантических слова далеко друг от друга в пространстве эмбеддингов, то теперь рядом со словом X будут лежать сколько-то семантически _не_ близких примеров (которых должно оказаться больше, чем случайных синонимов), и сэмплер скоро подберет их и затолкает слово X обратно\n",
      "\n",
      "вопрос о стабильности такого обучения, конечно\n",
      "----------------------------------------\n",
      "&gt; сэмплер скоро подберет их и затолкает слово X обратно\n",
      "Почему обратно?\n",
      "----------------------------------------\n",
      "ну, куда-нибудь, где минимальное количество случайных синонимов. если нет такого места, где их ноль, то это будет место, где их один\n",
      "----------------------------------------\n",
      "triplet loss основан на разнице между исследуемым объектом с негативным и позитивным примером\n",
      "цель состоит в том, чтобы embedding был как можно ближе к позитивным и как можно дальше от негативных\n",
      "основная проблема - долго учится и нелегко находить тройки\n",
      "<https://arxiv.org/abs/1503.03832>\n",
      "----------------------------------------\n",
      "<@U0KQ5M6KX>: нет, триплет лосс тут ни при чём, тут речь про то, как адаптивно находить негативные примеры\n",
      "----------------------------------------\n",
      "Так же как в GAN'ах дискриминатор учится по мере обучения генератора\n",
      "----------------------------------------\n",
      "про скитания, пожалуй, неочевидно - если мы берем несколько, скажем, негативных примеров, и при условии общей похожести они взяты равномерно-случайно \"вокруг\" позитивного, общее направление апдейта все равно должно быть в сторону, где их меньше\n",
      "\n",
      "правда, я не очень уверенно себя чувствую в таких высокоуровневых рассуждениях. спасибо за инсайт, подумаю)\n",
      "----------------------------------------\n",
      "почему нет? когда слово отошло от слова-товарища, оно оказалось в пространстве несоседских слов. любой шаг в сторону будет означать то, что сэмплер подберет негативный пример, соответствующий этому направлению, и запретит туда ходить. если мы соберем несколько таких примеров в батче, они в среднем будут вести обратно к слову-товарищу - потому что в том месте все еще жили часто встречающиеся с искомым слова, а случайный-синоним-товарищ на данной стадии не видим сэмплером и не дает никакого пенальти. мне кажется, это больше похоже на колебания туда-сюда. которые все еще не очень сходятся, конечно\n",
      "----------------------------------------\n",
      "В незалежной локализацие наверняка звучит как Taras\n",
      "----------------------------------------\n",
      "Всем привет. А не подскажите как правильно в керасе  последовательность кадров сначала подавать в сверточную сеть, а потом  в rnn сеть. На входе данные с `shape = (batch_size, seq_len, n_channels, height, width)`.  Я думал сделать  reshape  слой так чтобы преобразовать входные данные в  `shape = (batch_size*seq_len, n_channels, height, width)` затем пропустить через сверточные слои, flatten слой и потом обратно сделать reshape, чтобы получить `shape = (batch_size, seq_len, conv_out )`. Но че-то не понял как это сделать в керасе, так как в документации для reshape слоя написано, что он не затрагивает batch_dimension:\n",
      "&gt;&gt;&gt;target_shape: target shape. Tuple of integers, does not include the samples dimension (batch size).\n",
      "----------------------------------------\n",
      "там как раз насколько я помню  шейп входа как у тебя\n",
      "----------------------------------------\n",
      "очень хотел это как глаза агента потестить, но времени нет\n",
      "----------------------------------------\n",
      "тогда у тебя последовательность кадров в последовательность и будет преобразовываться , а потом расплющивай как угодно (через conv3d или return_sequences=False и Flatten)\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC>  а зачем тогда деконволюшн слои если страйд 1?\n",
      "----------------------------------------\n",
      "И не могу найти ничего, потому что вся выдача гугла завалена наоборот жалобами на то, что \"всё жрёт, как ограничить\"\n",
      "----------------------------------------\n",
      "```\n",
      "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95)\n",
      "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))```\n",
      "Если у тебя на той видео точно ничего не висит, то должно отработать. И  CUDA_VISIBLE_DEVICES=”0” (если твоя нейрокарта первая) лучше задай, чтобы тф даже не пытался куда-то ещё лезть. По крайней мере с такой командой оно честно попытается отъесть 95% памяти видеокарты. Вот если она недоступна будет, надо копать кто жрот.\n",
      "----------------------------------------\n",
      "```\n",
      "gen1 = image.ImageDataGenerator()\n",
      "trainGen = gen1.flow_from_directory(‘data/train/’, …, seed=0)\n",
      "model.fit_generator(trainGen, ...)\n",
      "```\n",
      "а кто нить пробовал такую связку в Keras? ImageDataGenerator при этом не делает никакую augmentation\n",
      "как с загрузкой GPU? у меня почему то очень низкая и время обучения относительно большое, если предобработать данные а потом просто `fit ` то скорость возрастает на порядок\n",
      "похоже на баг\n",
      "----------------------------------------\n",
      "Ок, я понял, что softmax и формула логистической регрессии естественно вылезают, если предположить, что каждому классу соответствует нормальное распределение фич, где у каждого класса свой mean и variance\n",
      "----------------------------------------\n",
      "Какое предположение нам нужно принять, чтобы финальный слой был softmax?\n",
      "----------------------------------------\n",
      "sim0nsays: тут я склоняюсь к тому, что говорил <@U04ELQZAU>: softmax -- это гладкая (полезность гладкости упоминал <@U13E1AWCX> ) аппроксимация максимума. По поводу предположений: то, что фичи с последнего слоя распределены нормально -- подходит, но это нам немного дает, поскольку мы их не наблюдаем. Вообщем, я не знаю, почему берут именно эту функцию, надеюсь, другие участники дадут более строгую аргументацию.\n",
      "----------------------------------------\n",
      "Но почему мы применяем его к deep networks?\n",
      "----------------------------------------\n",
      "Собственно, точно тот же вопрос, почему для бинарной классификации нужно сигмоиду применять\n",
      "----------------------------------------\n",
      "Сигмоида тоже не единственный вариант для двухклассовой классификации.\n",
      "Софтмакс обобщение сигмоиды, и как следствие тоже не единственный.\n",
      "----------------------------------------\n",
      "дамы и господа, а какой сейчас самый модный и молодёжный ami со свежими tf теанами и прочим каффе?\n",
      "----------------------------------------\n",
      "Какие можно использовать архитектуры для классификации последовательностей тектов? На ум приходит что-то вроде LSTM, которая на первом урочне получает последовательность текстов, а на втором смотрит на текст на последовательность слов\n",
      "----------------------------------------\n",
      "<@U0XF4GAM8> а где ты такой датасет взял?\n",
      "----------------------------------------\n",
      "чат, а кто как собирает глобальные статистики для batch normalization в lasagne?\n",
      "----------------------------------------\n",
      "но выглядит как штука со своим трехэтажным набором абстракций, в дополнение к TFовским\n",
      "----------------------------------------\n",
      "хз в каком канале спросить, перенаправьте, если что.\n",
      "А есть какие-то готовые нейронки, которые на лицах обучаются, чтоб ее можно было обучить (или взять готовую) отрезать последний слой и попытаться что-то свое дальше доучить, т.е. типа тех же реснетов и т.п. только наученное на датасетах из лиц ?\n",
      "----------------------------------------\n",
      "Я правильно понимаю, что для того, чтобы сделать FCN из pre-trained resnet надо оторвать последние слои и прикрутить на их место пару итераций UpSampling + convolution?\n",
      "\n",
      "Где-нибудь кому-нибудь попадался пример того, как это на keras делается, чтобы я не изобретал?\n",
      "----------------------------------------\n",
      "Вот как это в tf slim для vgg - я думаю это прям один в один в керас переводится\n",
      "----------------------------------------\n",
      "обрати внимание как pool примешивается\n",
      "----------------------------------------\n",
      "Все-равно не понятно.\n",
      "\n",
      "Эксперты по Keras, как мне pre trained которые приглагаются к керасу перебить под FCN?\n",
      "----------------------------------------\n",
      "Я у модели слои оторвать оторвал, а новые как добавить пока непонятно.\n",
      "----------------------------------------\n",
      "А она у тебя как sequential?\n",
      "----------------------------------------\n",
      "попробуй какой-то слой оттуда подать как входные данные для, положим, для теста, conv слою\n",
      "----------------------------------------\n",
      "Кто помнит, какие mean values по каналам на ImageNet?\n",
      "----------------------------------------\n",
      "<@U34Q3KU8H> так и как же?\n",
      "----------------------------------------\n",
      "Я даже не знаю кто больше наркоман: <@U34Q3KU8H>  что такие вопросы задает, или я что это помню\n",
      "----------------------------------------\n",
      "Ну точнее лого как раз нарисовал знакомый дизайнер в порыве креатива\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC>: не суть как важно, какой именно там полином, всё равно он немонотонен\n",
      "----------------------------------------\n",
      "на каком круге ада я нахожусь если куда не работает с неизвестной ошибкой, при том что нвидиа-сми спокойно видит гпу?\n",
      "----------------------------------------\n",
      "А есть кто-то из чата, кто когда-нибудь заказывал машины на <https://www.hostkey.ru/dedicated/ru-gpu/> ? \n",
      "Можете поделиться отзывами?\n",
      "----------------------------------------\n",
      "Добрый день коллеги! Есть ли у кого опыт работы с TFlearn? Наткнулся на странную ошибку, хотел бы обсудить...\n",
      "----------------------------------------\n",
      "А какой сейчас топовый способ обучения joint embedding между изображениями и текстом, если есть пары текст-изображение?\n",
      "----------------------------------------\n",
      "<@U428C5XRN>  а какой шейп у тензора после tf.expand_dims\n",
      "----------------------------------------\n",
      "возьми keras тогда, там тоже примеры по классификации есть\n",
      "сложно тебе чем-то помочь, как я понял ты не понимаешь что в коде происходит :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "как написал - код не мой... взял его без изменений... до автора не достучался... тут скорее проблема в сборке TFlearn...\n",
      "----------------------------------------\n",
      "ну может api давно изменилось, проблема в том, что надо пониматься как эти слои и  само api tflearn работают, чтобы их поправить :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "tflearn это какое-то адское поделие, скопированное с keras :slightly_smiling_face:\n",
      "не знаю кто его использует, пришёл чувак и скопировал многие моменты из keras \n",
      "уж если собрался изучать все плотно, то надо голый tensorflow брать :be-a-man:\n",
      "тем более автор keras говорил, что его библиотека будет официальной высокоуровневой абстракцией для tf\n",
      "----------------------------------------\n",
      "Зависит от твоего средства виртуализации, вангую, что просто карточку не пробросишь, so не надо возиться с кудой и гонять CPU only. Ничего страшного в этом нет.\n",
      "----------------------------------------\n",
      "Существуют гайды, как деплоить :more-layers:  модель в продакшн?\n",
      "Особенно интересно — обязательно ли для предикта использовать тачку с ГПУ?\n",
      "----------------------------------------\n",
      "Ну в общем, можно) Благодарю!\n",
      "А про гайды — может не про deep, а про обычные модельки — есть какие best practices? Просто ссылочки было бы достаточно)\n",
      "----------------------------------------\n",
      "Где секси картинки, я вас спрашиваю\n",
      "----------------------------------------\n",
      "<@U049HDR2Z> У меня поугас интерес к ганам. Я статью, открыл, подумал и закрыл. В общем не стоит ожидать от меня ревью. Как опять интерес проснется, наверное с нее и начну\n",
      "----------------------------------------\n",
      "<https://xnor.ai/index.html>\n",
      "кто знает что у них под капотом? свои модели? или они удачно портируют?\n",
      "выглядит очень круто\n",
      "----------------------------------------\n",
      "Вау результат, это когда может генерировать картинки способные “обмануть” человека. Ну хотя бы одну из 100\n",
      "----------------------------------------\n",
      "Кстати, а какое определение у deep learning'a с точки зрения математики?\n",
      "----------------------------------------\n",
      "А зачем ты даешь код неспециалисту для запуска?\n",
      "----------------------------------------\n",
      "Есть группа энтзиастов клипмейкеров. Я хотел им попробовать сделать сетку для интерполяции промежуточных кадров. Вот думаю как им потом дать на запуск\n",
      "----------------------------------------\n",
      "zfturbo: Если что, готов потестить как это всё таскать. Всё равно хочу разобраться как нейросети под виндой мутить\n",
      "----------------------------------------\n",
      "Отлично. Можно даже не ждать той сетки, а попробовать на чём-нибудь другом ) Я напишу как будет что-то готово.\n",
      "----------------------------------------\n",
      "<@U43FTJQ2V> я для себя в ближайшие дни буду делать live USB ubuntu для десктопа, но я буду конкретно для 960 карты делать, не знаю как оно будет работать на других \n",
      "----------------------------------------\n",
      "Ну вот собственно первый раз вопрос поднятый выше у меня и возник когда я хотел дать этот код дать другому человеку. )\n",
      "----------------------------------------\n",
      "<@U1CF22N7J> есть гайды как это делается. Расскажи на каком этап застрял подскажу.\n",
      "----------------------------------------\n",
      "Не, пока не разобрался.\n",
      "\n",
      "[1] Они действительно лучше чем UpSampling + Conv?\n",
      "[2] Как перебить Unet под Deconv?\n",
      "[3] И вообще все детали мне очень интересны.\n",
      "----------------------------------------\n",
      "После Deconvolution надо Concolution наворачивать, для сглаживания, как после Upsampling или он сам это делает и в этом его сила?\n",
      "----------------------------------------\n",
      "<@U053R9RS6> А LeakyReLU из каких соображений? Она лучше отработала чем ELU, PRelu ?\n",
      "----------------------------------------\n",
      "Мне кажется оно все еще пока не особо работает, как ни крути\n",
      "----------------------------------------\n",
      "Задача про спутники на кагле, которую мы массово пытаемся решить. в <#C3Z5S49GV|proj_kaggle_dstl2017> Половина из топ 10 с Public LB там.\n",
      "\n",
      "Есть спутниковые снимки, вроде как с Вьетнама, на 25 из них попиксельна промаркирована классовая принадлежность. Надо создать модель, которая будет для других снимков (test = 425 images) тоже ее определять. Классы не взаимоисключающие.\n",
      "\n",
      "В общем надо чтобы визуализация предсказаний выглядела как-то вот так:\n",
      "\n",
      "<https://www.kaggle.io/svf/650368/89902d556d0fa19464b7d6c3f306bff5/6120_2_2.png>\n",
      "----------------------------------------\n",
      "<@U34Q3KU8H> я с ходу как-то не понимаю а где здесь upsampling?\n",
      "----------------------------------------\n",
      "На приведенном выше происходит предсказание части исходной картинки, это да. Тут я согласен. Но в целом, если не кропать центр при осуществлении shortcuts, а брать целиком, как мы сейчас и делаем - ну очень похоже на autoencoder.\n",
      "----------------------------------------\n",
      "Да, в задаче надо предсказать класс для каждого пикселя.\n",
      "\n",
      "Выход не такой же как и вход. На выход подается маска. В этой задаче у нас 10 классов =&gt; (10, N, M) на выход. То есть это не autoencoder.\n",
      "\n",
      "Но если бы мы поменяли выходной слой так, чтобы он output выдавал в том же формате, что и было на входе. =&gt; можно было бы использовать эту архитектуру как Autoencoder.\n",
      "\n",
      "То есть Unet без crop (на картинке выше мы crop используем, я на такую вариацию только что наткнулся, когда картинку искал.)- это не AutoEncoder, но архитектурно очень похож, с точностью до выходного слоя.\n",
      "\n",
      "Даже есть идея преиницилизировать Unet, используя его как AutoEncoder на всех 450 картинках, а уже потом fine tune под предсказание масок.\n",
      "----------------------------------------\n",
      "В общем есть мнение, что пытаться предсказать пиксели, используя sigmiod - это мало, хочется добавить некой глобальной структуры в loss function. Но так как ответ у нас бинаризированный, то и каждый пиксель бы хотелось предсказывать более-менее точно. На задаче про сосуды, чтобы была летом, я не заморачивался  и просуммировал обе loss function. Ту что за пятна (глобальная структура) и ту, что за пиксели (локальная структура).\n",
      "\n",
      "В общем функция потерь выглядит это вот так:\n",
      "\n",
      "```\n",
      "def jaccard_coef_loss(y_true, y_pred):\n",
      "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)\n",
      "```\n",
      "----------------------------------------\n",
      "Я не удивлюсь если кто-нибудь похожую идею на целый текст на arxiv растянул с аргументами почему это может сработать.\n",
      "----------------------------------------\n",
      "то есть -K.log(jaccard_coeff) у тебя как пенальти работает за неправильное предсказание?\n",
      "----------------------------------------\n",
      "Я помню работу, где в сегментации помогал crf \n",
      "----------------------------------------\n",
      "Где-то тоже попадалась. Но я не придуумал как CRF сделать частью loss function, да и во многих работах он используется уже именно как post processing, чтобы улучшить качество предсказания. Хотя если кто-то поможет мне прикрутить его к функции потерь в Keras, буду премного благодарен.\n",
      "----------------------------------------\n",
      "Подскажите такую штуку. Когда у нас в сети convolution layer и подаётся 3 канала на вход, то это значит что одинаковые фильтры применяются ко всем трём каналам?\n",
      "----------------------------------------\n",
      "Что у нас должно быть на выходе первого такого слоя? Куда денутся три входных канала? :confused:\n",
      "----------------------------------------\n",
      "Я чувствую, что если бы получал по доллару каждый раз, когда рассказываю как работает CNN, можно было бы давно не работать \n",
      "----------------------------------------\n",
      "Да, я тоже подумал, что не стоит мучать Семёна свёртками в очередной раз и пойти глянуть Карпатого, вчера смотрел лекцию как раз перед свётками, надо продолжить\n",
      "----------------------------------------\n",
      "У живого человека спросить - очень важная штука, какие бы хорошие иллюстрации не были \n",
      "----------------------------------------\n",
      "Вот в твоём объяснении \"Пусть в следующем слое 10 нейронов\" -- следующий слой -- это какой и после какого?\n",
      "----------------------------------------\n",
      "Ок, т.е. если input (x, ,y, *20*) --&gt; conv2(10) --&gt; output (x, y, 10), то у каждого нейрона будет пропорционально больше параметров и всё, дальше по сети пойдёт информация такого же размера как в первом случае?\n",
      "----------------------------------------\n",
      "Да я всё про спутники, там вопрос в том, как лучше делать: натренировать пару (тройку) отдельных сетей на обычные rgb изображения + всякие инфракрасные и потом объединить предсказания или достаточно будет объединить все каналы в одну толстую картинку и дальше одна сеть разберётся сама. Ну вот исходя из нашего разговора выше, в тонкостях взаимоотношения каналов будет по сути разбираться только первый слой, а остальные будут видеть информацию уже через призму его понимания. И, имхо, этого слоя будет недостаточно, чтобы найти нужные взаимоотношения, т.к. у нас разные каналы реально очень разные (одни и те же объекты могут совсем по-разному выглядеть и масштаб разный и даже сдвиг может быть нехеровый)\n",
      "----------------------------------------\n",
      "IMHO Если бы у нас было данных как на ImageNet - можно было бы смело в 20 слойный бутерброд. А так как с данными у нас вилы - лучше разные входы и по середине или ближе к концу склеивать.\n",
      "----------------------------------------\n",
      "Весь вопрос где объединять, да\n",
      "----------------------------------------\n",
      "Есть работы, где объединяют перед последним слоем, который предсказание делает и сравнивают с тем, что добавляют сеть, которая смотрит на предсказания каждой половины и активации последнего слоя и учится их балансировать\n",
      "----------------------------------------\n",
      "Идея в том, что каждая половинка по идее и сама должна дать +- правильный прогноз, но может ошибаться, где-то, где другая права и вот этот последний кусочек должен выучить кто-где ошибается и выбирать правого\n",
      "----------------------------------------\n",
      "sovcharenko: Не понял, как можно получить дескрипторы, используя <http://www.robots.ox.ac.uk/~vgg/software/vgg_face/> Авторы утверждают, что \"This page contains the download links for the source code for computing the VGG-Face CNN descriptor\", хотя у модели на Torch на выходе 2622   чисел, то есть сеть тренировалась на классификацию, а не на embedding.\n",
      "----------------------------------------\n",
      "Надо брать выходы с fc8. А модель, где embedding триплетами обучали, они не выложили.\n",
      "----------------------------------------\n",
      "всем привет! кто что помнит свежего про генерацию музыки? давайте попробуем собрать актуальное (6-12 мес) в тредик. статьи, гитхабы, ключевые слова из памяти, что угодно. многим было бы либо полезно, либо интересно\n",
      "----------------------------------------\n",
      "Тогда почему тебя волнует производительность!\n",
      "----------------------------------------\n",
      "о, я кажется понял, почему во враппере жестко прописан ЦПУ для эмбеддинга\n",
      "----------------------------------------\n",
      "Когда считаешь w2v, эмбеддинги на GPU дают 30% прирост скорости. Наверное для LSTM уже не так важно, там другие затычные места.\n",
      "В примере принудительно положили на CPU, да\n",
      "----------------------------------------\n",
      "Всем привет! \n",
      "Есть какие-то бест практис как организовать хранение сериализованых моделей?\n",
      "----------------------------------------\n",
      "Одна из проблема на задаче про спутники, с которой я мучаюсь - это нахождение Вьетнамских тарантасов с высоты пртичьего полета на спутниковых снимках. Image Segmentation не справляется. Есть идея использовать оконный детектор. Кто мне подскажет грамотную литературу, а лучше сразу сетку, которая с этой задачей может успешно сработать?\n",
      "----------------------------------------\n",
      "Т.е. почему они в итоге лучше обычных работают\n",
      "----------------------------------------\n",
      "(если использовать autoencoder как generative model)\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC>: вопрос как раз в том, как пространство кодов регуляризовывать \n",
      "----------------------------------------\n",
      " А как сделать так, чтобы код был интерпретируем? Вроде срепени улыбки на лице?\n",
      "----------------------------------------\n",
      "<@U1LNBRZ29>: без внешнего источника supervision 'а -- не знаю как \n",
      "----------------------------------------\n",
      "Ну вот да, мне видится основное приемущество VAE как мощную регуляризацию на пространство\n",
      "----------------------------------------\n",
      "Мне кажется, и тот, и другой совсем не такие, как в VAE\n",
      "----------------------------------------\n",
      "Но я не знаю как в реальности\n",
      "----------------------------------------\n",
      "чот уже третий из заявленных спикеров\n",
      "----------------------------------------\n",
      "этот чел говорит как будто “здравствуйте, я тоже алкоголик\"\n",
      "----------------------------------------\n",
      "и так как на углях программируешь\n",
      "----------------------------------------\n",
      "там китаец вышел, он выглядит как будто он написал половину тф\n",
      "----------------------------------------\n",
      "<@U2TP5JELS> по видео оно вроде как отвечает на вопрос \"_какого цвета_ водолазка?\"\n",
      "----------------------------------------\n",
      "libfun: а mean как у людей нету?\n",
      "----------------------------------------\n",
      "Если это live, то почему субтитры показывают текст который еще не произнесен =/ причем явно не машинный перевод, только что видел в субтитрах слово \"operaors\"\n",
      "----------------------------------------\n",
      "а вообще, зачем теперь :theano:  объясните? я правда ее не юзал ниразу\n",
      "----------------------------------------\n",
      "mtrofimov: Как я понимаю, RF с элементами ET.\n",
      "----------------------------------------\n",
      "воо, когда мы делали артисто, там модель тоже с кучей связей (стайл лосс), как и в юнет, там тоже межслойные есть; у нас тогда проблемы были что тф слишком много памяти аллоцировал, и то что делалось в теане с батчем 30 в тф 1-2\n",
      "----------------------------------------\n",
      "mephistopheies: На спутниках не желаешь показать высокий класс или просто порассуждать как бы ты к этой задаче подошел?\n",
      "----------------------------------------\n",
      "Да че вы спорите, когда есть mxnet\n",
      "----------------------------------------\n",
      "а чо мхнет разве могет? куда ему до монстров типа теаны, тф и торча?\n",
      "----------------------------------------\n",
      "А с каким объемом ты играешься, сколько минут аудио?\n",
      "----------------------------------------\n",
      "У меня пока руки не дошли это правильно оформить + для этого нужен GrandMaster, а до него мне пока как до луны.\n",
      "----------------------------------------\n",
      "<@U041SH27M> А как ты на тему спутников? Не желаешь?\n",
      "----------------------------------------\n",
      "И еще память так не отжирает как тф\n",
      "----------------------------------------\n",
      "но ведь всё равно за тф будущее, на нем будут писать все, как на самом популярном. Все новички будут тф учить сперва\n",
      "----------------------------------------\n",
      "На TF будут писать те, кто очень сильно в теме.\n",
      "----------------------------------------\n",
      "Это как писать ассемблерные вставки )\n",
      "----------------------------------------\n",
      "Кто нибудь в курсе, как в керасе задать количество потоков?\n",
      "----------------------------------------\n",
      "Для начала по пальцам постучать тем кто новый дизайн выкатил\n",
      "----------------------------------------\n",
      "Из кераса, как я понял, хотят сделать высокоуровневую спеку, чтобы можно было готовыми кубиками сети клепать, а реализацию спеки -- на откуп желающим.\n",
      "----------------------------------------\n",
      "Кто тут работал со слоем Cropping2D в Keras? Есть пара вопросов?\n",
      "----------------------------------------\n",
      "А как у тебя crop1 соединяется с моделью, не вижу что-то\n",
      "----------------------------------------\n",
      "Как в :tensorflow: делать аналог `theano.clone`? Что-то я так и не понял есть там это или нет\n",
      "----------------------------------------\n",
      "Какой framework наименее прожорливый по памти? Хочется больших сетей с большими батчами. :mxnet:  ?\n",
      "----------------------------------------\n",
      "Дык эта, зачем большие батчи?\n",
      "----------------------------------------\n",
      "Runner/Queue выглядит как преждевременная оптимизация, если человек еще совсем не разобрался. Обычный предрасчет и векторизация вычислений дадут значительный буст.\n",
      "----------------------------------------\n",
      "<@U0ZJV6E5Q> сэмплер как раз и работает на видеокарте, основан на динамике Ланжевена. Я на ней считаю градиенты для перехода в mcmc\n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> я как раз np.array и использовал всегда. Просто увидел эти строки в доке, и засомневался\n",
      "----------------------------------------\n",
      "То есть с помощью очередей можно запихивать новый батч в память видеокарты еще перед тем, как досчитался прошлый? Мне кажется, у меня узким местом является именно работа с памятью gpu\n",
      "----------------------------------------\n",
      "Ну там надо смотреть где что разместится. Но да, не ждать вычислений очередного батча и пересылки на GPU.\n",
      "Попробуй профайлер <https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659>\n",
      "----------------------------------------\n",
      "а кто-нибудь добился с этими очередями одновременной пересылки следующего батча host-gpu одновременно с расчетом текущего?\n",
      "НЯП оно только помогает следующий батч приготовить в host ram, а пересылка стартует только когда session.run() вызываем, т.е. уже после обработки предыдущего.\n",
      "----------------------------------------\n",
      "<@U1UBZLMKK> <http://deeplearning.net/software/theano/library/#theano.clone>\n",
      "В краце это утилита, чтобы подменять части вычислительного графа. Полезно, когда есть вычислительный граф от инпута, а мы хотим поменять источник инпута. Использую, когда надо схлопнуть все стохастические переменные в один длинный вектор и получать этот вектор из апостериорного распределения. Тем временем апостериорное эффективно считается с помощью векторных операций\n",
      "----------------------------------------\n",
      "Пытаюсь для классификации собак и котэ обучить сетку при помощи mxnet. Взял архитектуру <https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol_resnet-28-small.R>, картинки сделал квадратными (аспект не менял, добавил пустого пространства по краям) и уменьшил до 50х50. Обучается быстро с батчами по 100 картинок, качество растет с самого начала процесса. Пытаюсь обучать на картинках 224х224 с батчами по 20 картинок (пробовал и 10) - эпик фейл! Топчется на 50% точности. С чем может быть связано, куда копать? Неужели :more-layers:?\n",
      "----------------------------------------\n",
      "Попробуй визуализировать сетку как здесь:\n",
      "<http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/>\n",
      "----------------------------------------\n",
      "n01z3: спасибо, я сразу не осознал, какой размера слой получается после `mx.symbol.Flatten`. Поэкспериментирую с более подходящими архитектурами. Визуализация в R, кстати, похуже выглядит: <https://statist-bhfz.github.io/mxnet_intro.html> (и даже такое не сразу получилось нарисовать)\n",
      "----------------------------------------\n",
      "Так и я говорю, что экзотика :slightly_smiling_face: Что уже работает \"из коробки\" на питоне - в R требует плясок с бубном. Зато когда что-то получается, можно полчаса гордиться собой.\n",
      "----------------------------------------\n",
      "Всем привет. А у меня вот такой вопрос. Есть статья: <http://ieeexplore.ieee.org.sci-hub.cc/document/7738816/?reload=true> про идентификацию диктора по голосу с помощью сверточных сетей на TIMIT. На второй странице авторы описывают архитектуру небольшой сверточной сети, которая должна работать. Но при попытке воспроизвести она у меня не работает и учитывая то, что сам датасет маленький, а 2 последних полносвязных слоя размером в 6300 и 3150 - я даже в теории не представляю как она может работать и не оверфитится. Собственно мой вопрос - кто-то сталкивался, как можно заставить такие (с такой “шапкой\") сетки работать?\n",
      "\n",
      "По моим расчетам размерность перед полносвязными слоями: 32 * 25 * 64 (далее полносвязный слой на 6300).\n",
      "\n",
      "Мое видение реализации сетки из статьи: <http://pastebin.com/KnDxsdaZ>\n",
      "----------------------------------------\n",
      "gsoul: второй конв слой  разве не model.add(Convolution2D(64, 3, 3, activation='relu')) должен быть вместо 32?\n",
      "И что значит не работает, оверфит? Если оверфит, возможно делаешь слишком много итераций обучения. Выдели validation set, смотри как там ведет себя лосс в зависимости от итерации и останавливай обучение когда там начинается рост. <https://en.wikipedia.org/wiki/Early_stopping#Validation-based_early_stopping>\n",
      "----------------------------------------\n",
      "а train accuracy? если train не меняется, скорее всего что-то с параметрами оптимизатора (learning rate слишком большой или слишком маленький итд). Попробуй для простоты выкинуть пока dropout и добиться оверфита тупо на трейне. Увидишь какие параметры оптимизатора работают, сможешь плясать от них потом уже с дропаутом.\n",
      "----------------------------------------\n",
      "1 конечно уже много кажется, я бы попробовал и уменьшить тоже. Но чисто для отладки можно попробовать задрать абсурдно высоко и убедиться, что train accuracy хоть как-то меняется, необязательно в нужную нам сторону :) Если не получится, мб что-то с самой процедурой обучения, параметры не обновляются. Или с инициализацией проблемы (хотя в керасе conv слои вроде инициализируются по умолчанию как-то разумно).\n",
      "\n",
      "Еще чтобы проблемы с сетапом исключить, можно просто сделать 1 полносвязный слой картинка-&gt;софтмакс и его обучить. Когда там будет разумный результат, вернуться к нужной архитектуре.\n",
      "----------------------------------------\n",
      "Ну и кстати Адам кмк не любит большие дропауты (т.к. шум градиента от дропаута увеличивает знаменатель в размере шага и шаги затухают быстрее чем надо). мб просто Нестеров, как в статье, будет надежнее работать.\n",
      "----------------------------------------\n",
      "Навскидку - задача прямолинейная и можно работать в лоб. \n",
      "\n",
      "29 классов и  1.9млн фото - это очень хорошо, причем даже если данные средней паршивости. (Бывает хуже. 10 классов и 25 картинок, как в задаче про спутники). Плюс, как я понял, тебе не надо решать задачу локализации ( где именно какие коробки), не надо считать сколько коробок каждого класса. А просто сказать - есть данный brand или нет.\n",
      "\n",
      "=&gt;\n",
      "\n",
      "Дергаешь pre-trained resnet, перебиваешь последний слой на 29 классов вместо 1000, вешаешь на него sigmoid и понеслось. К вечеру будет работающий прототип. Ну а дальше визуальная оценка предсказаний, подкрутка, эвристики и прочая магия, чтобы это отправить в production и оно там таки заработало как требуется.\n",
      "----------------------------------------\n",
      "я прост как раз сейчас понемногу считаю количество коробок конкретного вида на фотках, вроде классический цв норм подходит для поиска лого\n",
      "----------------------------------------\n",
      "Как написали в соседнем чатике про проблемы производительности тензорфлоу: \"Ну, да, бывает есть небольшая просадка, когда серверов параметров становится за тридцать\" :troll:\n",
      "----------------------------------------\n",
      "а на 2х гпу ты как считаешь?\n",
      "----------------------------------------\n",
      "Я пока на двух GPU не считаю. Но думал прикупить.\n",
      "\n",
      "Если TF на 40 процентов медленнее TH на одной GPU, то на двух у них как раз и будет паритет.\n",
      "----------------------------------------\n",
      "Мб дело в том, как именно на 2 карты раскидывается работа?\n",
      "----------------------------------------\n",
      "<https://arxiv.org/abs/1701.08734>, для тех кто хочет саму статью\n",
      "----------------------------------------\n",
      "Интересно, когда они следующее поколение выкатят\n",
      "----------------------------------------\n",
      "Все, кто хотел -- вполне могли себе купить в Штатах :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Скорее да, чем нет. Я постоянно использую. Правда не в режиме lr = f(n_epoch), а как вышли на плато так lr /= 10 Хотя и первый вариант можно будет попробовать.\n",
      "----------------------------------------\n",
      "Как минимум хуже стать не должно.\n",
      "----------------------------------------\n",
      "Я там прислал ссылку, где обсуждается вариант делать anneal когда validation перестаёт уменьшаться \n",
      "----------------------------------------\n",
      "<@U1BAKQH2M> Спасибо за инфу про reduce lr on plateau ) Еще вопрос - как вообще узнавать про новые фичи кераса при выходе новых релизов? Релиз-ноутсы fchollet не пишет, описания коммитов тоже неинформативны, гугл-груп на эту тему пустой и вообще... Я не умею искать? Или единственный путь - заново прочитывать доку и искать отличия?\n",
      "----------------------------------------\n",
      "Шутка, наоборот они хотят сделать качественное описание, как я понял\n",
      "----------------------------------------\n",
      "То есть, условно, надо интересоваться тем, как люди learning rate менеджат, а не фичами кераса\n",
      "----------------------------------------\n",
      "(смущенно ковыряет пальчиком) ну да, есть такое... Не всегда можно точно сформулировать одну проблему, которую надо решить с фреймворком. Например только прочитав новую доку я нашел, что в 1.2 ввели (наконец-то) precision/recall как метрику качества, сейчас вот про адаптив lr, ну и т.д. То есть вводят новые фичи, которые зело полезны, но тебе не приходит в голову их искать, потому что уже привык обходиться своими костылями. Релиз-ноутсы решили бы эту проблему полностью, не понимаю, почему fchollet забивает на это.\n",
      "----------------------------------------\n",
      "Привет! Есть у кого более-менее значимые результаты в классификации Музыки? Интересует определение жанра/инструментов/настроения и темпа композиций.\n",
      "----------------------------------------\n",
      "Ребят. Пытаюсь разбираться с tensorflow.contrib.learn и вот такой вопрос возник. Я так понимаю они намекают что бы я старался пользоваться input_fn для fit'a. Все бы круто а как организовывать минибатчи? Есть у кого-то пример как это вообще должно выглядеть? \n",
      "\n",
      "То есть если я не использую input_fn я передаю x, y, batch_size и все работает. Но вот если используется функция то тут у меня голова сломалась )\n",
      "----------------------------------------\n",
      "alex.ozerin: Не понимаю, почему это происходит. Ошибка возникает  всего-навсего при вызове метода predict, и вне функции обработки сообщений все отлично. \n",
      "----------------------------------------\n",
      "первый результат в гугле (<https://groups.google.com/forum/#!topic/keras-users/3NSKYn4FnVU>) говорит что это действительно может быть доступ из нескольких потоков, бороться с этим можно тем что обращаться с моделью в одном потоке - как конкретно уже зависит от фреймфорка где все крутится\n",
      "----------------------------------------\n",
      "Всем спасибо. Вроде получилось, когда сделал свою функцию для получения сообщений безо всяких там многопоточностей\n",
      "----------------------------------------\n",
      "А какая версия драйвера у тебя?\n",
      "----------------------------------------\n",
      "теперь, как я понимаю, автор работает в facebook и активно пилит с коллегами\n",
      "----------------------------------------\n",
      "в theano '0.8.2' были замечены странные баги:eww:, исправляется апгрейдом. Баг в том, что на минибатчах не было сходимости, хз где они именно\n",
      "----------------------------------------\n",
      "господа, если кто не в курсе, то у нас теперь есть <#C486WV5TR|ods_habr> и адепт <@U041P485A> пишет пост про :theano:, может среди нас найдется несколько человек которые после этого поста напишут аналогичные посты про :tf:, :mxnet:, :torch:, :caffe: ? если есть такие добровольцы то го в <#C486WV5TR|ods_habr>\n",
      "----------------------------------------\n",
      "А кто-нибудь работал с автоэнкодерами sequence to sequence или VAE, которые должны генерить последовательности? Я вот сейчас в блоге кераса натолкнулся вот на такой кусок кода:\n",
      "```\n",
      "inputs = Input(shape=(timesteps, input_dim))\n",
      "encoded = LSTM(latent_dim)(inputs)\n",
      "\n",
      "decoded = RepeatVector(timesteps)(encoded)\n",
      "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
      "\n",
      "sequence_autoencoder = Model(inputs, decoded)\n",
      "encoder = Model(inputs, encoded)```\n",
      "также видел примеры кода, когда примерно такую же архитектуру использовали для декодера в variational autoencoder.\n",
      "Мне показалось немного странным, что для декодера на каждом таймстепе используют один и тот же вход.   Это стандартная практика? Кажется, что на вход лучше бы подавать выход с предыдущего таймстепа или использовать teacher forcing (подавать на вход исходную последовательность). Кто нибудь может поделиться своим опытом?\n",
      "----------------------------------------\n",
      "Да просто тупо стенограмму доклада на непрофильной конфе выкладывать как статью\n",
      "----------------------------------------\n",
      "sim0nsays: Предъява не по сути, но у нее русский, как у меня английский - короткие рубленные предложения.\n",
      "----------------------------------------\n",
      "<@U34Q3KU8H>  у меня нет аккаунта на хабре, поэтому отвечу на твой комментарий здесь. В MegaFace ты можешь поучаствовать - он бессрочно продлен. В dsb3 я, лично, планирую участвовать как и в предыдущих двух. На спутники, увы не успел вписаться.\n",
      "----------------------------------------\n",
      "Спасибо. Это я комментарий написал скорее как провокацию, чтобы серьезный народ влился и в процессе рубки и после окончания, по обсуждениям, можно было бы много идей получить.\n",
      "----------------------------------------\n",
      "И еще вопрос про Keras. Пусть я хочу склеить несколько слоев в один, так чтобы было удобно сеть строить.\n",
      "\n",
      "Например, у меня в Unet много блоков вида:\n",
      "\n",
      "```\n",
      "conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(inputs)\n",
      "conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
      "conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
      "conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv1)\n",
      "conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
      "conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
      "```\n",
      "\n",
      "Как определить функцию или класс, чтобы работало как:\n",
      "```\n",
      "x = sandwich_layer(parameters) (input)\n",
      "```\n",
      "?\n",
      "----------------------------------------\n",
      "Следующий вопрос, как в Keras в цикле вот такую штуку сделать с произвольным числом слоев? <https://i.imgur.com/MMqosoj.png>\n",
      "\n",
      "Если бы не было shortcuts, я бы просто в цикле использовал functional api, но вот как это с ними сделать?\n",
      "----------------------------------------\n",
      "скоро, наверное, уже GUI сделают, где просто накидал слои, мышкой соединил, параметры подкрутил и пошло\n",
      "----------------------------------------\n",
      "Вот такой вопрос, к тому, кто сегментацией изображений занимался: “Правда ли что train / val loss ведут себя более шумно, чем при задачах классификации?\"\n",
      "----------------------------------------\n",
      "Сопутствующий вопрос, а как бы это сгладить? \n",
      "\n",
      "Может под сегментацию какие-то специальные оптимизаторы, которые не стандартно используемый Adam. \n",
      "\n",
      "Или может Momentum увеличить / уменьшить? \n",
      "\n",
      "Или слабую L2 регуляризацию влепить везде?\n",
      "\n",
      "Или разлениться и прикрутить Polyak Avregaing.\n",
      "\n",
      "В общем что у кого получилось для сглаживания кривой обучения. А то скачет как не родная и непонятно, плато, или нет или вообще что происходит.\n",
      "----------------------------------------\n",
      "Скажите как принято в вашем этом диплернинге: начал тренировать модель, сразу и валидируюсь (разделил 0.8 и 0.2). Так вот вопрос: мне нужно выбрать лучшую эпоху и перетренировать на всем датасете? (как и в классике) или тут свой подход?\n",
      "----------------------------------------\n",
      "Почему такой вопрос возник, потому что некоторые модели могут тренироваться днями-неделями, нужно и валидироваться сразу. Но вряд ли будут ждать еще неделю, что бы заново натренировать модель\n",
      "----------------------------------------\n",
      "Особенно потому что есть например техники, где гиперпараметры меняются в зависимости от значения лосса на валидации\n",
      "----------------------------------------\n",
      "мое ```[0.72019052723325172,\n",
      "  0.76180440504036739,\n",
      "  0.7832143553318005,\n",
      "  0.80174934216937921,\n",
      "  0.81397402787833595,\n",
      "  0.82630835201247443,\n",
      "  0.8369554624363712,\n",
      "  0.85023998635029674,\n",
      "  0.85928515739787592,\n",
      "  0.86805014130595415,\n",
      "  0.87723540589423943,\n",
      "  0.88564101938220352,\n",
      "  0.89396744955470131,\n",
      "  0.90086858006042292,\n",
      "  0.90731897476460432,\n",
      "  0.91011475490880123,\n",
      "  0.91721688918046884]``` и ```'val_acc': [0.73189747585630849,\n",
      "  0.77207387197135013,\n",
      "  0.78744761722615997,\n",
      "  0.7970714355674523,\n",
      "  0.79980021443890725,\n",
      "  0.80082350642276123,\n",
      "  0.80196861904297823,\n",
      "  0.79643796897401542,\n",
      "  0.80377156225494817,\n",
      "  0.79914238375421276,\n",
      "  0.80430757233232397,\n",
      "  0.80308936754870208,\n",
      "  0.80155442942769983,\n",
      "  0.80386901859093474,\n",
      "  0.80133515249601217,\n",
      "  0.80062859367527306,\n",
      "  0.79972712214189878]``` я бы эпох 7 гонял, если бы это был xgboost. То есть лучшей эпохой я считаю 6, но так как к-во данных выростет, то лучше увеличить время тренировки\n",
      "----------------------------------------\n",
      "sim0nsays: это ты про уменьшение lr , когда достигается плато?\n",
      "----------------------------------------\n",
      "Еще нубский вопрос: тренирую бинарную классификацию, метрика accuracy. Ориентиироваться лучше на val_loss или val_acc? у меня такая штука получилась, что когда модель выходит на плато, то acc – прыгает туда-сюда, а loss начинает рости. Значит лучше выбирать эпоху с меньшим loss?\n",
      "----------------------------------------\n",
      "Тогда у меня вопрос как ты его слушал)\n",
      "----------------------------------------\n",
      "или переставать учить в момент, когда начинает расти :idk:\n",
      "----------------------------------------\n",
      "хочу под ubuntu 16.04 desktop заставить работать встроенную intel карточку для рендеринга gym. Сейчас никакой display manager не запущен, хожу только по ssh. `lightdb start` или `status` выдает `update-alternatives: error: no alternatives for x86_64-linux-gnu_gfxcore_conf`, `prime-select intel` ничего не меняет. драйвера переустанавливал меньше месяца назад из nvidia ppa. Подсажите в какую сторону копать? Как сказать lightdm забить на nvidia?\n",
      "----------------------------------------\n",
      "Мне один раз удалось сделать как чувак советовал. Но потом обновил убунту и все по пизде пошло.\n",
      "----------------------------------------\n",
      "ага! спасибо, для gym как раз то же советовали, но я все хотел обойтись без переустановки драйвера. тогда как спутники закончатся снова попробую\n",
      "----------------------------------------\n",
      "При уменьшении ЛР на плато, кто сколько эпох обычно выставляет patience и потом на какой коэффициент уменьшает?\n",
      "----------------------------------------\n",
      "это при каком размере датасета и сколько времени на 1 эпоху уходило?\n",
      "----------------------------------------\n",
      "А может кто-нибудь подсказать : у меня есть object detector с инпутом 300*300, проблема в том, что входное изображение может быть совершенно произвольным и, часто, сильно больше, например 3000*1900. Однако, если я просто делаю ресайз, то качество распознавания сильно падает, так как есть много объектов, которые становятся слишком маленькими. Вопрос - есть ли какой-нибудь открытый фрэймворк, который бы оптимизировано бегал по всему фото и где-то делал ресайз, а где-то нет в зависимости от уровня достоверности распознанных объектов. То-есть и объекты по максимому находил и ресурсов много не тратил.\n",
      "----------------------------------------\n",
      "Подскажите по keras: если я добавляю `EarlyStopping` с `patience=3` и хочу сохранить лучшую модель, то мне нужно делать `ModelCheckpoint` и забрать последние сохраненные веса или модель по умолчанию “откатится” до лучшего loss, как в xgboost?\n",
      "----------------------------------------\n",
      "какая будет примерно закономерность на тесте?  как выбирать критерий остановки при тренировке?\n",
      "----------------------------------------\n",
      "С какими оптимизаторами нужно юзать в keras `ReduceLROnPlateau` и `LearningRateScheduler` для улучшения перформанса? Расскажите про свои хаки\n",
      "----------------------------------------\n",
      "какой метод из keras лучше использовать для уменьшения lr?\n",
      "----------------------------------------\n",
      "Когда нужно было обучить 100500 моделей, то я написал свой велосипед. Типа учил N эпох с запасов при фиксированом lr для SGD или фиксированном batch size для Adam. Потом выбирал лучшую эпоху, уменьшал LR или увеличивал батч сайз и стартовал обучение с нее.\n",
      "----------------------------------------\n",
      "Вот как раз для сохранения лучших весов юзал каллбэк специальный.\n",
      "----------------------------------------\n",
      "Когда учил lstm и cnn для текста. Сейчас не учу :mk_kapitan:\n",
      "----------------------------------------\n",
      "а хочется точность такую же как на цветных?\n",
      "----------------------------------------\n",
      "&gt;а хочется точность такую же как на цветных?\n",
      "Хочется гуглнет со сверточными слоями построенными на нормальном датасете, что потом свой датасет прокачивать.\n",
      "----------------------------------------\n",
      "Да. Так как в моём датасете есть ошибки т.к. лейблы не всегда правильно проставлены\n",
      "----------------------------------------\n",
      "А нельзя взять pte-trained веса googlenet и поменять веса только первых сверток, чтобы как раз конвертнуть для greyscale?\n",
      "----------------------------------------\n",
      "lr = 0.000983722 есть еще куда стремится :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "А как лучше пихать спарс матрицы в сеть? keras, вроде как не умеет на вход принимать такие.\n",
      "----------------------------------------\n",
      "Keras, сегментация. Выход сети (C, N, M), где C - число классов.\n",
      "\n",
      "Как прикрутить softmax?\n",
      "----------------------------------------\n",
      "Просто посмотри какой тензор прилетит в жаккард_коеф и поправь, чтобы по другим осям считалось всё, что там считается\n",
      "----------------------------------------\n",
      "Или я совсем не понимаю как это работает, или откуда сети это знать, если у тебя лосс считается усреднением всех классов, да ещё и по целому мини-батчу?\n",
      "----------------------------------------\n",
      "Вопрос обывателя. Как к такой архитектуре можно прийти? принять что-то? Сходу не сказать что есть какая-то логика.\n",
      "<https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAoQAAAAJDMyYzgyYzRhLTg1NjUtNDg1Yy1hMjY0LTJjZWFiMzJkOTk1NQ.png>\n",
      "<https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur>\n",
      "----------------------------------------\n",
      "Можно прийти к такой архитектуре побившись головой об стол, когда нихера больше не работает :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "почему же нет — разные эмбеддинги пар вопросов, потом всё в кучу и полносвязная сетка с BN, DO и PReLU\n",
      "----------------------------------------\n",
      "вот когда на inception смотришь, возникают вопросы о том, сколько машиночасов сожгли на перебор параметров сетки\n",
      "----------------------------------------\n",
      "Чуть менее тривиальная вещь -- поглядеть каких размеров получился замердженный вектор и сделать на нем большой дропаут\n",
      "----------------------------------------\n",
      "Нет, нельзя. свертки только там, где локальный порядок есть. С чего вдруг ему быть при мердже какой-то фигни?\n",
      "----------------------------------------\n",
      "сверток по фичам - это как? там же везде эмбеддинги, как сворачивать \n",
      "----------------------------------------\n",
      "Можете подсказать как мне в керасе проинициализировать сеть так, чтобы веса были одинаковые для tf и theano. Я задаю `np.random.seed(0)` до импорта кераса и получаю одинаковые веса, если несколько раз запускаю теано бекенд, и одинаковые веса если несколько раз запускаю tensorflow бекенд, но вот веса теано и тензорфоу отличаются. Пока проверяю на обычной dense сети\n",
      "----------------------------------------\n",
      "но как статистика оптимизатора влияет на предсказание?\n",
      "----------------------------------------\n",
      "у меня кстати в керасе были какие-то проблемы со статистиками оптимизатора как раз, криво подгружалась модель из дампа, приходилось выбивать вручную из дампа его поля\n",
      "----------------------------------------\n",
      "когда я читал документацию по керасу там было написано, что вся статистика оптимизатора сохраняется\n",
      "----------------------------------------\n",
      "есть кстати мысль что имеет смысл делать файнтюн другим оптимизатором, помню у <@U053R9RS6> что-то давал  файнтюнинг сгд поверх рмспропа, сгд как раз статистики не нужны :good-enough:\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> набросаешь оглавление, как самый стар^Wопытный?\n",
      "----------------------------------------\n",
      "вообще говоря, статья окончательно ставит точку в вопросе кто был первым\n",
      "----------------------------------------\n",
      "dremovd: а твой вариант чем хуже? забыл уже как он назывался\n",
      "----------------------------------------\n",
      "подскажите плз статьи по детекции end-to-end, где нет последним шагом никакого локального декодинга вроде супрессии. RL тоже подойдет.\n",
      "----------------------------------------\n",
      "не, я не про это. А про то, как теоретические наработки выше перечисленных ребят повлияли на DL, хоть это и не отражается в “Deep Learning” и обзорах подобных “On the origin of DL\"\n",
      "----------------------------------------\n",
      "Всем привет попрос по theano на cpu. Я сейчас запускаю разные архитектуры сетей на cpu в 2 вариантах:\n",
      "1) ограничиваю количество потоков через `OMP_NUM_THREADS=1`\n",
      "2) никак не ограничиваю количество потоков\n",
      "\n",
      "Получаю странные результаты, если я ограничеваю количество потоков до 1, то результаты сильно лучше, чем если совсем не ограничиваю.\n",
      "Когда запускаю htop или top с 1 потоком, то вижу утилизацию в 100% 1 потока. Когда запускаю без ограничений, то top показывает загрузку нескольких потоков, но суммарная загрузка процессора 25%.\n",
      "А htop показывает, что запускается 8 потоков, но много красных полосок.\n",
      "Я подозреваю, что как-то не так настроил теано, но не могу понять как правильно. В чем может быть дело?\n",
      "\n",
      "Ниже на всякий случай прикрепляю графики (дополнительно прикрепляю графики для tensorflow и для него все ок).\n",
      "Все делал в керасе меняя backend (dim ordering в явном виде задавал керасу перед построением сетей).\n",
      "В основном архитектуры которые в rl можно использовать.\n",
      "CPU - Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz - может работать в 8 потоков.\n",
      "версии: Theano==0.9.0.dev5; tensorflow-gpu==0.12.1\n",
      "----------------------------------------\n",
      "<@U24QAAZBL>  ясно, спасибо, я так понимаю не нашел способа как справиться с этим?\n",
      "----------------------------------------\n",
      "Привет, \n",
      "Появился шанс поехать на стажировку в универ Германии. Надо быстро разобратся с `liquid state machines` и `echo state network`(`RNN`)\n",
      "\n",
      "Я не давно начал разбиратся с ML, пока на уровне классических методов. План для себя накидал, как вьехать в эту тематику:\n",
      "\n",
      "- выборочные лекции с cs231n\n",
      "- курить deep learning book, чтобы прояснить узкие моменты\n",
      "- покурить echo state network tutorial\n",
      "\n",
      "Но пока запара с практикой.\n",
      "\n",
      "Посоветуйте куда копать на Python какие фреймворки учить?(TF?) И есть ли набор практических задач, чтобы освоить эти типы сетей? Или пытаться прикрутить к любой задаче классификации?\n",
      "----------------------------------------\n",
      "а где ESN/LSM используются - они разрабатываются как инструмент моделирования чего-то более похожего на мозг, или как методы для решения практических задач?\n",
      "----------------------------------------\n",
      "В описании к стажировке, нужно будет моделировать молекулярные структуры как динамические системы. В часности пептиды и протеины.\n",
      "----------------------------------------\n",
      "вот только там анализ сложный, и вообще ее заставить работать как надо проблематично, если данные сложнее игрушечных\n",
      "----------------------------------------\n",
      "Да, буду детально копать RNN. Я так понял хорошей идеей, будет все пытаться сделать на numpy/scipy, чтобы понять как они работают\n",
      "Спасибо, за papers. \n",
      "\n",
      "Кстати, есть мысли по поводу `reservoir computing`?\n",
      "Нашел сайт их <http://reservoir-computing.org> и как я понял, это такая deep RNN в которой мы обучаем только верхние слои?\n",
      "Буду благодарен за любую инфу.\n",
      "----------------------------------------\n",
      "&gt; судя по тому что сеть вообще не учится, я голосую за баги.\n",
      "если бы баги в препроцессинге были, то почему моя кастомная учится. Если баг в структуре - то тоже не совсем понятно, т.к. код предельно прост.\n",
      "\n",
      "Размер свертки - попробую поподбирать, спасибо. Дейтсвительно явным образом он там не указан, а (4 * 4) - это макс.пулинг\n",
      "----------------------------------------\n",
      "А таргеты в каком формате подаются сети?\n",
      "----------------------------------------\n",
      "День добрый. Я только начинаю реализовывать на питоне нейросети (направление - рекуррентные, автоэнкодеры). Мне нужно определиться на чем работать. Из понятного мне: (1) keras, (2)lasagne, (3)Theano и (4)TensorFlow. Подскажите, исходя из практики, какую связку фронт-бэкэнд лучше всего использовать? Почитал, что пишут, но однозначный вывод сделать не смог.\n",
      "Спасибо\n",
      "----------------------------------------\n",
      "но все же не 90круб, как за титан х\n",
      "----------------------------------------\n",
      "Чот дорого на нвидии.ру, 30% сверху\n",
      "----------------------------------------\n",
      "Как я понял, керас дает выбор бэкэнда, лзанья, как и говорили - только теано. На простых задачах я пробовал - переключается легко и это прельщает, один код. А вот что на сложных - вопрос. С другой стороны для теано есть предобученные модели (zoo).  Опять же, какой бэкенд быстрее  на GPU...\n",
      "----------------------------------------\n",
      "Насчет TH vs TF  <https://github.com/wjaskowski/tensorflow-vs-theano-benchmark> \n",
      "TLDR:   TF (cpu) быстрее TH(cpu)  в ~ 2-4  раза.  А TF(gpu)  медленнее  TH(gpu) в ~ 1.5-2.5 раза.  \n",
      "Интересено почему TF(gpu) так много медленнее TH(gpu)  ?\n",
      "----------------------------------------\n",
      "ipaulo: wtf, они в theano backprop меряют как forward + backward\n",
      "----------------------------------------\n",
      "Как жаль, что чейнером никто не пользуется почти\n",
      "----------------------------------------\n",
      "и как то это крайне подозрительно\n",
      "----------------------------------------\n",
      "если кому интересно, то вот тут  <https://github.com/fgvbrt/dl_rl_betchmarks> я правда в results.md вынес только gpu и cpu с однимо потоком, так как это типа для целей reinforcement learning больше подходит. Но в файлике results.txt лежит и для cpu на всех возможных потоках.\n",
      "----------------------------------------\n",
      "Чуваки, читаю статью <http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf> . Автор проводил кучу эксперементов с lstm и gru. Интересный момент с неиспользованием каких-то гейтов или созданием новой связи между ними. Если правильно расшарил документацию keras, то на нем нет никакой гибкости. В какой фреймворк нужно сунуться, что бы такое провернуть?\n",
      "----------------------------------------\n",
      "vradchenko: почему в keras нет гибкости?\n",
      "можно ж собственные слои писать :kekeke:\n",
      "----------------------------------------\n",
      "это похоже на кастомный слой, вот тут есть пример, как строят сеть с помощью кераса, потом из этой сети достают входной и выходной тензоры,  и пишут свой кастомный loss для rl агента <https://github.com/coreylynch/async-rl>\n",
      "----------------------------------------\n",
      "Накодил небольшую библиотеку на theano, забавы ради, можно сказать. Если я запускаю код в jupyter notebook единым блоком, всё работает отлично. Если запускаю тот же код кусок за куском, кернел падает без какого-либо трейсбэка. Если запускать через командную строку файл ноутбука, переконверченный в .py, всё работает нормально, но когда-то умирало с ошибкой floating point exception: 8, тоже без трейсбэка. Как эту жуть дебажить?:sweat_smile: до этого библиотека была на tf и работала исправно, в коде никаких очевидных ошибок нет\n",
      "----------------------------------------\n",
      "npetrenko97: у меня как то теана падала и валила за собой ядро из за версии scipy\n",
      "----------------------------------------\n",
      "не могу что-то загуглить. не подскажешь как это реализовать?\n",
      "----------------------------------------\n",
      "Вместо второй части про json можно просто использовать код создания модели, такой же как при обучении\n",
      "----------------------------------------\n",
      "хм. то есть когда я сделаю лоад, он загрузит в оперативку и по дефолту будет использовать CPU при операциях с моделью?\n",
      "я до этого использовал save_model / load_model и он загружал сразу на видео-карту модель\n",
      "----------------------------------------\n",
      "А, думал вопрос в том, как загрузить, а не в том, как выбрать цпу или гпу :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Может кто-то писал скрипт, который обрабатывает логи кераса и переводит в табличный вид? или скажите как залезть внутрь, что бы вытащить несколько чисел\n",
      "----------------------------------------\n",
      "Коллеги, приветствую!\n",
      "Решил начать знакомство с нейросетями с конкретной задачи: определения того, есть на фоторграфии номерной знак автомобиля.\n",
      "Нашел статью на хабре про keras <https://habrahabr.ru/post/321834/> где рассказыватся, как сделать transfer learning.\n",
      "Собрал выборки, дообучил модель, как показано в статье. Теперь я беру, загружаю модели из snapshot'ов, которые были созданы во время обучения и вызываю метод predict на двух классах обучающей выборки. И там, что в первом классе, что во втором подавляющее число предсказаний -- нулевые. При этом точность при обучении была 0.7. Классы сбаланстированы: половина выборки изх первого, половина из второго классов.\n",
      "Обучается сетка вот таким кодом: <https://github.com/MaxTitkov/Keras_InceptionV3_Binary_classification/blob/master/complete_model.py>\n",
      "\n",
      "Вопрос: если взглянуть на репозиторий из статьи: есть ли там особенности того, как следует загружать обученную модель из snapshot`ов? Ведь автор там над keras inception дообуча два полносвяных слоя прежде чем подать на логистическую регрессию.\n",
      "----------------------------------------\n",
      "Так эта. До того как сохранил в snapshot, результаты нормальные?\n",
      "----------------------------------------\n",
      "понадобилось немного с картинками поработать, как в ImageDataGenerator в керасе пихнуть препроцессинг с вычитанием константы из каждого канала?\n",
      "----------------------------------------\n",
      "пока кажется что немного никак и надо свой генератор писать (ужас то какой :scream: )\n",
      "----------------------------------------\n",
      "anyone? репощу из <#C1UEK73H6|mltrainings_beginners>, не уверен где спросить\n",
      "\n",
      "а в tf в rmsprop если указать моментум значение, оно будет делать нестеров моментум или что вообще происходить будет?\n",
      "<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/rmsprop.py#L25>\n",
      "mean_square = decay * mean_square{t-1} + (1-decay) * gradient ** 2\n",
      "mom = momentum * mom{t-1} + learning_rate * g_t / sqrt(mean_square + epsilon)\n",
      "delta = - mom\n",
      "и что такое g_t?\n",
      "\n",
      "Правильно ли я понимаю моментум нестерова\n",
      "сперва прыгаем на mom потом считаем градиент в новой точке и уже его добавляем к моментуму, и если подключать рмспроп то нужно делить этот добавляемый градиент на sqrt(m_s)?\n",
      "а то я как то раньше не задумывался и предполагал что если я при создании рмспроп оптимайзера напишу momentum=0.9 у меня магическим образом будет моментум нестерова одновременно с рмспроп\n",
      "----------------------------------------\n",
      "и еще такой вопрос.\n",
      "в LSTM оригинальной статье как я понял(читал два года назад) нет ограничения по количеству айтемов в последовательности и там используется truncated grdaient а в tf есть ограничение в айтемах\n",
      "это так? почему если так?\n",
      "----------------------------------------\n",
      "Люди, не подскажите кроме aws, какие еще серваки есть, идеальный вариант:\n",
      "- сервер с gpu\n",
      "- linux OS\n",
      "- доступ через ssh\n",
      "- c установленным cuda\n",
      "- оплата часовая (обучим сеть, нам этот сервак больше не нужен)\n",
      "----------------------------------------\n",
      "А знает кто-нибудь как на hetzer смотреть  gpu на аукционе ?\n",
      "----------------------------------------\n",
      "<@U06TZHSSJ> Ну как же, для: \"делать не только  предсказания (классификация / регрессия), но и  probabilistic reasoning about uncertainty\"\n",
      "----------------------------------------\n",
      "хз как на 4 шагах себя лстм может вести\n",
      "----------------------------------------\n",
      "<@U1SJ2U4V9> Можешь обриcовать как GAN вели себя в разрезе спутников, то есть для сегментации спутниковых карт по сравнению с сегментацией в лоб используя Unet?\n",
      "\n",
      "P.S. Меня на той неделе в Decartes Labs на интервью спрашивали как и что я могу рассказать за GAN и как они могут это использовать. Ничего на этот вопрос ответить не смог :disappointed:\n",
      "----------------------------------------\n",
      "У меня там ссылка и инвайт на Кружочки. Если придумаем как совместить - я готов!\n",
      "----------------------------------------\n",
      "Может, мы избалованы собственными лонгридами, но статья короткой показалась. И многовато англицизмов влезает туда, где можно русские фразы задействовать. Даешь импортозамещение!\n",
      "----------------------------------------\n",
      "Какие англицизмы больше всего режут глаз? Может поправлю\n",
      "----------------------------------------\n",
      "Какие есть практики работы с пропущенными значениями в нейросетках? У меня они ща как 999999 закодированы и это наверно ппц)\n",
      "----------------------------------------\n",
      "Думал попробовать нечто вроде замены их на свободные переменные и зафитить их. Как лучшее решение для nan в хгбуст, только лучший дефолтный инпут в сетке \n",
      "----------------------------------------\n",
      "Кто к курсе, можете прояснить ситуацию с Керас? Как там у них дела обстоят?\n",
      "То, что удалось выцепить: <https://github.com/fchollet/keras/issues/5299> обозначил, что Keras теперь - это не либа, а набор спецификаций API. Этот набор будет имплементироваться в Keras-2 (новая инкарнация либы) и отдельно в tf.contrib.keras (-&gt;tf.keras). keras-contrib, вроде как, будет содержать какие-то куски типа лосс функций, метрик, слоёв,  и т.д. которые недостаточно хороши для Keras-2 (которая, как бы, будет LTS), но и не полный мусор (потому что какие-то типы уже назвались мэинтейнерами и довольно критично всё ревьюят).\n",
      "Далее, вроде бы совместимость с TF 1.0 есть (<https://github.com/fchollet/keras/pull/5317>, <https://github.com/fchollet/keras/issues/5623> и т.д.), но модели в <https://github.com/fchollet/keras/tree/keras-2/keras/applications> используют старые ссылки на веса. Кто-н их тестил, они вообще работают теперь?\n",
      "В продолжение предыдущего вопроса, pre-trained модели теперь можно где-н найти рабочие? <https://github.com/tensorflow/models>, судя по issues&amp;PRs сильно отстают от движухи, и с TF 1.0 работают всего несколько штук. С <https://github.com/fchollet/deep-learning-models> ситуация вроде бы как с <https://github.com/fchollet/keras/tree/keras-2/keras/applications>.\n",
      "Последний вопрос: какой бранч Кераса теперь хостится в PyPi? Keras-1? Keras-2?\n",
      "----------------------------------------\n",
      "&gt; Keras-2 (которая, как бы, будет LTS)\n",
      ":yeah-sure:\n",
      "----------------------------------------\n",
      "<@U1UNFRQ1K> а это вы какими лонгридами избалованы? дайте почитать!\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC> сейчас неохота уже все перечитывать, но как примеры: ”чтобы человеку не нужно было придумывать loss function”, “если использовать как loss просто… “,  “вход дается input image”, “В пейпере люди” (жаргон)\n",
      "----------------------------------------\n",
      "Я вот пишу раз в год, вот где проблема :(\n",
      "----------------------------------------\n",
      "Это выглядит пока как нездоровое бурление. Думаю, стоит еще немного подождать. Сам пока юзаю Keras 1.2.2 с Theano бэкэндом, потому что быстрее.\n",
      "----------------------------------------\n",
      "У Хинтона есть отличная статья про то как строить и тренить RBM. В 17 разделе про пропущенные значения как раз. Ссылка на статью: <http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf>\n",
      "----------------------------------------\n",
      "Чем он лучше предыдущих? Быстрее тренируется? Достигает лучшего результата? Как это в цифрах показать?\n",
      "----------------------------------------\n",
      "На небольших датасетах - Cifar10/Cifar100/SHVN показывает одни из лучших результатов. Тут есть сравнительная таблица если кому интересно - <https://arxiv.org/pdf/1608.06993.pdf#5>\n",
      "Для ImageNet - достигает такой же точности как и ResNet но с меньшим количеством параметров - <https://arxiv.org/pdf/1608.06993.pdf#6> - figure 3.\n",
      "----------------------------------------\n",
      "Сейчас попробую 0.11 поставить, как на машине, на которой обучалось.\n",
      "----------------------------------------\n",
      "Поставил версию 0.11 (такую же как на машине, на которой обучался). Результат не изменился. Только я ставил версию без GPU (на ноуте старый GPU еще и с optimus).\n",
      "----------------------------------------\n",
      "Всем спасибо! Можно расходится. Виновником оказался keras, были разные версии. Поставил версию как при обучении и все работает! :tada:\n",
      "----------------------------------------\n",
      "чорт, я тут посмотрел в гугл школяре сколько ссылок на статью алекснет (ImageNet Classification with Deep Convolutional Neural Networks), и как бы был слегка удивлен, попробуйте угадать перед тем как идти смотреть\n",
      "----------------------------------------\n",
      "<@U06J1LG1M> ее первым делом все цитируют, когда начинают в интро \"recently, deep learning models demonstrated state of the art performance on many tasks\"\n",
      "----------------------------------------\n",
      "Всем привет!\n",
      "У меня возник такой вопрос:\n",
      "Если мы пременяем L2 регуляризацию к обычной модели логистической регрессии это тоже самое, с точки зрения лосс функции (максимального правдоподобия), что мы требуем чтобы веса модели, как случайные величины были распределены по нормальному закону.\n",
      "Если мы применяем L1, то тоже самое, но только мы говорим, что веса должны быть распределены по Лапласу. \n",
      "Эту логику я дальше переношу на всю сеть и не могу понять, почему в статьях на тему прунинга очень сильно восхищаются тем, что веса всяких VGG распределены по нормальному закону. Может быть я что-то не так понимаю или что-то упускаю?\n",
      "----------------------------------------\n",
      "obednikov.alex: это от куда такой вывод? То, что ты добавляешь в loss слагаемое, которое возрастает со значением весов линейно или квадратически ещё ничего не значит. Где тут предположения о распределении? Это больше про подход Баеса)\n",
      "----------------------------------------\n",
      "Да, тоже хотел это написать. Вообще за априорное распределение часто берут наиболее общее распределение, если нет особых догадок. При нормальном количестве данных апостериорное будет таким, каким должно быть:)\n",
      "----------------------------------------\n",
      "Чат, киньте в меня ссылкой или мыслями на тему плиз. Кто-нибудь пытался совместить SGD с динамическим обновлением весов примеров по типу adaboost?\n",
      "Мотивация такая: в задачах классификации, если датасет сильно несбалансированный, то классы, у которых гораздо больше примеров, задавливают общий лосс и на редких классах модель может сильно ошибаться. Это можно лечить колхозом с увеличением важности редких классов ручками (гемор, добавляет гиперпараметры). Либо делать что-то (не знаю точно как) в ключе GAN. С другой стороны, Adaboost должен автоматически отрегулировать веса примеров оптимальным образом. Тренить много сеток как weak learners при этом особо не надо, т.к. обычно capacity одной сети уже избыточна. Получается, что SGD на одной сетке с Adaboost весами должно работать лучше на классификации (игнорируя оверфит для простоты). Логично или что-то упускаю?\n",
      "----------------------------------------\n",
      "никто не знает как можно в lasagne сделать ExpressionLayer, у которого нелинейная функция - это другая нейросетка?\n",
      "или как-то можно в середину одной сетки встроить другую?\n",
      "----------------------------------------\n",
      "А какие есть датасеты с русской речью? Пригодные для тренировки speech-to-text.\n",
      "----------------------------------------\n",
      "&gt;Кино с сабтитрами?\n",
      "да кстати реальная тема, на одном митапе в мейле выступал проф из инрии (или ирния, ну кароче известный вуз во франции), он же в конторе visionlabs работает, и говорит что для мультимодального обучения текст+речь+видео именно кино с сабами, показывал демки где по текстовым запросам запросам ищутся фрагменты видео\n",
      "----------------------------------------\n",
      "Если серьезно, то давно хочу для рыбок сделать детектор. Ты для какой задачи этим занимаешься?\n",
      "----------------------------------------\n",
      "Кто нибудь в курсе почему в YOLO не пользуют ResNet?\n",
      "----------------------------------------\n",
      "<@U0GV3LMUY> архитектуры действительно хороши, но ResNet много где решает\n",
      "----------------------------------------\n",
      "да автор постоянно что-то меняет, как ему взбредёт. В серьёзных проектах лучше не использовать, только для тестов на коленке, иначе в следующем апдейте всё может работать по другому и в тихую выдавать не тот результат\n",
      "----------------------------------------\n",
      "Так вот почему Keras + TF работает медленнее чем чистый TF или Keras + Th...\n",
      "----------------------------------------\n",
      "Может, когда в TF таки полноценно возьмут, доведут\n",
      "----------------------------------------\n",
      "ipaulo: tf.contrib.layers - отличная вещь, как керас, только уже в тф\n",
      "----------------------------------------\n",
      "а почему вы таки спrашиваете?\n",
      "----------------------------------------\n",
      "Значит ты вы этом разбираешься.... :slightly_smiling_face:\n",
      "\n",
      "У какой архитектуры есть потенциал на задаче для рака легких?\n",
      "----------------------------------------\n",
      "Это вы как до такого lr догадались? Кстати `*` лучше, наверно, заменить на `\\times` \n",
      "```\n",
      "AdaM with learning rate of 27 ∗ 10−6\n",
      "```\n",
      "----------------------------------------\n",
      "И почему у вас бинарная классификация, а на выходе Softmax?\n",
      "----------------------------------------\n",
      "&gt; не опечатка ли\n",
      "опечатка, спасибо\n",
      "\n",
      "&gt; Это вы как до такого lr догадались?\n",
      "эту часть <@U0B4374S1> писал, он гонял тесты\n",
      "----------------------------------------\n",
      "Кто-нибудь пробовал Batch Renormalization <https://arxiv.org/pdf/1702.03275.pdf> или проблема маленьких батчей пока ни у кого не стоит?\n",
      "----------------------------------------\n",
      "Чат, а какие есть архитектуры сверточных сеток с приличным качеством на imagenet (==сравнимо с AlexNet хотя бы), но компактные и чтобы feedforward был быстрый?\n",
      "----------------------------------------\n",
      "Просто вычислительная сложность у squeezenet вообще чуть ли не ровно такая же, как у AlexNet.\n",
      "----------------------------------------\n",
      "Вообще, есть мало ресеча, когда люди стараются применить все современные методы к старым архитектурам, чтобы их оптимизировать\n",
      "----------------------------------------\n",
      "А есть где-нибудь почитать про то, как архитектуры оптимизируют?\n",
      "----------------------------------------\n",
      "sim0nsays: разве батчнорм не замедлит forward pass? Он же вроде как сходимость только ускоряет?\n",
      "----------------------------------------\n",
      "Может тогда вообще уйти от batch_norm <https://arxiv.org/abs/1603.01431> ? \n",
      "Интересно было бы узнать, кто на практике пробовал сравнивать normalization propagation vs batch norm.\n",
      "----------------------------------------\n",
      "Он, небось, batchnorm прикручивал, как Артур на спутниках пытался поначалу.\n",
      "----------------------------------------\n",
      "Динамических графов вычислений. Хотя хз как в ты с этим\n",
      "----------------------------------------\n",
      "а есть где почитать можно как это сделать? я сейчас запустил простую lstm на keras c tf\n",
      "----------------------------------------\n",
      "А как ты их кормишь сети?\n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> я правильно понял, что без разницы как пихать в lstm вектора?\n",
      "----------------------------------------\n",
      "а можно ссылочку на то, как без embeddings слоя пихать вектора? мне кажется, что где-то это есть уже написанное\n",
      "----------------------------------------\n",
      "можно LSTM как первый слой сети поставить, насколько я помню\n",
      "----------------------------------------\n",
      "&gt; как без embeddings слоя пихать вектора?\n",
      "заэнкодить еще в батч-генераторе. \n",
      "вместо [batch_size, seq_len] int32, передавать [batch_size, seq_len, dim_size] float32, а из сетки убрать embedding-слой.\n",
      "----------------------------------------\n",
      "И всем, кто побежал обновлять: там в паре мест сломалась обратная совместимость.\n",
      "Скажем теперь `y = np_utils.to_categorical(y, num_classes=666)`\n",
      "Раньше было `y = np_utils.to_categorical(y, nb_classes=666)`\n",
      "----------------------------------------\n",
      "Как и TH попросить преаллоцировать память и несколько ускорить вычисления\n",
      "----------------------------------------\n",
      "asobolev: и сидеть с лицом лягушки и подбирать бинарным поиском какую долю памяти под твою модель и батчайз можно аллоцировать :pepe_sad: \n",
      "----------------------------------------\n",
      "ramfs ещё как вариант, если в ram влезает\n",
      "----------------------------------------\n",
      "сами ворочайте этот ваш керас, зачем мне об него пачкаться :kekeke:\n",
      "----------------------------------------\n",
      "почему люди вообще этим пользуются?\n",
      "----------------------------------------\n",
      "каким местом нужно писать фреймворк, чтобы так получилось?\n",
      "----------------------------------------\n",
      "мама, как у меня пригорает от этого поделия\n",
      "----------------------------------------\n",
      "У mxnet'a кстати, все почти всегда хуево с доками. Мне приходится чужой код часто смотреть, чтобы понять как какую-нибудь штуку сделать\n",
      "----------------------------------------\n",
      "ну как минимум отвечает на багрепорты\n",
      "----------------------------------------\n",
      "ага но интегрируется всё в Keras легко, смотришь как у них то или иное реализовано и быстро своё допиливаешь удобно\n",
      "----------------------------------------\n",
      "pytorch ещё бета, как то не хочется тестировщиком выступать\n",
      "----------------------------------------\n",
      "на ворочанье dim ordering как минимум\n",
      "----------------------------------------\n",
      "Что-то жизнь меня не готовила к тому, что в дл фреймворке мне нужно ручками определить даже функцию, которая будет считать точность.\n",
      "Я точно должен все писать сам как тут или есть коробочные решения?\n",
      "<https://github.com/rdcolema/pytorch-image-classification/blob/331819df6732b0e1c3b321d6eebce6b97287c585/pytorch_model.ipynb>\n",
      "----------------------------------------\n",
      "в TF мне очень не нравится, не стабильность результатов, и кроме как пропатчить его самому я вариантов не знаю, Theano в этом плане надёжнее. Да даже если идти на низкий уровень и что то дописывать, это всё равно не значит всё с нуля и на низком уровне.\n",
      "----------------------------------------\n",
      "в сочетании с дешевым терабайтным storage, доступным там же, и отсутствием платы за трафик очень выгодно для тех, кто постоянно что-то крутит\n",
      "----------------------------------------\n",
      "сетап упал, был 150 евро\n",
      "может и на аукцион когда выстаят\n",
      "----------------------------------------\n",
      "нубский вопрос. а когда finetuning делаем для нового датасета, то препроцессинг картинок остается тот же? нужно ли к примеру mean, std пересчитывать для нового датасета ? (картинки отличаются от pre-trained)\n",
      "----------------------------------------\n",
      "какие у меня перспективы сейчас есть в области конвертации не совсем простой модели на каффе на что-то юзабельное?\n",
      "----------------------------------------\n",
      "Ну вот и вопрос, как свое замутить)\n",
      "----------------------------------------\n",
      "<https://opendatascience.slack.com/archives/C047H3N8L/p1489750760033503>\n",
      "а есть вообще причины почему он API поменял? с какой целью, некоторые решения выглядят спорно (раньше было лучше)\n",
      "----------------------------------------\n",
      "<@U14L5TKNJ> да проскакивала такая мысль, только зачем одно апи? важнее чтобы было удобно и последовательно\n",
      "----------------------------------------\n",
      "А cnmen какой у theano в конфиге?\n",
      "----------------------------------------\n",
      "А вот такой вопрос: вейвлетные разложения пытались совать вместо RGB картинок на вход сетям? Очевидная идея, но ни кто этого не делает, почему?\n",
      "----------------------------------------\n",
      "Там где раньше делали линейное PCA, сейчас есть сети, которые делают нелинейное PCA и т.п. :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "А зачем городить лишние этапы процессинга-препроцессинга, если и так работает отлично?\n",
      "----------------------------------------\n",
      "а где про это можно почитать?\n",
      "----------------------------------------\n",
      "Эм, даже трудно что-то посоветовать, любой курс/туториал по нейронкам описывает как кормить модель данными по частям\n",
      "----------------------------------------\n",
      "а если с фурье работать, то можно же как бы свертки на исходном изображении считать за линию в фурье спектре?.. О_о\n",
      "----------------------------------------\n",
      "Тогда каждую свёртку-фильтр надо делать размером как картинка, наверное по памяти плохо выходит. \n",
      "----------------------------------------\n",
      "зато перемножать то как быстро!11\n",
      "----------------------------------------\n",
      "ну и я тогда не понял, почему так все не делают)\n",
      "----------------------------------------\n",
      "Ну т.е. когда оперативки в карточках станет \"чуть\" больше так видимо делать начнут?\n",
      "----------------------------------------\n",
      "А как они там оптимизируют? По ядру и каждый раз новый образ находят?\n",
      "----------------------------------------\n",
      "а как ты tf устанавливал?\n",
      "----------------------------------------\n",
      "И как это связяно с TF vs TH?\n",
      "----------------------------------------\n",
      "Кто его знает, может тут загружает один гиг, а теано 10.\n",
      "----------------------------------------\n",
      "т.е. отслеживание watch nvidia-smi  + htop мне кажется должны показать где же оно реально крутится\n",
      "----------------------------------------\n",
      "может что-то не то, как оно патчи семплирует?\n",
      "----------------------------------------\n",
      "как тут уже писали, keras 2 сносит tensorflow-gpu по умолчанию, так что могло и на cpu соскочить. но если пишет о подгрузке куды, то вряд ли это твой случай\n",
      "----------------------------------------\n",
      "А расскажите какие вехи произошли в object detection за 2016 год?\n",
      "Условно, после Faster RCNN, YOLO и SSD\n",
      "Помню YOLO2\n",
      "Или все, задача считается решенной?\n",
      "----------------------------------------\n",
      "кто знает, в связи с чем поменяли? выходные слои в Keras applications.inception_v3\n",
      "было\n",
      "```\n",
      "if include_top:\n",
      "        # Classification block\n",
      "        x = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(x)\n",
      "        x = Flatten(name='flatten')(x)\n",
      "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
      "```\n",
      "стало\n",
      "```\n",
      "if include_top:\n",
      "        # Classification block\n",
      "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
      "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
      "```\n",
      "<https://github.com/fchollet/deep-learning-models/blob/master/inception_v3.py>\n",
      "в пейперах не нашёл детального описания выходных слоёв, только inception blocks, но помню где то видел именно последний вариант\n",
      "имхо он имеет ряд преимуществ\n",
      "----------------------------------------\n",
      "я так и не смог разобраться, как пайпить matplotlib плоты в binary, чтобы телепот не ругался, не сохраняя на диск\n",
      "----------------------------------------\n",
      "Я думаю, что в целом понятен use-case, но все же уточню: работаете вы над какой-то задачей (коллективом или самостоятельно) и хотите иметь некоторую витрину-табличку по всем парам модель и метрика чтобы выбирать какая лучше когда нет одного критерия\n",
      "----------------------------------------\n",
      "Ну и да, если знаете куда меня послать с этим -- скажите) Задачка возникла в контексте DL хотя понятно это не по адресу\n",
      "----------------------------------------\n",
      "А расскажешь почему не зашло?\n",
      "----------------------------------------\n",
      "FGMachine гоняется в докере, а надо было уметь раскидывать эксприменты по нескольким тачкам с несколькими GPU, с этим возникли сложности.\n",
      "Таких подробностей не знаю, там можно допилить, проект рабочий, но еще свежий.\n",
      "Скажем Суматра как средство регистрации результатов оказалась еще печальнее\n",
      "----------------------------------------\n",
      "1. Как понять, чем занята память на GPU ? Делаю finetune на vgg16 c отрубленной головой (без двух последних fc)  Через несколько итераций из gpu памяти остается порядка 100Mб (из 12Гб), причем никаких внешних shared vars кроме весов vgg16 без fc явно не создаю.  \n",
      "2. Как посмотреть, сколько памяти требуется theano для весов и промежуточных вычислений? Кажется для весов достаточно посмотреть на вывод `np.sum([p.size.eval() for p in lasagne.layers.get_all_params(&lt;last_net_layer&gt;)])`, а как посмотреть, сколько нужно для промежуточных вычислений ?\n",
      "----------------------------------------\n",
      "я не знаю точно как theano управляет памятью inputs, но регулярно наблюдаю, что после вызова функции остаётся занятая память\n",
      "----------------------------------------\n",
      "Как обычно называют Fold или KFold по-русски?\n",
      "----------------------------------------\n",
      "А можешь подсказать, пожалуйста, как из theano.shared сделать нечто, чтобы можно было подать в lasagne.layer? Как то неочевидно и документации такого использования нигде не могу найти\n",
      "----------------------------------------\n",
      "&gt; Теоретически, это задача распознавания образов (поправьте пожалуйста, если не прав)\n",
      "Это задача детекции объектов\n",
      "\n",
      "Из всех ответов ниже, самый понятный это faster rcnn, так как есть давно всеми пользуемая авторская библиотека на python\n",
      "----------------------------------------\n",
      "всем привет, помогите советом, есть корпус слов и выделено, какая из букв ударная, хочется на этом обучить сетку предсказывать ударения для слов.\n",
      "я так понимаю, что надо использовать character-level LSTM, но не очень понимаю, каким должен быть аутпут сети, чтобы это работало для слов разной длины?\n",
      "----------------------------------------\n",
      "Читал тут статью про новый способ metric learning <https://arxiv.org/abs/1703.07464>. Если кто-то читал расскажите как они выбирают proxy points? Просто случайные точки на сфере? Или их тоже учат через embedding? Ничего не могу найти про выбор этих точек в самой статье.\n",
      "----------------------------------------\n",
      "Описывается два способа назначения координат.\n",
      "Первый - статический. И тогда, как я понял, координата прокси это просто one hot encoding от id класса. Позитивному и негативному семплу батча назначаются координаты прокси, соответствущей их классу.\n",
      "----------------------------------------\n",
      "В динамическом методе за проксями не закрепляются семантические метки. Как выбираются координаты прокси - ни слова.\n",
      "----------------------------------------\n",
      "как раз таки в простонародии (местном) - кениг\n",
      "----------------------------------------\n",
      "Я бы чо-нибудь рассказал :slightly_smiling_face: У меня как раз брат туда переезжает 1 апреля\n",
      "----------------------------------------\n",
      "я что-то не понимаю как они определяют proxy через distance, когда distance и оптимизируется с использованием proxy\n",
      "----------------------------------------\n",
      "кажется (но это не точно), что они начинают с d=l2, фигачат условный k-means на обучающей выборке и выбирают proxy как центры кластеров\n",
      "----------------------------------------\n",
      "Мне кажется прокси выбирается не из эмбеддингов семплов из данных. В уравнении 5 написано только как выбрать прокси для семпла, но набор проксей уже предефайнен и обозначен P.\n",
      "----------------------------------------\n",
      "понимаешь как выучиваются, если d от θ явно не зависит?\n",
      "----------------------------------------\n",
      "Примерно понимаю. А почему d от  θ не зависит? d ведь через нейронку считаются. А прокси - тоже параметры сети. Например в керасе статическое назначение можно через Embedding layer реализовать.\n",
      "----------------------------------------\n",
      "Написал небольшой пост как профайлить tensorflow - возможно кому-то будет полезно. <https://medium.com/@illarionkhlestov/howto-profile-tensorflow-1a49fb18073d> .\n",
      "Как обычно английский не родной - есть замечания - пишите)\n",
      "----------------------------------------\n",
      "А можешь подробнее про \"каждому прокси назначается метка класса\". Как вычислить координату этой точки?\n",
      "----------------------------------------\n",
      "Я понял, у всех идеальный результат и вопрос как быстрее до него дойти\n",
      "----------------------------------------\n",
      "Вот такой вопрос в разрезе задачи на рыбки.\n",
      "\n",
      "У вас есть картинки с рыбками класса Акула, Тунец\n",
      "\n",
      "Метрика в задаче logloss по классам Акула, Тунец, other\n",
      "\n",
      "Если на картинке есть один тип рыбы, то другого быть не может.\n",
      "\n",
      "Что лучше брать как loss function:\n",
      "sigmoid по классам (Акула, Тунец) или softmax по классам (Акула, Тунец, other) ?\n",
      "----------------------------------------\n",
      "Вот такой еще вопрос - если вы использует pre-trained сеть, чтобы фич надергать, как я понимаю, нормализация не обязательна?\n",
      "----------------------------------------\n",
      "Как обычно, балансировать классы итд\n",
      "----------------------------------------\n",
      "Есть сеть натренированная на ImageNet. Если мы хотим чисто фичи поизвлекать - надо ли вычитать такой же mean, как был в данных при тренировке этой оригинальной модели?\n",
      "----------------------------------------\n",
      "Я свой mean считал, когда датасет был достаточно большой\n",
      "----------------------------------------\n",
      "И он сильный, как показывает финальная классификация\n",
      "----------------------------------------\n",
      "Каким образом это может быть \"не хуже\" - мне непонятно\n",
      "----------------------------------------\n",
      "А расскажите какие кто делал эксперименты по архитектуре и процессу обучения, особенно если вы верите что они были валидными.\n",
      "Вот, например.\n",
      "БатчНорм. Я понимаю что она сейчас много где используется, но вот я никогда не воспроизводил экспериментально, чтобы сказать как она влияет на итоговое качество, на скорость сходимости, на память и время работы модели в тест режиме (даже часть этой информации была бы полезна)\n",
      "А есть же еще куча нормализаций. По идее хороший подход в серьезной компании сделать бенчмарки по всем вариантам и иметь табличку. Но это ж дофига вложения ресурсов. А в статьях, как я понимаю, сразу несколько сильных байесов\n",
      "----------------------------------------\n",
      "Вот был пейпер, где кучу всякого препроцессинга для CNN попробовали\n",
      "----------------------------------------\n",
      "Я из этих оригиналов, где про это можно почитать?\n",
      "----------------------------------------\n",
      "Но я так и не разгуглил отчего и почему это, потому что все запросы \"фреймворк занимает не всю память\" выдают ответы на вопрос \"фреймворк занимает всю память\", как я не пытался сформулировать\n",
      "----------------------------------------\n",
      "я нашел вот это, думал может кто решение знает: <https://devtalk.nvidia.com/default/topic/901646/-980-ti-windows-10-cuda-7-5-out-of-memory-after-allocating-4-5-out-of-6gb/>\n",
      "----------------------------------------\n",
      "Но всякие gpu-z не показывают эту память как занятую\n",
      "----------------------------------------\n",
      "блин нет это тот же самый json.\n",
      "когда в нем меняю theano на tensorflow - начинает ругаться что нет такого бэкэнда\n",
      "----------------------------------------\n",
      "так а зачем он мне\n",
      "----------------------------------------\n",
      "Ну вот ты шутишь, а я eSXI скачал, когда совсем ушатают нейронки, попробую.\n",
      "----------------------------------------\n",
      "Я давненько от этой темы отошёл, но когда более-менее плотно этим занимался, там как раз это научились делать\n",
      "----------------------------------------\n",
      "Или размножающиеся как кролики фичемапы в densenet'ах\n",
      "----------------------------------------\n",
      "И еще я не понял, как у них per-pixel сегментация получается\n",
      "----------------------------------------\n",
      "Но регионы же разного размера - как эти 14x14 в пиксели на исходной картинке превращаются?\n",
      "----------------------------------------\n",
      "Народ, у меня как минимум до лета есть доступ к Nvidia Titax X Pascal (12 GB). Если кому-то надо посчитать что-то в Keras или Tensorflow и этот расчёт не займёт более суток, могу вам посчитать модель. Всё работает на последней Ubuntu. Есть 32 ГБ оперативки на хосте. Есть также возможность запустить всё это добро на Windows, но нужен будет мануал как быстро и правильно настроить среду.\n",
      "----------------------------------------\n",
      "Про Kaggle и рыбок ещё не читал. В Корее у нас, как ни странно, никто этим не занимается из знакомых рисёчеров. Жду код и датасет:)\n",
      "----------------------------------------\n",
      "Всем привет. \n",
      "Замучился с рекуррентными сетями в tensorflow, прошу помощи.\n",
      "\n",
      "1) проблема с переобъявлением в iPython Notebook\n",
      "Предположим, есть код: \n",
      "\n",
      "```x1 = tf.placeholder(tf.int32, [batch_size, seq_max_len], name=\"x1\")\n",
      "x1_len = tf.placeholder(tf.int32, [batch_size])\n",
      "embeddings = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1), name=\"embeddings\")\n",
      "cell = tf.contrib.rnn.LSTMCell(state_size)\n",
      "cell = tf.contrib.rnn.MultiRNNCell([cell] * 3)\n",
      "outputs, states = tf.nn.dynamic_rnn(cell=cell, sequence_length=x1_len, dtype=tf.float32, inputs=x1_embed) ```\n",
      "\n",
      "Этот код можно выполнить только 1 раз. Если я решу изменить в ноутбуке, например, batch_size, то перезапустить эти строки не получится: на tf.nn.dynamic_rnn tensorflow начнет ругаться (Variable rnn/multi_rnn_cell/cell_0/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope?)\n",
      "\n",
      "Как с этим бороться?\n",
      "\n",
      "2) если у меня есть x1 и x2, которые я хочу прогнать через динамическую рекуррентную сеть, как это сделать?\n",
      "на\n",
      "```outputs1, states1 = tf.nn.dynamic_rnn(cell=cell, sequence_length=x1_len, dtype=tf.float32, inputs=x1_embed)\n",
      "outputs2, states2 = tf.nn.dynamic_rnn(cell=cell, sequence_length=x2_len, dtype=tf.float32, inputs=x2_embed)```\n",
      "вылезет ошибка, аналогичная предыдущей\n",
      "----------------------------------------\n",
      "1. \"weights already exists\" -- вставь `tf.reset_default_graph()` в начале этой ячейки.\n",
      "\n",
      "2. используй `tf.variable_scope(\"my_awesome_var_scope\", reuse=False/True)`, где в первый раз `reuse=False` (не пытаться переиспользовать переменные, а создавать их), и в последующие разы `reuse=True` (переиспользовать уже созданные веса)\n",
      "----------------------------------------\n",
      "подскажите, как в керасе считать лосс только по части изображения? \n",
      "\n",
      "задача сегментации, есть некая маска. важны предсказания только тех пикселей, которые соответствуют маске, в остальных местах можно предсказывать все подряд.\n",
      "----------------------------------------\n",
      "теоретически ААЕ можно прикрутить, который как в статье разделяет стиль и семантику, но чот мне кажется такое обучить будет не легко, не мнист все таки\n",
      "----------------------------------------\n",
      "как я понял из описания там есть ground truth разметка\n",
      "----------------------------------------\n",
      "Призывается <@U1SJ2U4V9> \n",
      "\n",
      "Ты как GAN к спутникам прикрутил?\n",
      "----------------------------------------\n",
      "Если там все картинки, как та, что выше и их просто надо посчитать - задача прямолинейная по самое не могу. Это не рыбки или рак.\n",
      "----------------------------------------\n",
      "ну смотреть как у тебя шатаются данные в исходной выборке и так же шатать их руками\n",
      "----------------------------------------\n",
      "чот мне кажется что результат такого порно будет из серии онеме cool devices, если вы понимаете о чем я\n",
      "----------------------------------------\n",
      "какой must заботать по deeplearning кроме <http://cs231n.github.io>?\n",
      "----------------------------------------\n",
      "хватит emoji ставить, лучше бы посоветовали что-нибудь кто что ботал :disappointed:\n",
      "----------------------------------------\n",
      "хотя не знаю где это написано в доках, но сохраняет :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Это же не из ганов. Это как google deep dream примерно, просто визуализируют что сеть выучила. Через gan порн был бы гораздо интереснее. Например как pix2pix, только вместо сумок и коттиков фотки. Они бы вообще не такие абстрактные были.\n",
      "----------------------------------------\n",
      "хм, нет - никогда не проверял какого типа инты)\n",
      "----------------------------------------\n",
      "То есть ты предполагаешь, что во время :kaggle: Ты читаешь меньше статей, чем без него? По факту скорее больше, но они очень тематически сфокусированы - например по сегментации тебе каждый, кто работал над спутникам даст весь расклад, но за GAN или локализацию не скажет ничего.\n",
      "----------------------------------------\n",
      "Привет! Я думаю, здесь уже задавали этот вопрос, но четкого ответа  не нашла ) Какие курсы по deep learning рекомендуются для новичка?\n",
      "----------------------------------------\n",
      "Навскидку это как раз не ко мне, а к любителям постакать.\n",
      "----------------------------------------\n",
      "versus: это правда так хорошо работает, как пишут в статье? \n",
      "----------------------------------------\n",
      "<@U3PLVL3RD> а почему не взлетело, есть интуиция? \n",
      "----------------------------------------\n",
      "Доброго времени суток! На стажировке столкнулся с такой задачей computer vision: надо найти на скрине веб-страницы все присущие ей элементы (кнопки, текстовые поля, чекбоксы). Можете подсказать, какие способы можно попробовать или по теории материалы какие-то, которые помогут решить именно эту задачу. CS231n уже начал. Спасибо!\n",
      "----------------------------------------\n",
      "то есть хотите автоматически определять когда что-то съехало?\n",
      "----------------------------------------\n",
      "Когда keypoints - да, регрессия точек, только без sigmoid\n",
      "----------------------------------------\n",
      "Когда прям bounding box и регионы - то Faster R-CNN\n",
      "----------------------------------------\n",
      "Я все понять не могу как этих рыбок локализовать :disappointed:\n",
      "----------------------------------------\n",
      "<@U411PKASW> как минимум можно определять пересекаются ли элементы, которые не должны и какие между ними расстояния. Но неясно как идентифицировать элементы - у тебя может быть несколько чекбоксов и полей на странице, будет сложно понять где какой наверное\n",
      "----------------------------------------\n",
      "<@U3R32B38W> Надо определить границы всех элементов. Как второй этап задачи уже, это понять что в пределах этих границ находится чекбокс, текстовое поле, кнопка…\n",
      "----------------------------------------\n",
      "Данных ~3k и рыбок на самих картинках мало\n",
      "\n",
      "=&gt; один из подходов кропнуть куски где есть рыбки и уже их классифицировать, как в том году делали, когда китов классифицировали\n",
      "----------------------------------------\n",
      "Что-то я никак не пойму, как Faster R-CNN работает. \n",
      "\n",
      "Я правильно понимаю, что берется любая FCN, после последнего CNN слоя сигнал идет в два бранча, в каждом из них делается 1x1 convolution, после этого в каждом flatten, dense, а после этого в одной голове sigmoid или softmax на классификацию, а в другой  регрессия на bounding boxes?\n",
      "----------------------------------------\n",
      "Подскажите пожалуйста, как можно использовать KerasClassifier в sklearn Pipeline с feature селектором? В nn модель необходимо передавать input_shape, а в такой ситуации количество выбранных признаков мы априори не знаем.\n",
      "----------------------------------------\n",
      "нет, как раз `value.lower()` выше по коду есть\n",
      "----------------------------------------\n",
      "правда я впервые вижу чтобы кто-то делал аргумент функции case insensitive, когда там всего два значения возможно\n",
      "----------------------------------------\n",
      "<@U3HM4KY14> убрал девайс. Запустилось. По данным nvidia-smi аллоцирована память ток на одной тачке, CPU-Util в пике 20%. Почему не 100% или около того? Значит ли эта, что карточка не используется? Как проверить, что карточка нагружается? По скорости по сравнению с cpu улучшения заметного нет.\n",
      "----------------------------------------\n",
      "<https://medium.com/towards-data-science/howto-profile-tensorflow-1a49fb18073d> - недавно кто-то в чятике пиарился, попробуй посмотреть кто и что и где гоняет, я думаю <@U0H7VBQQ1> прав, у тебя в графе что-то тормозит.\n",
      "----------------------------------------\n",
      "народ, как бы вы стали решать задачку: \n",
      "имея\n",
      "- изображение здания\n",
      "- координату фотографа и направление камеры при съемке\n",
      "нужно разметить на фото геометрию видимой части здания (углы и ребра, и прикинуть их координаты)\n",
      "----------------------------------------\n",
      "Есть вопрос по Keras. В последних версияз появились соответственно Pretrained сетки, которые можно вызвать.\n",
      "from keras.applications.vgg16 import VGG16\n",
      "vgg16 = VGG16(include_top=False, weights='imagenet')\n",
      "\n",
      "Вопрос в следующем. Как потом сюда прикрутить в конец, например, Dense? Я это сделал, но через танцы с бубном, вытащив саму функцию и переделав под свои нужды. Мне кажется должен быть элегантный вариант в одну строку.\n",
      "----------------------------------------\n",
      "И Dilated Convolutions сразу станут быстро работать, а не как обычно, сразу после установки cudnn 6?\n",
      "----------------------------------------\n",
      "А где доступно написано про регуляризацию градиентами? У меня есть физичный мир: \"чем больше положишь - тем больше получишь\", я хочу, чтобы нейронка учитывала эту физику градиентами\n",
      "----------------------------------------\n",
      "довольно детерменированный процесс есть, который хорошо предсказывается. Но иногда, когда я анализирую свои градиенты, я замечаю, что есть пара выбросов, когда градиент по инпуту отрицателен, хотя физичный мир говорит, что положителен\n",
      "----------------------------------------\n",
      "Я вот все равно ни черта не понял. Что за физический процесс? Зачем его предсказывать если он детерменирован? Что за сеть?\n",
      "----------------------------------------\n",
      "Хотя, я видел пейперы, где нейросети заменяют расчёты, потому что расчёты по мелкой сетке -- долго, а нейронка -- хуякс, хуякс и 98% точности за мало флопсов\n",
      "----------------------------------------\n",
      "cepera_ang: так ли мало? И настолько ли точно?) \n",
      "\n",
      "Видел пару применений, где довольно сложные мат., модели аппроксимируют НС. Плюс товарищи, которые пытаются модель ошибок всяких акселерометров и гироскопов аппроксимировать ими же. Но тут понятно-  в данных шума полно.\n",
      "\n",
      "Но мне почему-то это кажется юношеским максимализмом. Неужели нет чего-то получше и попроще для регрессии?\n",
      "----------------------------------------\n",
      "\"в лоб\" такую задачу в CV решают, зная: параметры внешнего и внутреннего ориентирования (плюс дисторция), и еще нужно знать \"мировые\"   трехмерные координаты трех точек снимаемого: для восстановления третьей координаты. Либо знать что-то о геоиетрии здания (напр., размер дверного проема). Если не в лоб - можно попробовать поискать, как делают и обучают какой-нибудь \"угадыватель\" трехмерки по размеченным съемкам зданий,  но никакой точности там, пожалуй, не будет. Если чего не пропустил из \"новенького\".\n",
      "----------------------------------------\n",
      "n01z3: а примерную утилизацию CPU+GPU можно? Может mxnet более эффективно использует ресурсы CPU там где оно хорошо подходит\n",
      "----------------------------------------\n",
      "Как сказал Андрей Киселев: И че? \n",
      "----------------------------------------\n",
      "А кто может подсказать где искать статьи на тему примерного решения навье-стокса с нс?\n",
      "----------------------------------------\n",
      "Стохастическая регуляризация плюс ко всему, так как сетка корректирует предсказание  баесовской модели.\n",
      "----------------------------------------\n",
      "А кто какое железо использует? Есть кто на aws или у всех топовые титаны пачками воткнуты?\n",
      "----------------------------------------\n",
      "А какой правильный термин для такой штуки: когда картинку по которой надо сделать предикт, крутим вертим предсказываем несколько раз и потом усредняем и говорим ответ?\n",
      "----------------------------------------\n",
      "Вообще когда фраза состоит из одних существительных -- это пиздец, так не говороят, поэтому и читать такое невозможно\n",
      "----------------------------------------\n",
      "<@U43FTJQ2V> если для людей пишешь -- пиши так, как в чате бы написал, если для формальных нужд -- похер, множь энтропию, пиши в стиле \"множественная деформация единичного семпла во время выполнения операции предсказания\".\n",
      "----------------------------------------\n",
      "Теперь совсем жесть. Как раз в диссер вставить, лол\n",
      "----------------------------------------\n",
      "Прям как в изначальном вопросе :joy:\n",
      "----------------------------------------\n",
      "взаимное расположение наблюдателя и здания, полагаю, неизвестно? тогда я бы тоже попробовал сначала edge detector'ы. как только есть ключевые точки (углы, дверной проем, т.д.) и их референсные значения (в мировых координатах) - вкручиваешь гомографию и готово.\n",
      "----------------------------------------\n",
      "Друзья, а как выбирать window_size и количество скрытых нейронов в lstm? Как они к друг другу должны соотносится? Если какие-то эвристики? Мне наивно кажется что 1 к 1 норм, но я даже и это нагуглить не смог :dolan: \n",
      "----------------------------------------\n",
      "Пусть есть длинная последовательность (длиной L), которую ты хочешь RNN-кой свернуть, если нет необходимости сделать за один присест, тогда побьешь на  отрезки длиной seq_length, а при обработке будешь сохранять и передавать state RNN. `seq_length ~10-30`, `rnn_size ~0.1k-2k`\n",
      "Какое из чисел L или seq_length ты называл окном -- не знаю.\n",
      "----------------------------------------\n",
      "Хммм. Я подразумевал количество символов/точек на основе которых мы предсказываем. То есть берём window_size токинов типа фичи и window_size+1 таргет. \n",
      "Или как обычно эту величину величают? Я может поэтому загуглить не смог. \n",
      "----------------------------------------\n",
      "Только не language, а multivariate time-series. Но кажется для понимания каким должно быть окном тут конкретики задачи не важна. \n",
      "Спасибо! :+1: \n",
      "----------------------------------------\n",
      "<@U040M0W0S> мне кажется они вообще про разное, я не понимаю как они связаны\n",
      "----------------------------------------\n",
      "Прикольно, что основная идея простая как две копейки\n",
      "----------------------------------------\n",
      "А как FCN получится прогнать?\n",
      "----------------------------------------\n",
      "Я думаю патчами получится, как pix2pix делает\n",
      "----------------------------------------\n",
      "кстати, MSE упало до как раз примерно std и встало\n",
      "----------------------------------------\n",
      "Кто-нибудь сталкивался с задачей понимания что делает каждый фильтр в CNN ? Конкретнее, у меня CNN для time-series  c 1D свертками, мб есть какой-то более четкий способ чем то как делают в статьях про CNN и котиков (я нашел что выбирают или даже как-то частично генерят котиков которые сильнее всего активируют конкретный фильтр и рисуют feature map)\n",
      "----------------------------------------\n",
      "Есть спецы по лазанье?\n",
      "Кейс: есть физический процесс, 12 картинок в семпле. \n",
      "Я их сую в одну переменную шейпа [None,12,256,256]\n",
      "Теперь мне нужно предсказать конфигурацию системы через какое-то время (3 класса).\n",
      "Сейчас я просто делаю сверточную сеть. \n",
      "Может вариант как к этому прикрутить rnn/lstm?\n",
      "----------------------------------------\n",
      "А то получается что-то типа \"а если данные -- картинки, какие архитектуры посоветуете\"\n",
      "----------------------------------------\n",
      "Как известно, resnet50 тренированный на кошках и собаках отлично ловит хоть рак, хоть рыбу :noise:  \n",
      "----------------------------------------\n",
      "А где лучше почитать, как визуализировать активации на исходной картинке?\n",
      "----------------------------------------\n",
      "Вот тут что похожее делали <http://www.idiap.ch/~gatica/publications/FaselGatica-icpr06.pdf> специфических слов нет. Как и в статье <http://visual.cs.ucl.ac.uk/pubs/harmonicNets/> Называй как хочешь\n",
      "----------------------------------------\n",
      "а что значит отрезать? это у тебя какая размера картинка выйдет?\n",
      "----------------------------------------\n",
      "Например в рыбах - если не знаешь где рыба, при повороте может быть проблема.\n",
      "----------------------------------------\n",
      "Сейчас как минимум 3 на дл и еще долго будут идти\n",
      "----------------------------------------\n",
      "<@U07V1URT9> ну вот возьмем к примеру Intel &amp; MobileODT Cervical Cancer Screening - идея важная. Но, кому и как поможет моё участие, если мой результат будет где-то в топ 30%\n",
      "----------------------------------------\n",
      "Что такое \"общеполезный прикладной проект\" и под какую из этих категорий не подходит керас?\n",
      "----------------------------------------\n",
      "Ну это же фреймворк, \"инструмент\", какую общеполезную прикладную задачу он решает?\n",
      "----------------------------------------\n",
      "Как я уже писал в другом среаде, кому будет полезен мой код и мой труд, если мой скор будет на уровне 30% или даже 20%? Как пример, если бы велась сейчас МЛ разработка на Mycroft AI - это совсем другое дело. Добавил какой-то функционал, стянул апгрейд и твой личный voice assistant получил новые возможности.\n",
      "----------------------------------------\n",
      "Нубский вопрос, можно ли взять что-нибудь сложное из model zoo, типа U-net, и тренировать его как BNN, или там нужны другие архитектуры? Какие преимущества у BNN перед традиционными DNN?\n",
      "----------------------------------------\n",
      "Друзья, я столкнулся с проблемой выравнивания изображений по цвету/тону. У меня есть сырое изображение, например, c google maps, и на нем отчетливо видно, что снимки сделаны в разное время при разной освещенности. Мне нужно решать задачу image segmentation, но с такими изображениями это трудновато. Я пробовал equalizeHist, но результат неудовлетворительный. Подскажите, пожалуйста, как стоит выравнивать по цвету?\n",
      "----------------------------------------\n",
      "Господа диплернеры, какие крутые примеры применения DL кроме распознавания/генерации изображений, аудио, текстов и self-driving cars вы можете подсказать? Мне нужно подготовить презентацию, чтобы впечатлить всех возможностями DL\n",
      "----------------------------------------\n",
      "Распознавание лиц/эмоций и когда её обманывают паттерном на очках и она думает, что это Бред Питт.\n",
      "----------------------------------------\n",
      "вот не надо. Там где нет данных с локальной корреляций между фичами - а это 95% всех случаев xgboost будет рвать нейронки на британский флаг.\n",
      "----------------------------------------\n",
      "И как они могут обмануть весь этот ИИ в очках с паттерном Бреда Питта— типа, вот, смотрите, есть куда расти— купите.\n",
      "----------------------------------------\n",
      "Да всё как обычно срут\n",
      "----------------------------------------\n",
      "Каждый раз, когда пишут про сознание у нейросетей хочется вспомнить сатрое \"shut up and calculate\"\n",
      "----------------------------------------\n",
      "<@U0R55NS1M> расскажи про текущее соревнование data science bowl, где рак распознают по томограммам\n",
      "----------------------------------------\n",
      "Кто там хотел примеров сетей без картинок, звука и текстов? <https://twitter.com/KyleCranmer/status/848959715472277505>\n",
      "----------------------------------------\n",
      "И снова старые песни о главном.\n",
      "\n",
      "Пора слезать с Keras - в какой framework сунуться?\n",
      "\n",
      "[1] Легко прототипировать - не сильно сложнее Keras\n",
      "[2] Быстро работает\n",
      "[3] Легко параллелится на 2 GPU\n",
      "[4] Не жрет память неадекватно.\n",
      "[4] Много внятной документации, лучше с примерами.\n",
      "[5] Много всякой инфраструктуры работает если не из коробки - то легко находится в интернете.\n",
      "\n",
      "Я правильно понимаю, что [3] ограничивает мой выбор на Tensorflow и mxNet, а [4] и [5] ограничивает это до Tensorflow?\n",
      "----------------------------------------\n",
      "Говорю как человек, который постоянно хакает керас и попробовал tf)\n",
      "----------------------------------------\n",
      "ну это смотря, видимо, как глубоко проваливаться :slightly_smiling_face: Но если пишешь слой в керасе (и не заботишься о том, чтобы он работал с theano backend), то керасовский тензор -- это tf-тензор, и операции можно выполнять вообще прозрачно.\n",
      "----------------------------------------\n",
      "Лично мне что бы съехать на TF не хватает тупо например Kernel'ов на том же Kaggle для известных задач, написанных как можно проще и логично на TF.\n",
      "----------------------------------------\n",
      "сложности возникают со служебными вещами, когда хочется добавить assert nodes, или считать что-то помимо вычисления выходов сети\n",
      "----------------------------------------\n",
      "а я вот писал на голом tf и очень радовался, когда изучил керас, то есть двигался как раз в другую сторону\n",
      "----------------------------------------\n",
      "керас 2.0 (или раньше тоже?) ещё хитро умеет спрашивать у tf размерность всех тензоров, поэтому о согласовании размеров надо думать только там, где это содержательно надо\n",
      "----------------------------------------\n",
      "Привет, я до конца не понимаю чем mkldnn отличается от mkl2017 и почему в моих тестах слои показывают показывает сравнимую производительность. \n",
      "Типо нужны совсем илитарные xeon для того чтобы заметить разницу?\n",
      "У кого-нибудь был опыт? (интересует в первую очередь backward)\n",
      "----------------------------------------\n",
      "для классификации при помощи word lstm видел как рандомное слово на &lt;unk&gt; заменяли\n",
      "----------------------------------------\n",
      "мб у кого завалялся,или знаете репозиторий-нужна сиамка/triplet наученный на каком-нибудь разнородном датасете фоток вроде Imagenet(лица не предлагать)? преимущественно на тф или керасе\n",
      "----------------------------------------\n",
      "Есть простая (по меркам DL) задача определения облаков и их теней на спутниковых снимках. Есть обучающая выборка, но она не очень большая, поэтому хочется попробовать что-нибудь unsupervised. Наверняка есть какие-нибудь ресурсы, которые помогут сделать это быстро и на коленке как proof of concept, желательно на тф. Спасибо!\n",
      "----------------------------------------\n",
      "да нет, вполне обчно, я тоже так говорю когда от амазона счет получаю\n",
      "----------------------------------------\n",
      "Всем привет. У меня очередной затык с rnn-ками в tensorflow. Надеюсь на вашу помощь.\n",
      "\n",
      "Вопрос: как работает у dynamic_rnn параметр sequence_length? Как брать последний выход и последний скрытый слой? (он копируется в конец массива или нет?)\n",
      "\n",
      "Конкретная проблема: есть простая сеть\n",
      "```x_input = tf.placeholder(tf.float32, [None, max_steps, num_features])\n",
      "x_input_len = tf.placeholder(tf.int64, [None])\n",
      "y_input = tf.placeholder(tf.int64, [None])\n",
      "cell = tf.contrib.rnn.LSTMCell(128)\n",
      "outputs, states = tf.nn.dynamic_rnn(cell=cell, inputs=x_input, sequence_length=x_input_len, dtype=tf.float32) \n",
      "output = outputs[:, -1, :]\n",
      "logits = tflearn.fully_connected(output, 10)\n",
      "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_input)\n",
      "loss = tf.reduce_mean(cross_entropy)\n",
      "optimize = tf.train.AdamOptimizer(learning_rate).minimize(loss)```\n",
      "\n",
      "Она не сходится (на 10-классовой классификации выдает accuracy стабильно 8.75% и дальше не идет).\n",
      "Проблема решается, если:\n",
      "- заменить output = outputs[:, -1, :] на output = states.c\n",
      "- убрать параметр sequence_length\n",
      "Не понимаю, как это работает и почему это так. Прошу объяснить.\n",
      "\n",
      "Я полагал, что последний output (т.е. outputs[:, -1, :]) - это то же самое, что states.c, но, почему-то это не так\n",
      "----------------------------------------\n",
      "И еще сразу задам вопрос:\n",
      "когда мы пишем\n",
      "```lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
      "stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm] * number_of_layers)```\n",
      "у нас эти lstm имеют одни веса, или это разные lstm в количестве number_of_layers штук?\n",
      "----------------------------------------\n",
      "Гугл как всегда, максимально к оптимальному производительность/цена. Вообще пора бы уже и в другую сторону двигать науку. К нейросетям, которые требуют большей точности чисел и за счет этого решают проблему затухания градиента.\n",
      "----------------------------------------\n",
      "Размер весов кстати можно серьезно так уменьшить я просто не задумывался и выодил как {:.12f} там куча нулей и тому подобной чуши.\n",
      "----------------------------------------\n",
      "Из любопытства - а тебе зачем такое?\n",
      "----------------------------------------\n",
      "<@U36Q9NJMD>  Я более склонен к словам Дональда Кнута. Он сказал типа \"Лучше делать хорошую науку, чем популярную. Если что-то популярное, то это по-моему не правильное :whaaaaaat: :wat:\" - хотя звучит как отмазка  :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "<https://github.com/precedenceguo/mx-rcnn>\n",
      "а как вообще сюда корректно подключать новые датасеты? писать самому адаптер(когда то столкнулся в <https://github.com/rbgirshick/py-faster-rcnn>) или есть легче пути?\n",
      "----------------------------------------\n",
      "Это пример как получить предсказания с промежуточного уровня в Keras.\n",
      "----------------------------------------\n",
      "А где грязь, не могу найти?)\n",
      "----------------------------------------\n",
      "Я бегло поглядел, sonnet судя по всему должен быть удобен для генеративных моделек.\n",
      "Как раз там, где керас очень неудобен.\n",
      "\n",
      "Для динамических графов есть TensorFlow Fold\n",
      "----------------------------------------\n",
      "друзья, а че-то у меня Керас после `load_weights` обучение как будто заново начинает :hm-squared: \n",
      "у меня LSTM и RMSprop. нагуглил, что оно у кого-то еще себя так ведет: <https://github.com/fchollet/keras/issues/2378>\n",
      "но не нагуглил решения :pepe_sad: может кто сталкивался и победил? :muscle:\n",
      "----------------------------------------\n",
      "попробую как они предлагают через новый бэкенд libgpuarray\n",
      "----------------------------------------\n",
      "если что-то не компилится, то надо посмотреть, как оно компилится - не инклудящийся файл найти и его папку проверить в include paths `-I`, не линкующуюся библиотеку найти и ее папку проверить в `-L`\n",
      "переменные окружения еще полезно export-ить на всякий случай, но тут вряд ли этот случай\n",
      "----------------------------------------\n",
      "как мне казалось этого достаточно\n",
      "----------------------------------------\n",
      "А какую картиночную модель сейчас моднее всего применять для поиска по картинкам? Все ещё везде VGG из-за большого выбора юзабельных боттлнеков или резнеты и прочие инсепшны лучше работают?\n",
      "----------------------------------------\n",
      "Скажите, а cudnn с кудой встаёт на Ubuntu 17.04? А то у меня 16.04 LTS, и я хочу обновиться до 17.04, но боюсь, что TF не заведётся :pepe_sad:\n",
      "----------------------------------------\n",
      "omtcyfz: поставь рядом (или где нибудь на амазоне спот на часок) \n",
      "----------------------------------------\n",
      "И куду сразу 6, чтобы контрольный :gun: дать\n",
      "----------------------------------------\n",
      "<@U1CF22N7J> амазон споты - это неудобно. а про куду 6 не очень понял :pepe_sad:\n",
      "----------------------------------------\n",
      "Как подготавливают данные при задачах локализации?\n",
      "\n",
      "Для классификации можно просто кропать, для сегментации тоже.\n",
      "\n",
      "А вот если у нас есть картинка 400x1000 и на ней что-то выделено в bounding box.\n",
      "\n",
      "Что обычно делают? Перегоняют под квадрат, забивая на то, что изображение сплющит по горизонтали?\n",
      "\n",
      "Кропают?\n",
      "----------------------------------------\n",
      "Например, при локализации plate numbers точно не меняют aspect ratio, так как он имеет значение. Но вот _вписать_ и _отмасштабировать_ во что-то можно.\n",
      "----------------------------------------\n",
      "Необходимо на изображении выделять обпределенные объекты в bounding box. Сами объекты могут быть сильно разные по размеру. Какой метод на данный момент лучше выбрать? Желательно чтобы сеть это выполняла в один проход. Пока остановился на сети yolo <https://pjreddie.com/media/files/papers/yolo_1.pdf>\n",
      "----------------------------------------\n",
      "Может есть какие еще однопроходные решения?\n",
      "----------------------------------------\n",
      "Вопрос по tensorflow, как сохранить / загрузить веса? Поиск по SO особо не помог.\n",
      "Я выше привел код сниппета - при первом запуске получаем модель (раскомментируем строку), затем снова запускаем - все работает, готовая модель предсказывает нормально. Python 3, tf 1.0.1\n",
      "\n",
      "Если теперь закомментировать первый “with tf.Session() as sess:” и еще раз запустить, то будет модель загружаться, но предсказывать полный рандом. \n",
      "Вопрос: почему так, что вообще происходит?\n",
      "----------------------------------------\n",
      "Попробуй сохранять-загружать как тут - <https://www.tensorflow.org/programmers_guide/variables#saving_and_restoring>\n",
      "----------------------------------------\n",
      "Ну да, но у меня без global_variables_initializer и созданием saver как там - все работает :idk:\n",
      "----------------------------------------\n",
      "можешь показать код? а то я как обезьяна тыкаю\n",
      "----------------------------------------\n",
      "такой вроде как заработал, спасибо!\n",
      "----------------------------------------\n",
      "а не подскажешь, почему все таки в одном случае нормально веса грузятся, а в другом криво (исходя из первоначальной постановки вопроса)?\n",
      "----------------------------------------\n",
      "С чего проще начать - учить чистый tensorflow :tensorflow:  или keras :keras: ? Где больше документации, меньше багов, подводных камней и проще порог вхождения?\n",
      "----------------------------------------\n",
      "я и говорю что как пидор, ну ваще по нему видно что он жучара, не то что :schmidhuber:\n",
      "----------------------------------------\n",
      "кто-то файнтюнил ResNet-50? хотелось бы узнать какие блоки лучше дообучать. я сначала навесил dense размером в 2048 (также перед и после дропауты в 0.5). \n",
      "первый шаг - обучаю конкретно этот dense \n",
      "на втором шаге - позволяю тренирвоатся последнему конволюционному и двум identity блокам. Результаты очень хороши в сравнении с VGG16 и InceptionV3, прмерно +12% на валидации. Но хотелось бы узнать или прочесть как лучше всего тюнить эту сеть. Или все зависит от набора данных и их специфики?\n",
      "----------------------------------------\n",
      "А как в керасе проще всего sampled softmax заюзать? \n",
      "----------------------------------------\n",
      "Я не вижу проблем сделать выход на 1000 bounding boxes. Я вот понять не могу что туда при тренировке подавать. И в какой из выходов подавать правильный bounding box?\n",
      "----------------------------------------\n",
      "при тренировке? да, как и для меток класса\n",
      "только у одного класса стоит метка 1 и bbox с ненулевыми координатами\n",
      "во всех остальных метках и координатах нули\n",
      "----------------------------------------\n",
      "Так это прекрасно. Но это проблемы post processing, до них дожить надо.\n",
      "\n",
      "Собственно на ImageNet на Localization надо классы + bounding boxes для топ 5 классов,\n",
      "На detection для всех.\n",
      "\n",
      "То есть неулевые предсказания мне как раз и нужны.\n",
      "----------------------------------------\n",
      "А зачем мутить?\n",
      "\n",
      "Можно начать либо с rescale, либо вставлять в большую картинку. А потом как что-то пойдет можно глянуть на SPP Net, мужики в статье с Mask R CNN - пишут, что это просто обязательно.\n",
      "----------------------------------------\n",
      "Я в общем тоже, поэтому и хочется на этой задаче разобраться кто чей брат, тем более, что с валидацией проблем нет.\n",
      "----------------------------------------\n",
      "Так чтобы батч пополам побить и потом собрать корректно - я не знаю, какой фреймворк это прозрачно умеет\n",
      "----------------------------------------\n",
      "Господа, помогите разобраться начинающему, пожалуйста.\n",
      "\n",
      "Разбираю пример классификации изображений: <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb>\n",
      "\n",
      "Цель — на своих данных распознавать не 10 классов (как в примере), а 250.\n",
      "(Картинки как в примере — 28х28,просто чёрно-белые, визуально распознаются норм)\n",
      "Похоже что не достаточно изменить только размер выходного слоя (сеть просто не сходится), а нужно изменить ещё гиперпараметры (?)\n",
      "\n",
      "Наверное надо поставить batch_size такого размера чтобы в каждом шаге были примеры каждого класса.\n",
      "Опытным путём на 200ах классах пришёл к\n",
      "batch_size = 500\n",
      "patch_size = 15\n",
      "Буквально небольшое изменение — сеть опять не сходится.\n",
      "\n",
      "Ну ок, обучаюсь. Сохраняю граф вычислений для использования в другом месте, а он весит 100 мегабайт, как-то многовато.\n",
      "\n",
      "Вопрос 1. Верны ли мои рассуждения относительно параметров и сходимости сети? Или причина несходимости сети при увеличении классов на порядок может быть в другом?\n",
      "Вопрос 2. Почему граф такой простой сети столько весит? Что я делаю не так? Как уменьшить?\n",
      "----------------------------------------\n",
      "попробуй способ выше, вроде размер это тебе уменьшит, я в inception не копал внутрь, не могу сказать, оптимизированную версию они предоставляют или как есть.\n",
      "----------------------------------------\n",
      "Собственно решил не переобучивать Inception на своих данных из-за того что посчитал что это слишком сложный инструмент для моей простой классификации. Поэтому оказалось неожиданным что получившийся у меня результат ощутимо больше весит чем оригинальный Inception, рассчитанный на куда более сложные картинки\n",
      "----------------------------------------\n",
      "Привет, возможно вопрос - дубль или оффтоп, но задам:\n",
      "Порекомендуйте большой мануал, а лучше книгу по CNN. Хочу разобраться с различными слоями типа pooling например, как работаю каждый, когда и зачем нужен. Вокруг все по крупицам есть конечно, но может кто-то уже находил клад.\n",
      "----------------------------------------\n",
      "ребята, вот я взял resnet50, отрезал последние слои (include_top=False), но вектора, которые получаются - не решают мою проблему (детект фоток \"близких\" вещей), как я понимаю, мне надо дообучать сеть для моей задачи, правильно? Порекомендуйте релевантный материал на данную тему.\n",
      "----------------------------------------\n",
      "Есть ли какое-н исследование, которое показывает как различные глубокие архитектуры (fully convolutional, resnet, inception, etc) работают в задачах обучения embedding'ов? <@U041P485A> , <@U06K9ELB1> ?\n",
      "----------------------------------------\n",
      "привет) Как сделать, чтоб нейросеть не только выдавала ответ, но еще и уровень её уверенности в этом ответе?\n",
      "----------------------------------------\n",
      "Если argmax от вероятности, тогда уверенность -- как раз вероятность\n",
      "----------------------------------------\n",
      "В общем случае делать дополнительный выход, вопрос только в том, как его тренить.\n",
      "С генеративной все сложно :slightly_smiling_face:\n",
      "Для регрессии задача похожа на что-то с байесом: предсказывать величину и разброс, штрафовать за большой предсказанный разброс, штрафовать за несоответствие тому, что должно быть\n",
      "----------------------------------------\n",
      "Именно такого исследования не встречал, попадалось в некоторых статьях сравнение разных архитектур. В целом тенденция такая, что более современная архитектура оказывается лучше более старой :slightly_smiling_face: так какое-то время назад с AlexNet-ов перешли на GoogleNet/VGG, сейчас в ходу в основном разные ResNet-ы, ну и гугл свои усовершенствованные Inception-ы использует.\n",
      "\n",
      "Fully convolutional - видел вроде для какой-то специфичной задачи, где embedding-вектор - это не вектор, а такой embedding-feature-map (т.е. каждый \"пиксель\" feature map - это embedding-вектор), подробностей не помню.\n",
      "----------------------------------------\n",
      "Как вариант можно использовать софтмакс и распределение по уровням предсказывать.\n",
      "----------------------------------------\n",
      "Если ещё подумать, то я бы попробовал предсказывать мат ожидание и десперсию случайной величины как в VAE только на выходе. Интересно что получится :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "<@U436BP343> вот это глянь. тут тебе автор (бывший президент Кеггла) покажет в Экселе (sic!) как эти слои работают: <http://course.fast.ai/>\n",
      "----------------------------------------\n",
      "На замену ему в названиях статей или как работающему механизму?)\n",
      "----------------------------------------\n",
      "<https://arxiv.org/abs/1506.03134> - это как seq2seq, но на каждом таймстемпе декодера он аттендится на входную последовательность\n",
      "----------------------------------------\n",
      "Согласен на 100% лишь бы не заоверфитить :slightly_smiling_face: но вопрос был в том как с результатом выдавать ещё и увереность модели.\n",
      "----------------------------------------\n",
      "Мне кажется, что если мы хотим, чтобы модель умела оценивать свою уверенность, то по-хорошему, ей надо бы иметь возможность свои оценки проверять. То есть нужны не просто данные вида \"для входа x ответ y\", а \"для входа x ответ может быть от a до b\",  иначе нужной информации просто нет (кроме случая когда один и тот же вход x повторяется в выборке несколько раз с разными y)\n",
      "----------------------------------------\n",
      "<https://openreview.net/forum?id=B1KBHtcel&amp;noteId=B1KBHtcel> -  у нас не очень понятная табличка была, наша joint модель, которая больше, чем просто pointer network, была в табличке обозначена как PN - я думаю ревьюверы нет понял вообще, что именно joint model устанавливает state of the art :disappointed: Ну и один ревью был вообще ни о чем, 1 строчка - это ужасный ревью\n",
      "----------------------------------------\n",
      "Вообще, читается как довольно straightforward задача, не без трудностей, но и не видно ничего такого, для чего нужен был бы PhD по математике скрещённый с Корменом, как это обычно выглядит, когда читаешь вакансии...\n",
      "----------------------------------------\n",
      "Есть интересный вопрос. Я сейчас смотрю на загрузку GPU, особенно это видно на Inference - она близка к 0, потомучто почти всё время уходит на генерацию батчей. Как бы это сделать в параллели? Кто-то писал код на Keras который это решает? Мб ссылочки есть?\n",
      "----------------------------------------\n",
      "Тогда один шаг обучения будет выглядеть как-нибудь так:\n",
      "```\n",
      "loss, _ = sess.run(model.loss, model.train_op)\n",
      "```\n",
      "Тут не будет feed_dict и это даст немножко скорости (кажется, размер передаваемого не очень важен, важно передаешь или нет)\n",
      "Когда заработает, можно в QueueRunner все запихать и лишний код выкинуть\n",
      "----------------------------------------\n",
      "Ну, это часть \"цикла\" в котором он обещает вот-вот раскрыть все секреты и показать, как наши нейронные сети безнадёжно устарели\n",
      "----------------------------------------\n",
      "Особенно когда читаю комментарии типа \"Гениально! Продолжайте, продолжайте!\"\n",
      "----------------------------------------\n",
      "<@U1CF22N7J> нет не бред, просто графомания, не пости это сюда. когда будет про deep learning то кидай. Все ведь просто\n",
      "----------------------------------------\n",
      "разбирался кто с новой вариацией триплетов proxy loss от гугла? <https://arxiv.org/pdf/1703.07464v2.pdf>\n",
      "----------------------------------------\n",
      "Я так разобрался, что до сих пор стыдно :grinning: \n",
      "И написанная за два вечера имплементация на theano не до конца работает. Но вроде понятно как закончить.\n",
      "----------------------------------------\n",
      "Я с <@U0ZJV6E5Q> уже в кружках обсуждал, но так до конца и не понял как учить сами прокси\n",
      "----------------------------------------\n",
      "<@U4U5DNNPJ> в lasagne я положил прокси в embedding layer, как Александр предлагал. Но можно было просто матрицей обойтись.\n",
      "----------------------------------------\n",
      "Саша в кружках хорошую картинку выкладывал с пояснением как это должно быть. Видел?\n",
      "----------------------------------------\n",
      "Дан батч картинок и их меток. Для всего батча вычисляешь нейросетевые эмбединги. Метки классов картинок используешь, как индексы, чтобы взять прокси картинки из embedding слоя.\n",
      "\n",
      "Имея эти данные можно напрямую записать NCA loss.\n",
      "----------------------------------------\n",
      "Да ни у кого на её написание времени нет :good-enough:\n",
      "----------------------------------------\n",
      "Друзья, никак не могу врубиться в принцип работы рекуррентных сетей, в частности, LSTM.\n",
      "Допустим, у меня есть временной ряд длины m и мне нужно предсказать следующее значение этого ряда. Есть выборка из таких рядов и соответствующие им отклики.\n",
      "\n",
      "Если бы мы рассматривали этот временной ряд просто как вектор признаков, то как бы он проходил по нейронке с полносвязными слоями, я понимаю. Не ясен принцип, как строить рекуррентную сеть для указанной задачи. \n",
      "Сейчас будут глупые вопросы.\n",
      "1) Если первый слой LSTM, то сколько в нем должно быть нейронов? Весь временной ряд проходит через каждый нейрон этой сети?\n",
      "2) На выходе этого слоя что мы имеем?\n",
      "----------------------------------------\n",
      "То есть если члены моего временного ряда  - это просто действительные числа, то стоит один нейрон, через который последовательно проходит этот ряд. Далее либо на каждый член ряда есть некоторый ответ, которые далее также обрабатываются как последовательности, либо последний ответ, и тогда мы можем прикрутить, например, полносвязный слой. Так получается?\n",
      "----------------------------------------\n",
      "А как тогда будет тренировка выглядеть?\n",
      "----------------------------------------\n",
      "Кто может напомнить, почему в VGG / Resnet на вход идут BGR, а не RGB?\n",
      "----------------------------------------\n",
      "<@U34Q3KU8H> скорее всего как раз связано с использованием OpenCV. Там по дефолту BGR\n",
      "----------------------------------------\n",
      "а есть вообще разница для сети в каком порядке ей каналы кормить?\n",
      "----------------------------------------\n",
      "вот , я кажется понял как правильно спросить. есть батч векторов который прилетает в лосс , из embedding по соответствующим label получаем прокси, тогда для каждого примера будет положительный прокси а все остальные отрицательные, то есть если батч 128 то для каждого будет одна положительная прокси и 127 отрицательных , так ?\n",
      "----------------------------------------\n",
      "поэтому и спрашивал куда ошибку то складывать если они из embedding не придут\n",
      "----------------------------------------\n",
      "В этой статье <https://arxiv.org/abs/1611.01236> для inception применили, но там в основном рассматривали именно устойчивость к adversarial examples, а не как регуляризацию.\n",
      "----------------------------------------\n",
      "Тут сверху писали, что для батча нужно брать 1 прокси как положительную, а остальные как отрицательную. Имеется ведь в виду, что брать остальные прокси (то есть для остальных классов), а не остальные прокси для всех остальных элементов из батча?\n",
      "----------------------------------------\n",
      "Что значит получаем из сетки? Они представляют собой просто веса сети, учатся обычным backprop'ом как и остальные веса сетки.\n",
      "----------------------------------------\n",
      "Кто-нибудь подскажет ссылки на работы, когда авторы добавляли batch normalization к VGG и как это что-то меняло?\n",
      "----------------------------------------\n",
      "Какую статью не читаю, за редким исключением все тренируют с Relu, хотя есть целый зоопарк чуть более адекватных функций активаций из этого семейства. Или же как оптимизатор используют SGD + momentum, хотя у меня всегда Nadam / Adam сходились заметно быстрее. Хотя иногда можно было полирнуть в конце через SGD.\n",
      "\n",
      "Понятно, что выбор optimizer’a сильно зависит от типа данных и функции потерь.\n",
      "\n",
      "Вот вы все тоже только SGD используете при тренировке сетей?\n",
      "----------------------------------------\n",
      "Хотя если можешь себе позволить: `LR is reduced by 10x after 1.8M steps`, то, наверно пофиг как быстро что сходится.\n",
      "----------------------------------------\n",
      "как обычно проверяют сходимость параметров?\n",
      "----------------------------------------\n",
      "Adam юзаю только когда непонятная ебанина и соответственно не ясно когда уменьшать LR.\n",
      "----------------------------------------\n",
      "посоны, а поясните ньюфагу, чем tf.layers от tf.contrib.layers отличается и зачем так?\n",
      "----------------------------------------\n",
      "`tf.contrib` - это contribution от community (типа, мы (гугл) посмотрели, вроде все норм работает, да и штуки полезные, но никто ничего не гарантирует)\n",
      "\n",
      "`tf.layers` входит в основную часть tf и _вроде как есть надежда_, что из версии в версию будут поддерживать совместимость API, да и внутри все оптимально выполняется\n",
      "\n",
      "слои из `tf.contrib.layers` постепенно переезжают в `tf.layers`\n",
      "----------------------------------------\n",
      "Как вариант из головы: сохранять скользящее среднее раз в k итераций и смотреть на np.allclose с соответствующим rtol. \n",
      "----------------------------------------\n",
      "Ооо, спасибо большое!!! Наконец-то всё прояснилось. Как думаете, если попробовать другую реализацию Faster-RCNN не из mxnet, тоже с этим можно столкнуться?\n",
      "----------------------------------------\n",
      "Всем привет. Сейчас занимаюсь проблемой распознавания шоковых изображений (человеческая расчленёнка) и надо это сделать в виде одноклассовой классификации (нельзя использовать бинарную классификацию, так как множество всех не-шок картинок бесконечно и неизвестно как делать репрезентативную выборку). У меня достаточно скромный датасет - 1500 для обучения, по 500 для тестирования и валидации. Первоначальная идея была использовать свёрточные автокодировщики, где ошибка восстановления шоковых изображений будет меньше, чем ошибка восстановления любых других изображений и можно будет установить некий порог ошибки. Первоначально за кодировщик использовал VGG16, но сработало плохо. После множества экспериментов хорошие результаты показала сеть, где по 4 слоя tanh/maxpool на кодировщике и декодировщике (120 эпох, есть data augmentation), но проблема в том, что ошибка восстановления вообще не зависит от класса. Мне советуют использовать VAE, но боюсь на таких данных он будет работать ещё хуже. Подскажите, пожалуйста, какие-либо статьи по одноклассовой классификации изображений. Знаю про BoW+SVM, но хотелось бы использовать глубокое обучение.\n",
      "----------------------------------------\n",
      "<@U041P485A> каким образом? cnn не рассчитан на обучение по одному классу. я читал на stackoverflow идею обучить сеть на одном классе и поставить сигмоиду вместо softmax на последний слой, но нигде в статьях этого не читал. Фактически это выглядит как сомнительная бинарная классификация без одно класса.\n",
      "----------------------------------------\n",
      "<@U1YK4SV6W> а где датасет шок-контента собирал :youknow: ?\n",
      "----------------------------------------\n",
      "Если я правильно помню, в случае с One-Class SVM идея упирается в то, чтобы регуляризацией давить вообще все предсказания в ноль, но при этом штрафовать ошибки на сампле как обычно. Т.е. роль \"второго класса\" играет условный ноль, к которому модель пытается стащить все свои предсказания кроме сампла.\n",
      "\n",
      "Мне кажется что тот же принцип мог бы быть применим вне зависимости от выбранной модели, дип она или не дип.\n",
      "----------------------------------------\n",
      "В случае с deep-ом наверное идеологично оставить все промежуточные слои как обычно, а давящую регуляризацию весов привинтить в самом конце.\n",
      "\n",
      "Получится по сути то же что и в случае с SVM только фичеры будут браться (и возможно выучиваться) из глубокой сети.\n",
      "----------------------------------------\n",
      "<https://opendatascience.slack.com/archives/C040HKJF1/p1492339850507115> как можно было бы задиплернить штуку, собирающую осмысленные картинки вроде этих из каких-то суперпикселей, представленных маленькими картиночками?\n",
      "----------------------------------------\n",
      "а почему все же нельзя использовать бинарную классификацию? какая-то выборка не-шок картинок все равно ведь нужна чтобы оценить качество работы\n",
      "----------------------------------------\n",
      "Я пробовал Clockwork RNN, что почти тоже самое - на задаче классификации последовательностей работало не очень, хуже, чем LSTM. Есть еще PhasedLSTM на таком же принципе, и ClockworkLSTM - их не пробовал, хз как они будут работать.\n",
      "----------------------------------------\n",
      "котаны, а какой наименее геморройный и наиболее быстрый способ перестать бояться и начать файнтьюнить инсепшены/резнеты в :tensorflow: ? slim?\n",
      "----------------------------------------\n",
      "от этого как раз и :bombanoolo:  , так то можно все и самому собрать, но хочется понять максимально быстро, имеет ли смысл в таких времязатратах:)\n",
      "----------------------------------------\n",
      "Я для двух авитовских конкурсов успешно файнтюнил и инсепшны и резнеты из двух разных подрепов гугла, но не разобрался при этом ни в одном из них, понятней стало только когда начал руками делать обучение и вывод картинок в тензорборде для u-net с нуля\n",
      "----------------------------------------\n",
      "<https://github.com/tomrunia/ClockworkRNN> - я делал свою имплементацию на базе вот этой, только, насколько я помню, там был какой-то баг с периодами, непомню точно какой :disappointed:\n",
      "----------------------------------------\n",
      "Я бы попробовал как гипотезу слова синонимы позаменять.\n",
      "----------------------------------------\n",
      "Всем привет! А какой сейчас самый быстрый фреймворк для RNN’ок? Tensowflow все еще отстает от торча и теано? Или уже нагнал и перегнал? Не смог найти свежих бенчмарок на эту тему.\n",
      "----------------------------------------\n",
      "привет, играюсь с задачкой Intel Cervix с Kaagle, и думаю над таким вопросом - поиграться с размером batch при тренинге при этом меняя learning_rate для SGD, и вот какая там зависимость получается ? если батчи побольше, то lr тоже побольше ? например был batch=8 и lr=0.001 , а если batch=16 то lr=0.01, есть в этом какой то смысл ? спасибо\n",
      "----------------------------------------\n",
      "кто нибудь пробовал MR-RNN ?\n",
      "----------------------------------------\n",
      "товарищи, а как в Keras взять модель, загрузить в нее заранее посчитанные веса и присобачить наверх другой модели?\n",
      "----------------------------------------\n",
      "я посмотрел на то, как автор предлагает расширять модель - <https://github.com/fchollet/keras/issues/4040>\n",
      "----------------------------------------\n",
      "я кстати попробовал по-быстрому перейти на второй керас но там с весами стандартными под resnet вообще дичь какаято. теановские веса не добавлены а присутствуют только  для tf но при этом все равно используются когда у нас бэкэнд th. в итоге обучение замедляется в десятки раз...\n",
      "----------------------------------------\n",
      "есть ли здесь те, кто делал siamese network, в которой каждая из двух веток - resnet или какая-то другая подобная (по размеру) сеть без последних полносвязных слоев?\n",
      "----------------------------------------\n",
      "как я понимаю, это все очень затратно и по памяти и по времени, правильно?\n",
      "----------------------------------------\n",
      "как думаете, какой объем картинок будет достаточен, чтобы добиться более-менее точности?\n",
      "----------------------------------------\n",
      "<@U1BAKQH2M> ну вдруг выходной слой жирнее становится, но это как организуешь\n",
      "----------------------------------------\n",
      "а какие ресурсы нужны на finetuning resnet'а?\n",
      "----------------------------------------\n",
      "и еще, какой объем картинок нужен для обучения siamese с двумя resnet?\n",
      "----------------------------------------\n",
      "<@U041P485A> 4гб - это resnet50 с каким размером батча? 8?\n",
      "----------------------------------------\n",
      "и такой вот нубский вопрос: как правильно курится resnet50? картинки грузятся в память батчами? а если учиться только на CPU?\n",
      "----------------------------------------\n",
      "эм, от фреймворка же зависит. но вообще обычно готовят батчи на CPU а потом грузят в GPU, где делают forward-backward и шаг оптимизатора\n",
      "----------------------------------------\n",
      "и в догонку вопрос: как распараллелить resnet.predict? keras+theano. Заюзать tf?\n",
      "----------------------------------------\n",
      "Оффтоп от темы параллельных вычислений. Немного нубовские вопросы интересуют. Подскажите, как выбрать метод оптимизации для задачи классификации изображений, используя модели типа resnet50, inception v3? Различные методы находят минимумы с разной скоростью (Adadelta, Adam). Какие методы вы используете и почему? И как правильно подобрать learning rate, на скольки эпохах?\n",
      "----------------------------------------\n",
      "В mxnet вроде всякие такие штуки можно делать ещё <https://arxiv.org/abs/1604.06174> а как с этим в tf?\n",
      "----------------------------------------\n",
      "<@U06K9ELB1> а почему это важно?\n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> собственного нет, как и нет собственного кластера из машинок, если бы был, то и код бы был. В туториалах тоже рабочий код есть. Лично я видел бенчмарки lstm-мо подобного кода на три машины, каждая с 8 GPU (причем там могло быть 3*dgx-1, поэтому всё хорошо было с сетью и обменом информации между GPU, но я не могу сейчас найти точную ссылку). И выглядел он понятно.\n",
      "А вообще можно засамонить <@U0XR20SA1> . У него есть код:) \n",
      "----------------------------------------\n",
      "аналогично тем постам как уже есть\n",
      "----------------------------------------\n",
      "я могу про deeplearning4j накатать, но им точно мало кто пользуется :java:\n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> для тех, у кого есть кластер и тех, кто смог distributive TF поставить - проблема реально для левой пятки:просто запустил, указал ip:port воркеров, сказал эту считать тут, а это тут с помощью with (ну да, тут нужно небольшое понимание архитектуры, чтобы не сделать глупость) и готово\n",
      "----------------------------------------\n",
      "из ноутбука норм работает. я как раз оттуда и запускал. \n",
      "только вопрос возник -- он же фичимапы рисует не по-настоящему выходит? т.е. он берет картинку и применяет только фильтры из выбранного слоя, игнорируя преобразования которые идут до?\n",
      "----------------------------------------\n",
      "И если писать не а фрэймворке, а как, например используя его, с примерами кода подъехать к задаче про амазонские леса - это будет более востребовано, как минимум потому что халявщиков, которые хотят натянуть амазонию гораздо больше, чем специалистов по NN которым это может быть интересно.\n",
      "\n",
      "+ байки про классификацию амазонок для бонуса.\n",
      "\n",
      "Артур, вроде хотел показать как используя mxnet можно взгромоздится на тюленя и уехать в топ кагла, но замотался.\n",
      "----------------------------------------\n",
      "Хайп начнется, когда мы с Романом разберемся что к чему и сможем задачки закидывать железом, через mxnet\n",
      "----------------------------------------\n",
      ":likeaboss: когда раздел хайп вокруг mxnet, не показывая с его помощи никаких результатов \n",
      "----------------------------------------\n",
      "Что меня, что его останавливает, что на Керасе все между просто и очень просто, как в силу привычки, так и замечательного API, и работая с ним я чувствую себя умным, а читая доки по mxnet я начианаю сильно сомневаться в своих умственных способностях.\n",
      "----------------------------------------\n",
      "Я пока загорелся mxnet, ну и тот факт, что существует такой человек как Артур мотивирует в нем таки разобраться, надо просто сильно себя озадачить.\n",
      "----------------------------------------\n",
      "А кто-нибудь mxnet в проде использует? Мне tensorflow как раз нравится не только скоростью разработки, но и простотой развертывания в проде (через serving). Я на текущей работе продвинул serving, теперь его используем. До этого были модели на mxnet, но там все так неудобно было - свой враппер на плюсах, подключается как либа, еще и модель каждый раз заново с диска грузится, короче неудобно.\n",
      "----------------------------------------\n",
      "Будет работа где надо будет развертывать в prod - будет tensorflow,\n",
      "\n",
      "Но чисто под :kaggle: так чтобы бысто и на несколько GPU из коробки без боли - это вроде mxnet\n",
      "----------------------------------------\n",
      "Френз, в статье про U-Net авторы всё пишут, что ей нужно \"very few training images\". Кто реально использовал, very few - это какого порядка размер датасета?\n",
      "----------------------------------------\n",
      "Зависит от степени упоротости данных, если как в спутниках, где у снимков адский динамический диапазон и вообще глобально разные виды на картинках, то лучше побольше.\n",
      "----------------------------------------\n",
      "&gt;адский динамический диапазон\n",
      "<@U1CF22N7J> что имеется ввиду? Dynamic range как количесство полутонов картинки?\n",
      "----------------------------------------\n",
      "Скажите, а как в сверточную сеть (картинки на входе) подмешать ещё своих признаков? На ум приходит только выдирать признаки с последнего слоя сети и обучать отдельную модель на этих признаках + свой вектор. \n",
      "----------------------------------------\n",
      "у менять есть похожая задача, только там один источник - картинка, второй - временной ряд\n",
      "если я обучаю все сразу, результаты получаются такие же как если бы использовал только временной ряд\n",
      "если обучаю отдельно, потом вставляю веса и дообучаю - получается норм\n",
      "----------------------------------------\n",
      "<@U49NJJXNJ> Как обычно все зависит от задачи.\n",
      "----------------------------------------\n",
      "<@U49NJJXNJ> На какую именно тему?\n",
      "----------------------------------------\n",
      "как объединять разнородные данные в dl\n",
      "----------------------------------------\n",
      "а как загуглить? data fusion? \n",
      "или может были на кагле подобные соревнования, как вот авитовское?\n",
      "----------------------------------------\n",
      "Друзья, а как ретрейнутый граф тф в керас загрузить? :confusedparrot: \n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> а мне понравилось тулза, которые слои визуализирует. для кераса она работает. а для тф -- не знаю. подумал, что проще будет сконвертить, чем разбираться с тем, как оно с тф работает-то\n",
      "----------------------------------------\n",
      "Коллеги, а есть кто-нибудь, кто реализовывал/воспроизводил результаты статьи \"No fuss distance metric learning Using proxies\" (<https://arxiv.org/pdf/1703.07464.pdf>)?\n",
      "----------------------------------------\n",
      "<@U042UQC96> привет, привет! :smiley: а в каком месте не воспроизводится? у меня на данный момент получилось, что NMI и R@1 на CUB200 +- такой же (значения топовые для модельки), на Stanford CARS 196 где-то чуть лучше уровня стандартных. Мой NCA loss без нормализации в отличие от того, который ты скидывал. И ещё из неприятных моментов - если у авторов статьи R@1 непрерывно растёт,то у меня он растёт на протяжении 4 эпох, а потом начинает падать. Аналогично и с NMI. Меня смущает, что не указано для CUB, Cars196 там они используют static/не static + количество проксей. А у тебя как дела обстоят на текущий момент?\n",
      "----------------------------------------\n",
      "Вяло делаю в свободное время и пока ещё ищу баги. На cars NCA loss снижается на протяжении 50 эпох, затем растёт. На новых классах r@1 стабильно ниже 0.1.\n",
      "А есть способ понять место где не воспроизводится?\n",
      "----------------------------------------\n",
      "Не воспроизводится в результатах, хехе) плюс в поведении r@1 как на графиках, ну, эт то что на поверхности. Поигрался с регуляризацией в знаменателе nca loss-не особо влияет на общую картину. Но,повторюсь, у меня ненормализованный вариант - в статье они говорили, что на практике на результаты эт не оч влияет(дословно не помню). Вообдем надо будет еще попробовать с нормализацией, посмотреть что из этого выйдет\n",
      "----------------------------------------\n",
      "Здравствуйте! Тоже вопрос по одной работе :slightly_smiling_face:\n",
      "Знаком кто-то с этим алгоритмом (<https://arxiv.org/pdf/1506.04878.pdf>)? Можете объяснить как точность меряется?\n",
      "----------------------------------------\n",
      "Привет\n",
      "\n",
      "А если у меня стоит видюха Intel® Haswell Desktop , и у Intel есть OpenCL для работы с GPU, но только для Виндоус :disappointed: а у меня Дебиан Линукс, то как быть? У вас у всех стоят GeForce? (аренда инстанса на Амазоне пока не расматривал)\n",
      "----------------------------------------\n",
      "В интеле точно делали свой фреймворк для DL, и там вроде как заявлена поддержка OpenCL. Не знаю правда, как у них сейчас дела. \n",
      "----------------------------------------\n",
      "Привет) А не подскажете такие статьи, в которых нейросеть \"широкой специализации\" управляла бы нейросетями \"узкой специализации\"?  Как, типа, менеджер, который знает, кто лучше справится в данной ситуации - и тому кидает задачу. Ответ берется как функция ответов экпертов, назначенных \"менеджером\" на этот таск. Обучаются все с нуля.\n",
      "----------------------------------------\n",
      "Берем простую двухслойную сеть и смотрим на неё под следующим углом:\n",
      "\n",
      "Каждый нейрон первого слоя суть \"эксперт узкой специализации\". Нейрон выходного слоя - это \"менеджер\", который знает \"кто лучше справится\", и \"кидает задачу\" именно им (это определяется весами, которые влияют и на влияние каждого \"эксперта\" на предсказание, и на то, кто из \"экспертов\" будет больше платить за ошибки).\n",
      "----------------------------------------\n",
      "Ну тогда разберись чего ты хочешь добиться, потому что исходя из твоей спецификации так вполне \"идет\".\n",
      "\n",
      "Более того, одно из самых понятных обоснований, поясняющих почему нейросети осмысленны - как раз вот такое.\n",
      "----------------------------------------\n",
      "В областях вне нейросетей многослойность не является столь естественным аспектом, и там можно найти более специфичные методы, где действительно \"отдельно тренируются эксперты или фичер-экстракторы, отдельно делается комбинация оных\". Самое простое - тот же стекинг. Из более специфичного сейчас приходит в голову клевый метод из kernel methods где бралось несколько кернелов и тренировался новый кернел, являющийся взвешенной комбинацией оных. Это всё впихивалось в SVM и выглядело довольно наукоемко.\n",
      "\n",
      "Но опять же, с колокольни именно нейросетевого диплернинга - это всё просто end-to-end learning глубокой сети.\n",
      "----------------------------------------\n",
      "Странно кстати что они софтмаксом веса скалируют, логичнее имхо их рассматривать как отдельные бинарные классификаторы.\n",
      "----------------------------------------\n",
      "Кто понял из сайта, будут записи?\n",
      "----------------------------------------\n",
      "Вообще по-моему такого типа input-dependent stacking можно куда угодно прикручивать (пусть не везде получится end to end, всё равно лишняя простая нелинейность). Надо чтобы на кагле кто-нибудь попробовал, хватит уже линейно стакать хгбусты, нужна революция!\n",
      "----------------------------------------\n",
      "На саму конфу не отпустят с работы, а вот пересечься с теми кто поедет за рюмкой чая я бы пересекся.\n",
      "----------------------------------------\n",
      "кто писал кастомный генератор для Keras есть вопрос, падает после нескольких эпох и причина не понятна?\n",
      "----------------------------------------\n",
      "да насчёт неполных батчей определённо идея, но вот сейчас не упало почему то… конец эпохи я не знаю :disappointed: или пока не нашёл метод который за это должен отвечать\n",
      "----------------------------------------\n",
      "Как не знаешь? Твой же генератор (функция trainGen) по данным идет, как до конца дошел, вот эпоха и закончилась\n",
      "----------------------------------------\n",
      "Да, верно, но это ничего не меняет, новый параметр это старый параметр // размер батча.\n",
      "\n",
      "Но твоя-то функция которая батчи генерит, она-то знает когда данные закончились (она не может не знать, она по ним в цикле идет).\n",
      "\n",
      "И да, неполные батчи керас не умеет \"All arrays should contain the same number of samples.\"\n",
      "----------------------------------------\n",
      "Всем привет! Мы в DeepSystems запустили сервис рекомендации фильмов на основе Deep Learning.\n",
      "\n",
      "<https://movix.ai/>\n",
      "\n",
      "В ближайшее время планируем опубликовать технические посты и лекции о том, как мы это сделали.\n",
      "\n",
      "Не стесняйтесь, тестируйте. Будет рады узнать ваше мнение и критику :slightly_smiling_face:\n",
      "\n",
      "Вот немного о сервисе:\n",
      "\n",
      "<https://thenextweb.com/apps/2017/04/26/movie-reccomend-artificial-intelligence/#.tnw_Y76TLVpI>\n",
      "----------------------------------------\n",
      "а где La La Land?:confused:\n",
      "----------------------------------------\n",
      "Привет! Не подскажите как меряют точноть в задачах object detection? Есть координаты правильных bounding box’ов и предсказанных.\n",
      "----------------------------------------\n",
      "Кому-нибудь попадался туториал для чайников как перебить в mxnet SSD или Faster RCNN чтобы это тренировалось на другом датасете? Там какие-то нюансы с созданием rec файлов и что-то где-то у меня идет не так.\n",
      "----------------------------------------\n",
      "<@U07V1URT9> Как эксперт по mxnet, тебе ничего такого не попадалось?\n",
      "----------------------------------------\n",
      "Пока что просто задача стоит так, что надо сначала найти нужные объекты (они разных классов, но это пока неважно). Можно было бы использовать и сейчас mAP и потом, когда уже придем к распознаванию классов, просто использовать ту же метрику.\n",
      "----------------------------------------\n",
      "Кто-то встречал работы в которых объясняется как одноцветный фон изображений  влияет на процес обучения в сверточных сетях? Например, у нас есть датасет где много изображений не квадратны, соответственно чтобы их поместить в квадрат мы вертикальное или горизонтальное оствшееся место заполняем каким-то цветом. Иногда фон может занимать больше 50%. Я вот заметил, что черный цвет это плохо, белый - хорошо (у меня были почти все изображения где фона больше, и при черном я получил оверфит тупо в один класс, в то время как с белым фоном все было ок). Моя нубская догадка в том что при белом цвете (из-за максимального значения), мы находимся в максимуме из которого проще скатиться в минимум и наоборот для черного. Что вы думаете?\n",
      "----------------------------------------\n",
      "Ок, попробую на своем датасете все три(noise, mirroring, full scale) подхода и отпишусь какой дал лучшие результаты\n",
      "----------------------------------------\n",
      "<@U0G29N5U4> Я так понимаю, когда сдвигают и есть пиксели вокруг - то конечно надо брать их\n",
      "----------------------------------------\n",
      "Ну, почему для этого хочется вырезать?\n",
      "----------------------------------------\n",
      "Всем привет! Как и обещали, выкладываем первый пост с техническими аспектами о найшей рекомендательной системе фильмов Movix\n",
      "\n",
      "<https://medium.com/deepsystems-ru/movix-ai-%D1%80%D0%B5%D0%BA%D0%BE%D0%BC%D0%B5%D0%BD%D0%B4%D0%B0%D1%86%D0%B8%D0%B8-%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D0%BE%D0%B2-%D0%BD%D0%B0-%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%B5-deep-learning-6f8917666ad9>\n",
      "----------------------------------------\n",
      "кто что знает про Running Minimum stopping criteria?\n",
      "----------------------------------------\n",
      "Может кто знает, как в keras/tensorflow решить проблему, когда мне в кастомной функции лосса нужны дополнительные Variables, которые участвуют в подсчете значения лосса и по которым также должен идти градиент?\n",
      "----------------------------------------\n",
      "<@U1BAKQH2M> как это решит проблему с доп. переменными? я их включаю в лосс, но они остаются константными\n",
      "----------------------------------------\n",
      "что за хрень, почему lasagne так отстал от жизни? там депенденси theano 0.8.0 до сих пор\n",
      "----------------------------------------\n",
      "Сейчас Сережа скажет: Фреймворки переписывают только те, кто сразу хорошо писать не может :keras:\n",
      "----------------------------------------\n",
      "В последние недели мучаюсь со стэнфордским курсом cs231n, где учат, как писать сетки с нуля на numpy и без крутых библиотек.\n",
      "Если feed forward я вполне осилил, то на CNN я сдался - forward pass для convolutional layer ещё смог написать, но с backward уже не смог справиться.\n",
      "В самом курсе это описывается совсем мельком. Про backprop для этого случая вообще одно предложение написали - типа это легко, надо только повернуть фильтры...\n",
      "Что можно почитать, чтобы научиться понимать это? Или учиться самостоятельно писать backprop на практике особо не нужно (достаточно посмотреть как это сделали другие и понять)?\n",
      "----------------------------------------\n",
      "какой лосс может быть лучше, если не закапываться в метрик лернинг?\n",
      "----------------------------------------\n",
      "<@U4ZRFG1V0> так по аналогии сделай, там же надо суть уловить, а не то, как он применяется к конкретному кейсу\n",
      "----------------------------------------\n",
      "Как у Keras c tf backend сказать tf, что вот это GPU можно использовать, а вот это трогать не надо?\n",
      "\n",
      "А то сейчас tf резервирует под себя оба GPU, а считает только на одном.\n",
      "\n",
      "В th это решалось ` THEANO_FLAGS=mode=FAST_RUN,device=gpu1,floatX=float32 python resnet.py`\n",
      "----------------------------------------\n",
      "Spatial? Я смог понять, как это делать, но никак не мог красиво реализовать. Написал с циклами, сильно больше 5 строк. А потом нагуглил решение в 2 строчки с использованием transpose, о чем я раньше не знал.\n",
      "----------------------------------------\n",
      "avasilev: Очень этот вопрос тоже интересует. Я понимаю, как раскидывать различные операции по разным GPU, но вот так чтобы одной строкой как в mxnet или pytorch, так я пока не умею. :disappointed:\n",
      "----------------------------------------\n",
      "привет, один товарищ спрашивает, можно ли решить задачу распознавания цвета волос (по шкале от 1 до 10) по снимку обычной камерой в обычных условиях. датасет готов собирать, какой нужен будет (например, снимок волос и рядом образец известного цвета). я так понимаю, тут проблема в текстуре и балансе белого может быть. кто-то что-то подобное делал или слыхал?\n",
      "----------------------------------------\n",
      "Зависит от того, какие настройки и какой white balance\n",
      "----------------------------------------\n",
      "но вопрос в том, сталкивался ли кто с этим уже? может, баланс белого уже сетки научились определять на снимке\n",
      "----------------------------------------\n",
      "тестовые - нет. тренировочные - как скажете, я так понял. я не знаю почему, но не хотят давать палитру, хотят сфотать и чтобы все само\n",
      "----------------------------------------\n",
      "Кажется на Керас в качестве основного фреймворка для глубокого обучения рассчитывать не стоит :disappointed: Почти все реализуют свои идеи в чём угодно, только не в нём, а сторонние реализации все как одна кривые до ужаса. Видимо сказывается лёгкость освоения и привлекает тем самым всех индусов, как легкий вариант по умолчанию.\n",
      "----------------------------------------\n",
      "<@U0AD1L5NC> да с калибровочной шкалой и я бы осилил, но поэтому и спрашиваю тут, что чуваку хочется сервис без нее. я сам далеко не уверен, но почему бы не спросить\n",
      "----------------------------------------\n",
      "Ну если датасет есть, почему не попробовать\n",
      "----------------------------------------\n",
      "У меня нескромный вопрос -- где там батч-сайз устанавливается?\n",
      "----------------------------------------\n",
      "Короче надо на MXnet все это пилить, но я никак не пойму как данные скариливать сети.\n",
      "----------------------------------------\n",
      "я одно не пойму: почему не запустить генерацию данных на диск, а потом с него читать, когда гоняешь с разными параметрами? тормозит же жестко...\n",
      "----------------------------------------\n",
      "Блин, какой ужас, там весь код делает не то, что написано\n",
      "----------------------------------------\n",
      "Да чот не особо дохрена...\n",
      "----------------------------------------\n",
      "Я пока не смог допилить. На уровне подавать данные правильно. Плюс там вроде аугментации нет через D4, как есть в Керасовской, а данных маловато.\n",
      "----------------------------------------\n",
      "Меня не отпускает тот факт, как быст mxnet и как хорошо он параллелится. Я еще не отступился от идеи натравить его на ImageNet и на AWS на 8 GPU, ну и хоть сабмит сделать.\n",
      "----------------------------------------\n",
      "Привет! Помогите, пожалуйста. Как установить tflearn для анаконды?\n",
      "----------------------------------------\n",
      "господа, а кароч амазон пишет что у них к80, но в версии p2.xlarge доступно 12 гб, на сколько я помню в к80 просто две к40 каждая по 12, это они типа программно блокируют доступ ко второй половине и кто то юзает вторую на одной железяки? или чо?\n",
      "----------------------------------------\n",
      "Только собрал под этим вашим линуксом yolo, начал разбираться как этим пользоваться и переучить на другие классы, как оказалось, что единственная нормальная документация у порта на винду :but_why: (и что такой порт вообще существует, где ты раньше был!)\n",
      "----------------------------------------\n",
      "и что процесс обучения идёт нормально? у меня линукс версия крашилась по непонятным причинам… где то в недрах Yolo\n",
      "----------------------------------------\n",
      "привет. вопрос к тем, кто шарит в керасе. там есть какой-нибудь функционал, который позволит разделить натренированную сиамскую сеть? хочется написать это одной-двумя простыми строчками\n",
      "----------------------------------------\n",
      "А под какие framework'и есть имплементация Mask R-CNN ?\n",
      "----------------------------------------\n",
      "<@U43FTJQ2V> мне кажется в README будет круто добавить про то как pretrained веса помогают быстрее сходится\n",
      "----------------------------------------\n",
      "Вопрос по tf.\n",
      "\n",
      "У меня есть список bounding boxes и список соответствующих вероятностей.\n",
      "\n",
      "У каждой картинки разное число bounding boxes.\n",
      "\n",
      "То есть на входе что-то вот такое:\n",
      "```\n",
      "new_probs\tshifted_boxes\n",
      "0\t[0.97138309, 0.89845979, 0.9296, 0.8547526, 0....\t[[48, 48, 96, 80], [448, 0, 480, 32], [432, 10...\n",
      "1\t[0.99968529, 0.9976089, 0.99978441, 0.89575303...\t[[96, 320, 144, 400], [96, 314, 144, 394], [11...\n",
      "2\t[0.99892837, 0.85288972]\t[[1490, 1804, 1538, 1900], [1484, 1804, 1532, ...\n",
      "3\t[0.88441133, 0.99754095, 0.99913651, 0.9104014...\t[[400, 32, 448, 112], [176, 1016, 224, 1096], ...\n",
      "4\t[0.96327102, 0.98539644, 0.88011599, 0.8040579]\t[[224, 1400, 272, 1512], [224, 1394, 272, 1506...\n",
      "5\t[0.81249487]\t[[692, 516, 756, 628]]\n",
      "6\t[0.93350941, 0.86235744, 0.89811134, 0.8266245...\t[[48, 368, 80, 416], [64, 368, 80, 400], [176,...\n",
      "7\t[0.95371836, 0.938133, 0.99871385, 0.89310324,...\t[[16, 1192, 112, 1240], [160, 522, 240, 570], ...\n",
      "8\t[0.9980374, 0.98452294, 0.99228573, 0.99924421...\t[[1224, 1128, 1272, 1240], [1240, 1256, 1288, ...\n",
      "9\t[0.99193442, 0.87001282, 0.89822716, 0.9991089...\t[[304, 32, 352, 112], [272, 1192, 320, 1272], ...\n",
      "10\t[0.92838627, 0.99157351, 0.90044743, 0.9988871...\t[[1416, 32, 1464, 128], [1288, 1208, 1336, 130...\n",
      "11\t[0.91346675, 0.95700151, 0.99657744, 0.9861850...\t[[416, 1602, 448, 1682], [416, 1596, 448, 1676...\n",
      "12\t[0.99615979, 0.98410827]\t[[1288, 224, 1400, 288], [1298, 224, 1394, 288]]\n",
      "13\t[0.82114112, 0.93216109, 0.92051238, 0.8621713...\t[[0, 1314, 32, 1346], [1320, 320, 1352, 368], ...\n",
      "14\t[0.80408633]\t[[682, 16, 730, 96]]\n",
      "```\n",
      "\n",
      "Как это пропустить через `tf.image.non_max_suppression` ?\n",
      "\n",
      "проблема в том, что число bounding boxes в каждом ряду разное, и вследствие этого я не пойму как определить Variables, которые идут на вход `non_max_supression`\n",
      "----------------------------------------\n",
      "Хочется картинку, про val loss как функцию от числа итераций. С одной кривой за dice, с другой за softmax, ну и третий за сумму :slightly_smiling_face:\n",
      "\n",
      "Там сразу будет видно кто быстрее сходится и у кого итоговый результат выше. (Я сам думаю такую набить, но все руки не доходят.)\n",
      "----------------------------------------\n",
      "Бог с ним, а разрезе tech report про спутники, я сам прогоню на данных с нормальной валидацией. Уж больно интересно как добавление dice меняет поведение на задачах сегментации. У нас сетки похожие, сразу и посмотрим.\n",
      "----------------------------------------\n",
      "хмм, теперь осталось понять, как найти scores от этих профильтрованных bboxes…\n",
      "----------------------------------------\n",
      "bboxes_selection это не отфильтрованные ббоксы, и их индексы, так как tf.image.non_max_suppression возвращает не сами ббоксы, а именно индексы, gather потом выбирает по индексам ббоксы в ячейке 8\n",
      "----------------------------------------\n",
      "<@U04ELQZAU> если карта может кушать данные быстрее, чем линейно читает хдд -- хоть как готовь, не успеешь. А хдд читает всего 100-200мбайт/сек (зависит от модели/места на блине)\n",
      "----------------------------------------\n",
      "настоящую асинхронную подготовку батчей сложно сделать, не обойдешься тыканьем async/await, как с сетевой асинхронностью. кажется, подход <@U0ZJV6E5Q> должен быть оптимальным трейдоффом между простотой и эффективностью\n",
      "----------------------------------------\n",
      "Кто мне расскажет, как можно аугментировать данные для локализации / детекции.\n",
      "Сходу в голову приходят:\n",
      "\n",
      "[1] D4 group\n",
      "[2] translation\n",
      "[3] zoom in / out\n",
      "[4] random channel shift\n",
      "\n",
      "Как обстоит ситуация с поворотами? На небольшие углы, наверно, можно bounding box для повернутого изображения брать как максимальное вложение от повернутого исходного прямоугольника. А если хочется на 45 градусов? По новой вручную обводят?\n",
      "----------------------------------------\n",
      "Ну, ладно, половина фрейморков раз в полгода ломают апи или меняют/добавляют/убирают всякие промежуточные уровни, как пример -- в релизе 1.1 тензорфлоу всё поменялось, керас 2 -- много поменялось, теано переходит на либгпуаррей, торч съезжает с луа на питон, про менее популярные вообще молчу\n",
      "----------------------------------------\n",
      "Вот тут много интересной аугментации - в 2.2. и в 3.6. <https://arxiv.org/pdf/1512.02325.pdf> - растяжения-сжатия, дорисовка полей вокруг картинки средним цветом (чтобы объекты относительно полного изображения более мелкими были) и тп\n",
      "Ещё вот тут <https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf> и тут <https://arxiv.org/abs/1312.5402> разные цветовые искажения, яркость-контрастность, блюр и прочее\n",
      "<https://github.com/kevinlin311tw/caffe-augmentation>\n",
      "<https://github.com/ShaharKatz/Caffe-Data-Augmentation>\n",
      "\n",
      "Повороты на небольшие углы - обычно хватает просто брать новый bounding box, так чтобы \"повёрнутый\" в него полностью входил, можно подзапариться и оставлять в разметке повёрнутые боксы, помимо координат предсказывая угол поворота (что-то такое <https://arxiv.org/abs/1612.02742> (upd: ссылку обновил на правильную)). Можно также (если это делается на более поздних этапах обучения, когда сеть уже что-то детектирует) просто повёрнутые изображения пропускать через детектор и брать наилучшие предсказанные окна (в плане уверенности и в плане пересечения со старой разметкой) в качестве новой разметки.\n",
      "----------------------------------------\n",
      "Я к Packt Publishing слегка скептически отношусь, т.к. их редакторы регулярно спамят предложениями типа \"я смотрю вы в гитхаб выложили пару строчек на питоне, а мы как раз хотим издать книгу про использование Питона в гитхабе. Не хотите ли стать автором?\".\n",
      "----------------------------------------\n",
      "Имя автора как из Италии или Испании -- что не сильно лучше :kekeke:\n",
      "----------------------------------------\n",
      "Как перевести на английский комитет нейронных сетей?\n",
      "----------------------------------------\n",
      "Привет, а где можно найти, что значат теги в аннотациях PASCAL VOC? Интересует sub-tag &lt;depth&gt; в &lt;size&gt;, &lt;segmented&gt;, &lt;truncated&gt;, &lt;difficult&gt;.\n",
      "Хочу сгенерировать аннотации для своих данных, но непонятно что записывать в эти теги\n",
      "----------------------------------------\n",
      "<@U411PKASW> на сайте PASCAL есть PDF, с каждым датасетом идёт вроде как\n",
      "----------------------------------------\n",
      "<@U0FF52P7D>, а как в питоне такой кеширующий меммпа сделать? \n",
      "Вообще по хардкору - это просто загрузить весь датасет в оперативку да и всё :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "Подскажите, а как правильно строить граф в tensorflow, если есть разные части для train'а и test'а (но различия очень маленькие)? \n",
      "\n",
      "В документации есть такой код:\n",
      "```if mode == \"train\":\n",
      "  helper = tf.contrib.seq2seq.TrainingHelper(\n",
      "    input=input_vectors,\n",
      "    sequence_length=input_lengths)\n",
      "elif mode == \"infer\":\n",
      "  helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
      "      embedding=embedding,\n",
      "      start_tokens=tf.tile([GO_SYMBOL], [batch_size]),\n",
      "      end_token=END_SYMBOL)```\n",
      "\n",
      "Как этим пользоваться?\n",
      "что такое здесь mode? Это переменная, которая передается в функцию построения графа? Или надо написать tf.cond для mode (или что-то в этом роде)?\n",
      "----------------------------------------\n",
      "Как правильнее настраивать гиперпараметры нейронок и какие из них важнее?\n",
      "----------------------------------------\n",
      "Подскажите, почему у меня высасывает всю оперативку когда создаю батчи через np.load\n",
      "----------------------------------------\n",
      "&gt; А если хочется на 45 градусов?\n",
      "\n",
      "А почему каждый угол известного bounding box нельзя просто повернуть на тот же угол?\n",
      "----------------------------------------\n",
      "нет, сюда я пришел после того как прочел доку Кераса. Просто по писанию оттуда я не совсем представил как оно работает. Я понимаю что такое обычная свертка, но впервые слышу про spatial convolution и pointwise convolution.\n",
      "----------------------------------------\n",
      "Спасибо, так и сделаю. Думал, может какой более хитрый способ есть.\n",
      "----------------------------------------\n",
      "Никому не попадались попытки научить сетки решать задачки вроде этой? Я думаю, что кто-то пытался, скорее интересно, есть ли какие успехи в этом напрвлении?\n",
      "\n",
      "<https://opendatascience.slack.com/files/ternaus/F3J3V3X0T/matrix15n.gif>\n",
      "----------------------------------------\n",
      "И скорее это не он нас обошел, а мы его пытались догнать всю дорогу. В отличие от остальных участников, которые каждый день меняли места на LB, он как в начале ушел на первое место, так оттуда и не слезал.\n",
      "----------------------------------------\n",
      "Есть вероятность что наш OpenDataScience спутают с каким то другим OpenDataScience который захватил хештег и твиттер акканут\n",
      "----------------------------------------\n",
      "сейчас обкатываю две куды в разных папках, stay tunder.\n",
      "----------------------------------------\n",
      "Там же можно переменную окружения указать, где куднн лежит\n",
      "----------------------------------------\n",
      "дыа. но тфло глючило. если поставить куду в две разные папки и сделать там разные куднн - всё работает.\n",
      "----------------------------------------\n",
      "В какие фрэймворки поддержку cudnn6 впилили?\n",
      "----------------------------------------\n",
      "Это только в Linux так, в винде ищи можно с одной кудой два cudnn, но придется в ручную переключать переменные окружения\n",
      "----------------------------------------\n",
      "<https://opendatascience.slack.com/archives/C047H3N8L/p1494392839446736>\n",
      "стоит посмотреть на загрузку GPU в процессе обучения если она равномерная и высокая вряд ли поможет, если нет то можно оптимизировать… на практике с ImageDataGenerator бывает по разному, + где то валялся код для утилизации всех ядер CPU на лету\n",
      "----------------------------------------\n",
      "Котаны, решаю задачку детекции нескольких объектов на одной картинке, где ground true боксы пересекаются друг с другом (например, отдельные люди в толпе людей) . Детектор тоже дает россыпь баундинг боксов, которые тоже пересекаются между собой.\n",
      "\n",
      "Какие метрики для такой задачи лучше/можно применять?\n",
      "Если IoU, как ее правильно считать (все со всеми/в окрестности/с наибольшим пересечением)?\n",
      "----------------------------------------\n",
      "не, их по плану должно быть много, это как раз норма :slightly_smiling_face:\n",
      "----------------------------------------\n",
      "так были когда то статьи сколько стоит обучить разные Deep Learning модели на amazon какие то парни всё в деньгах подсчитали\n",
      "----------------------------------------\n",
      "Расскажите мне про размер изображения, который подается на вход сети при детекции\n",
      "\n",
      "Как я понимаю, когда делается детекция, картинки рескейлятся до какого-то разрешения, и часто это разрешение ниже исходного =&gt; downsampling =&gt; теряется информация и поэтому Faster RCNN в оригинальной статье используется (1000, 600), а в статье про SSD результаты (500, 500) выше, чем при (300, 300)\n",
      "\n",
      "На задаче про машинки ситуация другая, там разрешения и так маловато (5см на пиксель), поэтому кропаются куски из исходных 2000x2000 картинок.\n",
      "\n",
      "Я пробовал кропать 1000x1000 и 500x500 и кормить Faster RCNN. Есть нестойкое ощущение, что 500x500 работает лучше. Сравнение не совсем честное, и фрэймворки разные и параметры тренировки коих для Faster RCNN море тоже, чуть отличаются.\n",
      "\n",
      "Но в целом точность заметно разная.\n",
      "\n",
      "bounding boxes по размеру варьируются от 16 пикселей (короткая сторона мотоцикла) до 256 (длинная сторона автобуса)\n",
      "\n",
      "Кто мне расскажет, как это надо делать по уму?\n",
      "----------------------------------------\n",
      "<@U3NTG7CCS> Как эксперт, что скажешь?\n",
      "----------------------------------------\n",
      "Тут еще может быть такой эффект, так как куски на которых нет объектов нет не используются, может получиться что среднее число пикселей, которые не background, в тренировочных данных при 500x500 кропе выше, чем при кропе 1000x1000\n",
      "----------------------------------------\n",
      "В оригинальном Faster'е что делают: берут картинку и рескейлят её меньшую сторону до 600 пикселей, а большую сторону пропорционально меньшей и подают на вход сети. В твоем случае надо понять: кропы 1000 и 500 идут уже на вход сети, или там еще какой-то препроцессинг делается? Если идут сразу на вход, то разницы быть по идее не должно, т.к. масштаб то не меняется. Если там делают как в оригинале, то получаем, что кроп 1000 уменьшает средний размер объекта в ~1.5 раза, а кроп 500 увеличивает на 20%...поэтому кроп 500 может работать немного лучше в твоем случае.\n",
      "----------------------------------------\n",
      "Кто шарит как красиво объединять сетки? Обучил несколько сеток, собрал фичи с последнего слоя в csv и у той и у другой. Потом пробовал и новую сетку обучить на этих фичах и xgb, но результат получается даже хуже чем у любой отдельно взятой. Даже просто усредненные вероятности дают лучше результат. если вместе учить, объединив через ConcatLayer полная беда со сходимостью (кажется,  что одна сеть просто быстрее учится и забивает другую). \n",
      "\n",
      "----------------------------------------\n",
      "как собрать торча без конды?\n",
      "----------------------------------------\n",
      "и не совсем понятно как из python пользоваться :kekeke:\n",
      "----------------------------------------\n",
      "Товарищи, поясните, как такое может быть. обучаю lstm и всегда падает после первой эпохи с ошибкой out of memory (использую :keras:  + :tensorflow: ). Это баг что память не чиститься вовремя? целая эпоха то проходит. в интернетах на такое даже сам франсуа отвечал мол делайте меньше батч. а оно падает что с батчем 2048 что с 256 на титане с 12гб памяти. (оригинальный код у человека обучался с батчем 2048)\n",
      "----------------------------------------\n",
      "<@U0L4KM9R9> у меня такое было когда начинает считаться валидация и используется другой code path (например честный softmax вместо sampled softmax)\n",
      "----------------------------------------\n",
      "если такой тензор то это не похоже на softmax, видимо производные какие-нибудь - не знаю как это отладить нормально в keras. я считал валидацию на CPU\n",
      "----------------------------------------\n",
      "а как ее отдельно на цпу можно кинуть?\n",
      "----------------------------------------\n",
      "вряд ли есть такая прям опция. если модель для валидации/трейна одна и та же примерно, то и на gpu должно работать. я бы попробовал для начала не использовать встроенную валидацию, а запустить ее руками. при валидации не нужно считать градиенты, и памяти наоборот должно меньше потребляться. как это сделать на tf/keras не подскажу\n",
      "----------------------------------------\n",
      "случаем никто не знает, где можно взять batch iterator для unbalanced классов?\n",
      "----------------------------------------\n",
      "n01z3: огого, где такое обещают? я только про v100 видел\n",
      "----------------------------------------\n",
      "чат, а подскажите как мне в tf динамически сконструировать shape на основе размерностей 2х входных тензоров?\n",
      "я хочу что-то в таком роде сделать:\n",
      "```\n",
      "        n, c_main, h, w = tf.shape(tensor1)\n",
      "        n, c_other, h, w = tf.shape(tensor2)\n",
      "        padding = tf.zeros((n, c_main - c_other, h, w), dtype=tensor1.dtype)\n",
      "        other = tf.concat([tensor1, padding], axis=1)\n",
      "```\n",
      "----------------------------------------\n",
      "я делаю так, но скорее всего, как это всегда бывает c tf, есть какой-то более правильный но незадокументированный способ\n",
      "----------------------------------------\n",
      "Привет! Такой вопрос: пытаюсь улучшить разрешение карты загрязнения воздуха (из 34х34 в 102х102) с помощью сетки для сегментации (тирамису с двумя входами для картинок размерностью 2 и 6 (а не как 3 у ргб)). По пути у меня несколько решейпов на данные ( для таргета из (102, 102, -1) в (-1, 102, 102, 1), например), потом получаются такие картинки (слева таргет, справа предсказанное). Почему такие разбросы в значениях, откуда артефакты (и как от них избавиться)? Спасибо\n",
      "----------------------------------------\n",
      "Всем привет! Обучаю inception подобную сеть на cifar100 с батч-нормализацией. Проблема в том, что алгоритм adam странно себя ведет и время от времени делает скачки, которые резко увеличивают значение функции ошибки, затем опять ошибка начинает медленно уменьшаться. Сталкивался ли кто-то с такой проблемой? Или, может быть, кто-то обучал такие сети, расскажите, какие методы и параметры использовали для обучения.\n",
      "----------------------------------------\n",
      "Нет, вот скрин из tensorboard. Находил у других людей графики, где adam ведет себя точно так же, если использовать батч-нормализацию в сети.\n",
      "----------------------------------------\n",
      "Я рекомендую обучать SGD с Nesterov Momentum с понижением LR когда лосс выходит на плато. С картиночными сетками с имаджнета этом работает лучше всего для меня. \n",
      "----------------------------------------\n",
      "Спасибо! попробую. Можешь подсказать какой точности тебе удавалось достичь, если ты обучал на cifar100?\n",
      "----------------------------------------\n",
      "какую максимальную длину ты брал?\n",
      "----------------------------------------\n",
      "ну это полнятно. но мне на самом деле нужно ухватить как можно большую длину, наверное\n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> какую максимальную длину в одном патче можно захватить?\n",
      "----------------------------------------\n",
      "А когда накидаешь больше фильтров, там число параметров уже распухнет\n",
      "----------------------------------------\n",
      "ну вот да. поэтому и встаёт вопрос, какую максимальную длину рецептивного поля разумнее всего брать\n",
      "----------------------------------------\n",
      "Проверь на какой нибудь эпохе что она там вообще выдаёт на валидации\n",
      "----------------------------------------\n",
      "Ты задачу то и не описала толком. Если последовательностей много, но разной длины, тогда сама длина может быть дурацкой суперфичей.\n",
      "Какая природа у последовательности btw?\n",
      "ДНК какая-нибудь? Тогда можно почанкать и word2vec посчитать.\n",
      "----------------------------------------\n",
      "<@U065VP6F7> у меня получится потом каким-то образом развернуть то, что свёрнуто dilated convolution в последовательность такой же длины как на входе?\n",
      "----------------------------------------\n",
      "dilated -- это точно такая же свертка, `mode='SAME'` или как в твоем фреймворке задают, и выход будет такого же размера, что и вход\n",
      "----------------------------------------\n",
      "советую Kaldi ASR (C++) для Speech To Text. А с Text to Speech не работал, но слышал такие фреймворки как Lucida на Java\n",
      "----------------------------------------\n",
      "ну в общем это всё равно не объясняет почему размерность останется той же\n",
      "----------------------------------------\n",
      "Когда размерность меняется? Когда ядро сворачивается на краях тензора. \n",
      "Можно западдить, так чтобы размерность не менялась. Именно это и происходит, когда `mode='SAME'`, размерность уменьшается когда `mode='VALID'`.\n",
      "Прочти уже про арифметику сверток\n",
      "----------------------------------------\n",
      "Да, это довольно много, у меня цель - достичь значения 1 на валидации. Сейчас нашел исследование ребят из Microsoft, у них лучше всего работал SGD c Nesterov, как советовал n01z3 и RMSprop c learning rate 0.001. Сейчас запустил с этим алгоритмом, посмотрим.\n",
      "----------------------------------------\n",
      "Попробуй любое из вышеперечисленных, где есть реализация на фреймворке с которым ты знаком нормально\n",
      "----------------------------------------\n",
      "Правда довольно поеблись, но если что можем подсказать чего куда\n",
      "----------------------------------------\n",
      "Мы взяли тут и правили то, как оно жрёт данные: <https://github.com/precedenceguo/mx-rcnn>\n",
      "----------------------------------------\n",
      "Как думаете, с помощью чего можно добиться похожих результатов как в этой статье? <https://geektimes.ru/post/288548/?mobile=no>\n",
      "----------------------------------------\n",
      "И кто имеет опыт работы с голосом при обучении нейронных сетей? Очень интересуюсь  этой темой сейчас, отзовитесь кто занимается, пожалуйста )\n",
      "----------------------------------------\n",
      "Хочется сделать по аналогии с тем как они реализованы в самом tf\n",
      "----------------------------------------\n",
      "<@U58H63MMW> какой сейчасть State of art на имаджнете?\n",
      "----------------------------------------\n",
      "А как из него удалять дообучение? :grinning:\n",
      "----------------------------------------\n",
      "<@U58H63MMW> какой лучший результат на imagenet?\n",
      "----------------------------------------\n",
      "как же чешется начать над ним издеваться, но канал жалко\n",
      "----------------------------------------\n",
      "где то можно посмотреть на код ?\n",
      "----------------------------------------\n",
      "kohrah: а как был получен новый shape изображения? Вообще канонично к маске применить такое же преобразование\n",
      "----------------------------------------\n",
      "а почему плохо просто маску отресайзить?\n",
      "----------------------------------------\n",
      "это уже зависит от того, какое преобразование было применено к исходному изображению.\n",
      "\n",
      "например, у вас есть картинка и маска к ней\n",
      "вы ресайзите картинку, например, сплайном\n",
      "но маску так ресайзить нельзя - она перестанет быть маской, а станет каким-то странным изображением непонятно чего\n",
      "----------------------------------------\n",
      "я нубас, поэтому торможу, как это так в одну строку сделать, чтобы область np.array(), заполненная единицами (маска), отресайзилась в тот же shape, что изображение\n",
      "----------------------------------------\n",
      "<@U0KQ5M6KX>  я не до конца понял идею, о каких параметрах маски идет речь?\n",
      "----------------------------------------\n",
      "если, например, вот такая маска: \n",
      "<http://i.imgur.com/L0KOmvN.png>\n",
      "как расчитать её по преобразованию картинки?\n",
      "----------------------------------------\n",
      "ну вот, к примеру, датасет Cars (<http://ai.stanford.edu/~jkrause/cars/car_dataset.html>) интуитивно мелкие детали играют очень большую роль, но при этом объекты крупномасштабные, как тут быть?\n",
      "----------------------------------------\n",
      "Это ML детка, какие тут абсолютные правды\n",
      "----------------------------------------\n",
      "Парни, а подскажите как получают character-level embeddings? Тренируют skipgram модель с минимальной длиной char ngram 1?\n",
      "----------------------------------------\n",
      "Ну да. Я как бы и не писал что для всех сетей это так. Для большинства из тех, с которыми работал, это так\n",
      "----------------------------------------\n",
      "skipgram можно добавить как дополнительный loss в обучении, но обычно не нужно\n",
      "----------------------------------------\n",
      "Если посмотреть на то, как распознает картинку человек, то, если разрешение маленькое, он может увидеть только контуры и определить объект, но, если разрешение большое, можно будет создать сети, которые, даже, определяют, например, авиакомпанию и модель самолета, плюс множество других, более мелких объектов.\n",
      "----------------------------------------\n",
      "Это какие-то философские рассуждения пошли. \n",
      "\n",
      "&gt; просто, меньше параметров было в модели и она в память GPU помещалась\n",
      " овермного видеопамяти съедается для обеспечения вычислений, редко в какой модели весов на гигабайты.\n",
      "\n",
      "&gt;  создавать дополнительные слои с размерностью 71х71 и 147х147,\n",
      "&gt; Я говорю про размерность картинок(тензоров подаваемых на вход слоев)\n",
      "Эм.\n",
      "\n",
      "Можно запилить сетку, которая хавает 2к х 2к картинки и делает что-то полезное на 1 GPU.\n",
      "----------------------------------------\n",
      "Можно, но архитектура будет как у обычной сверточной сети. Приведу реальный пример с параметрами, у inception их, примерно, 22 млн. Соответственно, чтобы ее обучить на 1 GPU с 11Гб памяти, потребовалось взять батч - размером 100.\n",
      "----------------------------------------\n",
      "А на чем допиливаешь Wav2Letter? Я как раз хотел сравнить CNN-RNN с wav2letter\n",
      "----------------------------------------\n",
      "<@U041P485A>\n",
      "&gt;  о каких параметрах маски идет речь?\n",
      "\n",
      "Если маска имеет какую-то определенную геометрическую форму (прямоугольник, эллипс и т.п.), то лучше хранить параметры этой формы, а маску перерассчитывать при преобразованиях\n",
      "\n",
      "&gt; как расчитать её по преобразованию картинки?\n",
      "\n",
      "Если строгой формы нет, то, вообще говоря, никак. В таких случаях можно применять то же преобразование и к маске, а затем ее бинаризировать по заданному порогу: \n",
      "`mask[mask &gt; threshold] = 1`\n",
      "`mask[mask &lt; 1] = 0`\n",
      "----------------------------------------\n",
      "я правильно понимаю что каждому символу в этом случае выставляется в соответствие некий вектор (инициализированный рандомно скорее всего), который подается на вход сети, а затем после forward pass, когда считается backprop, туда придет антиградиент от лосса и изменит этот рандомный вектор в нужную сторону?\n",
      "----------------------------------------\n",
      "Да, но в этом нет ничего необычного, просто embedding layer, есть в любом фреймворке. Если не укажешь явно `trainable=False` или как он у тебя, вектора будут учиться.\n",
      "----------------------------------------\n",
      "Где-нибудь пролетал бенчмарк о том, что по скорости при параллелизации на несколько GPU мизинцем левой ноги на различных фрэймворках? Как я понимаю, паралеллизацию мизинцем на данный момент поддерживают только mxnet и pytorch?\n",
      "----------------------------------------\n",
      "хмм. Я думал там надо вручную расписывать какую операцию на каком девайсе делать.\n",
      "----------------------------------------\n",
      "А зачем веса на cpu?\n",
      "----------------------------------------\n",
      "<@U0H7VBQQ1> веса на *CPU* размещать не обязательно, а если есть быстрый DMA между картами, как в случае NVLink, то и не желательно. Подразумевается, что input pipeline уже есть нормальный, иначе CPU быстро станет bottleneck-ом, если использовать feed dict-ы\n",
      "----------------------------------------\n",
      "Не видел, но мне кажется справедливый бенчмарк тяжело сделать, так как всегда есть нюансы у либ + зависимость от конкретной задачи.\n",
      "----------------------------------------\n",
      "Если они примерно одинаково работают то и бог с ним, а если у одоного отставание, как у TF по ссылке выше - то это везде проявится.\n",
      "----------------------------------------\n",
      "zfturbo: блин, я даже не подумал, что так может быть :disappointed: Думал как обычно глючит какой-нибудь блокировщик рекламы\n",
      "----------------------------------------\n",
      "подскажите как мне можно выход contrastive loss преобразовать в вероятность принадлежности пары объектов к одному классу?\n",
      "----------------------------------------\n",
      "alexzenin8: Это хороший вопрос, я бы сам почитал.\n",
      "\n",
      "У меня на лучше всего заходил he_uniform / he_normal.\n",
      "\n",
      "Но, как я понимаю есть три градации:\n",
      "\n",
      "[1] Pre trained. Причем лучше чтобы она была pre trained на похожей задаче.\n",
      "[2] he_uniform, grlot, etc\n",
      "[3] Криминал вроде все в нули, или все в непонятный рандом.\n",
      "\n",
      "И что конкретно ты используешь в [2], на сложных сетях вообще не важно. Batch Norm улучшает неидеальную инициализацию. Я думаю, что весь зоопарк активаций в семействе relu, тоже во многом работает в этом же направлении.\n",
      "----------------------------------------\n",
      "Я сейчас тренирую inception подобную сеть, где после каждой свертки Batch Norm. \n",
      "В итоге из трёх вариантов - \n",
      "1. He_uniform\n",
      "2. He_normal\n",
      "3. Ничего\n",
      "\n",
      "Лучше всего работает второй.\n",
      "----------------------------------------\n",
      "Так и все - вопрос решен, для keras + inception - he_normal - самый нормальный вариант. :slightly_smiling_face:\n",
      "\n",
      "Если бы мне денег за эта платили я бы и теорию какую-то под это подвел с касивыми уравниями, но если, по честному, эмпирический факт.\n",
      "\n",
      "А почему не pre trained инициализация?\n",
      "----------------------------------------\n",
      "Как раз первым делом я оставил там один из пулингов, который уменьшал картинку до 15х15, работало быстро, но не очень хорошо.  Теперь картинка, как была 32х32 (cifar100) , так и остается. Вопрос в том, оставлять ли там несколько сверток, которые стоят в линию или сразу же отправлять вход в inception блок с параллельными свертками.\n",
      "----------------------------------------\n",
      "Это преждевременное теоретизирование на деталях до хорошего не доведет.\n",
      "Изначально сетка именно с такой архитектурой хорошо работала на другой задаче. \n",
      "Если твой тюнинг даст буст на cifar100 то отлично, но это частность и на иной задачке не полетит.\n",
      "Именно поэтому очень странно взывать к чужому опыту насчет такого небольшого изменения. \n",
      "В ряде статей есть обсуждение входного каскада, например в ENet авторы рассказывают, как и почему делали входной steam именно таким.\n",
      "Там есть чуток теории, которая натянута на эмпирику.\n",
      "Только чтобы теория была содержательной, они более осмысленные изменения делают (в частности prelu вместо relu и анализируют обученный наклон в зависимости от номера слоя).\n",
      "----------------------------------------\n",
      "Ну и условно просто рассуждать о сетках со свертками, как только появляются residual connections все становится совсем не тривиально.\n",
      "Интересно было бы послушать мнение [теор]физиков насчет сверток и срезок, похоже ли это на вторичное квантование, но не уверен, что из этого можно какие-то полезные следствия вывести\n",
      "----------------------------------------\n",
      "Я рассуждаю с позиции того, как выглядят свертки, если их визуализировать. Получается, что слой stem из картинки 299х299 выделяет немного признаков из областей большого размера свертками 3х3. Гугл провели небольшое исследование и сравнили как влияет размер на точность. \n",
      "\n",
      "<https://arxiv.org/pdf/1512.00567.pdf>\n",
      "пункт 9\n",
      "----------------------------------------\n",
      "Ребята, хотел обратиться к вам с просьбой. Мы хотим открыть доступ к нашему внутреннему инструменту по разметке данных для машинного обучения. И для того, чтобы понять насколько это актуально, и какой функционал требуется, мы узнаём экспертное мнение. Опрос по использованию размеченных данных не займёт у вас больше 2-3 минут. Буду вам крайне благодарен если вы пройдёте опрос или скинете ссылку на него вашим друзьям! Пройти опрос вы можете по ссылке: <https://goo.gl/forms/8QKv5GjtVrOByq6i2>\n",
      "Надеюсь на ваше понимание.\n",
      "----------------------------------------\n",
      "Это похоже на обучение VAE на данных, после чего (или одновременно) тренировка предсказания таргета по скрытому представлению. Я делал такое для картинок, работало хуже, чем просто по картинке предсказывать таргет. Но тема интересная, тоже было бы интересно узнать, у кого был такой опыт\n",
      "----------------------------------------\n",
      "Сделал такое\n",
      "```x = 2\n",
      "y = 3\n",
      "a = tf.add(x,y)\n",
      "b = tf.multiply(x,y)\n",
      "c = tf.pow(b,a)\n",
      "with tf.Session() as sess:\n",
      "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
      "    print(sess.run(c))\n",
      "    writer.close()```\n",
      "поясните за стремный граф. почему там какие-то непонятные массивы, а не по одному add, mul и pow?\n",
      "----------------------------------------\n",
      "tf.reset_default_graph() первой строкой в ячейке, где строится  граф\n",
      "----------------------------------------\n",
      "Меня заинтересовала работа artwork1 -&gt; artwork2. Не подскажешь где можно поискатьстатьи или хоть слайды с этой конференции?\n",
      "----------------------------------------\n",
      "<@U0LSE3J2E> спасибо. Видел это уже. Думал, там что-то новое. Они не рассказывали, как это делали?\n",
      "----------------------------------------\n",
      "С точки зрения логики разницы быть не должно просто веса будут больше. Но как на практике точно не знаю.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _text in map(itemgetter('text'), questions):\n",
    "    print(_text)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
