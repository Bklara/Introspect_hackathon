"я немного заработался на этой неделе и забыл дать объяву. в общем, все как планировалось",
"мне норм. вообще идея на буднях мне больше нравится чем на выходных - в формате обсуждения что есть куда дальше какие проблемы, а не в формате парного программирования на 100500 человек",
"<@U041LH06L>: Саш, а про какие именно курсы на edx ты говорил",
"natekin [11:44 PM] 
в вс будет. формат тот же что и раньше, попробуем быстрее рассказывать кто что сделал и хочет делать :simple_smile:

в Москве место выбил с 13 часов. списки обновил. в Питере могу тоже прийти с 13, но аудитория совбождается с 16:30 только (будем сидеть и прогать на диванах)
NEW MESSAGES

sovcharenko [11:47 PM] 
а телемост во сколько?",
давайте проголосуем. кто когда хотел бы прийти?,
может там как раз аналитики сидят и цифры рисуют?,
"""Какие есть способы устранения пропусков(NA) в данных и/или где об этом можно почитать"".
Я знаю несколько:
1. Выпиздить плохие данные
2. Заменить их средним/модой, если данные численные
3. Использовать NA как категориальный признак, если данные категориальные
4. Предсказывать значение какой-нить регрессией по другим параметрам

Какие есть еще варианты?и в каких случаях стоит применять какой способ?",
"1. выкидывать можно не только строки, но и столбцы
2. дозаполнение наиболее частым значением. для непрерывных - среднее\медиана, для факторов - самый частый фактор
3. na - уровень фактора. в том числе, можно предсказывать na в классификации (уместно в некоторых исследованиях, эквивалетно ""воздержался бы от ответа"")
4. да, наиболее экстремальный способ - randomforest ""все против всех""
5. помимо этого, есть вариант дозаполнения матрицы за счет факторизации аля svd (и схожими по смыслу методами, решения систем уравнений). получается занятный ЕМ алгоритм на поиск факторов, чтобы ""видимые"" данные оставались теми же, но факторы строились с учетом всей дырявой матрицы
6. другой вариант, как смесь 2, 4 и 5 - то как заполняют user-item матрицы в рекомендательных системах. про это лучше почитать (поискать презентации Сергея Николенко, у него отличные туториалы по рекомендациям)",
"в каких случаях - зависит от размера и постановки. чаще всего это либо 1 - годно, дешево сердито. либо, в заивисомсти от объема, 2 или 5 (но это потребует обоснования).

3 это скорее хак
4 возможен только на небольших данных (умеренная размерность до пары сотен)
6 появляется только на соответствующих задачах",
"так а какие ситуации для 

&gt; 1. выкидывать можно не только строки, но и столбцы

это можно безболезненно делать, а в каких болезненно? Ну те если абстрагироваться от случаев, что весь столбец - NA",
"уголок знатоков, кто трогал дипленинг для текстов?",
"&gt; еще можно решать задачу (например, строя модели), постепенно добавляя столбцы\строки 

а может же быть такое, что для одной модели добавления одного столбца/строки дает улучшение, а для другой нет. В каком случае тогда нам стоит его добавлять?",
"ну те если переформулировать вопрос, как считать, что какая-то фича или наблюдение неинформативное для нас",
"или есть какая то фишка в стиле ""если это неинформативно для линейной регрессии например, то это неинформативно вообще """,
"""а может же быть такое, что для одной модели добавления одного столбца/строки дает улучшение, а для другой нет. В каком случае тогда нам стоит его добавлять? ""

такое может быть и без пропусков, по разным причинам. вопрос про информативность - отдельная опера про отбор признаков :simple_smile:",
У меня вот такой вопрос: почему на курсах по Data Mining почти не встречается дискриминантный анализ?,
"&gt;&gt;""Какие есть способы устранения пропусков(NA) в данных и/или где об этом можно почитать"".",
"А деревья обычно сами разбираются, как это учитывать",
кто нить здесь разбирался с word2vec?,
"вот он как раз на пальцах объясняет, откуда они берутся",
"<@U041SH27M>:  большой датасет поисковых запросов. и хотелось выделить темы поисковых запросов, хотя бы в грубой оценке. а потом свести к пользователям, и попытаться разделить пользователей на группы по типам запросов. ну и  еще немного других задач на предсказательную классификацию - к какой группе будет относится пользователь с такими запросами",
"ну каков. я взял lda, и им пилил на разное количество тем. а потом руками смотрел топ слов для каждой темы и смотрел, какое количество тем мне больше всего подходит/выглядит осмысленным для моих задач. 15 тем, и случайный лес, обученный на классифиакацию на две группы, дает ROC порядка 0,92. грубовато и на скорую руку, но мне хватило пока.

а word2vec... вещь хорошая, но синонимы к каждому слову - слишком круто для моих задач.  плюс, я работаю в r, а тут нет приличной реализации, вроде как.",
а зачем реализация в R? можно обучить вектора и потом читать их в чем угодно. или я чего то не понимаю,
а у питона есть обертка вроде как <https://radimrehurek.com/gensim/models/word2vec.html>,
с поисковыми запросами еще нужно думать как от слов перейти к векторам запросов. самое простое усреднить вектора слов.,
"в 16.30 где - в делойте?
я подъеду, наверное",
а как ты их связывать собрался?,
"К тому моменту, как я до этого доберусь, ещё сотни обёрток успеют сделать",
"я, скорее, даже про верхнеуровневое сравнение - какие пакеты есть и для чего предназначены
потому что вот про d3 я только слышал, не юзал, но хочется хотя бы примерное представление иметь, что эт такое и для чего нужно",
"<@U041T0UHM>: да, D3 просто боженька в этом отношении, где то тут ссыль была на примеры кода, поищи",
"Вопрос знатокам: как вы визуализируете и первоначально анализируете многомерные данные?

Вот у вас есть выборка N элементов по M фич, для простоты, считаем их все численными и без NA. Какой джентельменский набор визуализаций и преобразований, который любой уважающий себя датамайнер должен проводить с только что попавшим в его распоряжение датасетом?",
"стоит начать с базовых свойств (тип, число уникальных значений) и статистик по переменным(summary) . это даст понимание того, что вообще имеется в наличии

затем можно попробовать нарисовать их гистограммы, чтобы прикинуть какие распределения имеются. есть ли интересные особенности в распределениях (смеси, скосы, повышенные частоты отдельных значений, хвосты, выбросы, ...)

затем можно визуализировать. можно по-старинке (splom от pca + график с распределением вкладов компонент в pca), если точек немного, можно параллельные координаты построить. <@U041LH06L> мне рассказывал\показывал hiveplot-ы (я их не умею готовить). 

если хочется поизвращаться, можно понастроить нелинейных проекторов (например tsne, чтобы понять как могут группироваться точки)",
"о, раз пошла такая пьянка.
какой ""теор.минимум"" по статистики/теор.веру можно считать необходимим/достаточным?",
до каких пор стоит в них углубляться,
"и смотря на каком уровне. если что-нибудь из области computation neuroscience/psychology - то ту да же байесовщина, итеративные алгоритмы оценки правдоподобия, бутстрепы, линейные модели со смешанными эффектами и прочее. и плюс хорошее понимание проблем p-value и вообще проблем statistical inference",
"например, для регрессии - проверить на мультиколлинеарность - это проверка на значимость данных для модели. то есть если строишь модель, смотришь summary и видишь, что все твои коэф незначимы (малое p-value), то стоит почекать ее на мультиколлинеарность, в R делается командой vif(модель), не помню из какого пакета, какого-то базового.",
"<@U041SH27M>: и то и другое. Смотрю на графики объясненный дисперсии и понимаю, какая эффективная размерность у данных. (Но не факт, что оптимальная)
Ну и непосредственно рисую - если есть какие-то явные закономерности или разделимость, то их обычно видно",
<@U04422XJL>: а ты только дефолтный PCA (и только его юзаешь) или разные вариации пца + другие какие способы уменьшения размерности?,
"я как раз собирался написать про план :simple_smile: я вернусь в Питер только во вторник, поэтоу нужен кто-нибудь кого можно назначить всех впускать (и согласовать это с ВК, пропуск только у меня)",
"итак, план!

1. заканчиваем часть проектов и устраиваем себе маленький выпускной (и презентацию для людей снаружи)
2. с мая начинаем новую группу проектов, более укрупненную
бонус: несколько мероприятий где можно и поучаствовать и воспользоваться своими результатами :simple_smile:

проекты по которым стоит зафиксировать результат (финальный или для второй стадии):
-порно (финальный)
-депрессия (финальный)
-волатильность (возможно финальный)
-майндмепы (промежуточный?)
-спорт (промежуточный)
-ретинопатия (промежуточный)
-саенс слэм (финальный)

12.04 (вс) - возможное занятие с 13 до 20 (как обычно). доделываются проекты, обсуждаются планы
 мб можно в Питере провести без меня, либо в ВК, возможно сорганизоваться и собраться в другом месте (ОК? Ченджлаб? Жетбрейнс? ...?). 
15.04 (ср) - встреча вечером, с 19 часов. обсуждаем что нужно чтобы закончить текущую пачку проектов.
19.04 (вс) - доделываем проекты, финализуем анализ
22.04 (ср) - встречаемся вечером с 19, проводим маленькую внутреннюю презентацию. финализуем картинки
24.04 (пт) - открытая презентация части проектов. потом пати хард. возможный формат: мини-саенс слэм, презентации на 10 минут
25.04 (сб) - сколтеховский urban хакатон в Москве. все кто может - присоединяются
29.04 (ср) - встреча где-нибудь, официальное начало второй части",
"_________________________________
актуальные вопросы:
1. что делаем в это воскресенье в Питере?
2. кто кроме Сережи хотел бы поучаствовать в хакатоне <http://hackathons.skoltech.ru/|http://hackathons.skoltech.ru/> ? я там читаю и жюрю вместе с <@U041LH06L> 

3. какой формат лучше для нашей презентации? 
-если делаем саенс слэм формат, то это круто, но возможно только в одном городе (проще в Питере). 
-если делаем открытый формат с телесвязью, то можно делать это непрерывно на оба города, но это не так эпично (и стоит в Москве найти площадку побольше)
4. кто готов презентовать по проектам и побыть звездой ля слэма? :simple_smile:",
"может перенести открытую презентацию на после-хакатона? там как раз и майские праздники, проще будет собраться или до Питера например доехать",
"господа, зачем извращаться с названиями, когда и так понятно, что происходит в питере? :simple_smile:",
"Аттеншн! Есть такое предложение - вчера увидел в ленте вк такое сообщение - <http://vk.com/deeplearning?w=wall-44016343_2445%2Fall|http://vk.com/deeplearning?w=wall-44016343_2445%2Fall>, написал туда и договорился о том, что эти товарищи могут прийти к нам в вск на следующей неделе, рассказать о своих планах, послушать о нас, может кому будет интересно поработать с ними. Как вы на это смотрите?",
"Ребята, как на встречу в СПб вписаться? ",
"<@U04422XJL>: нет :simple_smile: более того, все кто уже в списках, по дефолту имеют пропуск на 3 месяца вперед",
"я недорассчитал свою логистику и достиг инета только щас. есть предложение провести хэнгаут завтра, или же просто в среду скоординироваться в Питере у кого что получилось. а пока просто обсужать в своих чатиках что вышло\не вышло",
"Друзья, такой вопрос:
""Какие способы работы с порядковыми переменными есть?""

Для примера, порядковые переменные будут численными и с  возможностью пропусков (1,2,3,7, 20)

Я знаю 2 варианта:
1. one-hot-encoder - как будто с категориальными работаешь
2. Либо просто как с обычными количественными данными работать.

В каких случаях какая информация потрачивается и какие еще варианты  работы есть?",
"<@U041SH27M>:  очень уж общий вопрос, задача-то какая? я в примитивных опросных задачах смотрю частотники, и временами ту же шкалу лайкерта редуцирую до меньшего количества категорий.

<@U041LH06L>: это в какой же книге? впрочем, спор это долгий и темный, Митина, нежно любимая психологами, например, говорит, что таки можно делать факторный, но в том случае, если расстояние между рангами достаточно однородно.",
<@U041LH06L>: и почему я не удивлен,
"<@U04423D74> какой средний размер базы, с которой ты работаешь в R на работе?",
какой средний размер платья с которой ты работаешь в R на работе?,
"я ж тебе выше написал:
средний размер моего датасета -  2,5млн строк на 20-100 переменных
варианты бывают разными, иногда пилю датасеты под 100-1000 млн строк
сама sql-база вроде как в районе терабайта весит",
"а можно я побуду наглецом и попрошу вас в личку с этим дискашеном, а то может кто еще захочет на вопрос ответить, а в потоке флуда не заметит вопроса ;(",
"Последний день больше всех понравился: выступает Александр Крот, который вроде как посещает в этом слэке :simple_smile:
Думаю сходить",
Это вы на каком языке сейчас разговариваете?,
"Коллеги, напишите, плз, будет ли сегодня встреча и если да, то где и когда -- я просто в первый раз собираюсь. )",
к тому же в понедельник RUSSIR и дедлайны для сборника AIST. все кто в теме должны сидеть ближайшие 3+ дня фигачить как проклятые :simple_smile:,
\\я как раз такой проклятый,
"как я понял, на воскресенье",
а встреча обсуждается общая или какая то локальнлпитерская?,
О! Это будет интресно. Помню слушал их Тех дира когда они в 2013 приходили в инкубатор ИТМО. Тогда у них был только алгоритм похожести картинок на основе хэшей и расстояний между хэшами.,
серега где то интсрукцию кидал кстати,
"ребята, а расскажите, пожалуйста, про ссылку выше - зачем оно надо? я просто пока локально играюсь с теаной, IMPORT TENSOR AS T, все дела :simple_smile:",
там пошагово описано как запустить теано на амазоне,
а чувак смотрит на тебя с фотки как продюссер,
"Кто-нибудь пробовал инициализировать веса CNN левыми сингулярными векторами случайной матрицы, как предложено в этой статье: <http://arxiv.org/pdf/1312.6120.pdf|http://arxiv.org/pdf/1312.6120.pdf> для задачи регрессии?
У меня неплохо работает при классификации, однако в случае регрессии приводит к переполнению весов.",
"и смоделировать данные, там где у нас их нет",
"кстати, а какие модели для этого посоветуете? особенно если NA не только в одной фиче встречаются, а как то равномерно рассыпаны между всеми переменными ? Деревья?",
"Либо я его плохо знаю и не понимаю как он тут применим, или он и правда не для этого",
"ну вот пример, выборка где то 100к, у них 40 фич, все категориальные, NA как-то случайным образом раскиданы ( те у одной записи может быть половину NA, может не быть NA вовсе, а может быть 1-2).",
"представим, что я посчитал сейчас свой пульс за одну минуту. получил 60 ударов. теперь я предположу, что мой пульс не может выйти из какого-то диапазона. допустим 40-120. и допустим он не меняется за минуту более чем на 20. как я мог бы обладая такой скудной информацией пусть очень грубо, но создать еще 1439 переменных? :simple_smile:",
"<@U040M0W0S> можно, а какими еще фичами мы обладаем? и сколько всего наблюдений?",
"про другие фичи, не понимаю. какие тебе приходят на ум?",
<@U041SH27M>: допустим пропуски во всех фичах возможны. но кто отменил что фичи все нужны? может они не нужны.,
Я постараюсь прийти на след неделе на встречу (после дедлайна russir) - можно как раз там и посмотреть уже,
"да, я сам пока не знаю как подойти к задаче этой((",
"можешь сказать, как определить g++ и что это такое?",
"Напоминаю, что завтра к 13:00 придут товарищи из Кузнеча, они занимаются обработкой изображений. У них есть проект по раковым опухолям, судя по всему, в процесее зарождения. так что приходите, послушать их будет как минимум интересно.",
Кто решал Restaurant Revenue Prediction ? (может чат создадим?),
"я не решал, но если будет какое-то обсуждение, то попробую подключиться - чтобы поучиться, как это делать",
"ребзя, а для тех кто потратится к часу, продублируйте плиз как нить важную инфу :simple_smile:",
"Какой план на встречу в Питере, а то я ничего не успеваю ",
можно какое то миниописание текстовое для тех кто в танке и прослушал презу?,
"Угадай, какой из них мой? :wink:",
<@U041SH27M>: <@U04AR6WF0> создал чятик <#C04FEUZHZ> где скинул описание.,
"Извиняюсь, что с Кузнечем такая история вышла, мне Света утверждала, что будет техдиректор. Но может позже придут, она вроде как впечатлилась происходящим.",
А какая там история вышла?,
"Довольно любопытную штуку сегодня придумал, вдруг кому-то пригодится.
Сижу я, значит, решаю задачку по регрессии. Пробую разные алгоритмы. Получилось, что примерно одинаково хорошо себя показали алгоритмы GBR, SVR и Lasso-Lars. Качество предсказаний на CV показывают примерно одинаковое, так что выбрать какой из них однозначно лучше не могу. Любопытства ради построил в python pandas такой dataframe:
- True prediction - правильный ответ
- Prediction_GBR (_SVR, _Lasso)  - три колонки с предсказаниями от трех этих алгоритмов
И значит решил я посмотреть какой ошибка была бы, если бы в каждом случае я брал предсказание от того алгоритма, который был ближе всего к правильному. У меня получилось, что ошибка была в таком случае процентов на 17 ниже, чем для любого из трех алгоритмов по отдельности. При этом какой-то явной закономерности что ""в таком-то подмножестве данных лучше себя показывает этот алгоритм, а вот в том - вот тот""
После этого я построил такой dataframe для тренировочных данных:
- Features - фичи на которых изначально тренировались все три алгоритма для регрессии
- DesiredPredictor - номер того алгоритма который показывает результат самый ближний к правильному. 0, 1 или 2.
Потом я на этом dataframe прогнал тренировку GBC (gradient boosting classifier) чтобы он предсказывал значение DesiredPredictor. Ну а потом уже на CV данных я просто сначала прогонял этот классификатор чтобы он мне сказал, какой алгоритм наверное лучше всего себя покажет, и потом просто брал предсказание сделанное выданным им алгоритмом. Получилось точность улучшить на 9%",
"Тут есть как бы 3 подхода, но они все об одном и том же",
"Mixture of experts - решение принимается как сумма мнений отдельных экспертов, помноженных на некий *локальный* вес. Идея с том, что в разных областях признакового пространства должен преобладать (иметь больший вес) какой-то свой эксперт",
"Stacking - мы не хотим заморачиваться с ручным подбором коэффициентов для суммы экспертов или париться с весами. Хотим на ответах твоих трех алгоритмов обучить еще один, который сам будет определять, как лучше скомбинировать ответы",
Ибо я пока толком не вкуриваю как он работает,
"Мне бы тоже такой материал пригодился, если есть у кого",
"у меня периодически возникает идея такую большую статью написать, ибо у меня немецкий диссер про это отчасти :simple_smile: большой такой отчасти, процентов эдак в 50

а еще так сложилось, что мой русский диссер - как раз mixture of experts - более сложный нелинейный подход к сшиванию моделей (и от этого куда менее устойчивый на практике)

BMA это клево в теории, но на практике слишком мало информации для того чтобы апостериорные распределения были осмысленны. на практике хорошо работает честный bagging и линейный stacking. есть всякие усложнения типа feature-weighted stacking (читал, не пробовал ибо нет открытой реализации) и подходы к сшиванию как в теории финансовых портфелей (Марковитц, minvar портфели, это вот все)",
Нелинейный слой. Когда на втором уровне - случайный лес или бустинг над деревьями,
"сперва хотели навесить штраф так, словно у нас был бы gamnet, на аддитивные функции и их размах. а взаимодействия добавлять сперва на тех кто сперва прошел этот штраф, а потом ""пускать всех"" как это в regularization path происходит",
"кто завтра в DO идёт? помимо, очевидного:wink:",
"как обычно в последнее время, ни одной формулы",
"&gt; немного за этикой
все как мы любим :smiley:",
" Мне кажется, что  задача, по-любому входящая в ТОП10 задач всея ETL должна уже быть решена раз 100 и такому гумнонитарному быдлу как я достаточно только выбрать самый удобный.",
А когда будет известно про встречу в это воскресение?,
"Про 'gputools' слышал, но хотелось бы послушать тех, кто пользовал.",
"<@U042UQC96>: не, у меня же карточка не нвидиа. может позже, когда вырасту большой и смогу купить :simple_smile:",
<@U04AR6WF0>: ты про какой именно слайдер? Powerange?,
"<@U040M0W0S>: да, powerange, вроде там есть всё, кроме возможности картинку поверх добавить
<@U040HKJE7>: а как это сделать? мне же надо, чтобы она была там же, где слайдер, и я вряд ли могу просто по координатам нарисовать? я, если честно, не особенно хорошо разбираюсь во всём этом, если можешь кинуть ссылку на пример какой-нибудь, будет очень хорошо :simple_smile: спасибо :simple_smile:",
когда я учился этого не было :disappointed:,
"Премолаб закончился? Мне казалось, Спокойный как был в Физтехе, так и остаётся. Или он в вышку перешёл?",
"но как интерпретировать вот такие графики, я вообще понятия не имею",
Если с Питера кто надумал пойти на лекции - можем скооперироваться,
"Премолаб как мегагрант -- закончился. Спокойный завязался с физтехом\вышкой, связи остались, да. Не уверен, как сейчас все это дело называется -- может по-прежнему премолаб",
"у нас последнюю неделю была заварушка. завтра в 20 часов расскажу что да как :simple_smile:

Питер - Changelab (там где занятия были в самом-самом начале)
Москва -Deloitte
<https://pp.vk.me/c624326/v624326301/2d35b/499sxdxLOkY.jpg>",
там же где ты в воскресение был :simple_smile:,
"2ой этаж корпуса Б, а куда потом?",
"кто в Москве будет приходить, набегайте в Цюрих!",
"где, как я понял, вероятно пройдет летняя школа. либо если не школа, то просто выезд.",
"А много ли факторов, примеров и как определяешь значимость?",
"<@U041SH27M>: <https://www.coursera.org/course/nlp|https://www.coursera.org/course/nlp> этот же, да?
предлагаю его здесь обсуждать. ты с какой скоростью смотреть планирую? предлагаю как-нибудь договорится о скорости :simple_smile:. это будет немного помогать.",
потом можно спарсить какой нить датасет с ревьюшками на русском и  такой же пассаж провернуть,
кто нить кстати русские аналоги сайтика типа yelp знает/пользуется со всякими отзывами по заведениям?,
"их вроде не один, но кто ими пользуется",
"только как пользователь - 4sq мне нравится, без него точно было бы хуже",
"А можно что-нибудь сделать с кучей персонализированных логов? Есть куча событий на пользователя, где событие — одно из набора действий (существенно различных) + время (разброс довольно большой), хочется из всего этого щастья как-то извлечь представления этих пользователей, для кластеризации, например. Ну или в другом классификаторе использовать",
"Котаны, а тут как то было обсуждение про stacking и blending. А чем они отличаются? В первом случае - на вход модели верхнего уровня, помимо предсказания моделек нижнего уровня подаются и сами обучающие данные, а во втором случае - нет?",
а как валидировать и оптимизировать параметры в таком случае надо - целиком всю схему или на уровне каждой модели?,
кстати как раз его они вроде в продакшен и не пустили :simple_smile:,
"&gt;&gt; кстати как раз его они вроде в продакшен и не пустили
Ага, не пустили )",
А почему в первом линейные должны быть лучше?,
"А почему леса -- оверкилл? Настраивать их почти не надо, лишь бы примеров в обучении хватило. Запустил, подождал. счастье.",
"коллеги, возникла задача - научиться определять семантическую близость двух поисковых запросов (плюс выводить в результате степень близости, наличие одинаковых слов, наличие синонимов и так далее). что думаете, как пилить сие? датасет под сотню миллионов запросов",
"подожди. вопрос не в списке синонимов. а в определении семантической близости поисковых запросов. если для этого потребуется словарь синонимов - это решаемо.
а вот как оценить близость запросов - в этом и вопрос",
точнее будет doc2vec - где документ 1 запрос,
"и то хорошо
если не на встречах в делойте, то может просто на пару в какой кафешке? :simple_smile:

корпус текстов - ты имеешь ввиду, результат выдачи?",
"в делойте можно самоорганизоваться, как я понимаю, практически в любое время.",
хотя как от векторного представления отдельных слов перейти к работе с предложениями я пока не дослушал лекции  :simple_smile:,
"А вообще да, под задачу Филлипа как раз подходит в2в, причем примерно об этом была статья на хабре пару месяцев назад",
"но опять же, на каких то отдельных задачах :simple_smile:",
"но думаю в любом случае надо плясать от задачи, где то мб хорошо одно, где то другое",
"Котаны, немного туповатый вопрос: 

""А есть ли какие нибудь эвристики, связывающие размер датасета, количество фич и сложность обучаемой модели, чтобы она хорошо обобщала? Или это делается только с помощью гридсерча и кроссвалидации?""

Что-то в стиле: ""если датасет маленький (&lt;10к), фич штук 10, то лучше юзать линейные модели с такой-то регуляризацией или неглубокие деревья (max_depth &lt; 4) "" (пример абстрактный)",
А качество получается как правило весьма хорошее,
Линейные как правило проигрывают по качеству на таких датасетах,
"Кто знает, как там оценивается важность признака?",
"Скажем так -- я не знаю теоретических работ, которые бы говорили, как из ансабля деревьев получить честную важность фич. (вполне может быть что они есть, но просто мне не попадались)
А те эвристики, которые используются -- он зачастую работают, но во вмогом это ""вилами по воде писано"".",
"А что ты понимаешь под чёстной важностью фич? Мне, например, неясно, как определить её для простого решающего дерева",
"Верно. Но это хоть что-то.
Вообще я под интерпретацией имел ввиду не важность фичей, а понимание модели. 
В линейной понятно, что конкретно сработало в том или ином случае или какую ручку нужно дернуть, чтобы получить желаемый результат.",
"Возможно кстати здесь вот в чем разница
В линейной модели у тебя жесткие предположении о виде модели и ты можешь, накладывая дополнительные ограничения какие то выдавать различные статистические гарантии для коэффициентов
В лесах же вид модели не задан (кол-во узлов и вообще кол-во деревьев - определяется во время обучения), поэтому так легко интерпретировать значимость не получается

Вообще скорее всего чушь, но хотел подчеркнуть что в линейной модели ее вид зафиксирован, а в лесах нет",
"Такой еще момент:
Ведь для деревьев мы даже по определению обычно не используем оптимальное разбиение пространства предикторов, а просто top-down greedy подход
А для линейной модели у нас оптимальные коэффициенты given наши допущения о модели

Возможно поэтому говорить о важности предикторов, когда мы даже не уверены, что оптимально разбили пространство предикторов при построении модели, не совсем корректно",
"Ребятки, хочу спросить у вас совета.
У меня задача в которой надо предсказывать упорядоченные категориальные переменные (0, 1, 2, 3, 4). Использую я для этого нейросеть, есть ли у кого какие мысли как лучше кодировать целевую переменую для сети?
Я пробовал: 
регрессию с округлением к целому (работает, но не особо)
обычную классификацию (работает получше)
""ординальную регрессию"" как описано в статье: <https://web.missouri.edu/~zwyw6/files/rank.pdf>
Соль в том, что у сети, в моем случае 4 выходных нейрона с сигмойдами
0 кодируется 0000
1 - 1000
2 - 1100
4 - 1111
Эта штука здорово улучшет результат по сравнению с классфикацией/регрессией, но я вот думаю как бы её улучшить :simple_smile: может у кого есть какие соображения?",
"Если у тебя есть контроль над оптимизируемым функционалом, то можно так сделать, только слагаемых в нём будет O(N²) где N — количество семплов",
"не очень понимаю как мне это может, я же в итоге отранжирую свою выборку, так?",
как мне это преобразовать в категории?,
"Теоретически, если сеть умеет ранжировать объекты, то (поскольку последний слой не сильно отличается от линейного классификатора на каких-то новых фичах) можно снять выход, использовать сеть как feature extractor и поверх обучить самую простую классификацию — ранжированность фичи из сети уже должны учитывать",
"Ну или, там, для предсказания пройтись по всей обучающей выборке и посчитать, объекты какого класса ""ближе всего"" к классифицируемому. С вычислительной точки зрения не очень приятно",
Но я не могу сообразить как :(,
"как раз хотел спросить, а выделяют ли помимо классификации и регрессии еще какой-нибудь класс алгоритмов. а вы тут про это как раз.",
мне больше интересно какие фичи удалось вытащить,
"Правда я не помню, каких именно.",
ну тут ясно: если репост от навального (людей с большим pagerank) то это как плюс для лайков,
"использовать одну численную фичу как время в сутках не хорошо - так как время закольцовано
использовал 24 класса?
или меньше классов, например как ночь, утро, день, вечер",
а какие ты фичи делал из временной шкалы,
"Не уверен в ответе. Это работало как предиктор, а логика была такая -- если в окрестности много лайкали, то и этот пост с большей вероятностью лайкнут",
"<@U040HKJE7>: по какое число действительны прошлые списки в Делуа? В прошлый раз я мельком увидел, кажется, май. ",
"как всё-таки понять, что линейной модели нужен просто нелинейный параметр? и как понять какой",
"но я бы сказал, что метод проб и ошибок -- это и есть просто работа))) а как бы эту работу делать по-быстрее.",
"господа, у меня немного странный вопрос. machine learning в общем виде - это куча моделей или даже их ансамблей, и основная цель - качество предсказания/классификации. но что делать в том случае, когда мне содержательно надо проинтерпретировать, какая переменная является более сильным предиктором/делает больший вклад и т.д.? потому что на куче фич я могу создать классификатор, не проблема. но мне нужно понять, какая их этих фич выгоднее, и потом долго думать почему именно она и какой профит из этого уже можно извлечь (уже за пределами ML)",
Есть ещё понятие feature selection — оно как раз про выбор самых лучших фич,
"ага, есть. но это не решает проблемы - в ситуации, когда мне важно содержание, должен ли я пренебрегать дающими лучшее качество, но неинтерпретабельными методами?",
"<@U04423D74>: тут, как я понял, ситуация ""или/или"". ""Машинисты"" над интерпретабельностью не парятся, с нашей же стороны баррикад она важнее. В конце-концов у нас и объекты более эфемерные. По крайней мере в социологии. ",
"коэффициента у члена в линейной модели или, как минимум, значимость переменной в random forest",
"<@U041LH06L>: я вот так и подумал, что зависит от аналитика и задачи. у меня совершенно тупые задачи типа ""есть пользователи в группах а и б, мне надо понять, какими типами документов эти группы предпочитают пользоваться"".",
"... и не важно, почему и имеет ли это смысл. ",
"не, вот потом, после того, как будет понятно, какими документами - тогда и надо будет думать, а чо это оно вдруг так",
"Получить некоторую значимость признака можно -- например так. как я сказал. А вот понять, как изменение одной крутилочки повляиет на результат -- можно не у всяких моделей. Конечно, можно перебрать всевозможные варианты и запомнить, как что влияет -- но это долго и неинтересно.",
"<@U04422XJL>: ну вот это-то и хреново. потому что у меня цель, как правило, не качество модели запилить, а именно выцарапать, что хорошо объясняет реальность",
"<@U04422XJL>: ну видимо так и получается - смотришь на модель и думаешь ""хм, интересная идея, надо проверить, как согласуется с прочими нашими представлениями о реальности"". то есть модель не сколько для предсказания, сколько как вариант для инсайта",
с их помощью поймём как модель справляется с задачей,
а меня интересует что-то типа начиная с какого числа параметров можно в принципе отказываться от линейной модели,
"а 10к -- это тексты, изображения. где еще такое пространство признаков?",
"Большая размерность пространства часто возникает там, где у фичей много категорий (ибо они раскалыдваются по one-hot\dummy кодированию)",
"Я тут в википедию случайно попал:
&gt; Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. ...

и такой вот неочень сформулированный вопрос. модели в ML -- часть это статистические модели? а часть какие еще? нейронный сети к какого рода моделям относят? а вот из теории игр применяют что-то в ML?",
"Нейросети, как мне кажется, ещё можно втиснуть в рамки статистических моделей, а вот какие-нибудь деревья — нет",
а как поиск терминов зовется?,
себе нужен корпус где это встречается,
"не знаю зачем тебе это, но посомтри в сторону Perceptual hashing",
а как nlp для русского языка делать в R?,
"ну как я понимаю, что для начала нужен русский стеммер в R.",
"синтаксис, как я понимаю, не используется. по-крайней мере в NLP индоевропейских языках.",
а в R как mystem юзать?,
"mystem &lt;- function(doc) {
  library(stringr)
  sdoc &lt;- system('D:/mystem.exe -nl -e CP1251', intern = T, input = doc) #тут нужно указать свой путь до mystem.exe
  sdoc &lt;- str_replace(sdoc, '\\|.*$', '') #При получении нескольких вариантов mystem разделяет их вертикальной чертой. Удалим черту и варианты.
  sdoc &lt;- str_replace(sdoc, '\\?', '')    #Если mystem сомневается в результате, он добавляет знак вопроса. Удаляем. NB! удаляет знаки пунктуации самостоятельно!
  sdoc &lt;- paste(sdoc, collapse="" "")
  attributes(sdoc) &lt;- attributes(doc)
  sdoc
}

&gt; test &lt;- c(""Мама мыла офицера Горчакова горячим мылом в подъезде, где он курил"")
&gt; test &lt;- mystem(cleaner(test))
&gt; test
[1] ""мама мыло офицер горчаков горячий мыло подъезд он курить""",
"ну и выхлоп сноуболла:
&gt; SnowballC::wordStem( c(""Мама мыла офицера Горчакова горячим мылом в подъезде, где он курил""), language = ""russian"")
[1] ""Мама мыла офицера Горчакова горячим мылом в подъезде, где он кур""",
"хы. я вот до сих пор стесняюсь коллегам показывать свое быдлокодерство %)))
хотя вроде как уж года три без продыху...",
"``` Встреча Hadoop-сообщества.
Тему выбираем из:

    * Spark VS Hadoop (сравнение, когда что использовать)
    * Machine Learning over big data
    * Инструменты обработки потоковых данных
```",
"Тем более, что на Дримлиге вроде как NaVi будут с VP играть...",
"=) речь про ближайшее или следующее, когда митап?",
Вопрос - как посчитать стандартное отклонение для среднего от ( N негативно корелированных переменных ) ?  Выходит корень из отрицательного числа местами. Есть понимание копать в сторону определенности матрицы ковариации но в интренете все мутно про связь. Есть у кого не будть мысли ?,
"Так как попарные корреляции сл. величин у вас не равны 0, то без ковариаций не обойтись",
"работаю с классификацией, прогнал несколько алгоритмов, получил что метод ближайших соседей работает хуже, чем простая логистическая регрессия. Лучше всех показал себя метод опорных векторов, а на деревья решений вообще получил 100% результат, ошибка закралась по-любому :simple_smile: А у вас как бывало?",
"Бывают же задачи, где качество реально 99.99",
"а что, azure кто то использует серьезно ?",
"Нет, вопрос в том, как ты разбивал на обучение и контроль (и разбивал ли)?",
"О, как раз хотел вбросить эту ссылку",
<@U04JUF21N>: мы рассказывали зимой на Microsoft Big Data Hackathon как за 4 ночи построили там анализатор/визуализатор твиттОра в реальном времени,
"коллеги, вдруг кто собрался на митап в мейлру, ссылка на регистрацию:
<https://corp.mail.ru/ru/press/events/83/>",
"дак они ж только в апреле опубликовали его
просто почему бы это сразу не написать, а то я был честно уверен, что функционал уже полноценно рабочий...",
я кстати дозыриваю курсик и мне кажется можно какую нить конфигурацию рекуррентной нейросетки подобрать под твою задачу как раз,
"если кто поедет - держите в курсе, как я понял там не супер сложно пройти",
"друзья, а как от бана перестраховаться при парсинге/скрепинга сайтов?",
"точнее, я уже вижу, что эти параметры передаются в виде ключей при make из консоли, типа такого
&gt;make if [ ! -e text8 ]; then wget <http://mattmahoney.net/dc/text8.zip> -O text8.gz gzip -d text8.gz -f fi time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15 ./distance vectors.bin

но видел ли кто боле-менее приличную документацию по всем этим параметрам?",
"вот за это короче отвечает, какой функцией писать в файл <http://stackoverflow.com/questions/6552907/how-to-write-an-integer-to-a-file-the-difference-between-fprintf-and-fwrite>",
"с min_reduce честно говоря до конца не могу понять чо как, но она тоже как то урезает словарь",
"Котаны, а как в R сгенерировать последовательность от 0000 до 9999? Гугл не помог.",
"<@U04422XJL>: спасибо. Как раз, кстати, с помощью Avito генератор пишу :smiley:",
"мне кажется, следующий мегапрорыв будет тогда, когда научимся моделировать концентрацию на чём-то конкретном",
как именно подумать о стуле?,
"или вспомнить, что стулья ""разбегаются как тараканы""
это будет мультимодальность в правильном её смысле)",
"сейчас мы там просто какую-нибудь логистическую регрессию делаем и понимаем, какие нейрончики про стулья",
"потому что в реальности нам нужно не просто понять, что перед нами вот эти 25 объектов, а что-то потом сделать с ними, и как правило мы 24 из 25 вообще не замечаем)",
"или хочу трансформировать объекты, которые я вижу, в то, как крутить руль у машины; не уверен, что тут употребляют слово ""модальность"", но суть та же",
"я так понимаю, что сейчас устроено так:
input -&gt; [deep model] -&gt; high level activations -&gt; [relatively simple model] -&gt; action
а на самом деле вместо relatively simple тоже что-то сложное происходит, как мне кажется",
"хотя, возможно, это само собой получится, когда мы научимся хорошо объединять несколько глубоких сетей с разных модальностей",
"коллеги, а задача-то какая в итоге? что хочется посмотреть?

да, про что речь вот в этом утверждении?
&gt; мозге же конечное число этих самых уровней, 5-6 вроде",
а deep learning над текстами (тот же word2vec) устроен так же как над изображениями?,
"а так как я понял, больше всяких рекуррентных веселушек используют для nlp",
"DL — это, как минимум, глубокие модели, а кроме нейросетей таких нету",
"а какие еще модели бывают? должны быть какой-нибудь антоним, по-крайней мере",
"С жадным подходом так не получится. С другой стороны, весь хайп вокруг этого deep learning'а как раз начинался со стекинга RBM с жадным предобучением, но сейчас как-то заглохло всё",
"коллеги, если вдруг кто работал с doc2vec, что значат параметры модели size и window? window я еще хоть как-то понимаю, а вот size уже не очень.",
имхо дип в лернинге такой же баззворд как и биг в дате,
"друзья, а если у меня есть сотни тысяч не очень больших документов с датой. на один день сотни документов. представим, что это посты в блоге. как я мог бы посмотреть как меняется что-то в течении времени. то есть какие метрики для текстов можно вот посмотреть, чтобы сравнить?

банально на ум приходит средняя длина слова. а если что-нибудь более семантичное :simple_smile:? подумалось так же, что можно объединить все документы за месяц и так месяцы попарно сравнивать на самые популярные слова. наверняка, еще масса интересного используется.",
сделать топик модел и посмотреть какие популярны когда,
"Это да. А если каждый срез представить как отдельный документ, и посчитать tf-idf для каждого слова в нем?",
"если срез -- отдельный документ, то какой idf?) документ-то один. просто можно посмотреть на самые частые слова.",
"<@U040M0W0S>  По идее, если считать tf-idf для каждого слова из среза (объединенные доки), то наверху как раз и будет слово (слово с наибольшим tf-idf), которое максимально релевантно для текущего среза и реже повторяется в других",
"Возникла тут задачка, которую наш программист не знает, как оптимально решить в SQL. У меня же есть подозрения считать, что она вполне решается там, без monkeyjob.",
"Т.е. у тебя 5 строк с одним телефоном, по каждой из них Х принимает какое угодно значение.",
Ща чуть позже напишу как мы решили,
"коллеги, возникла такая задача:
есть пользователи, есть сессии пользователей, есть временные интервалы сессий.
как можно посчитать, с каким количеством пользователей работает одновременно тот или иной пользователь?

я использовал пакет IRanges в R, но там просто подсчет пересекающихся интервалов (т.е., сессий). А мне надо подняться до уровня пользователя.",
"<@U04423D74>: имелось в виду ""с каким количеством сессий работает одновременно тот или иной пользователь""?",
"нет, именно с каким количеством пользователей.
то есть, это как нагрузка на сайт/форму - сколько пользователей одновременно находятся на сайте в каждую сессию каждого пользователя.
но из данных - только времена начала и конца сессий пользователя.",
"Лол, я как раз вчера на лекции в вышке вспоминал этот хакатон. А по какой метрике там качество прогноза измерялась?",
"только сейчас понял, где хакатон проходит)))",
Так идёт кто в итоге?,
Я когда в первый раз увидел решение аж испугался,
А кто такой Gilberto Titericz?,
а как люди доходят до такой архитектуры? :simple_smile:,
"почему 33, а не 55, например?",
"Ребят, вопрос нарисовался. Есть определённая база клиентов, по ним строится модель - например, скоринговая. Потом, допустим, по части клиентов мы смогли надыбать ещё данных, допустим, из соц.сетей. Стоит задача проверить, что использование этих данных может улучшить модель. Как это лучше сделать? Я пока вижу три пути - построить две модели, на исходных данных и на новых, сравнить по какому-нить показателю, например, Gini. Самый простой подход, но не супер строгий. Есть ещё вариант, если модель линейная, посмотреть на значимость коэффициентов при новых переменных. И, наконец, есть лютое извращение - набутстрэппить подвыборок из этих двух множеств, построить на них модели, посчитать целевую статистику и бахнуть тестом на равенство средних. Правда я пока не знаю, по какой доле клиентов есть доп.инфа.
Может есть что ещё?",
А почему последний вариант -- лютое извращение?,
"кстати интересно в целом, отличается ли обучение на таких данных (я бы назвал их блочно-разреженными) от обычного обучения? 
Под блочно разреженными я имею ввиду, что часть данных мы можем обогатить данными из соц-сетей, часть наблюдений - по мобильным операторам, часть - еще по чему-то. Конечно, его можно рассматривать как обычный датасет с большинством миссингов. Но мб можно сделать что-то еще... Что приходит на ум - строить модель по каждому набору переменных, а потом сверху модель накручивать... Но опять же, может есть что-то еще?",
"такой вопрос: кто может порекомендовать хорошую библиотеку для того, чтобы сделать красивый timeline на js?
гугл выдаёт вот это:
<http://timeline.knightlab.com/>
мне вполне нравится, как оно выглядит, но у меня, скорее всего, будет очень много дат и событий (а тут TimelineJS is optimized for 20-30 entries), поэтому хочется, чтобы (1) был зум и (2) чтобы можно было грузить только заголовки, а данные про отдельные события подтягивать ajax'ом",
а если визаулизовать как ганта?,
как ганта с древней греции до наших дней?,
"а какие есть библиотеки, чтобы на много событий в ганте?",
какие на много - не знаю,
"котаны а кто нить вытаскивал тексты статей из дампов русской википедии и че использовал для их предобработки (выпиливать там разметку, ссылки, прочую шляпу)?",
"хм, ну я это и хотел заюзать как раз",
для топик моделинга собственно. посмотреть как топики в новостях меняются во времени.,
"а для тех, кто в танке - о каких ""китах"" речь идет?",
а как ты вообще туда попал?,
"<@U040M0W0S> я бы делал lda, lsa, plsa - каким нибудь из этих методов. благо они есть - например в gensim (для питона)",
"да, а weka делает рандом форест, но я ей не очень люблю пользоваться как программой (если не на джаве пишу - там я все таки библиотеки вызываю)",
"интересно! спасибо. и доки там хорошие, как я вижу.",
"да, я думал об этом, но это, наверное, должно работать хорошо, когда источники обогащающих данных взаимоисключающие. Например, мы по номеру телефона у разных операторов запрашиваем информацию. А вот когда они независимы - такое вряд ли сработает.
Ну да ладно, в общем-то, не так важно.  Наверное серебряной пули нет и тут)",
<@U040M0W0S> а где ты это смотришь?,
Так идёт кто или нет?,
там вроде как регистрация закрыта,
"я точно не на фб видел. Где-то в анонсе мероприятия было что-то, мол ""участникам ВПЕРВЫЕ будут предложены самые что ни на есть настоящие данные от крупнейшего интернет-магазина - Озона"". Как-то так. Ток хз, где эо было.",
"Мне показалось или тут кто-то писал о том, какую задачу там предстоит решать?",
ну. а мне бы как -- я посмотрел бы на результат :simple_smile:,
ну там вроде присоединен какой то размеченный датасет,
"вообще пообщаться бы с тем, кто решил так продавать",
"не знаю, кто конкретно там у них движитель этого курса",
"Репин - как директор, Турилин как орг лицо",
так что - кто то поехал на хакатон?,
"ну там вчера дали задания. Первое - что-то вроде титаниковского датасета. А еще два - от озона. Первый содержит запросы пользователя, описание выбранного им товара и количество таких пар ""запрос"" - ""товар"". Надо предсказать это количество. Примечательно, что они не знают, как будут это тестировать=)",
"я дома мержу спарсенное с озона с тем, что выдали. при этом выданное оказывается не такое почищенное, как подразумевалось :simple_smile:. не мог долго в бд поэтому загрузить -- будет мне уроком) но времени вообще не осталось, как я полагаю. но уже интересно проверить гипотезу.",
"<@U04422XJL>: Мих, а ты когда w2v обучал - ты сам реализацию писал, гугловую натравил или еще чью-то?",
"но, как я понимаю из своей Москвы, пока нет никакого расписания",
как эту матрицу они потом использовали?,
как она им помогает отранжировать для конкретного запроса итемы?,
"так, еще раз Как мы вчера выяснили , задача - предсказать клики, а не степень соответствия товара запросу)",
кто эта команда?) проверим твою гипотезу),
"еще одна примечательность: как мы вчера узнали про третью задачу, по одному товару могло быть много записей (для разных каталогов). Так вот народ, когда кросс-валидировался, делил выборку так, что записи по одному товару были и в train и в validation",
"спросил у них первым делом ""а как по вашему озонатор выглядит""",
так. а дальше они какой вывод сделали?,
<@U04423D74>: а бывают разные? :wink: я ни на каких пока не бывал,
"есть встречи, которые организует <@U040HKJE7> 
планируются митапы по R относительно регулярно (раз в три-четыре месяца)
я слышал про питоновские митапы, но эт на сайте митапов вроде как смотреть надо
может что-то еще есть",
"Кто участвовал расскажите, по каким критериям была оценка?",
Так а это -- кто в итоге взял топ и что они сделали?,
"Я прочитал, ага. А как по факту было?)",
"<@U061WBASK>: Кать, как в итоге судилось?",
Дельно - когда более-менее детерменировано :smiley:,
сами чуваки из МС в Ажуре не бог какие эксперты,
"&gt; Александр Семенов - младший научный сотрудник Международной лаборатории прикладного сетевого анализа НИУ ВШЭ, раскажет про анализ социального графа Twitter, и как он влияет на взгляды людей в нем.",
"А кто из NPL выйграл? как связаться, они здесь есть ?",
"Можно как бонус, небольшой Q&amp;A про событие.",
<@U04URBM8V>:   а коля марков там заявлен как event host. вы вместе с ним делаете?,
"<@U041LH06L>:  я лично не имел дела. насколько я понимаю, там многое зависит от того, какой айтрекер, в каком виде данные отдает и какие задачи стоят.",
"<@U041LH06L>: напиши мне вечером в личку в вк, я тебе сброшу ссылки на тех, с кем можно пообщаться",
"почему-то не вижу красивых графов, как ожидал",
но как видишь браузерный клиент позволяет визуализирвоать,
посмотри какая красота хотя бы,
"<@U04423D74>: пока просто прикидываю возможности. декларативный язык для графов. как SQL -- декларативный язык для таблиц. мне вот преза показалась занимательной: <http://www.slideshare.net/neo4j/meetup-analytics-with-r-and-neo4j>

поиграюсь -- попробую ответить на любые вопросы)",
"когда много камментов -- они сворачиваются. для разворота надо сделать клик. клик -- js. я юзаю `scrapy` для парсинга. для работы с js он юзает какую-то тулзу, которую ставить на винду надо через docker.",
"коллеги, немного глупый вопрос.
в какой момент заканчивается machine learning?
когда уже больше ничего не придумывается? когда машинное время стоит дороже возможного улучшения модели? или еще что-то?",
Когда усилия становятся непропорциональны результату,
"угу. я это и имел ввиду, когда говорил про машинное время
спасибо",
ну я не знаю как назвать,
я про timeline где видно как одна тема меняется другой,
а кто сегодня на митап в ДО идет?,
а как бы мне в R просто хоть как-то работающий `named entity recognition` получить?),
"а какие существуют метрики длинных текстов? :simple_smile: в частности, как _проще всего_ посчитать степень появления слов, которые ранее в тексте не встречались :simple_smile:",
"кстати, ведь наверное если посмотреть на рост употребления новых коллокаций -- это как раз покажет новые ммм темы",
если взять какой-нибудь словарь за нулевую точку. и смотреть как газетные статьи вносят новые слова -- можно за трендами последить типа,
а сейчас ты как их выделяешь?,
"<@U064DRUF4>: Михаил, привет. а как сейчас с NER для русского дела обстоят?))",
"Как с русским NER - не знаю, не занимался. Ну надо тренировочные данные где-то брать, и модель потом строить какую-то; что-то стандартное вроде CRF пробовать сначала.",
"<@U064DRUF4> а где бы взять тренировочные данные?)) кстати, ты к opencorpora отношение имеешь?",
<@U04CH4QBD>: почему бы не воспользоваться чужими?)),
вряд ли кто даст хорошие,
"Не знаю, где брать) в opencorpora вроде что-то начинали размечать. Но я этого не очень понимаю - NER же с какой-то целью делают обычно. А на практике у всех клиентов свои задачи =&gt; свои тегсеты. Хотя может PER/LOC/ORG и частый случай.",
а как вот эту строчку понимать? comparable? достойные результаты?,
а числы оси ординат как стоит интерпретировать? :simple_smile:,
"коллеги, а никто не видел описание алгоритма LDA вот так, чтобы совсем для далеких от темы?
а то описывать алгоритм как ""оценка совместной встречаемости слов, и группы часто встречающихся слов есть одна тема"" как-то ну совсем уж некорректно",
ну какая деревня без коров?!,
"&gt;Модель латентного размещения Дирихле относится к моделям вероятностного тематического моделирования - способам построения модели коллекции текстовых документов, которая определяет, к каким темам относится каждый из документов. Вероятностные тематические модели описывают каждую тему как набор терминов, с разной вероятностью принадлежности к теме, а каждый документ – как набор терминов, с разной вероятностью принадлежащих к разным темам, т.е. смесь тем.

Некоторые базовые предположения о документах и терминах (словах) для метода латентного размещения Дирихле таковы:
•	порядок документов в коллекции не важен;
•	порядок слов в документе не важен, документ — «мешок слов» (bag of words);
•	слова, встречающиеся в большинстве документов, не важны для определения тематики, их обычно исключают из словаря и называют стоп-словами;
•	слово в разных формах — это одно и то же слово;
•	в качестве функции распределения принадлежности термина к теме и документа к темам используются распределение Дирихле

Так как документ или термин может относиться одновременно ко многим темам с различными вероятностями, вероятностные тематические модели осуществляют так называемую “мягкую” их кластеризацию. Это позволяет решать проблемы синонимии и омонимии терминов, возникающие при “жёсткой” кластеризации (документ или термин относится только к определенной тематике). Модели со скрытыми (латентными) переменными, такие, как LDA, особенно эффективны для выявления скрытых структур в текстовых коллекциях и используются для решения таких задач, как классификация документов, поиск схожих документов, многоязычный поиск, выявление ключевых слов в документе, выявление зависимостей между терминами, выявление трендов в различных областях интересов и др.",
"грубая компиляция, аж стыдно. но лучше не знаю как описать...",
как в примере из которого взял)),
"&gt; будет необходимо разработать собственный алгоритм или улучшить открытое решение, созданное командой Deep Mind, с целью получить программу, способную самостоятельно обучиться играть в любую из компьютерых ретро игр Atari. Только в финале, лучшие команды узнают на каких из 49-ти игр Atari будут соревноваться за победу их боты.",
"<@U045BH6SM>: Никит, а где лекция завтра будет? в Кубе?",
"прокинешь меня, как в прошлый раз? -__-",
"<@U040M0W0S>: Разницы  особой нет, но я любитель красивостей, и тут есть еще куда искать",
"Ребята, а вот скажите - когда нужно остановиться? :simple_smile: я прогнал уже несколько моделей, данные масштабировал, но результат все-равно слабоват, ну как слабоват, неплох, но хотелось бы лучше. Как можно понять, что из этих данных уже ничего не выжать? Или надо продолжать искать до победного свиха крыши?",
"В общем случае останавливаться надо тогда, когда дальнейшая работа не стоит свеч",
"Тут недавно <@U04422XJL> отвечал на подобный вопрос. Идея заключалась в том, что прекращать играться с моделями надо тогда, когда получаемый результат перестаёт окупать затрачиваемые усилия",
"Ну да, все правильно. Благодарю! В моем случае, действительно, важно понять, как все работает, а небольшая разница - дело десятое.",
"Я еще вроде слышал, что получить результат с точностью на test set более 85% (вроде на курсере это было в одном из курсов) практически нереально, и дескать имеет смысл останавливаться в тот момент когда удалось такой точности добиться",
а почему русский ворднет такой какой есть? люди чем-то другим пользуются?,
"Еще раз, что 12-го было? Где авито про свой каггл расказывали?",
"А где это мероприятие проходило, и как оно называется?",
"Неа, сегодня как раз объявил, что летом собирать народ трудно -- регулярных встреч не будет. Отдельно кого-то выцепить или о чем-то поговорить группой в 5 человек -- можно.",
"Нет, анонсов на <http://meetup.com|meetup.com> нет. Есть группа в ФБ, где выкладываются объявления",
а как бы мне нарисовать карту миру в виде типа графа. т.е. из одной страны в другую направленные дуги чтоб шли.,
а как мне их спроецировать на карту мира?,
а как я их поставлю? как-то руками можно?,
"да нет, на той карте что про Create a flight connection map using R используются просто широты/долготы как координаты",
"смотрел я на nlp-тулзы и думал, зачем нужно определение языка. а теперь смотрю на `ཤོག་ངོས་འདི་ནང་ད་གནས་ཡིག་གེ་མིན་འདུག། ཁྱེད་ཀྱིས་ཤོགངོས་གཞན་ཁག་ནང་ ཤོག་ངོས་འདིའི་འགོ་བརྗོད་འཚོལ་རོགས། དེ་དང་འབྲེལ་བའི་དོ་ཟླའི་ཐོར་འཚོལ་རོགས།` и думаю, что было бы неплохо.",
"и вот если такие тексты кластеризировать, то можно лучше понять как работают алгоритмы)))) потому что никаких иллюзий по отношению к понимаю смысла текста не остается",
"Привет! Мб у кого-то был опыт с Learn to Rank задачами?
Какими библиотеками пользовались (особенно интересно что-то под R, главное чтобы не только под Python?
Я под R видел только одну pairwise модель в gbm, но она у меня крашит R Session рандомно :simple_smile:
+ что то в пакете rSofia, но я его не знаю.
И схожий вопрос: пользовался ли кто-то RankLib? Как результаты? Я почитал что RankLib неплохая тема, но было бы интересно услышать информацию из первых уст, так сказать :simple_smile:",
"А кто в курсе, что за ""топ кэггла"" собирается выступать в Мэйле?",
а когда руками темы задают -- это как называется? классификация? а если нет обучающей выборки?,
"если допустим сделать как в поисковых системах. оценивать релевантность документов некой строке слов. только не сортировать документы, исходя из этой релевантности, а просто как-то ее демонстрировать.",
td-idf для поиска -- он как работает? там перемножение матрицы на строку?,
"ну ты говоришь “а когда руками темы задают""",
"наверное, ты понял иначе, чем я имел в виду про ""руками"". в топик моделинге мы делим все слова на несколько множеств, и топ из этих слов -- он для нас выражает собственно топик. а есть варианты, когда мы сами создаем множества таких топов слов, а потом смотрим как по корпусу они распределяются.",
ну то есть это очень похоже на поиск. но в ТМ же как я понимаю иная математика. вот как ее использовать :simple_smile:,
"<@U04422XJL> спасибо за наводку.
Хотя меня больше интересуют listwise алгоритмы.
Знаю что для SVM есть модификации для listwise подхода в том числе (вроде SVM Map), но вроде как под R реализации нет.",
"Скольких математиков я ни спрашивал, мало кто мог объяснить ""физический смысл"" собственных векторов и чисел.",
"товарищи, а подскажите, пожалуйста, из вашего опыта - применение PCA к данным не самой большой размерности, допустим 50 параметров - давало вам какой-нибудь ощутимый результат? вообще оно может здорово помочь в задачах регрессии, если у данных наблюдается гетерскедастичность, а вот в задачах классификации, как оно бывает? или может есть какие-то другие методы? я что-то слышал про ICA, но ничего конкретного сказать не могу, подскажете?",
О какой предметной области идет речь?,
"&gt; На мерояприятии мы покажем, как, используя всего 1000 запросов к API ВКонтакте, найти топ50 самых влиятельных людей в крупнейшей социальной сети. Расскажем о том, как можно искать экстремизм в социальных сетях, анализовать поведение пользователей и самое главное - как это превратить в работающий сервис",
"Так как я о Спарке вообще ничего не знал, было полезно)",
"вообще, я бы сам пошел на mail и хочу тоже такие мероприятия проводить, но пришлось сначала это сделать. Да и вообще, если иметь ввиду задачи на kaggle - их имеет смысл решать, когда кол-во участников до 10, наверное",
как это проходит по субботам в ШАДе,
"эм, ну к <http://mail.ru|mail.ru> я не имею отношения - поэтому это к ним, а к этому хакатону - тут тоже уже ничего не перенести, я сам хз, почему так вышло",
"Она упоминает всех, как если бы ты написал его юзернейм <@U0441TBSV>",
"&gt; я думаю если 35-40 сделает сабмит и пришлет заявку то это успех, цифра выбрана как раз из предположения, что именно столько людей пришлет правильные заявки, так что думаю никто не останется не приглашенным, если сделал все верно; конечно если заявок внезапно окажется чуть больше, то позовем всех",
"Если штатными средствами, то только если таблица уже есть и схема совпадает. Но, да, можно использовать какой нибудь инструмент. Вот еще есть вариант <http://www.convertcsv.com/csv-to-sql.htm>",
"&gt;&gt;Да и вообще, если иметь ввиду задачи на kaggle - их имеет смысл решать, когда кол-во участников до 10, наверное
пока это все эксперимент, посмотрим как пойдет; в этот раз я планирую что мы после лекционной части потратим минут 20-30 на знакомство и разобьемся на подгруппы человек по 5 примерно, и работы будет внутри подгрупп идти

ну и раз в час-полтора будем общий брейншторм устраивать, типа а мы вот юзали то то и то то и получили такой то результат -)",
"И как насчет не очень подготовленных участников? Поприсутствовать, послушать, набраться опыта.",
"&gt;И как насчет не очень подготовленных участников? Поприсутствовать, послушать, набраться опыта.
это пока эксперимент, и я основываюсь на нескольких отзывах о прошедших подобных мероприятиях; например на этом <http://blog.kaggle.com/2015/03/25/if-you-cant-beat-them-invite-them/> серьезной проблемой было то, что не сделали фильтр по тем кто вообще не в теме; там рассматривали задачу по лип лернингу, а пришли люди которые вообще даже о машинном обучении не знали, не то что машин лернинг

в итоге Зандер и Макс (приглашенные гуру) сидели у компа кодили весь день, а толпа сзади стояла и смотрела просто",
"но пока это наш первый такого рода семинар, помотрим как пройдет, соберем отзывы и следующие скорректируем",
"мы же не ставим плану что выше какого то уровня пройти нужно, а просто сабмит -) что бы не тратить время на изучение данных, а сразу начать кодить",
"&gt;а можешь поподробнее рассказать? а то я за тем событием тоже следил :simple_smile:
с Максом за пивом лично обсуждал -) а провторой конкурс общался с администрацией кегла, они там рассказали немного чо как прошло",
"вообще мы думали на тему стоит вводить какой то приз или нет, решили что не стоит, тк соревновательность убьет идею общения и обмена идеями в процессе, и тогда семинар превратится в хакатон, а конкурсов как бы в мире и так хватает",
а иначе зачем вообще идти на мероприятие по кегглу?),
"а хз, мы просрочили оплату, и система митап.ком по непонятным для нас причинам взяла да и выкинула организатора, и поставила какого то левого чела",
через какое то время мы выкатим свою систему аналогичную митапкому и все будет там,
"лол. как я понял, если митап теряет хозяина, то любой может вызваться. но я подумал, что вы специально.",
"вот только я с ним не сталкивался. это как стандартный правила грамматики румынского. ну да -- многим известны, не значит, что мне, например :simple_smile:",
инструмент...мммм. из какой субд в какую грузить надо?,
"пусть автоматизация. или другое слово. мне вот хочется понять, как бы это сделать :simple_smile:",
почему гугл не дал ответа,
"Кстати, всегда опредлял тип данных столбцов как ""строка"", в целях первичного захвата данных",
"так. а какой инструмент ETL мне сейчас скачать, чтобы получить инфу о колонках csv, которуя я мог бы просто превратить в SQL",
С каких пор Tableau и Tibco стали open source?,
<@U040M0W0S>: как раз хотел написать про csvkit ),
"Системное мышление - как работает система (inside out) + архитектура. Обычно, этого хватает для понимания где и что копать.",
не знаю как у других.,
"<@U06J1LG1M> - о, круто, Вы организатор мероприятия в <http://mail.ru|mail.ru>? Рад знакомству!) это хорошо, что такие мероприятия стали проводиться, аналог я знаю только один - в ШАДе проводится по субботам. Там очень круто, спасибо <@U04422XJL> за это. Есть только один вопрос, который немного не понятен насчет условий с kaggle - почему код, который генерится на мероприятии должен быть обязательно там выложен? Или я что-то не понял?",
"я про шад сам недавно узнал, уже после того как затеяли все это, Дьяконов поведал",
как возобновятся если пустят то приду -),
"&gt;почему код, который генерится на мероприятии должен быть обязательно там выложен? Или я что-то не понял?
это условие администрации кегла что бы были все в равных условиях, либо мы на семинаре объединяемся в одну большую команду (если конкурс позволяет делать большую) и участвуем в нем как  одно целое, либо публикуем наработки сделанные на семинаре",
шучу куда может быть командировка.,
"я уже писал выше про так как я вижу процесс: мы типа такие ботаем час-полтора в подгруппах, делаем сабмит, ну можно поочереди в подгруппе что бы сабмитов больше было, потом 20-30 минут рассказываем по очереди, может представители подгрупп, о том чо юзали и какой скор на лидборде получили",
"интересно, честно говоря, не смотрел, что там за задача, но если еще учесть, что нужно время какое-то на то, чтобы запрогать (а это, как правило, делает кто-то один) и чтобы обучиться (часто тоже не малое время, но зависит от задачи) - в итоге, наверное, имеет смысл, чтобы каждая подгруппа/группа проверяла какую-то одну гипотезу и только ее",
<@U040M0W0S> хочешь похвастаться как попросил Елену занять диванчик?,
"вообще не очень понятно, зачем этот вебинар устроили))",
"<@U06J1LG1M>:  а что происходит с Data Science Meetup - ко мне вчера в IBM подходил Александр Белугин, мы с ним разговаривали о ""судьбе группы"", я так ничего и не понял, кто там всем сейчас заправляет и что планируется проводить. Хорошая такая группа была, здравые мероприятия, а сейчас ничего не понятно",
"<@U0441TBSV>: 

mephistopheies [2:38 PM] 
а хз, мы просрочили оплату, и система митап.ком по непонятным для нас причинам взяла да и выкинула организатора, и поставила какого то левого чела

mephistopheies [2:39 PM]2:39
в общем кроме того что тот чувак там сейчас числится в админах - ничего не поменялось (edited)

mephistopheies [2:39 PM]
через какое то время мы выкатим свою систему аналогичную митапкому и все будет там

mephistopheies [2:39 PM]
мы на них обиделись кароч

i [2:42 PM] 
лол. как я понял, если митап теряет хозяина, то любой может вызваться. но я подумал, что вы специально.

mephistopheies [2:45 PM] 
ну мы сами удивились от такой системы",
"мы не знаем кто такой Белугин и что ему нужно, у нас митапы продолжаются -)",
"странный тип на самом деле, какой то опрос там в группе проводил -)",
а чо там в ибм норм было? я как то был там на митапе по R и было половину времени потрачено на рекламу ибм,
"но как-то все лайтово, как и предполагалось видимо",
"группы уже в митапах отжимают ))
да, там у них конечно как-то слишком много ибм обычно, хотя вчера не был, на первой встрече вроде как про Hadoop, 100% инфы было от ибм",
"ой, да я сам уже пожалел, что связался с <http://data.gov.ru|data.gov.ru>, если честно - там получилась такая история, что несоклько дней назад они ко мне пришли (я там раньше с некоторыми из них работал) и попросили - мол выручай, народ не идет. И я решил сделать хороший примеры на основе данных <http://data.gov.ru|data.gov.ru> и данных из соц.сетей. Например, показать, как можно всего за 1000 запросов к API найти ТОП50 вершин с максимальной степенью в графе соц. сети (из-за ообенностей самого графа такое можно сделать) ну и прочие такие вот штуки и на примере таких вот идей сделать хороший хакатон - думаю, было бы очень научно и интересно с одной стороны и их бы потребности по открытым данным тоже бы были удовлетворены. Но в результате, как-то все не совсем организованно прошло - но, еще неделя, контент и интересные примеры мы все равно подготовим. А вот насчет организации (что так поздно получилось) - ну уж как есть)))",
"подустал чета от них, взял паузу, потом решил снова делать, но, честно говоря, утомляло прилично переводить названия статей на русский и решил, что вроде как если по-английски, то нельзя на хабр )) хотя в общем-то пофигу наверное, это ж дайджест",
"но пока я так постыдно все делаю как тетеньки в банках ручками, по старинке",
а там на какую глубину можно собрать фид? я бы потопикмоделил ради фана.,
"да, я через фидли как раз читаю и он вроде всю историю хранит, но api у них нет кажется, так что думаю, что проще просто взять opml и пройтись по каждому фиду отдельно и ссылок набрать",
"да, все как ты описал, но я бы не сказал, что прям вот какие-то неожиданные тренды, все больше про Deep Learning + GPU, все больше и больше про Python в связке с scikit-learn особенно, но про R не становится меньше, как мне кажется, упоминаний Azure ML  все больше, ну и конечно же стремительный рост статей и упоминаний по Apache Spark, не зря ж мы на курс-то записались ))",
<@U0441TBSV> а почему хакатон по Открытым Данным проводится на площадке АЦ при Правительстве РФ? Они-то какое ко всему этому имеют отношение?,
а на какой курс мейла вы даете там ссылку? ну чисто иниересно ),
не стал юзать? я как понял у тебя такая задача была,
"значит ошибся, слышал что их рекомендовали от кого то, вот решил уточнить",
"В принципе можно. Но я не совсем релевантен, я на бюджете там был :) Нужно скорее мнение тех кто платил (или за кого платила компания) - это будет честнее",
"чо, такая разреженная матрица? ее б тебе как сократить, что ли...",
"<@U04CH4QBD>: да вроде ж как дополнительные материалы -- давали рекомендации на видосы, например, курсеры и того же мейла",
<@U06J1LG1M>:  а когда где и для кого это все будет?),
и как нех наберешь пару сотен тысяч примеров,
интересно вообще кроме яндекса дип лернинг где в России применяется,
"причём я знаю, где можно найти много неодетых девушек",
<@U041P485A>: а где еще Яндекс есть? в Турции?,
<@U040M0W0S> ну какой вопрос такой и ответ :simple_smile:,
а кто нибудь теану под маком в гпу режиме запускал?,
"оно мне пишет какую то дичь CUDA is installed, but device gpu0 is not available  (error: cuda unavilable)",
а сама куда в тестах выдает что все ок,
"ребята, подскажите, пожалуйста - в чем отличие survival analysis от обычной вероятности? То есть, как я понял, survival analysis   - вероятность наступления события, без привязки к наступлению другого, как в ассоциативных правилах. Чем он отличается - сферой применения или еще чем-то?",
"там есть тонкость - в анализе выживаемости учитывается, сколько данных у тебя нет или не будет
так как это обычно в мед.исследованиях используется, то там испытуемые иногда уходят из исследования",
<@U0441TBSV> и те кто были на хакатоне - как прошло?,
"коллеги, подскажите глупому, как выбросы или аномальные пики во временных рядах определяются?
на глаз как-то уж очень стремно пилить",
"Как бы меня не закидали тапками :simple_smile: Первое, что приходит в голову: 
signal - median(signal, 3..5) &gt; threshold
median - медианный фильтр, thresh подобрать",
А где можно подробнее прочитать про разные способы фильтрации таких выбросов ? Может кто нибудь посоветовать ? ,
"был, как и планировал. в общем, там не было ни у кого ML, да  и какого-то математического анализа тоже не было.",
"а вот такой вопрос - скажите, пожалуйста, чем отличаются деревья решений от случайного леса? ну то есть они оба подходят и для регрессии и для классификации, случайный лес, например, делит параметры по следующему принципу: считает суммы квадратов остатков и на том срезе, где они минимальны - делит.  Может деревья решений делают это как-то по-другому или в чем отличие?",
как и в жизни :simple_smile:,
а почему тогда называется деревьЯ решений,
<@U049NHC4X>: лучше в чём? в какой метрике?) по скорости работы — нет,
кому нибудь удалось найти красивую визуализацию карты с данными? как js библиотеку например или хотя бы для отрисовки. вроде <@U040M0W0S> занимался,
"посоны а есть у кого пдфка недавняя Y. LeCun, Y. Bengio, G. Hinton (2015). Deep Learning. Nature 521 ? не могу найти чот хотя была в интернетах где то",
тут показывают как надо  в S3 файлы загружать),
а какие железяки дадут в пользование участникам?,
"Другой фактор, что если рассматривать хакатон, как некоторый коллективный генератор идей, то эффективней обмениваться идеями по ходу и на месте.",
"ограничения на число людей в команде жесткого нет, но как показывает практика оптимально - 2-4.",
а гпу инстанс на амазоне это какой аппарат? а то вдруг там игровая карта на 1гб,
я пока не представляю как идет взаимодействие с игродвижком,
как происходит ввод и вывод,
"предполагается, что у каждой команды в начале будет виртуальная машина, на которой уже будет работать код  DeepMind из статьи в Nature . Дальше можно насиловать это как угодно.",
"&gt; Приглашаем Вас на традиционное неформальное летнее мероприятие Oracle Cloud BI Day, посвящённое бизнес-аналитике, хранилищам данных, большим данным и другим смежным технологиям, которое состоится 9 июля 2015 г. в Москве.
&gt; ...
&gt; Как обычно, Вы сможете задать докладчикам интересующие вопросы, а также, в неформальной обстановке обсудить с другими участниками свой и их опыт работы с продуктами Oracle за кружкой холодного пива.
<https://eventreg.oracle.com/profile/web/index.cfm?PKWebId=0x2349554644>",
"да, мне тоже понравилось, как они там клипы какие-то мутят из моих фоток и видео, музон даже в тему накладывают поверх",
На этом питерском митапе будем разбирать как раз статью из Nature :smiley:,
как обычно что то не найдено,
Но как-то не хочется тратить вечер. Ни у кого собранной caffe под винду не завалялось? не поделитесь? :simple_smile:,
"ну, в большинстве маков нет возможности использовать GPU в принципе,  как и в моем например)",
"Ребят, а какие статистические оценки можно применить для того что бы оценить скученность значений градусов 0 - 360 ",
"а если несколько тысяч текстов, как понять, что один частично копирует другой? bag of words, как я себе представляю, не очень походит ж.",
"ну самый простой способ как раз почти бэг оф вордс, строишь вероятности эн грамм и потом сравниваешь модели",
"&gt; а если несколько тысяч текстов, как понять, что один частично копирует другой? bag of words, как я себе представляю, не очень походит ж.

дедовский способ - шинглы (мегашинглы)",
"как думаете, пост писать на английском или на русском где советы и магический способ всё поставить запилить?",
"коллеги, вдруг кто из вас знает пакет stan или просто способен понять вот это:
&gt;            // Daniel's solution
&gt;            for (n in 1:N)
&gt;                increment_log_prob(log(lambda3) - y[n]*lambda3);
&gt;
&gt;            // Bob's solution
&gt;            increment_log_prob(N * log(lambda4) - sum(y * lambda4)); 

оба решение идентичны, но одно - в цикле, другое в векторной форме. оба используются для Log probability экспоненциальной функции (точнее, ее mcmc-моделирования)
но я не понимаю, логику второго решения. почему идет домножение на N, например?",
"Я не знаю, что там происходит, но выглядит так, как будто к некой скалярной величине прибавляется log(lambda3) - y[i] * lambda3",
"да, именно это и происходит, добавление значения

если без подобных ухищрений, то другая аналогичная функция выглядит так
&gt; y ~ exponential(lambda)

то есть, там где у нас есть вектор значений y - (а N - это длина  вектора Y), мы используем sum
а там, где просто параметр - домножаем его логарифм на N? 
попробую покрутить на других функциях, спасибо",
"Я в свое время не нашел, как это можно делать для текста. Загонял в питоновский filter() и map() ",
"там ничего особо интересного или нового, просто объясняю, как все работает",
"там вон какая ""идея"" есть)",
"как бы не вышло ""скрытый и явный текст без смысла!""",
"я думал, что он платный, но не могу найти где там цена",
но он как раз на дни нашего <#C071Y3R26> приходится,
есть идея как улучшить коллекторские процессы  с помощью апи соцсетей,
правда он какой то чрезмерно сумбурный,
<@U040M0W0S> смотря где и когда,
"<@U041LH06L>: если верить опыту, то главное — где и когда собраться",
чот не угодали они со временем,
"коллеги, возник вопрос. нид хелп, можно сказать.
есть у меня три группы пользователей. A, B и С. Первые две группы различаются по доступу к разным корпусам текстов. A читают одни тексты, B читают другие тексты. третья группа, С, имеют доступ к обоим корпусам текстов.
Мне каким-то образом надо разделить группу C на тех, кто похож на A и кто похож на B. Я правильно понимаю, что уже существующие A+B нельзя использовать в качестве обучающей выборки, так как у них физически нет доступа к текстам другой группы, поэтому то, что они их не читают - не показатель принадлежности?
и как тогда быть? может, натравить какое-нибудь обучение без учителя или какой метод редукции данных на данные по группе C? и вырезать документы, которые есть у всех/читают все?",
"да. это однозначно микс. так как корпусы документов - это юридические документы, бухгалтерские пакеты, и все вместе. это если очень грубо описывать",
"а какие данные, что ты хочешь на них обучиться?",
"пользователь, его тип корпуска документов и какие документы он открывал",
а тот кто покупает релевантен чтоб по нему судить что он купит,
"что сказали то и купил, а что он смотрел - какая разница он же менеджер",
"а если обучаться не совсем корректно, как ты сказал:
&gt; обучаться можно, но не совсем корректно (ты правильно заметил почему)
то можно ли как-то проверить, насколько некорректным решение будет? например, оценить равномерность использования разных корпусов разными группами, которые мы получаем при классификации? то есть, мы определили пользователя группы C как похожего A, и он не особо открывает документы пользователей группы B - будет ли это критерием того, что хоть обучение и было на не особо корректных данных, все равно результат приемлем.
обратно, если некорректность обучения все же высока - размеченный пользователь, напротив ожидания, легко открывает документы и того корпуса, которого, по идее, не должен.",
Не приходит в голову как так быстро,
"понимаешь, у меня только спец.документы
кодексы, проф.пресса и книги и так далее. то есть, пол тут практически не играет (ну я не могу представить, как именно он тут может играть)

плюс надо еще как-то учесть/вычесть документы, которые есть во всех трех корпусах, то есть, все три группы могут их одинаково активно читать.

ладно, буду думать дальше. спасибо.",
"Всем привет, 

Может ли кто-нибудь дать совет по определению схожести разбиений данных на кластеры двумя разными алгоритмами? (Кластеров одинаковое количество)

Я, для наивного начала, сделал биекцию между кластерами обоих алгоритмов (минимизировал джаккард дист, субоптимально, но всё же), а потом этот же средний джаккард дист и считал

Вышло порядка 0.9, что было оценено как не самое схожее разбиение))

Сейчас стал копать глубже, говорят о Variation of Information, но это уже как-то глубоковато, может есть ещё какое на поверхности решение.

Спасибо)",
"О каких гарантиях можно говорить, если мы не знаем распределения для пикселей на фотках, например?",
"<@U041LH06L>: ну просто так как эталонного разбиения нет, и классы не обозначены никак, мне кажется не совсем очевидным, как именно сопоставить одно разбиение другому, какой кластер из первого сравнивать с каким из второго",
"&gt; Сергей Плис, доктор наук, выпускник Университета Нью-Мексико, директор по машинному обучению в the Mind Research Network, расскажет, как машинное обучение используется в подразделении the Mind Research Network — Datalytic Solutions чтобы справиться со сложностями работы с данными нейровизуализации. Лекция состоится в эту пятницу в 19.00 в офисе Future Technologies Media Group. Это уникальная возможность для всех, кто интересуется машинным обучением. 
<https://ftmg.timepad.ru/event/225169/>",
"И в Долгопе завтра что-то со странным названием:
&gt;  Виртуальные смарт машины как инструмент экспоненциальных инвестиций
&gt; Тренды применения Big Data и Predictive Analytics с использованием прототипов Искусственного Интеллекта. 
<https://ftmg.timepad.ru/event/224197/>",
а где завтра в МФТИ Плис?,
"<@U070Y25AS>: Ну мне кажется тут уместно --- кто ясно мыслит тот ясно излагает, готовиться, делает пезинтации аккуратно их оформляет и понимает про что рассказывает. Ну это холивар конечно. 

Давайте о хорошем Руслан и Доша были огонь.",
"А какие существуют методы сравнения рейтингов? Например, у меня есть два TOP10 чего-либо (спортсменов, продаж, чего угодно), которые между собой естественно отличаются позициями участников/товаров, и я хочу их сравнить. Есть для этого что-нибудь ""эдакое""? Мне достаточно ключевых слов, нагуглить я и сам смогу.",
там как раз задача про рейтинги,
Даже вот какая бородатая классика отыскалась: <http://link.springer.com/article/10.1007/BF02288803>,
"Понятно, что в них более-менее тот же набор фильмов расставлен по-разному. Но как эти сходства/различия можно количественно описать?",
"тебе лучше кого маркетологов посмотреть. да и релевантнее будет

да вообще, можно тупую меру разброса взять '- либо размах различий (разница между рангами одного и того же фильма), либо sd",
"ndcg, map, concordance - это вроде как оценка качества ранкирования (т.е. сравнение с идеальным ранкированием)
На практике мы идеальное ранкирование не знаем
для сравнения похожести двух рейтингов может быть тебе подойдет вот это
 <https://en.wikipedia.org/wiki/Inter-rater_reliability>
в частности, каппа и ранговые корелляции",
"ты прав concordance = inter-rater_reliability

я под concordance имел ввиду % правильно отсортированных пар среди всех пар)
есть такая метрика. в библиотеки gbm почему то concordance называется, поэтому я перепутал)",
<@U041P485A>: а сам датасет там как посмотреть? :simple_smile:,
"а если дождь завтра, то как на Стрелке поступают?",
"Ну как я понял количество ребер, которые можешь обрезать, чтобы не разбивать на два подграфа граф. Но с каким-то ограничением на локальность перемещения)",
"когда cmake ом конфигурировал, поставил BUILD_opencv_python3 = 1 ?",
"Меня просто смущает еще такой вопрос. Вот я делаю make, получаю бинарки. Но каким образом они в принципе попадут в пайтон? Откуда либы возьмутся-то",
а opencv ты какой версии собираешь?,
"но вообще вопрос
&gt;  Но каким образом они в принципе попадут в пайтон?
с анакондой тебя будет преследовать постоянно :simple_smile:",
зачем conda когда есть pip?,
"Ну, нет. Мне до сих пор снятся кошмары как я ставил theano + nolearn (lasagne) через pip",
"как думаете, это не фейк?)",
<@U04URBM8V> ты 4й кто мне эту ссылку кидает) вы реально думаете я не видел?))),
"Да, в общем на анаконду все просто встает как выяснилось",
А есть кто-то кто был и на хакатоне <http://deephack.me|deephack.me> и на MS Research Machine Learning Summer School ? Салахутдинов тоже самое рассказывает?),
<@U04422XJL>: как тебе школа кстати? годно?,
и как его зовут?) надо узнать где он будет выкладывать,
"ну я так втягиваюсь потихоньку, но как и многие верчу xgboost по всякому, в общем ничего выдающегося не могу рассказать",
"чую будет как у этойй же конторы был конкурс fireloss, когда после дедлайна топ-100 переколбасило полностью, я тогда неожиданно поднялся с середины в топ-10%",
"Гайз, у нас тут маленький неформальный митапчик завтра вечером планируются в мск около 7-8 в районе белорусской, желающие - велкам, можете отписаться мне в личку, скажу чо как и куда",
так опиши сюда что как и где,
или фк/дк как в лучших клубах москвы?,
"Когда время точно будет известно, напиши плиз",
"есть варик сначала в пивнячок заглянуть, а как леша подтянется передвинуться в кальянную :simple_smile:",
"<@U041SH27M>   ну так не понял, где искать вас. в субботу тогда? ;)",
"А для тех, кто интересовался методологией организации процесса -- вот ссылка на crisp-dm, <http://www.machinelearning.ru/wiki/index.php?title=Crisp-dm>",
и вообще какая логика стоит за фильтрами именно 3х3,
ну я тоже так подумал да. А почему центр так важен? :simple_smile:,
"&gt;&gt;и вообще какая логика стоит за фильтрами именно 3х3

они там пишут об эквивалентности большой свертки стеку меленьких
например 7х7 = 3х3 + 3х3 + 3х3
если считать по параметрам то это 49 против 9*3=27, почти в 2 раза меньше параметров - это первый аргумент, типа получаем свертку 7х7 за меньшее количество параметров, да и ваще любую большую выгоднее представить мелкими

второй  аргумент такой у них: как то год назада выкатил фб коммит в торч где можно большие свертки делать через fft, и типа это быстрее работает; но не для сверток 3х3, такие быстрее делать по обычному, и оказалось что вычислительно быстрее делать три раза 3х3 чем один раз 7х7 через ффт",
вопрос скорее в том почему именно на 3х3 остановились :simple_smile:,
но сраная луа и сраный торч не дали -) не рабзобрался как там,
"у меня там почемуто добавление полносвязного работало, а после добавления сверточного, вылетала строка в dqn где  делалось клонирование сети, на строке где было .clone()",
"Коллеги, кто посещает лекции Сколтех, их записывают организованно? Если да, то подскажите, где можно найти",
"ребята, а вот такой вопрос - если мне надо построить модель, которая на выходе дает 50 разных точных значений, какую задачу решать - классификации? (многовато классов). Регрессии? - может дать неточные значения (можно попробовать привести их к нужным). Есть какая-то регрессия, то ли орто - какая-то, то ли другая, не подскажете?",
"&gt;&gt;(многовато классов)
у <http://www.robots.ox.ac.uk/~vgg/> есть статья где делается классификация на 83000 классов -)",
"ну да, 50 разных значений, короче либерти с каггла, как ты решал, <@U070Y25AS> ?",
"ну кроме букв, которые однозначно категории, есть еще цифери - так вот часть из них тоже вроде как категориальные",
"Регрессия или классификация — вопрос ошибки. Если предсказать x+1 так же плохо, как и x+30, то это классификация",
"&gt;&gt;это, я поглядел на датасет - показалось, что большая часть данных к категориальным относится, ты пробовал переводить?
я их представляю тремя способами: все как категории, все как числа (кроме букв), и смешанно (по распределениям посмотрел и исходя из вида распределения и неизвестно чего, штук 6 отнес к числам, там например по одной явно видно что это какой то процент от 0 до 100)",
ну а из этого уже какие то фичи пробую извлекать,
"я сегодня-завтра еще собираю ключевых спикеров и жду, как мне забугорные товарищи ответят",
"мы еще придумаем, как это по-веселее обыграть. а то яблочные железки это както бездушненько и типичненько",
"о, классическое судейство  как оно есть на самом деле - та еще неоптимальная метрика. но есть какое-нибудь понимание, как сделать его более прозрачным?

Делать голосвание лайками из зала и усреднять счет с судейским? 
Или lasso - давать судьям только 2-3 голоса и просить рассказать в двух словах свой выбор?",
"нельзя, читай первое сообщение. конкурс без точностей, как классиский хакатон а не явно-выраженное соревнование",
"как сравнивать что одни сегментируют поведения юзеров, другие предсказывают интерес и сезонность, а третие просто визуализируют интересы и связки товаров?",
"<@U040HKJE7>: основная беда <#C074F6E1K> как раз была в том, что никто до хакатона не ставил почти нужный софт и не читал ничего. Требование для регистрации запустить бенчмарк эту проблему снимает",
"не думаю что придет так много народу, чтобы не влезли в api moscow. тоесть онлайн-конкурс может быть лишним, хотя просто для тренировки data munging может сойти. и конкурс вида предскажите количество просмотров категории такой-то через месяц. и бонус - через два, чтобы точные данные появились как раз к хакатону :simple_smile:

но давайте лучше про лучшие\худшие практики поговорим

1. прозрачность в судействе. да, надо думать
2. подкорм - да
3. открывать ли данные о хакатона? и делать онлайн-тур? может быть
4...
5. про что все забывают?",
"тоесть сабмит как требование чтобы зарегистрироваться - да. а отбирать людей, кого приглашать\ не приглашать (как было в одноклассниках) скорее нет\",
"<@U054DU76Y>: а про Yac ничего не слышно, когда будет?
я то в прошлом году объявили за месяц, ровно в даты моего отпуска :disappointed:",
"изза такого формата возникает  куча bias'ов и возможностей хакнуть метрику. можно сделать вообще двухфакторную систему:
сперва все вывешивают свои результаты в формате 1 слайда и каждому дается по 30 секунд рассказать кто что сделал. лучших 10 выбирают всем залом. а потом все уходят на обед, а эти 10 команд допиливают свои презентации до формата 3+2 минуты",
"выражаясь птичьим языком, ...
объективность экспертных оценок  начинает значительно снижаться как с ходом времени, так и с ростом числа оцениваемых",
"<@U054DU76Y>: как так не пустили? О_О

<@U041LH06L>: надо хоть раз попасть на него все же. годя четыре не получается уже :disappointed:",
"короче, вещи которые стоит сделать по хакатону.

А. до мероприятия:
1. выложить семпл
2. выложить пару стартер-скриптов
3. финализация регистрации - ответить на вопросы про датасет (типа ID самого популярного товара в категории X, ротация вопросов)

Б. перед началом:
1. краткий рассказ про логистику мероприятия, про жюри (чтобы понимали кто, что, где и когда)
2. краткий рассказ про данные, показать где лежат, как открыть, что внутри
3. поделиться базовыми идеями для проектов
4. ___???___ както сгруппировать  участников (понятия не имею, каждый раз это полуслучайно)

В. по ходу:
1. подкармливать и по расписанию, и для подзарядки силами
2. ___???___ лекции наверное не нужны? а доступ к экспертам нужен? менторы? или лучше чтобы не мешали?
3. ___???___ на ночь какиенибудь пожелания есть?
4. ___???___ пожелания по развлечениям, отвлечению?

Г. финал:
1. нужно прозрачное судейство
2. ___???___ чтото сделать с тем чтобы презентации не затянулись. мб двух-ступенчатые схемы
3. ___???___ чтото еще?",
"&gt;&gt; краткий рассказ про данные, показать где лежат, как открыть, что внутри
желательно что бы это был не снепшот базы -) а то щас на кегле 5 конкурсов в которых тупо выгрузки из таблиц, типа сиди мерджи сам",
"скорее всего это будет здоровенный csv самого cliskstream. где будет session_id, item_id, time 

плюс отдельный csv со свойствами товаров, названиями и т.д. (в целях экономии места clickstream)",
"я думаю одну строчку в R участники осилят написать чтобы смерджить одно с другим, когда это понадобиться :simple_smile: иначе кликстрим увеличится в размерах раз в 30",
"все же понимают, что это как рыбалка или в гараж пойти",
как отличный вариант такого лёгкого отдыха,
"Но не слишком много -- примерно так, как это было в одноклассниках",
"Главное, чтобы не получилось, как в Urban Data Hackathon, когда самая важная лекция с рассказом о том, какие и где бывают городские данные, была сделана непосредственно перед представлением результатов работ участниками.",
про конференцию - когда цены приемлемы на билеты,
бери пример с <@U040M0W0S> “нет ни одного хакатона где бы он не был” :smiley:,
"только те, где он победил!",
"но мы то знаем, почему <@U040M0W0S>  на них ходит :simple_smile:",
"ну ок, кто возьмет нуба в команду? :)))",
"Так, о каком из мероприятий тут идёт речь? Я тоже собрался к Лёше на хакатон.",
а кто на Дататоне без команды и хочет победить?),
"<@U040M0W0S> ща посмотрел, интересно посмотерть как будет выглядеть",
"эцсамое, есть кто в чятике? я б через часик выложил на суд общественности демо афиши для дата феста",
мол странно почему именно америки,
"хороший вопрос, так как чукча не особо писатель :simple_smile:",
"<@U040HKJE7>:  тебе виднее, 
но я бы все равно сделал большой пост/страничку, где все мероприятия коротко и ясно описаны
и на этой афише поместил ссылку на этот пост)",
Я б как минимум текст сделал чуть крупнее и выровнял по краям,
а почему лазанья а не пицца? попахивает хипстотой,
"ладно ща к другому придерусь, почему первые три кружка синие а вторые зеленые?",
небоскреб с надписью бигдата к который врезается самолет с какой нибудь надписью,
"а про ексель было что то год или пол года назад, мс устраивал хакатон по баг дата, где нада было принести ноут с установленным экселем и плагином каким то",
":smiley: кто-то теано изучает, как я посмотрю",
ну тогда будет как евротур в братиславе,
тишина и все думаю как ответить на этот анализ -),
мы не были ни от кого зависимы,
ну т.е. хоть где то должно быть место где можно отвести душу,
"ты лучше расскажи, ты когда свою статью для хабра писал, ты locally-sensitive hashing не использовал?",
оо кстати думаю где фон видел,
издалека пятно над Data Mining Competitions как звездочка выглядит,
"расстановка продиктована во многом тем, из какого часового пояса докладчик",
"так как автор xgboost из штата Вашингтон, то он  будет ровно в 11, открывающим лекции (в 10:00 ооткрытие, в 10:40 приветственное слово на 10 минут)",
"спрошу завтра у посона одного, как раз вувпал бигдата и хайлоад проект",
"Почему на постере 11 сентября, а на сайте 12-ое?",
"<@U040HKJE7>: Леш, а когда будет примерный список спикеров известен?",
а как правильнее называть нграммы из слов?,
а по оффлайн спискерам есть какой то список?,
"<@U040HKJE7>: я еще познакомился с девочкой, которая в DO отвечает за пиар. и собственно такое количество людей -- ее работа. могу у нее узнать, как она это сделала :simple_smile:.",
"не знаю, как Артем отреагирует на такую штуку))",
"_______________________________
убер-вопрос: афтапати BDSM'а

кто остается?
кто что хочет?
кто что точно не хочет?

\\все равно больше 100 человек к Яндексу не убегут",
А у нас наберётся 5 человек на команду для Доты? Я сегодня как раз нашёл ещё один рекомендательный сервис по подбору героев: <http://www.dotawizard.net/>,
а девочек ты где возьмешь?,
я не понял какое BDSM пати имел ввиду леша,
"лол, а еще у него прикольный акцент (теперь я знаю как звучат Ново-Зеландцы)",
"Кто уже читал ""From Word Embeddings To Document Distances” ?
<http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf>
<@U04CH4QBD>:  ?",
как только она размечена - она попадает в базу,
как только будет 4к размеченных новостей по 4 человека на новость - корпус будет доступен для свободного скачивания и раз в неделю обновляться,
А как выгрузить размеченные данные? ),
появится ссылка как только появятся размеченные данные),
только я так и не понял как он размечен,
"не, как меняется апостериорная оценка p с каждым новым броском",
"<@U04CH4QBD>: мм погоди, щас гляну как заново парсится)",
"питерцы, кто 12 сентября не будет в москве, ивент для вас:
<https://vk.com/wall-89201406_41>",
"<@U041SH27M>: ты так говоришь, как будто это что-то плохое...",
ты дома за theano сядешь или куда сгоняешь отдохнуть?,
"<@U041LH06L>: те как пить 4 дня - это пожалуйста, а как на наш митап прийти - то ты не в городе?",
"а вот если у меня есть в данных, допустим какие-то записи, стринги, я их конвертирую в цифры, как категории. Потом запускаю, допустим, РСА, он ведь берет эти данные именно как цифры, ему неважно какой они природы. И он начинает считать собственные вектора, средние и прочее — и вот тут не заключается ли какая-то ошибка, то есть эти цифры ведь по сути ничем не являются, а тут они несут в себе прямой смысл и влияние. Допустим у меня получилось 200 категорий, в какой-то записи будет стоять число 200 - оно ведь может серьезно исказить картину. Как в таких случаях поступают?",
Я уж не помню где советуют категориальные столбцы делать на 1 меньше чем всего категорий,
"для обхода этого (в реализациях что я видел) самый часто встречающийся уровень (либо возможен вариант самого редкого) считался ""нулевым"". тогда не требовался еще один столбец - этот уровень уходил на случай когда все остальные равны нулю",
но случаев когда это напротив ухудшает - нет?,
"В планируются ли в Питере встречи как раньше, по выходным? Лето вроде практически кончилось.",
"угу. натекин сознался про  фест тогда, когда питерцы уже договорились с лекторами и все такое. переносить было бы неудобно.",
"Такой вопрос - вот у меня есть датасет, в котором порядка 5000 столбцов (после one-hot encoding), около половины из которых 0 и 1. Какой метод лучше подходит для уменьшения размерности подобных датасетов? Просто из опыта или поделитесь соображениями? И лучше будет, если провести это преобразование именно только над столбцами с 0 и 1, а к остальным, в которых более менее осмысленные числовые данные, попробовать применить что-то другое?",
"сырой PCA врядли. 

NNMF вариации на тему PCA - может быть. вообще есть специальный категориальный PCA (так и гуглится, categorical PCA)

мысль про то, что категорийники отдельно, непрерывные отдельно в тему

а вообще надо подумать как привести категорийники к божеским фичам и работать уже с ними. например если в каждом факторе много уровней, то к их рангам",
"да методов то куча, вот только какой из них взлетит, вот в чем мой вопрос",
"поцики, а кто могет запилить лекцию по всяким вот этим вот факторизационным штукам, чо, когда и как применять, какие у чего есть полезные свойства и вот это вот все. <@U040HKJE7> , <@U04422XJL> ?",
"а я уже про ""поцики, а кто могет запилить лекцию по всяким вот этим вот факторизационным штукам""
из опыта - каждому приложению свой инструмент",
"<@U049NHC4X>:  а какой датасет был исходный, до dummy-кодирования? В смысле, сколько фичей и каких (вещественная \ категориальная. если категориальная. то сколько уровней)?",
А как категориальные к вещественным приводят?,
"&gt; А как категориальные к вещественным приводят?
Например, через ранги. Каждому уровню ставится в соотвествие ранговый номер (упорядочиваем по частоте).
Веротяностные замены (считаем веротяность клика при условии этого фактора)
Обобщенные средние (арифметическое \ геометрическое \ придумай-свое) по какой либо другой переменной. Например, по целевой.",
"<@U040HKJE7>: тащем та про опыт я и хотел спросить, кто где пользовался и чо лучше",
"&gt; Дорогие подписчики-лингвисты!
&gt; Профессиональный опрос: какими корпусными инструментами вы чаще всего &gt; пользуетесь для своих исследований?
<http://vk.com/wall-89094852_12>",
"Просто я пытаюсь понять, в каких случаях это уместно и чем это можно оправдать?",
"это также как крематорий в Питере - обьект есть, крематория на месте нет (но гдето вдали за леском говорят он там есть). Легенда гласит, что называется",
"ну вопрос-то не только в добавление данных, но и в том, как их потом достают. вот может косяк с последним :simple_smile:",
"О том, как их доставать я тут уже задавал вопрос в начале лета :smiley: Ты же SQL запрос и написал.",
"Я спрашиваю потому, что скорее всего придётся обсуждать эту проблему с коллегами и руководством и убеждать их в том, чтобы подобное не повторилось. Но для этого мне надо понимать, почему такое в принципе могло произойти.",
"_____
я наверное был весьма сонен и очень близок к упоротости, так как я едва смог прочитать вышенаписанное

лично у меня от такого выбора - полный диссонанс, так как хочется и на тусовочку сходить, и наконец-то попрыгать ",
а тем временем вопрос на миллион: как обновить страницу на митапе во встрече чтобы про нее узнали уже подписавшиеся?,
а программа где-то уже есть? кроме как в общем виде на сайте <http://meetup.com|meetup.com>,
а еще это гипер-круто когда у тебя команде есть дизайнер который может взять все это на себя :simple_smile:,
Как вообще тренируют сети где будет binary threshold unit?,
просто я не шутил когда говорил что у меня есть зоопарк :simple_smile:,
"о. как свеже-понаехавшему, у вас ни разу не бывавшему.
просто подхожу к ресепшену и говорю что ""я такой-то, на митап"", верно?",
"эх ребзя, как хотелось бы, тчобы вы митапы не в 6 проводили %)",
"да, я к тому, что т.к. backpropagation у тебя разбивается на подсчёт градиента и изменение весов для каждого слоя в отдельности  — что тебе тогда мешает обучать слой с binary threshold так же, как обычный перцептрон?",
"Его-то может быть, но вопрос в том как я буду делать backprop до слоев которые лежат ЗА binary слоем. Мне ведь нужны производные будут от слоя с бинарным порогом, а я не смогу их посчитать",
<@U04ELSPTY>: так же как ты делаешь backpropagation для выходных нод - у тебя есть desired output для бинарных нод - от них считаешь ошибку,
чот я зря пропустил митачик?),
я тоже часто отключаюсь когда читаю про рнн и лстм,
"Коллеги, а никто не слышал про хакатон от Билайна? Вроде как тоже где-то в конце сентября должен состояться.",
"желтые квадраты это какой либо лернинг, а черные это хардкодинг",
все зависит от какого рекола ты смотришь,
"вот мне интересует какой сделать вывод если логлосс хороший, а аук плохой и наоборот",
"кажется, что логлосс может гораздо сильнее реагировать на аутлаеров в ответе классификатора, тогда как на аук они не будут влиять почти совсем, если данных много",
тогда как логлосс будет плохим в такой ситуации,
<http://www.meetup.com/Moscow-Data-Fest/events/224856462/> как думаете сколько людей придет? рег так дохрена,
"кофе/чай и горячительное предусмотрены
но кто ж знал про такую поганую осень(",
"не
это для тех кто замерзнет на 2м этаже",
можно как уровни ботов из кваки,
также как вариации на тему jedi/padavan,
"когда 2 неочевидно, что это ранговое сравнение",
Learning curve как у покойника.,
Какой же я всё-таки чёткий поцик.,
"Привет, кто в Стали был, как прошло?",
"Любопытно. Кто Сутскевера солушал, у него стандартный DNN доклад был, или что то интереснее?",
"надо повторить для тех, кто отсутствовал",
"господа, а записи/слайды с мероприятия когда появятся?",
"Ребяты, а какую метрику лучше всего выбрать для несбалансированных классов? например, у меня 95% положительных и 5% негативных. Тот же аккураси говорит - ништяк, 0.97, а на деле то он вообще все по нулям посчитал. Смотришь рекол - и правда, нули.",
Я не понял как может измениться результат линейной (или логистической) регрессии от масштабирования ,
как минимум вот связь: изменение масштаба -&gt; изменение параметров логрегрессии -&gt; а тут будет влияние регуляризации,
<@U040M0W0S> как ночь яндекса прошла?,
"гайз, а кто мастер гуглокалендаря, как на него ссылочку получить с расписанием мероприятий на ближайший месяцок, например",
<@U041SH27M>: Саша прав. Ссылка в описании канала как раз на месяц: <https://www.google.com/calendar/embed?src=oln8j0q24rm8i5h0a0bukb5fv0%40group.calendar.google.com&amp;ctz=Europe/Moscow>,
"кстати, кто хочет -- могу дать права на добавления. я бы его вообще открытым сделал, но это, кажется, невозможным.",
"эцсамое
типа-аннонс

я на дата фесте говорил - наш ритейл хакатон переносится как минимум на месяц. в конце сентября\начале октября в diginetica никого не будет на месте (Москве)  :disappointed:",
мы просто еще не выбрали когда это удобнее всего провести,
<@U040HKJE7>: а когда видосики с прошедшего можно будет посмотреть.,
"Нет, серьёзно, разве лог. регрессия без регуляризации где-то кроме как в учебных примерах существует? Там же в случае линейно разделимых данных без регуляризации оптимизационная задача не имеет решения",
"Да, когда обучащую выборку можно точно разделить прямой",
"я собственно почему сказал - мне казалось, что дефолтный способ делать logistic regression в R не делает регуляризацию",
Видимо когда predicted probabilities уже близко к 0/1,
"Но это не регуляризация, как я понимаю",
"Про катеогриальные вещественные. Перепечатваю по памяти коммент, который писал в этой же ветке несколько месяцев назад.
Есть 2 крайности: все фичи вещественные -- юзай деревья, все категриальные -- юзай линейные модели. 
Это правила большого пальца, в них есть исключения, но в целов все примерно так.
Когда данные смешанные -- нужно смотреть, как их лучше привести -- все к вещественным или все к категориальным. Нюансов много, но в целом -- чего больше, к тому и приводи.
Когда признаков примерно 50\50, и приводить их к одному знаменателю трудно -- да, работают ансамбли. В Tradeshift ансамбль из моделей отдельно на вещественные фичи и отдельно на категориальные показал топ1 результат",
А как рекомендуется приводить categorical к numeric?,
"<@U04422XJL>: именно это и является главной проблемой слака, как я говорил в субботу на afterpaty <@U041P485A> и <@U0AJU5AMA>. <@U041SH27M> вроде как хотел делать wiki для того, чтобы подобные знания не пропадали, <@U04CH4QBD> предложил запилить что-нибудь на Q&amp;A движке a la Stackoverflow.",
"slack write-only medium, как перл :simple_smile:",
Для меня в тот момент когда количество инсайдов кратно уменьшилось относительно секса все стало бессмысленным,
"ну и все остальные, кто рядом с Белорусской - тоже го :simple_smile:",
в здании авито как раз расположен,
а до скольки - как уйдете),
"<@U040HKJE7>: а про h2o будет запись с датафеста? и как вообще, интересного чего рассказали про него?",
"хотя нет, что-то не пойми совсем. Спарк не про распределенное хранение данных, у ш2о тоже есть RDD, как и у спарка. Тогда для чего слой Спарка между HDFS и  H2O",
а есть кто в Берлина у нас?),
"Он везде об этом говорит и пишет. Я не понимаю, почему на него многие так гоняют",
"я спросил кто занимается машинным обучением, руки подняли человек 10-15 из на вскидку 80",
в технической секции как каз наверно 80,
"на самом деле конференция, как и ожидалось — большой фейспалм",
"Как они зарабатывают на ""бигдате""?",
"На таких конференциях еще бывают инвесторы, бизнесмены, обеспеченные бездельники и просто интересующиеся, журналисты, разного рода случайный менеджмент (как обычный, так и ИТ), ну и ученые. Для всех них технические детали, даже самые простые - почти излишни.
Они почти и не обманывают: ""Конференция является местом презентации самых эффективных кейсов использования новых технологий Big Data в бизнесе, а также дискуссионной площадкой для отслеживания всех трендов в области анализа больших данных + место для объявления научных открытий и исследований"" 
Не NIPS :smiley:
<@U0A7CPPDL>:  а как они могут зарабатывать - вот тут кейсы для банков <http://firrma.ru/data/articles/6047/> (больше и правильно продать, чтобы меньше уходили, чтобы оборудование более оптимально работало, чтобы иметь сбалансированный кредитный портфель)",
"или просто, в рамках ""ярмарки тщеславия"" рассказать как все клево, биг дата отправит вас отсюда с места прямо в космос",
Кто там ещё на конференции? Напишите результаты чемпионата :simple_smile:,
"&gt; Кто там ещё на конференции? Напишите результаты чемпионата :simple_smile:
&gt; А ни у кого, часом, нет желания сегодня забухать?
<http://cs4.pikabu.ru/images/big_size_comm/2015-03_5/14270968775294.jpg>",
"<@U04422XJL>: выглядит так, как будто конференция была настолько плохой, что никто не выдержал до конца :disappointed:",
"хотя когда Стас Семенов и Дима Алтухов сказали, что они в одной команде, то вопрос про первое место как-то у меня отпал ))",
<@U054DU76Y>: а кто в итоге выиграл?,
хотел тебе при встрече рассказать как раз,
"как я понял, всем, кто вошёл в верхний топ (порядка 10) прислали приглашение",
"Впечатления с конференции:
На технологический трек специально не пошла (DCA и Анохин доступны бесплатно в любом объеме :simple_smile:  Я хотела послушать людей, которых на наших обычных митапах не встретишь. 
В целом доклады были очень верхнеуровневые, ну прям очень. Тот же Тинькофф рассказал, что одна из их главных фишек – активное использование внешних данных, но источники назвать отказался. Комментарий из зала: «Да, те же что и у всех – соцсети и операторы». Позже представитель Double Data подтвердил, что Тинькофф покупает у них данные соцсетей, однако Мегафон и Билайн заявили, что они свои данные на сторону не продают, но готовы делать совместные кейсы или разработать для вас сервис. Кто же из операторов тогда продает данные не понятно.
У Яндекса был интересный кейс (хоть и без подробностей) про прогнозирование увольнения сотрудников. ФНС сказали какой у них размер hadoop-кластера и в перерыве еще подробнее рассказали об архитектуре и с какими проблемами столкнулись в процессе. Порадовал А. Плешаков откровенным докладом о ситуации на рынке данных (касается RTB, DMP, DSP и т.п.). «Рынок данных если не черный, то точно серый». Удалось поймать А.Сербула не обвешанного пачками студентов. Остальные докладчики для меня мир не открыли, но для общего развития послушать было интересно",
Ещё напишите в комментариях кто что делал,
"вопрос к тем, кто участвовал в конкурсе сбера - а данные были за январь 2015? и далее по 2015? и отток - это уход из банка?
Если так, то данные же не имели смысла из-за макроэкономической ситуации. падения рубля, затаривания вещами/покупками квартир, самыми низками ставками сбера по сравнению с другими банками и т.п.",
"Ну он вроде как находил тех, кто его ДДоСил",
чот я смотрю платные курсы по мл и дл становятся модными,
"<@U0873FY94>: значит нужен национальный фокус на знание английского, как в китае. я понимаю, что в DL далеко со знанием одного русского языка не уедешь. но на самом деле так везде. а основные из бесплатных интересные лекции - на английском. 
поэтому пока у нас англ не знает каждый как отче наш - пусть хотя бы на русском сделают бесплатные. вот я о чем.",
"<@U0AJU5AMA> есть видеокурсы ШАДа и Техносферы по машинному обучению на русском языке. Не уверен, что национальный фокус - это решение, да и зачем знать каждому. Тут, как везде в ИТ - только саморазвитие и личная заинтересованность.",
"мы делаем задачку на inclass.kaggle с анализом данных твиттера. Собираем списки айдишек по двум взаимоисключающим категориям: в прошлом семестре было ""увлекается компьютерными играми”/""не увлекается”. На кэгл закидываются айдишки и категория, больше ничего. Студенты самостоятельно выбирают какую инфу им получать по этим айдишкам через апи, делают фичи и строят модельки. Победитель получает книжку (мерфи или бишоп). Все ДЗ оформлены в виде айпитоновских ноутбуков, всего их 5. Могу желающим скинуть те ноутбуки, но не данные. Но читать их довольно бесполезно - они пока не отшлифованы до конца. Кстати, лекции на ютубе это очень старый прогон (вроде, пару семестров назад — сейчас курс сильно изменился, например теперь 2 семестра вместо одного и <@U06J1LG1M> рассказывает аж 6 или 7 занятий про сети)",
"В пятницу, 25го сентября - новая гастроль Щепотки Соли в Москве. В программе - Deep Learning от Анатолия Левенчука. Подробности и регистрация - тут <https://thinksharp.timepad.ru/event/246564/>

Напомним правила:
• Первое правило клуба: «Позови друзей, которые круче тебя». Мы вот позвали.
• Второе правило клуба: «Лучшая часть конференции – это кофе-брейк». Ну и бар после.
• Третье правило клуба: «Человек – это лучший инструмент для обработки неструктурированной информации (по состоянию на август 2015)». Или нет. Как раз обсудим, встреча тематическая.

Что будет: 
- много информации на интересную тему;
- шанс задать вопросы и получить ответы (но это не точно);
- возможность познакомиться с новыми, не менее крутыми, чем вы, людьми (это-то точно).

Формат: не такой, как в прошлый раз, чтобы не привыкали.",
я придумал как промотивировать дойти до делойтта,
"первые 5 человек из этого чатика кто дойдет до офиса приглашаются на кальянное афтапати ко мне домой. там проектор, кальян, все такое. идти пешком 15 минут в сторону динамо",
вот почему пятница то уже занята :…(,
"Котаны, а кто-нибудь знает алгоритмы кластеризации, где можно явным образом задавать кластерные центры и применять на сетевых данных кроме k-means / k-medoids?",
А кто ведёт этот блог?,
"Потому что после моего доклада про ""псевдо-бимодальные"" сети самый популярный вопрос: ""А почему бы вам просто не прогнать на этих данных k-means, взяв за центроиды топовых юзеров?""",
"ну как вариант запустить affinity propagation, mean shift и k-means",
и посмотреть как далеко будут центройды от топовых юзеров,
"Label Propagation, кстати, на наших данных в R ""встал колом"", хоть в документации и значится как линейный по комплексности.",
<@U040HKJE7>: кто все эти люди и что здесь происходит?,
"Театр. Свет притушен. Полная тишина, идёт спектакль. Вдруг ряда из третьего раздаётся возглас: 
- Доктор! Есть в зале доктор?! 
Где то с балкона отвечают: 
- Я доктор. Что случилось? 
- Коллега! Что за хуйню нам показывают?! ",
"а где тут вход, одни рестораны",
"ребят, а что почитать на тему небинарной классификации и почему ее делают через бинарную. в чем тут соль?",
иерархическая кластеризация - нигде не встречал ее кроме как в литературе),
а кто сказал что есть “требуемая” иерархия?),
Плюс её можно использовать как предварительный шаг перед k-means. ,
<@U04CH4QBD>: а в какой литературе встречал? :simple_smile:,
"<@U040M0W0S>, не совсем понятно, как иерархическая кластеризация связана с иерархической классификацией, если иерархия уже задана ",
"Taras Sereda @Simon Kozlov
Пока пробую построит иерархический классификатор сверху над плоским. Может видел specificity/accuracy trade-off <http://www.image-net.org/projects/hedging/>
Если кратко, позволяет пройти вверх по дереву классов в случае неуверенности в предиктах на лиф нодах. В итоге получаем предсказание более обобщенного характера но уже с большей увереностью.
Simon Kozlov @Taras Sereda
Интересно. А как тренируется предсказание класса предка?
Просто это еще один из классов в output layer?
Или как-то сложнее?
Taras Sereda @Simon Kozlov
Тренируется поверх плоского классификатора. Задача построить классификатор который максимизирует реворд при заданном уровне точности. Допустим есть 50 лиф классов. Но мы знаем что их можно организовать в иерархию (если можно) Строим дерево. В нем будет количество нод = 50 + интренал_ноды. Где интернал ноды супер классы потомков. Дальше определяем реворд для предсказания на каждом из уровней. Ревордом может быть information gain. Который максимальный на лифах и 0 в руте. Реворд знаем распределение вероятностей тоже есть, вычисляем мат ожидание для предикта на каждой ноде. Дальше весь процес состоит в том что бы найти lambda. Ищем ее бинарным поиском. которая превращет реворд в reward += lambda при котором заданная точность должна находиться в пределах доверительно интервала. Если это так в дальнейшем используем найденную lambda в момент предсказания. Зависимость такая lambda -&gt; +inf приведет предсказания в рут, если только какая-то из интернал нод не имеет минимально заданную вероятность попадания.
Sep 18
Я вот перечитал что написал, может я не все достаточно ясно описал. если интересно могу показать в коде. так яснее должно быть ну и в статье тоже детальней описано, ссылочку выше давал.",
"&gt; ermakovpetr
&gt; иерархическая кластеризация - нигде не встречал ее кроме как в литературе)

&gt; i
&gt; <@U04CH4QBD>: а в какой литературе встречал? :simple_smile:",
<@U086VRZCL>: а где можно на код посмотреть?,
"кто выберет бар? :simple_smile:
мои пожелания - что-нибудь не очень шумное и с приличным выбором пива
<@U041LH06L>: или водочки, как собирались как-то? ;))))",
"а есть еще какие варики, чтобы это не было крафтовым пивом?",
1516 как то связан с бывшим баром 1920? :simple_smile:,
Как вы в такой ситуации решаете ?,
"Разумеется, все null samples должны быть репрезентативными, т.е. какие могут быть в тестовой выборке ",
можно кста крепыша какого вкусного заказать под прецедент,
"да следующий збс, расскажешь как там враги за океаном поживают",
"<!here>: в общем, приходите кто может сегодня в Каск на Китай-Городе к 19:00.",
"ок, кто может 6го - плюсуйте это сообщение",
а кто может 7го - плюсуйте это сообщение,
"<@U041LH06L>: чота рано, я в 19 только с работы выйду...",
так вы чо когда решили собраться?,
"Это поиск решения, которое минимизирует ошибку в случае, когда точного решения не существует",
"почему такой линии не существует? вдруг точки такие, что она есть. или где-то прямо говорится о том, что ее нет? я понимаю, что скорее всего ее действительно нет, но ведь мгут и ""вырожденные"" случаи, не? :simple_smile:",
"У нас на вычах даже говорилось, мол, вот есть СЛАУ Ax=b, можно его записать как ||Ax-b|| --&gt; min, добавить регуляризатор и решать уравнения, не имеющие решения",
Можно понимать как обобщение на другие системы уравнений. Можно как обобщение как системы оптимизации ошибки,
<@U040M0W0S>: это когда ты добавляешь штраф за норму вектора решения,
ага. а зачем нам наименьшая норма? :simple_smile:,
не понимаю норму :simple_smile:. или почему из всех норм мы выбираем наименьшую,
А на практике пайплайн проседает очень много где,
"<@U041SH27M>:  пока что обкатываем модели, которые могли бы хорошо работать для end-to-end диалоговой системы и смотрим что можно было бы соединить и как",
Что-то не сходится у меня математика. Вот смотри как я оцениваю - <https://gist.github.com/anonymous/9af7e168f13ac1a6619e>,
"Интересно же, как они с реальностью сочетаются :simple_smile:",
Посмотри какой у тебя настоящий размер плз :simple_smile:,
как она там сама себе считает forward и backward,
<@U041P485A>: тестировал neon vs cudnn на caffe? Или на какой библиотеке на caffe свертки делались?,
"камрады, а есть ли среди нас те, кто знает байесовскую статистику? и ее реализацию в BUGS/JUGS/Stan?
нид хелп, прямо говоря.",
"<@U04ELQZAU>: ну, собственно, регрессионные иерархические модели с заданными распределениями и распределениями парамеров, фитинг распределений и так далее.

плюс, у меня теоретический интерес - если я хочу оценить влияние методов препроцессинга данных (нормализация, удаление аутлаеров и так далее) на результат проверки гипотез  - могу ли я в качестве контроля брать результаты bayesian anova или каких аналогов.",
несколько раз прочитал название как отрытые инновации. теперь не могу их с ними не ассоциировать,
"о птичках и нейротехнологах -  <@U0989QUVC> рассказывала что планируется чтото клевое с лабой где Илья Захаров, эдакое мероприятие\площадка про просвещение на тему нейроинтерфейсов, нейронауки и прочего настоящего нейро, которое к нам не относится",
"<@U041SH27M>: нейронет нти это как весь нти про то как денег у государства взять на выживание, только про слово нейро и все кто к нему смогут пристроиться",
"суть всей движухи такая: есть такой Тимур Щукин, который говорит, что пару лет назад придумал слово “нейронет” про ""картирование мозга и наше всё”. Эту тему подхватил Дима Песков, который начал раскручивать ее как политическую. Но ему показало одних мозгов мало и он сделал НТИ раскопировав идею по секторам авто/аэро/еду и прочие “НЕТы”. Теперь на это дали денег, осталось найти кто покажет результат.",
"а руководители рабочих групп - это те, кто набирает людей для осваивания лавандоса в определенных нетах?",
"там не то чтобы набор людей под конкретных людей, а сетевая структура - т.е. все вместе сели, договорились, какие должны быть направления, определили сколько нужно всем денег, деньги выделили и они начали разбредаться по направлениям и конкретным структурам
т.е. новые не создаются специально, насколько я понимаю",
"Позже будет нормальный анонс, когда утрясу список рассказов и оргвопросы",
"А где есть ссылка на календарь ""Data Science"" из которого бот события постит, я бы на весь полностью подписался бы",
"а что значит нужно? -) ты можешь кластеризовать что угодно, например вдруг вот нужно (не знаю зачем правда) кластеризовать что то методом k-medoids если функцией расстояния является средняя корреляция фич",
такое определенно нада кластреризовать когда фичи скореллированы иначе будет расстояние между все ноль,
"Фишка в том, что  фичи  можно разделить на 2 группы в зависимости от того, каким образом были получены факторы: в одной группе было больше переменной для ФА, а в другой части из них была удалена.",
В вышеозначенной книге как раз есть пара приёмчиков для подбора числа кластеров и оценки их устойчивости.,
"Собственно, разница между чистым PCA и Factor Analysis как раз в том и есть, что часть инфы теряется, но это не страшно, т.к. мне важна интерпретабельность полученных факторов.",
"Уже не надо, весь день мучался, но стоило лечь спать как сразу вспомнил:
<http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf>",
"&gt; вдруг вот нужно (не знаю зачем правда) кластеризовать что то методом k-medoids если функцией расстояния является средняя корреляция фич
месье знает толк в извращениях",
"факторный по сути, та же регрессия, только без учителя. а для нее надо бы как минимум пять наблюдений на переменную, лучше 10-20",
"Всё-таки ""машинистам"" проще: увеличил AUC и норм. А тут: как ни крути эти оси -- смысла как не было, так и нет.",
"Чувствую себя как та проститутка из анекдота, которая увидела на улице купюру в $100 и обрадовалась, что нашла. А когда наклонилась, чтобы её поднять, поняла, что не нашла, а зарабатывает...",
"откуда такие чувства?

я имею ввиду, как они рассчитываются",
Как рассчитываются -- не знаю. И знать не желаю.,
Какие только буквы к NN не добавляют ,
"а что про среду где анонс или уже перенесли? <@U040HKJE7> к вам это
я уже массажистку знакомую позвала!!",
"<@U04422XJL>: они приблизительные :simple_smile:. ты когда афишировал -- сказал, что точные появятся позже. так где и когда?",
"вопрос к академическим коллегам по поводу правильной организации исследования. Допустим, я хочу проверить гипотезу, что девочки пишут сочинения лучше мальчиков. При этом, допустим, у меня есть достаточно большая база сочинений, скажем, 10 тысяч. При этом, хорошее сочинение то, которое высоко оценивается моей бывшей учительницей литературы (это я говорю, чтобы подчеркнуть, что ""хорошесть"" сочинения не есть суть вопроса).
Соответственно, я могу дать учительнице эти сочинения, пусть оценит, а я сделаю проверку на равенство средних. Но допустим, моя учительница старая и я, дабы обеспечить успех, хочу процесс ускорить (идею с мапредьюсом старушек не предлагать).
И вот, нашёл я чудный сервис, который умеет оценивать сочинения, но я хочу убедиться, что он согласован с моим учителем. Для этого я даю, например, 200 сочинений туда и туда, сверяю, убеждаюсь, что алгоритм ошибается, скажем, в 5 процентах случаев. Но меня это не пугает, потому что, общая тенденция в выборке должна сохранится.
А теперь, внимание, сама проблема. Как мне видится, моё исследование можно обвинить в том, что хер его знает, что там алгоритм наоценивал. Так вот как можно оценить влияние этой погрешности на результат исследования? Можно поутверждать что-то вроде - мы протестировали 100 сочинений, ошибка составила 5 процентов, тогда с уверенностью 95% можно сказать, что ошибка классификатора не превышает, например, 7%. Тест на равенство средних при этом уровне ошибок и в предположении о независимости оценок индикаторов ошибки дал такое-то p-value. Это прокатит? Как вообще такие исследования делаются?",
"Принято ли в таком случае убеждаться, что посчитанный мной показатель согласованности оценён нормально и не слишком зависит от того, какие 100 сочинений я выберу? Набутстрэппить там... Или до этого обычно не доходит?

Всё равно меня что-то гложет... Потому что точность я и так могу померить, с этим проблем не очень много.",
"<@U042PRW1V>: альфа кронбаха вещь достаточно примитивная - все же это, по факту, корреляция каждого пункта с общей суммой по пунктам. если у  тебя всего два ""пункта"", то есть, оценщика, то проще вообще сделать корреляцию между ними.
далее, на больших выборках оценка значимости различий почти всегда срабатывает - специфика конструкиции методов,  к сожалению.

если честно, я не поймал проблему целиком. можешь сформулировать конкретно, какую гипотезу ты проверяешь?",
"ну так это две задачи. первая - о различии мальчиков и девочек. решается относительно просто  -какой-нибудь метод сравнения групп (необязательно по средним, анова по дисперсии смотрит), и накрученный сверху бутстреп, с ограничениями на размер семплированной выборки, чтобы совсем больших выборок не было. просто на выборке в 10к наблюдений хи-квадрат, например, так же как и коэффициенты корреляции, всегда дадут значимое различие. (вообще, я на таких размерах смотрел бы уже просто средние/медианы).

вторая задача - качество оценки. и тут встает вопрос, с чем ты будешь сравнивать. но,  впринципе, дизайн почти тот же, смотришь либо корреляции как согласованность оценок, либо значимость внутригрупповых различий (оценщик как фактор внутригрупповой изменчивости).",
кто нить хочет синглмолтика на попоечку  на двоих-троих-десятерых заказать?,
"в среду, когда ж еще",
я лично рпиглашал тех кого вспомнил :simple_smile: <@U0AL6V5TN> заходи за компанию,
"<@U040HKJE7>: спасибо, может загляну, а как место называется?",
а кто со сбертеха у нас тут?,
"чтото про ДМ делать можно, но результат оценивать будут как мобильное приложение в общем конкурсе",
есть идея как раз поиск по фотографии человека замутить,
"лол, сказать что вы по фотке ищете наиболее релевантный список подписанных пабликов этому человеку. какая интересная случайность, что чаще всего этот список пабликов прилетает вместе со страницей пользователя :simple_smile:",
"<@U070Y25AS>: дада, искать страницы где больше фоток с этим человеком. вдруг человеку косплейщица на комикконе понравилась - а как ее найти? а где еще таких найти?",
"<@U041P485A>: <@U041LH06L> а до кучи, помните идею про порно? берем порнхаб и его скрины с видео. а потом ищем по фотке из вк - в каком ролике самая похожая на фоточку актриса",
"как бонус, можно быстренько накраулить порно-роликов в ВК и сразу ссылку давать",
"Яндекс, не хранит фотографии пользователей в ВК, а ВК не дает поиск по фото поэтому, как я понимаю
если выгрузить фотки с аккаунтов, связать их с именами и открыть доступ для поиска, то это незаконное использование персональных данных",
"речь о том, что нельзя искать тех у кого страничка не закрыта или вообще всех?",
"искать ты можешь) просто те кто поставил такую галочку - что не все видна вся информация - значит они не хотят делиться своими персональными данными. И если как-то выкачать их данные и дать доступ к поиску - это противозаконно.
Тут мутная тема с ПДн, поисковиками и соц сетями. Я бы рекомендовал не влезать, без хорошей юридической поддержки",
"Всем привет. Я из команды, которая участвовала в deep learning hackathon (<http://deephack.me|deephack.me>) в июле в МФТИ. Мы там заняли первое место. Сейчас решили попробовать собирать раз в неделю или раз в две недели людей, т.е. организовывать лекции или семинары, так как тема очень интересная, и все продолжаем ее изучать. Конечно, одним deep learning не ограничивается - любые интересные течения и направления, особенно связанные с моделированием поведения рационального агента в среде. Первый семинар планируем сделать 18 октября по механизму визуального внимания в нейронных сетях. Часть нашей команды (я туда не вхожу) сейчас пишет статью именно по этой теме, с целью ускорить обучение нейронных сетей в условиях игр Atari (статья Гугла в Nature за февраль 2015).",
"<@U0C1BGRB2>: а вы хотите собираться небольшой но плотной тусовочкой, или прямо сделать из этого митап как митап?",
"Я хочу именно небольшие группы. За исключением ситуации, когда будут приглашаться какие-то прямо крутые чуваки которых все захотят послушать.",
просто я если что могу помочь организовать все как митап с столько людей сколько захочется :simple_smile: но в таком случае средний уровень аудитории падает (хотя число тех кто постепенно вникакет в тему растет),
"ок, как надумаете - пишите :simple_smile: ну или мб <@U070Y25AS>  и <@U041P485A> захотят митап по DL замутить :simple_smile:",
"Я могу ссылку на евент сюда копировать и описание (если кто не зарегистрирован на фейсбуке), не вопрос.",
"Как к дому добраться легче, от Динамо или Белорусской? может автобус какой есть?",
а где час тусоваться тогда?,
стукнитесь ко мне когда начнете плюсы раздавать,
С увдовольствием попью кофейку завтра после 19 с тем кто его возьмет <@U040HKJE7>,
"всем спасибо :simple_smile: 

дата саенс был достаточно плотный чтобы по ходу серкл джерка до линкдина  ни у кого руки не дошли",
кто будет в Я сегодня?,
чот думаю к двум можно не приезжать -),
"Ребят кто ставил caffe под CUDA 7,5 &gt;",
"да мак говно, после обновления перестало много чо работать, включая теано и кафу, а переустановить нельзя тк xcode что то там не поддерживает и нада ждать следующей версии, или скачать старую, установить в ту папку где текущаа версия лежит, ну тупо перезаписать файло, и может заработает",
"<@U06J1LG1M>: ахахахах вот-вот. Но я скачал xcode 6_2, вот с ним как раз не заводится ничего",
"<@U042PRW1V>:  так как Big data понятие широкое, то стоит уточнить для удобства понимания)",
<@U04488690>: мы просто в основной секции выступаем и можем заглянуть посмотреть что и как,
э а как же проект от <@U040M0W0S> как самого трудящегося?,
а кто нибудь собрался на робоночь?,
<@U0873FY94>: нет сил на робоночь - 4 встречи за день и выжат как лимон,
как оно там из ваших глаз?,
"<@U06J1LG1M> это Робоночь в Сколково
роботов много, но мы ничего почти не видели т.к. играли в робототехнический квест и ура)) команда DeepHackLab хоть была и не в полном составе, но мы решили квест, заняв первое место из 20 команд))
там много Промоботов, три дрона, несколько каких то еще роботов и там вроде даже битва роботов была) 
но согласен, мероприятие больше гиковское и просто потусить)",
6. Так же бывает когда  объект не попадает ни в один класс,
"Если да -- то зачем тебе пляски с порогами, если все равно в итоге argmax берешь?",
когда я просто брал максимальную вероятность (не считая порога) точность была от 0.40 до 0.65,
когда я взглянул на пороги - они очень смешены для разных классов,
А как как и что ты учишь? Честный мультикласс или 33 one-vs-rest ?,
"Вариантов могу предложить 3: перейти на честный мульткласс, построить композицию (чтобы на втором уровне был честный мультикласс), использовать так, как сейчас используешь",
"<@U04422XJL>: попробую, вопрос скорее был наживкой - теперь знаю кто имеет какие-то знания на этот счет и кого при встрече теперь терроризировать",
Вторую вроде понятно как решать -- приводить к софтмаксу,
"какая неожиданность что pca не работает :simple_smile: 

кстати, glmnet в R умеет sparse pca (spca функция). его можно попросить топ 100 компонент чтобы в каждой было только 100 фич",
ментальный оператор это как то связано с наркотиками?,
"<@U040HKJE7>: где ж тут дзен? бери ручку, в которой кончились чернила, и никаких линий не будет :))))",
"котаны, а кто нить юзал RNN для предсказания time series  каких нибудь?Продаж например. Или слышал какие-нить успешные применения сего чуда",
"Просто вот смотрю на задачку по предсказанию продаж <https://www.kaggle.com/c/rossmann-store-sales>  - 1000 шопов, пару лет истории продаж по дням + какие нить дополнительные свойства магазинов. Складывается впечатление, что процентов 15 задач на кегле это прогноз продаж  чего-либо, ну и в целом задача классическая - что количественные продажи в штуках для айтемов, что в объемах для чего-либо другого, вотэвэ. В таких задачах есть и сезональность, и тренды +-

Поэтому  есть такое впечатление, что эта проблема неплохо ложится на RNN/LTSM архитектуры + всякими онлайн лернингами может обучаться даже в продакшене, прости господи. 

Вот хотелось узнать, слышал ли кто-нить чо нить про такое и желательно не в 200 страницах академического чтива :simple_smile:",
"о, так это прямо наша задача с почти готовым решением по банкоматам (кстати, статья тоже вроде как в доступе)",
мы для 150 банкоматов предсказывали в какой день сколько бабла снимут (и юзали это для оптимизации расписаний инкассаций),
Как в старой физтешной байке -- интеграл exp(x)dx,
"Пацаны, а есть ли в треде те, кого заинтересовала статья про тензирование нейросетей?  (<http://arxiv.org/pdf/1509.06569.pdf>)",
"Ну вот он как раз и будет, да)",
"Никто <https://ru.wikipedia.org/wiki/Hierarchical_Data_Format> не пользовался? нужно хранить много чисел с большой точностью (тысячи десятичных знаков), и таких данных терабайты. Числа получены с исследования дзета функции Римана, и это нужно все в открытый доступ отдавать. Какие базы для такого типично использовать?",
"или знает, где про это можно прочитать :simple_smile: (в статьях Гугла я это видел, но там не написано про ""как"" :smiley: )",
"Гугл рассказывал, как они тренировали на 8-ми GPU одновременно, первые 4-ре считали последовательно слои, последние 4 считали softmax (это для seq2seq)",
"на сколько я знаю пока нет эффективного моста между гпу (это нвидия планирует вот вот сделать) и данные идут через оперативу и это дорого; например можешь глянуть в код которым обучался алекснет, там обмен между 2 гпу, и типа как то оптимизировано что бы минимизировать обмены; форк каффы на которой тренировались в 2014 году VGG тоже модифицирован под 2 гпу; думаю полазив по репе алекснета и вгг ты найдешь что нибудь и на больше чем 2",
"они на какой то презентации говорили что будет на картах новый порт, и мост можно отдельно докупать будет, и типа купил и соединил карты",
кстати вот тесла к80 это в реальности 2 к40 соединенные вроде как чем то типа такого моста,
кстати кто не в курсах завтра лекторий образовача гиковский в музее москвы с 19 вроде часов,
чот накладывается сингулярность на дип лернинг тусу,
"<@U065VP6F7>:  задачи можно распихивать по отдельным ядрам, никто не спорит. Я говорю о ситуации, когда у нас одна задача (тренировка нейросети) и ей нужно очень много видеопамяти.",
"есть ещё <https://github.com/dmlc/mxnet>, где тоже multigpu заявлено, но я не пробовал",
"вообще сказка) а когда следующий релиз, не знаешь?)",
"мы как то тестили 3 либы для TTS - эпловую, гугловую и яндексовую. Гугловая зарешала.",
а кто сегодня идёт в Яндекс?,
"Котаны, а напомните относительному нубу, какие модельки используются для каких нибудь question-answering систем, может ссылки на статьи/еще какие веселушки. Недавно научно популярная статья на хабре даже выходила, где чуваки обучали что-то на субтитрах и получали такую систему, если кто помнит как ее найти, скажите плиз.",
"<@U041SH27M>:  это мы обучали, если какие вопросы, можем подробнее прокомментировать. Там самая примитивная модель, есть еще, как минимум 2 архитектуры подходящие - seq2seq и HRED",
"Гугл код зажал как обычно, у Бенджио есть",
"В смысле, почему в русском намного меньше?",
"коллеги, несколько наивный вопрос, но я немного торможу
у меня есть несколько а-ля логарифмических кривых, примерно до 20 точек  в каждой
мне надо выбрать несколько наиболее близких к одной определенной, с такой же формой

какую метрику мне корректнее использовать здесь? что-нибудь притивное типа коэффициента корреляции спирмена?",
"<@U04423D74>: а какой должен быть смысл в близости между кривыми? просто если они всегда одной и той же природы, то ты можешь сопоставить им какое-то описание в виде коэффициентов кривой(типа a*log(x + b) + c ) и задать метрику над ними",
нарот а со сверточными автоенкодерами имел кто либо какие нибудь отношения?,
"<@U06J1LG1M>: я на прошлом месте работы с ними работал. А какие имеются ввиду ожидания? На мой взгляд они неплохо отрабатывают при послойном обучении deep-сети; получалось неплохое начальное приближение для последующего  fine-tuning'а, что ускоряло его сходимость.",
а ты какие либы юзал?,
"просто по статьям у меня сложилось впечатление что глубокий сверточный автоенкодер есть в виде концепта, но никаких о каких практических применениях не пишут вроде",
например не натыкался еще на то что бы его использовали для какого нибудь семантического хеширования,
а всякие ЛеКуны пишут что зачем предобучать нейросеть если можно взять больше данных,
"Душа радуется, когда читаю, что где-то кэгглер жжот)",
"в одном параллельном чате про МЛ чел говорит что у них Абхишек работает, и как он не пройдет мимо него тот постоянно на кегле торчит -)",
"О чорд, как мир-то тесен",
знаете где её достать можно?,
"кстати, а почему нирвана и все остальные так переживают про ""недозагрузку"" GPU при тренировке сети? при тренировке больших рекуррентных архитектур загрузка GPU TitanX держится стабильно 97-99 (иногда 100)%  меряю через NVidia XServer
может я что-то неправильно делаю?) или это только к сверточным относится?",
"аа, а где можно померить сколько ядер работало?)",
"<!here|@here> по поводу саенс слэма... вопрос на миллион - какие как вам кажется самые интересные и доступные одновременно темы для широкой аудитории? 

недавно проскакивали ссылки про то, как объяснить МЛ простым детям. задача - и аудиторию развлечь, и про науку понятно рассказать, и позитив про какоето-светлое будущее чтобы остался",
Какой процент времени GPU занимался твоей задачей,
"&gt; Какой процент времени GPU занимался твоей задачей
&gt; По всем ядрам в целом",
"Ну смотри, как оно все работает",
"Из-за того, как твой код написан",
"<@U040HKJE7>: 
&gt; ссылки про то, как объяснить МЛ простым детям
напомни",
"эцсамое, а как поправить календарь? а то эта запись - совсем кулстори. помимо того что хакатон перенесен (на непонятно какой срок), так еще и api moscow закрылся",
Куда пропал <@U040M0W0S> :smiley: Видимо улетел в неизвестном направлении на выигранные на хакатоне деньги,
"Спасибо! В общем, придётся самому вкрячивать его.
А вообще у кого какое мнение по HFO? Имеет вообще смысл с ним заморачиваться - как я понял, пока нет единого мнения о его полезности",
а какие у нас поблизости мероприятия? или пока все конференции отгремили,
<@U070Y25AS> какого же ты обо мне низкого мнения),
"не знаю как <@U041P485A> , а я еще прихожу в себя после нашего статейного марафона. как-нибудь расскажем про наш 8-слойный random-forest и deep unrestricted forest machines :feelsgood:",
"Мария Степанова вот такое запостила в ВК: В этом году в рамках Dialogue Evaluation будет проводиться соревнование по извлечению информации из новостных текстов на русском языке. Участникам будут предложены дорожки по NER (named entity recognition) и по извлечению фактов.

Приглашаем всех желающих принять участие в независимом тестировании систем извлечения информации. В качестве участников мы будем рады видеть как опытных разработчиков, так и новичков, в том числе и тех, кто раньше не занимался этой областью.

Более подробную информацию о соревновании вы можете прочитать во вложенном файле.

По всем вопросам вы можете писать на адрес <mailto:evaluation@dialog-21.ru|evaluation@dialog-21.ru>.

С уважением,
Оргкомитет #FactRuEval :
Виктор Бочаров (OpenCorpora)
Ирина Ефименко (НИУ ВШЭ)
Анатолий Старостин (ABBYY)
Мария Степанова (ABBYY)
Светлана Толдова (НИУ ВШЭ)
Владимир Хорошевский (ВЦ РАН)
Анна Гусева (Диалог)

#dialog21
<http://www.dialog-21.ru/>",
<@U0989QUVC> Это где два дня на Сокольниках без наркоты но с VR ? Я был там сегодня,
<@U040HKJE7>: а как у вас статейный марафон?,
"Кто-то понял детали того, как model-level parallelism работает?",
Куда в итоге все сбежали?,
а как узнать могу ли я зайти в ИППИ?,
"<@U04CH4QBD>: есть идея, как бы записать трансляцию?",
"))) 615 есть на 6 этаже и портреты на стенах
просто его не было, когда я заходил =) сижу тут на 5 этаже, а то на 6 стульев не предусмотрено)",
а у кого ниудь получалось bigARTM на мак поставить успешно? -),
"не делал записи, но судя по предполагаемому уровню данного чата - там людям отсюда можно только:
1. искать себе практикантов (но не сейчас, а под конец курса, они пока учатся тиано ставить)
2. позадавать вопросы ведущему и его коллегам (там еще Миша Беляев присутсвует и читал сегодня)
т.е. если сравнивать его с семинаром, который 5vision организовал в future technologies,  в иппи entry level, а у 5vision - advanced

по содержанию  - ведущий рассказал про подбор параметров и раннюю остановку тренировки полносвязанных и сверточных сетей на основании прогноза функции ошибки (судя по статье дает 2-х кратное сокращение времени подбора) и ""случайного"" (подвязанного под чувствительность точности к параметрам) поиска гиперпараметров
Все по стаьям Бенджио, как я понял + еще было много базы",
"хз я сноубол юзал всегда, но и задачи у меня не особо нлп были; а в одноклассах где типа очень даже крутое нлп юзается апач люсьен метафон",
"тебе придется сидеть с счастливым видом обладателя продукции эпл, но про себя ты будешь лить кровавые слезы, потому что нихуя не будет работать так как ты ожидаешь, ну кроме браузера, потому что хром",
"а еще со временем ты поймешь что виндовс - отличная ось, особенно когда ты будешь открывать мануал на какую нибудль либу и будет: линукс - апт гет, виндовс - тыкните в икзешник, мак ос - смотри страницы 31-78",
"))) а вы как раз шли до метро, как оказалось",
А 5vision — это кто такие? Которые по воскресеньям?,
а че кто орги и спикеры?,
для тех кто в танке,
Может кто организует перископную запись?,
"вообще почему минт, а не убунту? :simple_smile:",
"делай как тематическое моделирование, сначала прокидывай тему, а затем слово",
"Дада, с mate -- как аналог столь милому моему сердцу gnome2",
"ну как ЛДА работает, прокидываем тему и затем из темы достаем слово",
просто когда объясняют лда на пальцах говорят что прокидывают кубик что бы получить тему слова <http://habrahabr.ru/company/surfingbird/blog/230103/>,
чот не с той стороны тогда имхо -) может лучше что то специализированное а потом нереальное?,
"сразу не взлетит , мало данных, хотя если у кого есть миллиард слов диалогов лишних)",
"угу))) как раз думали
но это не диалоги",
"но пьесы могут хорошо пойти как претрейн и не только пьесы, а и просто книги в прицнипе, наверное, но тут тренировки недельные) поэтому надо мудро выбирать",
"вот как раз на субтитрах и треним, просто их тоже мало и качество не айс, дефолтного датасета на русских субтиритрах нет (по сравнению с англ)",
"тут все как у всех - нет качественных данных и хз как сделать архитектуру, т.к. тренировки по 5-6 дней",
"рассматривается все :simple_smile: что хорошо работает
просто надо бы успеть это все попробовать, а ресурсов пока маловато, поэтому идет медленно
насчет генерации слов и доделывания окончаний и т.п. - можно наперевкладывать RNN на CNN по-всякому :simple_smile: и совместить посимвольные с пословными со стейтами и т.п.
все можно, надо экспериментировать, смотреть
еще можно довернуть оценку вероятностей и параллельно поднять вторую Q&amp;A модель на знаниях как в конкурсе Каггла и смотреть какая из моделей будет генерить хороший ответ - тот и брать",
"У кого есть Видеокарта от AMD-ATI с поддержкой OpenCL&gt;=1.2 ?
Мне удалось установить OpenCL-Caffe на ubuntu 15.04, Но моя карта поддерживает только openCL 1.0, не все тесты проходит.
В целом забавно )",
"Я тут какое то время назад кидал ссылку на презентацию Джефа Дина, там как раз как без огромного трупута на кластере учиться, а спакр под executor'ом довольно агностичен, даже прекомпиленые бинарники принимает",
"кстати это же семинар, а не просто лекция, может в зале расставить стулья не как обычно в ряды а полукругом вокруг доски или как то так, отвечаю так создается совсем другое ощущение от общения с людьми -)",
а кто на ВДНХ завтра едет,
"народ, кто-нибудь занимался collocation extraction? Если да, какие методы работали лучше всего? Я вот сейчас пытаюсь извлечь колокации из Common Crawl, point-wise mutual information работает нормально, но только для 2-грам, для 3 и 4 грам результаты похуже.",
"<@U04URBM8V>: Я как раз про ту презентацию Джефа Дина: он там пишет, что у них упор на ""Asynchronous Distributed Stochastic Gradient Descent"", на спарке такую штуку не сделать.",
ага у меня где-то студенческий завалялся как раз,
"Проблема в скорости обработки или в том, как быстрее написать код?",
"как бы ему нагенерировать из первичной такой информации ивенты, последовательно которых уже можно изучить с помощью пакетов",
"я так никогда не делал, но раз о регулярках речь, то как еще? :simple_smile:",
должно быть какое-то куда более простое решение -- не придумал пока как загуглить,
"гайз, смотрю первую неделю курса Нг. он говорит, что в линейной регрессии cost function выглядит одинаково. типа convex function. не могу понять почему она обязательно такая :simple_smile:. does anyone could give me a clue, please.",
А кто тут Azure ML пользует?,
"вот вот, это же как программирование мышкой",
Кто сегодня в ВМК Бартунова послушать едет?,
"Я всё равно не очень понял, что значит ""график выглядит одинаково"". Но если вопрос почему loss для линейной регрессии - convex function, то это вроде достаточно понятно ",
"я просто решил, как natenkin’у пришло в голову собраться в честь дня народного единства :smiley:",
кто то мучительно пытается за него сесть,
"я пробовал чот по мелочи, пока не успешно, но я тупо иксджибуст, никаких там арима и тому подобное",
"сегодня насиловал весь день ариму из statsmodel, так и не понял, как ей скормить больше одной exogenous переменной для обучения",
"Кто может пояснить что будет здесь : 
Data Fest 2 от Deloitte, DM Labs ?",
"В мэйле все обычно до конца докладов + 20 мин, дальше в бар. Когда заканчивается бар, зависит от упорства участников",
"пока трудно предсказать, как доделаю тут доделаю всё",
"а, ну давай я наберу тогда как выходить буду",
а кто работает в формате smart data?,
"&gt;а кто работает в формате smart data?
знаю этот формат .smdat",
"+ люди из Открытого Правительства, HP из тех, кого я знаю лично.",
чот википедия ничо не говорит,
Они как раз этим и занимаются. Но не все охотно отдают.,
как будто это что то плохое,
А как Сердюков и Ростех связаны с Открытым Правительством?,
"вот честно, по виду мракобесие
потому что строить выводы на основе остатков  - несколько странно
надо либо говорить, что есть какие-то факторы, которые не учитываются в модели, либо просто признавать остатки как ошибку измерения.

а тут вообще что-то странное. остатки от авторегрессии в одной модели как предиктор остатков авторегрессии второй модели...",
"мне неизвестно о существовании легитимных тестов причинно-следственной связи кроме полевых экспериментов и доменной экспертизы. PGM и байесовские сети это отдельный вопрос :simple_smile: у нас тут повседневная линейная регрессия

применительно к тексту, он выглядит неоднозначно. 
фичи:
-чувак взял две фичи и построил их приращения. ок

целевая переменная, часть 1:
-для каждой из двух фич чувак зафитил на одних и тех же данных две модели - предсказание одного показателя по второму в первый момент времени и во второй 
-затем чувак берет остатки регрессий по этим фичам и вычитыает друг из друга. просто потому что может

целевая переменная, часть 2:
-чувак сложил бля эти две фичи(разности остатков) в новую фичу и сказал что это новая целевая переменная. ну и что что черт знает какая у обоих была дисперсия, это наша новая целевая переменная",
а это у них прямо считается как анализ данных?),
А многие потом орудуют этими результатами как фактами.,
"Мол, если есть два фактора, они зависимы, и ты хочешь узнать в какую сторону причинно-следственная связь...",
"<@U06J1LG1M>: Открытое правительство -- это проект одно министра ""без портфеля"". Как я понял, он для набора политических очков служит. Потому что непонятно для чего еще. Например, есть там отдел, который борится за повышении эффективность бюрократического аппарата. В тоже время прямо влиять они не могут -- могут давать рекомендации. Но кому нужны всякие советы?",
"Тут бы спор решил эксперимент, но так как мы живём не в тоталитарном обществе a la ""1984"" он, к сожалению, невозможен. В таком же обществе, кстати, и выборочный метод в Social Sciences наконец-то нормально бы заработал. Но, увы и ах...",
"Откуда такая категоричность? Открытые данные -- это геморрой для многих министерств и ведомств и какую пользу можно извлечь, разгребая этот геморрой и убеждая всех этими данными делиться, я, откровенно говоря, понять не могу.",
Какой грант и какого госдепа?,
"как говорится в пари паскаля, потенциальный профит от веры в бога как бы перевешивает затраты, так и тут, вроде как хуже ничо не делают, ну и пусть работают, может реально что хорошое получится",
"котаны, я уже все написал. в таком подходе рандомный результат обеспечен уже с того момента как на одних и тех же данных учатся модели и на них же чтото делается с невязками. все, уже с этого момента начинается произвольный треш не говоря уже о чудо идее одни невязки вычитать из других(почему не сложить, не нормализовать, не перемножить или не сложить синус одних плюс косинус других?).

хотите больше ада - сделайте так чтобы у одной случайно величины был размах\дисперсия в 100 раз больше чем у второй

а хотите еще больше ада - генерите из байесовской модели с явной зависимостью и явной независимостью по 50 точек и замерьте в скольки случаях будет результат следуя этой схеме. если упороться то можно даже предельное распределение выписать, лол",
как мы в митингс к этому пришли?))),
"напомните, когда там следующее ближайщее заседание :simple_smile:",
"Тут можно было бы обсудить такое понятие как ""здравый смысл"", но для этого придётся открывать канал #philosophy",
Так а кто в итоге на этом мероприятии будет экспертом? Признавайтесь),
"Видимо имеется ввиду, что ее часто используют чтобы превратить некое распределение по всем числам (например прямую) в значение от 0 до 1, где большие отрицательные значения плавно стремятся к 0, а большие положительные - к 1",
"В английской википедии, где обычно больше деталей, похожей фразы нет, поэтому остается догадываться",
"а нет какой-нибудь книги почитать про то, как ""в 80-90-ых строилось куча инженерии вокруг""?",
"боюсь, там будут результаты и панегерики -- а хотелось бы про проблемы и то, как их старались решать",
"все вокруг нечетких управлений разными контроллерами. книжки чертовски похожи, мне на память приходит Борисов\Круглов, но думаю подойдет любое

особенно показательны книжки с ""зоопарком"" где в названии полный фарш - Fuzzy Logic, Genetic Algorithms  Neural Networks",
"&gt; Однако в 1969 г. бывший сокурсник Розенблатта Марвин Минский и Сеймур Паперт опубликовали книгу «Перцептроны», в которой привели строгое математическое доказательство того, что перцептрон не способен к обучению в большинстве интересных для применения случаев. В результате принятия идей и выводов книги М. Минского и С. Паперта работы по нейронным сетям были свернуты во многих научных центрах и финансирование существенно урезано. Так как Розенблатт погиб в результате трагической случайности во время плавания на яхте в день своего рождения, то он не смог аргументировано ответить на критику и исследования нейронных сетей приостановились практически на десятилетие.",
"There is no reason to suppose that any of these virtues carry
over to the many-layered version. Nevertheless, we consider it to be
an important research problem to elucidate (or reject) our intuitive
judgement that the extension to multilayer systems is sterile.

вот считается какая фраза убила НС тогда",
кого нет в этом чате,
ну тогда всех тех кто прочитает.,
"потому как передаточная функция у нас заранее известна, и не совсем понимаю, зачем ее символьно дифференцировать при каждом цикле обучения",
"На этапе построения модели автоматически дифференцировать некоторые фреймворки могут. Но не на этапе обучения, конечно, ибо зачем ",
"просто пытаюсь понять, о каком символьном дифференцировании в процессе обучения DNN говорил Левенчук. видимо, таки пошутил",
"&gt; еще раз - в процессе вычисления производной получается ее символьная формула (строка) или таки градиент (число) 
Theano как раз символьными формулами оперирует",
"Про ""сразу прыгнуть куда надо"" - я вообще не понял",
"&gt; сразу прыгнуть куда надо
Тут, видимо, предполагается сразу найти аналитическое решение, как в случае с лин. регрессией, где есть чудо-формулка w = (X^T X)^(-1) X^T y",
"а как можно правильно чистить датасет? такая вот задачка: есть датасет, в котором единственная фича x - случайная величина из равномерного распределения на (0, 1) и ответ y тоже самое. нужно показать, какую часть нужно минимально удалить, что-бы увидеть зависимость.
я предполагаю, например, что у меня линейная зависимость a*x + b, и беру, например, только те y, для которых (a*x + b) / y в каком то интервале около единицы [1-t, 1+t]. потом можно поварьировать (a, b) и куда нибудь сойтись (t относится к пониманию того, что значит зависимость и, видимо, можно на глаз смотреть). (при t = 0.5, a = 1, b = 0 достаточно взять половину; при t = 0.1, a = 1, b = 0 можно брать только 10%)
на сколько так адекватно делать? как еще можно сделать?",
"это все надо смотреть по power analysis
то есть, какого размера должна быть минимально выборка, чтобы увидеть определенного размера эффект и при определенных допустимых границах ошибок 1 и 2 рода при использовании определенного метода",
"<@U0AD1L5NC>: а зачем вычислять градиент в аналитическом виде даже один раз в начале, если все передаточные функции известны заранее, и, соответственно, их производные тоже? в каком месте здесь возникает необходимость в символьных вычислениях?",
"<@U0DJEEXE1>: Ну, мол, нейросеть состоит из слоев, у каждого функция своя. Когда тебе нужен новый слой, который еще никто не написал, то писать и функцию вычисления, и градиента руками - error prone",
"то есть ты считаешь, что когда он говорит ""вместо того, чтобы численно брать дифференциал, вы его возьмете символьно и прямо попадете, куда надо"", он имеет ввиду именно символьный вывод формулы производной при проектировании нового типа передаточной функции, а не процесс обучения НС?",
"Я думаю, что это имеется в виду до слов ""и прямо попадете куда надо""",
"Что такое ""попадете куда надо"" - мне не очень понятно",
"Кто-нибудь знает, есть ли кто из этой лаборатории в чате?
<http://sites.skoltech.ru/compvision/>",
"<@U06J1LG1M>:  Minsky по-прежнему смотрит на всех, как на говно! <http://www.technologyreview.com/video/543031/marvin-minsky-reflects-on-a-life-in-ai/>",
"<!here|@here> Коллеги, если кто планирует пойти на Smart Data Hack на выходных и есть вопросы по мероприятию -- пишите мне в личку или сюда. Я могу задать их оргам прямо сейчас.",
"Ребят, а можете пригласить сюда mrkonstantin ? не пойму как это делается..",
Привет! Кто нибудь знает простой и безболезненный способ вытащить из заданной сети в Keras производную произвольного выхода сети по произвольному весу?,
"<@U04423D74>  а просто посчитать корреляцию между функциями зависимости количества кликов в час от времени? (Например каждый баннер крутить по дню, получится 4 зависимости :1а 1б 1в и 3абв ну и посчитать корреляцию между 3абв и всеми остальными, у кого больше тот и прав)",
"Привет! Ближайший Moscow Data Science будет как бы юбилейным (2 года), мы решили отметить это дело выпуском серии наклеек на ноуты. Идея в том, чтобы выпустить 2 типа наклеек — “обычные” и “привилегированные”, для участников и тех, кто был хоть раз докладчиком/модератором соответственно. Есть лого группы, но возникла проблема - как понятно визуально обозначить разницу (обычный - покруче). Может у кого-то есть какие-то идеи или ссылки, откуда их можно украсть? Буду очень благодарен.",
"как заметил <@U04423D74> обрамление было бы в самый раз, как у всяких эпических и легендарных кард в TCG",
"Я мало знаю про Keras, но разве нельзя у модели вытащить input и output как theano function?",
когда я пришел - 2 команды по 4-6 человек,
"В профилях игроков доты, которые побеждали на The International, есть значок в виде Aegis of Immortal. Примерно так, как у <@U04CH4QBD>  на картинке. Но т.к. у нас нет универсальной иконки, которую можно было бы использовать как символ Data Science, не очень понятно, что там можно изобразить. Пиктограмму микрофона maybe?",
"нолики с единицами еще куда ни шло, но на этом фантазия прекращает выдавать релевантные варианты. потому что гистограммки, лайнчарты, нормальное распределение, схемы регрессий, барплоты, схемы сетей и plate notation - все это слишком неспецифично к DS в целом, не покрывая сути",
"поэтому все и рисуют диаграммы Венна, облака тегов и вариации на тему, потому что непонятно как еще подчеркнуть что мы занимаемся всем и сразу",
<@U065VP6F7>: там learn lyve надпись слева и справа от лого. можешь по домену посмотреть как выглядит,
"<@U06MTEXQQ>: мне кажется, что короны/шевроны и прочие знаки превосходства тут не совсем подходят, т.к. правильнее было бы отметить тех, кто выступал, а не тех, кто круче как Data Scientist. Поэтому цвет рамки, значок микрофона или что-то подобное, как мне кажется, подошли бы больше.",
чтобы знали где выступал :simple_smile:,
на секспросвет куда ж еще,
")))
максимум - поглазеть на шибури сессию
вон мы с <@U0AJU5AMA> сегодня наслаждались хехе, но реально круто

орги кстати зря не заделали при мероприятии темную комнату или что-то такое, как при гей-клубах в америке
соотношение м-ж было примерно 53-47",
"Типа, задаешь функцию, которая складывает текущий и прошлый элемент
Вопрос - как оно это маппит на GPU-то?
Там же нужно чтобы все было параллельно",
"<@U0EK52XLK>: Подсказать не смогу, но можешь сказать какую задачу нужно решать? Просто есть любопытство в данной тематике",
"у меня задача есть находить сливы топлива у фур, тоже думаю как к ней подступиться",
Человек гуглил. Но пока не особо удачно. А я в этой теме кроме как график нарисовать ничего предложить не могу.,
"<@U0EK52XLK>: а как звучит задача? в какой области, на каких данных?

может быть нужен классический outlier detection из коробки, а-ля n-sigma",
<@U040HKJE7>:  конечно гуглить не надо)) Задача звучит как обнаружение подозрительных транзакций в финансовых данных.,
если я правильно понимаю - тут как раз можно ячейки независимо гонять,
как бы только на совсем сомнительных сайтах в рекламных блоках не стали показывать :lips: :fist:,
А мы как раз недавно в параллельном чятике обсуждали Geek Big Dick и Geek Dick Pic.,
"<@U04422XJL>: когда ты сказал Петр засветился на пикабу, а картинка еще не загрузилось
я подумал, что или пьяный или голый",
<@U040HKJE7>: а чо на вс какой план?,
где уверенность - это уверенность в предсказанном числе,
"в честном виде - много кто хочет, никто нормально не умеет

чаще всего придумывают пост-процессинговые оценки. например, можно сделать раз 20 OOF построение прогнозов и посчитать их sd",
"квантильная регрессия - вариант, но у нее нет нормальной оценки ошибок, из-за чего попав в ситуацию где q25 может быть выше чем q75 (это же верно для других уровней) придется свесить лапки (или объявить отрицательный IQR)",
"нарот чот я туплю, кароч есть у меня матрица bag of words, и она же перевзвешенная tf-idf, как может xgboost выдавать разный результат? ему же должно быть пофигу на масштабирование внутри колонок",
"как так то блин не пойму, видать как то по хитрому он поиск делает в множестве значений, а в сорцы чот не хочется лезть )",
"А ты не совсем убитый будешь? Я же, как ты понимаешь, не просто так такой щедрый :smiley:",
Ок. Вопрос у меня как обычно -- для третьеклассника. Где встречаемся? Там розетка нужна будет.,
"да верно, не только в колонке, но как я понимаю это не должно никак влиять на фичу",
"Да я даже не знаю, что писать. Где можно накосячить в фильтре вида (q5_3 == 1 | q5_4 == 1)?",
"может косяк дальше с тем, как ты логический вектор гоняешь?",
я могу час рассказывать о том как криво его везде считают,
"А вот такой ещё вопрос у меня назрел. Есть переменная (вопрос в анкете), которая может принимать 5 значений.  Допустим, этот вопрос задавали людям (разным, но из похожей выборочной совокупности) 2 раза с разницей в год. За первый год у нас есть только распределение ответов по этим значениям (допустим, 15%, 15%, 20%, 25%, 25%), а за второй год -- как распределение (например, 17%, 13%, 25%, 20%, 25%), так и исходные данные. *Вопрос*: можем ли мы посчитать статистическую значимость различий в этих распределениях, не имея исходных данных за первый год? Если бы они были, я бы молча посчитал t-test или что-то подобное и был доволен. Но вот с тем, как посчитать это, имея только распределения, я сталкиваюсь впервые.",
"типа 95% доверительный интервал был бы 17% +- 2.5%, ..., 25% +-4.5%
как посчитать их аналитически я не уверен (все-таки Дирихле распределение), поэтому я бы забутстрапил их",
<@U0AF2AZCM>: как правило некорректный подсчет tf части,
"Тут уже даже мне стало интересно, как можно некорректно посчитать _tf_. Как корректно посчитать _idf_ -- да, вопрос.",
со стороны оно кажется простым как дверь. но я только готовыми реализациями пользовался. потому и интересно,
"<@U0AF2AZCM>: увы, можно делать нормализацию не на сумму всех значений, а на число всех кто больше нуля (типа бинаризовать), а в некоторых случаях могут вообще делить тупо на число столбцов (что вообще бесполезно раз это делают всем-всем строкам)",
А теперь посмотрите как в nltk и найдите 2 ошибки,
"<@U04CH4QBD>: мы как раз выше говорили про разные нормализации. почему нормализация это плохо в данном случае? или ты к тому, что надо это называть по другому или давать пользователю выбор через аргумент?",
отдельно почему l2 нормализация выхода плоха,
с итогом кто не лох,
как у тебя там успехи кстати?,
"как это так, а митап",
"просто  улингвистов есть роскошный linguistlist, где все - и работа, и пхд, и новые книги, и конференции
весьма полезно. но область все же специфичная, не все, что надо, есть

спасибо :simple_smile:",
"<!here|@here> знает кто-нибудь где можно скачать модели обученные на imagenet или других больших базах и совместимых с caffe?
Кроме этих:
<https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet>
<http://www.robots.ox.ac.uk/~vgg/research/very_deep/>",
"кто сетку обучает, тот старается в репу её запилить",
"это скорее обучающий пример, как раз",
"ИТ-лекторий ФКН ""Технологии и алгоритмы анализа данных социальных сетей"". Спикер - Дмитрий Бугайченко, <http://ok.ru|ok.ru>
Докладчик: Дмитрий Бугайченко, инженер аналитик, <http://ok.ru|ok.ru>
В рамках доклада мы расскажем о том, какие алгоритмы анализа данных применяются в социальных сетях, о том, какие технологии для этого используется, а также немного о тех ""кровавых подробностях"", с которыми приходится сталкиваться, масштабируя классические алгоритмы на терабайты данных.
Место проведения: Кочновский проезд, дом 3, лекционный зал Евклид",
Я чота тоже только сейчас вспомнил про регистрацию.,
"Хотел найти фрагмент фильма ""Snatch"" про то, как они проходили на подпольный боксёрский матч, но не нашёл.",
опс. плюсую к несчастным кто не успел записаться,
"Привет! Все, кто не успел зарегаться, приходите так. На всякий случай мой телефон есть в профиле, звоните если что",
Тут я как всегда пишу заметки - <https://closedcircles.com/chat?circle=14&amp;msg=5581781>,
"Also, если кто-то понимает как работает attention models - поговорите со мной!",
"если кто пропустит сам митап, то подтягивайтесь в кафе амиго-мигель рядом с мейлом, там стол заказан на 21-30",
"ну как стол, пол зала",
"Привет, если кто собирается сегодня ехать на машине и хочет припарковаться на нашей стоянке, самое время писать марки и номера мне в личку",
"господа, вопрос нуба.
интересует область text production - все вот эти алгоритмы генерации текстов и тому подобное

подскажите, пожалуйста, какие-нибудь программные тексты в это области, по каким ключевым словам гуглить и настраивать гугл-алерты, конференции, имена, базовый тулчейн?
в общем, все, чтобы более-менее быстро въехать в тему",
"в общем для начала обучаешь RNN которая просто генерит текст по этому корпусу; затем как генерируется текст в РНН - подается нули в скрытое состояние (представление о контексте - его нет) или не нули а что то случайное (что бы была генерация с рандомного контекста) и специальный токен который означает начало предложения; в твоем случае есть смысл подать на скрытое состояние не нули, а уже некоторое представление о контексте, например персонажи, тогда после токена начала предложения будет вероятнее сгенерировано слово из того контекста нежели из другого",
"ойвей, не так быстро, я не в теме нейросеток :disappointed:
я ж потому и спрашиваю - какой вообще тулчейн может быть, с какого боку народ грызет задачу...",
"приходи сегодня на мэйл митап, после пойдем пиваса попить, будет как раз самое время обсудить нейросети :smiley:",
"&gt;&gt;только лучше не RNN, а LSTM/GRU)
ну как бы лстм это частный случай рнн, так что не стоит говорить что вместо рнн нада юзать лстм -)",
Кто нибудь знает телефон Дмитрия кропотова из вмк,
кто будет покупать ослиную мочу? думаю никто,
"— Меня посадили, потому что я продал партию пива, которое на их взгляд отдавало мочой. Какого чёрта! Почему отдавало?! Это и была настоящая моча!",
"я так и не доехал. простите, друзья (( как все прош… проходит?",
Джин а ты чего ушел? а как же обсудить политоту? -),
"коллеги, меня попросили сказать, почему именно вот эта статья - чушь (они делают неформальную премию за идиотские и ненаучные исследования, и им нужна хоть какая-то аргументация)
<http://www.ssa-rss.ru/files/File/basimov12.pdf>

но я как открыл, так и сразу мне стало плохо
помогите, а? если уж совсем небрезгливо %(

<@U041LH06L>: тебе вопрос в первую очередь, так как речь о социологах...",
"&gt; Синергетический стиль мышления – это стиль мышления постнекласси-
&gt; ческой науки. Он представляет собой современный этап развития системного
&gt; и кибернетического мышления, многие элементы которого подвергаются
&gt; существенной переделке. «Нелинейность» – фундаментальный концептуаль-
&gt; ный узел новой парадигмы. Можно даже сказать, что новая парадигма есть
&gt; парадигма нелинейности

как будто новый пост в блоге левенчука",
"да мне-то все понятно, я аргументировать содержательно не знаю как, почему бред
какая-то совсем иная система координат 
%((",
Ну тогда формулу глянь. Какую форму имеет распределение ответов? Почему и как разбили двумя квантилями на 3 группы? Почему они получились взаимопересекающимися?,
"А кто-нибудь знает как ""обойти"" в ubuntu идею, что старый пакет одновременно/поверх с новым не ставится?",
как у тебя кстати скептикон прошел? скептики посрамлены?,
"некоторые уточнения - есть 2 папки usr/local/cuda и usr/local/cuda-7.5 
если я переименую обе, то у меня все зависимости (из терминологии винды - записи по реестру, но тут может нет никакого реестра) к чертям? :smiley:
во-вторых, переименовывать обе или одной достаточно? как думаете? :simple_smile: играть в русскую рулетку с пакетами и установками и потом восстанавливать систему с консоли - оставило приятные воспоминания о линуксе на всю жизнь))
и если потом поставить сверху 7.0, то как там с зависимостями? (вот у ребят по ссылке видимо скилл консольной магии высок :simple_smile: - никаких вопросов и проблем)

самый простой вариант - подождать пока гугл выкатит обновление, много людей жалуется и логично бы им выкатить
чуть сложнее - снести куду (но я не знаю, что там будет, если снести, а потом поставить 7.0, а потом все-таки хочется на 7.5 работать) и хочется и колется :simple_smile:

в общем, печаль :disappointed:",
"его можно удалить, поставить еще одну куду и потом симлинком переключать",
а то мне точно не хочется переустанавливать куду (хотя может это предрассудки ... :simple_smile: ),
"одновременно это
1. переименовал папку, поставил куду 7.0
2. запустил тензор флоу - все работает на 7.0
3. хочу чтобы код во фразе 
th train.lua 
using CUDA on GPU 0...
использовал CUDA 7.5 - о, тогда я пишу export CUDA_HOME=/usr/local/cuda-7.5/ и все работает под 7.5
4. хочу снова потренить тензор флоу, пишу export CUDA_HOME=/usr/local/cuda-7.0/

и все без проблем :simple_smile:",
"попытка не пытка, как говорил товарищ Берия)",
"ок, попробую, а дальше все аналогично, как и было?",
"если заработает, то почему бы и нет :simple_smile:",
вся инфа как раз в cuda-repo-7-0-local,
"мне вот очень кажется, что все кто выше по ссылке <https://github.com/tensorflow/tensorflow/issues/20>
знают что-то, что я не знаю)) и даже не догадываюсь
т.к. у них это дело - скачал - поставил, а не танцы :simple_smile:",
"""I've installed cuda toolkit 7.0 and changed the bash profile reference to the new one and it works""
или
""If you install version 7.0 in a separate directory from 7.5, and point tensorflow at it via the configure script (or LD_LIBRARY_PATH), it will work""
как у них - без распаковки по архивам и прочей теме)",
мы как раз на этапе install version 7.0 in a separate directory,
"на deephack похоже... 
""Пацаны, пацаны, как торч поставить? че там за лажа с luarocks? """,
"Краткое промежуточное резюме
1. run файл - решает все проблемы с кудой
2. потом надо прописать классические 
$ export PATH=/usr/local/cuda-7.0/bin:$PATH
$ export LD_LIBRARY_PATH=/usr/local/cuda-7.0/lib64:$LD_LIBRARY_PATH
3. И ошибка на тензор флоу изменяется на то, что он не поддерживает cudnn 7.0 v3 (надо ставить 6.5 v2)
пока в процессе",
Имхо правильне всеже оставить переменные окружения как есть,
"Теперь получилось так (перелинковал куду обратно на 7.5)
lrwxrwxrwx  1 root root    8 сент. 27 14:24 cuda -&gt; cuda-7.5
lrwxrwxrwx  1 root root   19 нояб. 29 22:34 cuda_7.0 -&gt; /usr/local/cuda-7.0

printenv LD_LIBRARY_PATH
/usr/local/cuda-7.0/lib64:/home/vladislav/torch/install/lib:/home/vladislav/torch/install/lib:/home/vladislav/torch/install/lib:/home/vladislav/torch/install/lib:

И при этом все работает (и вычисления на куде и tensorflow)",
"<@U040M0W0S> а мы на митапе еще трындели с челом, который раньше в инсилико работал датасаентистом, а щас в сбере <@U06J1LG1M> помнишь как его звали??",
"они же только недавно начали искать дата саенсов, маловероятно что кто то пришел и сразу ушел",
<@U040M0W0S> у кого такие наклейки остались? я б взял) на мейловую тусовку доехать то не успел тогда,
"но эти должны войти в историю, щас же монетки 30-летней давности с опечатками огого как стоят",
"крайне всесторонне развит, как и в науке так и в бизнесе",
<@U0989QUVC> когда уже ключ-биоключ запустишь),
"<@U06J1LG1M>: Паша, стал читать статью по твоей ссылке. Там есть такое вот сравнение RNN с программами на языке C
&gt;Are RNNs too expressive?
Finite-sized RNNs with nonlinear activations are a rich family of models, capable of nearly arbitrary computation. A well-known result is that a finite-sized recurrent neural network with sigmoidal activation functions can simulate a universal Turing machine [Siegelmann and Sontag, 1991]. The capability of RNNs to perform arbitrary computation demonstrates their expressive power, but one could argue that the C programming language is equally capable of expressing arbitrary programs. And yet there are no papers claiming that the invention of C represents a panacea for machine learning. A fundamental reason is there is no simple way of efficiently exploring the space of C programs. In particular, there is no general way to calculate the gradient of an arbitrary C program to minimize a chosen loss function. Moreover, given any finite dataset, there exist countless programs which overfit the dataset, generating desired training output but failing to generalize to test examples.
Why then should RNNs suffer less from similar problems? First, given any fixed architecture (set of nodes, edges, and activation functions), the recurrent neural networks with this architecture are differentiable end to end. The derivative of the loss function can be calculated with respect to each of the parameters (weights) in the model. Thus, RNNs are amenable to gradient-based training. Second, while the Turing-completeness of RNNs is an impressive property, given a fixed-size RNN with a specific architecture, it is not actually possible to reproduce any arbitrary program. Further, unlike a program composed in C, a recurrent neural network can be regularized via standard techniques that help

Ты понимаешь, в чем тут соль? Зачем вообще нужен градиент по программе считать и какой тут может быть loss?",
"ну как я понимаю тут как раз о том что обе модели тьюринг-полные, но только сишный код мы не знаем как оптимизировать для обобщения, т.е. каждая прога на си - это уже оверфит; а РНН мы знаем как оптимизировать, например бекпропом",
"имхо тут не нужно задаваться вопросом о том как продифференцировать сишный код, а нужно задаваться вопросом - а есть ли способ оптимизировать какую то прогу, в смысле некоторой функции потерь",
"ну это как я понял, может конечно не так -)",
и мы как бы умеем в теории оптимизировать РНН которая в теории опять же может генерить осмысленный код и выучить вообще все,
"есть теорема универсальной аппроксимации которая как бы говорит что можем, но как обычно в таких теоремах, она не совсем конструктивна, мы не знаем сколько ждать и какие размеры скрытых пространств выбирать",
"ну и так же, размерность на вход РНН можно подавать произвольную (смотря как закодировать токен, но всегда есть крайний случай one-hot-encoding)",
<@U0D8KLBFV>: во смотри как раз в тему нашего разговора <https://www.facebook.com/yann.lecun/posts/10153243711677143>,
"Да... Надо же как они одновременно, там Nando De Freitas (Deepmind) еще две аналогичные привел 
<http://arxiv.org/abs/1511.06279> 
<http://arxiv.org/abs/1511.04834>",
"но теперь то когда наклейки уже распечатаны, можно обсудить как надо было на самом деле сделать",
"как понять что убийства, шантаж и подкуп не пропадет зазря?))",
"Котаны, задачка следующая:

Есть датасет на 1.5кк объектов - названия айтемов в екоммерс магазике - надо найти среди них дубликаты друг друга. 

Супервайза тут нет, но есть возможность замутить в последствии некий реинфорсмент - делать выборку явно славных айтемов между собой и вручную валидировать.

Кто нить сталкивался с похожей задачкой и  чо посоветуете?",
"ну тут менее упоротая фигня, все таки слова как слова без рандомных сокращений",
а как ты этот NER проводил?,
а левенштайна ты all vs all в лоб делал или как то хитро?,
"Это ж как кластеризация
Все хотят, но никто не знает, что же именно",
"<@U04422XJL>: 

Ну, по факту - дубликаты это то, что физически является одним и тем же айтемом.  Но физической валидацией в инет магазе понятное дело никто не занимается, поэтому все пырят в описания, которые дает продавец.

Как это тебе математически описать - я хз, валидироваться это будет потом по каким-то доп полям, которые сейчас неизвестны и у различных категорий айтемов разные + ручная валидация по тому, что ты скажешь ""вот это еба очень близкие айтемы"".

Так-то да, это задача выбора расстояния и меры близости айтемов между собой. Собственно и вопрос, какие варианты есть чисто по названиям айтемов находить какие-то близкие вещи :simple_smile:",
"Как ни крутись, а без Левенштейна не обойтись.",
"единственное, что названия айтемов это все таки не какие то рандомные тексты и там может быть некая структура типа ""бренд + модель + цвет/другие качественные характеристики""",
Пробелы и прочие знаки расставлены как надо или как придется?,
"Пока могу сказать, что магический прием под названием ""триграмы+свд+жадный кнн"" работает не совсем хорошо, так как для чисто текстов это ок, а вот iphone 5 и iphone 6 его заставляют печалиться  

Да, я посмотрел это у Дьяконова :simple_smile:",
а ну да или как Дьяконов говорил по н-грамам,
"я еще не настолько упоролся, а это потом каким-то случайным посонам которые решили быть модными и писать на го имплементить где то в германии",
"<!here|@here>: кто-нибудь втыкался в проблему, когда раздувание датасета картинок путём их отзеркаливания ухудшало результат бинарной классификации?",
а для кого еще тот <@U04CH4QBD> ?,
Как обычно организовывается процесс треканья экспериментов? Ну то есть мне хочется хранить каждую эпоху (ну или каждую Н-ую) и полный лог процесса -- есть ли стандартные инструменты для этого?,
"Я вроде пользую FileLogger из dp -- но чото я не пойму, как его логи нужно читать",
"ок, как вернется, я ему покажу",
была статья недавно где матрица весов в нейросетке представлялась в виде факторизации двух поменьше,
я чот тогда картинку увидел и закрыл,
лови сразу наработки по тому как это в Lasagne запилить: <http://benanne.github.io/2015/11/10/arbitrary-expressions-as-params.html>,
"да я чисто с научной точки зрения, прост интересно почитать, я тут думал на тему можно ли так сделать для RBM, но в итоге лог правдоподобия получается не очень красивым; а с другой стороны, факторизация матрицы весов - это тоже самое что добавление нового слоя с тождественной функцией активации; ну и интересно узнать чо там исследователи пишут про это, и какой смысл заморачиваться над такой факторизацией, если все сводится тупо к новому слою",
"я тут недавно заливал про random forest в xgboost’e, нашёл наконец ссылку как это делать",
"для тех, кто любит OSS так же, как люблю его я: <https://blog.wikimedia.org/2015/11/30/artificial-intelligence-x-ray-specs/> . ctrl+F -&gt; Getting involved (ищут волонтеров в следующих областях: NLP, social, modeling, production)",
"коллеги, нид хелп.
на руках датасет - количество бабла по дням за последние пару лет. базовые фичи - количество людей, платформы, проекты и так далее.
хочу научиться предсказывать по первым N дням месяца суммарное количество бабла за весь месяц.

собственно вопрос. стоит ли вообще лезть во временные ряды? может, можно просто обойтись регрессиями и прочими изъебствами рядом, когда зависимая - количество бабла за месяц, а фичи - все, что есть по первым N дням?",
"Временные ряды в классическом виде - не стоит, так как в таких задачах N=1:7 а значит они будут маленькие и нерепрезентативные. Аггрегируй и фичи и целевую переменную :) ",
"понимаешь правильно, но мне кажется зря хочешь выкинуть прошлое, там польза таки бывает. почитай форум на rossman текущем, потом решения, как оно кончится, похожего на твою задачу много, но шума поменьше в данных (только 2 или 3 вида рекламы)",
"Я когда жил в стартапе, все из года в год адски менялось",
"Как предсказать, что в следующий год мы нащупали новое направление и теперь растем в месяц на 6%, а не на 1% как раньше - не очень понятно",
"А как предлагается оценивать overfitting, кстати?",
"ща узнаю номер кабинета, а то я только помню как туда пройти -)",
А людям без пропуска как быть?),
"сделал следующий трюк предсказания бабла за месяц по первой неделе.
беру день, отсчитываю от него +7, считаю бабло. потом отсчитываю +30, считаю бабло, и ряд других фичей (выходные, акции, и прочее).
получаю датасет где-то на 300 строк (работал только с данными за последний год, они наиболее полные)
прогоняю через RF, получаю качество модели под 0,89
корреляция предсказанного с исходным порядка 0,95

но меня гложут сомнения. 
нормально ли это, нет ли оверфита или еще какой херни? потому что как-то неправильно данные формируются, на мой взгляд.",
как же ты это тогда делаешь?,
<@U040HKJE7>: кстати как пчелки и шмели?,
Как в lasagne делают annealing?,
не там == где то на конференции,
"<@U0AD1L5NC>: подскажи пожалуйста, cs231n целиком был выложен, когда ты его проходил?",
<@U06J1LG1M>: а кто завтрашную тусню организовывает в Мейле?,
"<@U040M0W0S>: хз, но вероятно это как то связано с теми кто тут в организаторах <https://vk.com/setup_lc>",
кстати а как дела с ДС в китае?? там много кадров или дефицит в этой сфере? кто в курсе?,
"<@U0989QUVC>: Я этим летом был в Пекине и списывался с организатором пекинских ДС митапов, хотел пересечься, но не сложилось. Вроде у них с кадрами как везде - дефицит квалифицированных кадров есть определенный. Знаю, что IBM в ML research в Китай регулярно вакансии выкидывает.",
"<!here|@here> кто как использует в проектах диплеринг, поделитесь? Интересно просто, кто чем занимается, как применяет",
<@U0A7CPPDL>: он пока как подростковый-секс-ну-дальше-ты-понял,
"Может, кто ещё прибежит расскажет",
<@U065VP6F7>: а где там сухие? Только вот такие нашел с гелем <http://openbci.myshopify.com/collections/frontpage/products/openbci-gold-cup-electrodes>,
"Я просто искал в инете описание, с какой части головы какие сигналы можно собрать, но как-то очень мало об этом",
он в ИППИ пилит как раз ML на ЭЭГ,
А какие ЭЭГ они используют?,
"С учетом нынешнего ажиотажа вокруг openBCI вспомнился старый анекдот, который теперь может стать стартапом:
Придумал новое телешоу: ""Битва IT-экстрасенсов"". Лучшие админы проникают в головы юзерам и угадывают, что они нажали и почему ""ничего не работает"".",
"Но, например, классифицируем фоточки по полезным классам  - документ это или нет, какой категории конкретно",
"Вопрос про data augmentation для картинок, которые кормятся на вход CNN
Часто для augmentation хочется картинку сдвинуть или повернуть
И при этом в новой картинке получаются ""пустые"" пиксели, для которых нет информации
Ну тупо, повернул на 45 градусов, появились зоны, которые были вне картинки
Как их рекомендуется заполнять, чтобы кормить на вход CNN?
Средним цветом?
Нулями?
Повторять последний пиксель картинки (clamp)?",
"не добавляешь, а используешь искажённые картинки как вход для сети",
<@U06J1LG1M> мы же как раз это вместе на последнем circle jerk'е обсуждали,
"а какие есть традиционные решения, для того чтобы ввести в DNN инвариантность к зеркалированию?",
может их идею как то модифицировать для зеркалирования,
"например не вертеть на угол как там, а отражать",
"непонятно, правда, как подбор параметров учитывать,  в идеале в это ограничение еще бы и подбор прошелся, а не только обучение, но отследить это малореально, только вдумчивым чтением исходников на предмет констант всяких, руками вписанных",
ну вот там как раз оооочень хитрый слой -),
получится как билайн-конкурс (точнее его финал),
"<@U0FEJNBGQ> была такая идея задачи для хакатона: даются данные, где фичи это уже готовые предсказания 10000 деревьев, скажем, на миллионе точек. сами деревья тоже предоставляются.

задача - построить решение на основе этих 10000 деревьев, оптимальное с точки зрения сложности\точности. может быть вообще с точки зрения AIC\BIC в качестве целевой метрики (для фиксированных деревьев их наверное можно посчитать) 

исходные данные даются в ограниченном количестве и только для проверки на адекватность самих деревьев. скажем, исходных данных будет в 100 раз меньше точек, чтобы на них не могли заново построить деревья.",
"а то я до сих пор сайтик запиливаю, никому это походу не надо)

да я в принципе и сам саммари сделаю с позволения почтенных господ <@U041P485A> <@U06J1LG1M> <@U049HDR2Z> как только в _jobs утихнут",
"слак-бот
щас кстати мы мучаем как раз слак-бот для проекта, может научиться сразу",
"как напишешь так и вытащит
не знаю есть ли уже реализованные тулы такие",
<@U040HKJE7>: а че как ваш россман то ?,
а есть какой нибудь поисковик по ссылкам статей друг на друга?,
а какой сейчас state of art для проблем ranking?  что используют? какие модели? что стоит почитать?,
"какой подход используют? модель выдаёт релевантность и по ней сортируем? используется хотя бы иногда по-парное сравнение, когда в модель отправляется два элемента + запрос и она возвращает какой из двух сильнее соответствует?",
"я как то в отдел поиска зашел и спросил что почитать, мне дали ссылку на эту книгу :smiley:",
<@U06J1LG1M>: это ты про какой хардкор?,
"Думается мне, что не конкурсы метаморфируют, а участники -- будет еще более явно разделение на тех, кто угарает по фичам/анализу и по алгоритмам.",
1) кто кроме <@U040HKJE7> решает rossmann?,
"А потом пусть те, кто разберутся учат тех, кто не осилил",
Как это всегда и происходит,
"та же проблема с боязнью провафлить, поэтому стараюсь что-то делать в командах - мотивирует. ну, если кто еще захочет, давайте вместе подадимся",
"праздный вопрос, который меня смущает в Python, почему народ до сих пор сидит на 2ом? разве все нужные либы не портировали на 3?",
<@U0AD1L5NC>: как у него настроения?,
"другая тема, были ли на каггле конкурсы на регрессию с экстраполяцией, когда надо находить значения за пределами виденного в training set, да еще и такие, что экстраполяция константой плохо работает?",
"это возврат к вопросу о регрессии с доверительным интервалом? видел его месяц назад тут... у мерфи в гл. 7.6 очень подробно описана байсовская регрессия, которая применима к L2 регуляризации (она выводится, если применить prior на параметы) - советую начать оттуда. там как раз получается хороший результат, что чем дальше уходим от наблюдений, тем больше дисперсия posterior predictive",
"но вообще он мега-крут, и, к счастью, де фрейташ говорит, что он готовит второе издание, где будет до фига про deep learning еще теории",
"интересно глянуть на аналогичный график какой-нибудь недавно бывшей модной технологии,  можно ли по силе нарастающей волны хайпа предугадать, когда оно затихнет",
"предсказать окончание хайпа - это как предсказать, когда лопнет пузырь. практически невозможно. ну только разве когда каждому студенту не начнут предлагать сотку в год, как в 2000 - это неплохой индикатор",
кто ходил 15го ноября на ажур хакатон??,
"все правильно, извините, 28-29е
хотела спросить как там эти ребята в жюри которые были, рабблс
они классные вообще? ато тут спрашивают",
"надо было юзать ажур, мало кто был с ним знаком, поэтому победители просто стакнули 2 модели и все)",
"МС же так  и делает. мастер-классы устраивает ,поясняет как работать, раздает бесплатно часы. еще предлагает сфоткать на пресс-воле и подобных сооружениях.",
они щас вроде должны быть где то на нипсе :simple_smile:,
"котаны, а никто не искал/юзал/знает какую реализацию Truncated SVD или какой нибудь другой SVD на C++ ВНЕЗАПНО",
да че то там все переехало хз куда,
mystem он распространяется только бинарником и как же сделать питонячую библиотеку которая делает морфологический разбор посредством mystem’а? только так. и вполне рабочее решение,
Посему нужно разбираться с главными рисками как можно быстрее,
"Скрипт - полчаса, разбираться как делать ML на C++ - неделя, плюс еще и не гибко",
"точнее, какие еще могут быть риски у МЛ-проекта: 1) недостаточная обучающая выборка 2) медленно",
"После того как эти риски пройдены, возникают другие :simple_smile:",
<@U040M0W0S>: тут мне сбросили парадигму того как правильно надо шо то пилить <http://c2.com/cgi/wiki?MakeItWorkMakeItRightMakeItFast>,
"может эти ребята ничего, кроме го, не умеют и их очень беспокоит как потом поддерживать",
"еще вариант, они делают типа коробочный софт и хотят отдавать один исполнимый файл, как умеет го, и не просить клиентов ставить питон другой версии и еще кучу либ на их rhel 5",
"Первая встреча проводится совместно с Московским институтом социально-культурных программ (МИСКП) и посвящена возможностям применения результатов социологических исследований в градостроительной практике. Будет представлено масштабное социологическое исследование городской среды «Механика Москвы». На основании 150 статистических показателей и двух волн социологического опроса 24 тыс. респондентов в исследовании анализируется уровень развития более ста районов Москвы в различных сферах: благоустройство, безопасность, культура, образование, здравоохранение, экология, транспорт. Результатом работы является выявление группы московских районов, получивших название «Районы потенциальных изменений», куда вошли наиболее перспективные территории города, объединяемые высокой вовлеченностью жителей в районную жизнь и заинтересованностью в развитии места своего проживания.

После презентации состоится дискуссия, могут ли результаты данного исследования быть учтены в реальном проектировании городского пространства Москвы, каковы возможности и ограничения такого подхода и каким образом адаптировать результаты социологических исследований для конкретных проектировочных задач.",
"т.к. с большой зарплатой закрываются мгновенно, а всякий треш висит годами, и когда говорят ""средняя зарплата по хх"", получается называют средний треш, куда никто не откликается",
"ну это классический такой антипаттерн, когда надо сразу все новое вбухивать не разобравшись",
"орг офтоп от ДС
13го на этой неделе будет московский саенс слэм - финалы в йота спейсе
<http://scienceslam.rocks/>
так вышло, что у меня (и у Леши наверное) есть энное количество еще не израсходованных вписок

так что пишите в лс пожалуйста, кто хочет проходку на саенс слэм в вск!!",
Обычно слэм заканчивается в 2130 где то. В этот раз он в нормальном месте где можно накидаться перед выступлением и во время,
"ну кароче как минимум раз перед новым годом нужно собраться, обсудить так сказать тренды, <http://jatwood.org/blog/nips-deep-learning-tutorial.html>",
"<@U06J1LG1M>: совесть, говоришь? а кто тут недавно сиськи требовал в jobs? ^))))",
"Можно прочитать первую часть, где они дизайнят текстовый редактор",
"когда код уже написан, и автор с reviewer на пару его обсуждают? это да, т.к. в комментах оно затягивается, а голосом быстро получается",
"представьте, что вам прислали датасет, но не сказали, что за задача (регрессия или классификация) и не дали описания переменных. В том числе, не известно, какая переменная - таргет. 
Какие бы эвристики вы бы использовали, чтобы найти целевую переменную? 
Можно поискать переменные с названиями ""tagret"", ""response"", ""flag"", ""class"", мб что-то ещё
Можно посмотреть количество уровней переменной - плохо сбалансированный бинарный признак - отличный кандидат.
Какие-нибудь ещё идеи?",
"Посмотреть, какой колонки нет в test'е :-)",
"Ну, вообще, надо исходить из потребностей бизнеса. Предсказание какого признака было бы полезной фичей в плане повышения доходов",
"Например, какой признак сложнее (дороже) всего получить",
"как появилась такая странная задача? можно глядя на тип каждой колонки пробовать регрессию или классификацию её от остальных, кто совсем не предсказывается - тот скорее всего не target",
"<@U040HKJE7>:  ты прав) увидел в какой-то группе в контакте обсуждение про поиск аномалий в датасете, загружаемом через сат на сервер. Задумался, как бы я это делал)",
"практический вопрос: 
как в xgboost добавить свой личный вектор весов точек? 

обшарил документацию, но нашел только scale_pos_weight, а это не то",
"единственный вариант который я сейчас вижу, это переписать свою функцию потерь. но это че-то выглядит как overkill :neutral_face:",
"ага, я представил как я продажи гдето домножаю на 2 изза того что вес мощный :simple_smile: или на 20...",
"А как ты веса хочешь учитывать? Как коэффициент при невязке? Кажется, тогда семплинг и для регрессии ок будет работать, sum_i w_i (f(x_i) - y_i)^2 будет примерно равен sum_j (f(x_j)-y_i)^2, если каждое x_i повторить w_i раз",
"О, а я как раз завтра там в это время буду, спасибо)",
"Сказать себе ""Хм, да это же почти как текст"" -- и использовать все наработки человечества в области NLP",
<!here> кто сегодня идёт на слэм?,
"те кто еще сомневаются с саенс слэмом, могут обратиться за впиской в личку :simple_smile:",
 а чо вы где там?,
"<@U070Y25AS>: сбрею, как только допишу (или опубликую) статью. На неделе планировал добить черновик. Stay tuned...",
<@U0989QUVC> я уже года два как -) ,
"<@U041T0UHM> не помнишь, что было когда ставили умышленно 1?",
"Миша верно говорит что часто ее ставят равной 1, а потом множат

Мы как раз проверяли в свое время линейными моделями эту гамму (а потом тоже множили). По идее для L2 регрессии она должна быть аналитически равной 1",
"у меня такое мнение сформировалось:
- память: скорость не очень принципиальна, лишь бы побольше
- проц: помощнее если делать real-time augmentation (а как её на соревнованиях _не_ делать вообще не понятно)
- мать хз, на работе купили самый дорогой ASUS, но мне кажется это  оверкилл",
"да, стоить он будет видимо как чугунный мост",
"Кто знает, какая либо из библиотек имеет поддержку cuDNN v4 ? Или там просто стандартное API и его еще до-оптимизировали так что ничего менять в клиентском коде не надо?",
а почему бы и нет?,
"Была мысль засесть сегодня с 17.30 и до 20.00 где то в центре, посидеть пиво попить, есть желающие присоедениться ?",
чтобы посмотреть на оформление и как референс использовать,
"отлично, щас выберем место, какой нибудь очередной крафтовый бар",
"Сегодня идём в Дранк Крафт Баръ на Лесной 55, всем кто с белорусской будет удобно))",
а как гуглить на такую тему?,
как то так recurrent neural networks for sentence classification,
"датасет в полуручном режиме собрать из нашего чата, я думаю) построить интерфейс, как в тиндере -- и бысто насвайпать target. несколько тысяч сообщений в час можно разметить.",
<@U041SH27M> ты как то похож на  basic из гифки,
"&gt; recurrent neural networks for sentence classification
а тут же особый случай классификации: не n от m, а n от n+1 . как вот такую про такую вещь поискать?",
"сорри, не понял. да, я вот про бинарный ответ. но хочу погуглить как это делается. не приходит в голову как :simple_smile:. 
на вход как я представляю идет сам текст и мета: автор, время, канал. хотя может стоит в рамках одного канала рассматривать сообщения. .",
"расскажите, как у вас тренировки проходят?",
"какой формат, что вы там делаете?",
доклады по конкурсам с конкретными рецептами кто что делал :simple_smile:,
"просто это антипример того как кажется можно сделать, но это физически практически нереально",
по факту она держит меньше 1 запаса батарейки  так как на 720p 4-гиговый файл набегает быстрее разрядки,
я гопро упомянул как простое решение,
"&gt; просто это антипример того как кажется можно сделать, но это физически практически нереально",
"<@U0AF2AZCM>: я записывал кстати, ещё до того, как это было модно)",
а киеве сейчас. если бы кто взялся - я бы с удовольствием ходил),
я с Овеном кстати джин пил когда в бостоне был),
<@U04422XJL>: кто может кстати выступить? ты кого планировал?,
только главное не как в прошлый раз <@U04422XJL> :confused:,
кто за 25 лайкаем этот коммент,
кто за 26 лайкаем этот коммент,
а кто молодой и горячий?,
если вдруг кто из Питера есть - был бы рад встретиться-потрепаться на научные и аналитические темы вечерком на днях :simple_smile:,
Все кто был переехали в Москву. ,
"<@U040M0W0S> я не знаю как статус менять у треда) надо менять на 26е
<@U040HKJE7>: подтверди только",
") нет, надо всех на 1 день наметиьт, чтобы концентрация дата саентистов зашкаливала, ато на 2 дня все растекутся мыслями по древу как всегда",
в каком мейле -- почему не в Яндексе?,
"думаю, что недостаточно. потому что самому Мише не хочется организовывать. и следовательно, если тренировка нужна, то кто-то должен за нее взяться. 
кто вообще, кроме <@U041P485A> может попитчиться?",
ага! ну тогда надо как можно быстрее уточнить их желание и возможности. и запулить афишу.,
"мм. а как ты понял, что кто-то паникует?",
"ну то есть, не вижу кто паникует)",
"Как я понимаю - фестиваль бесплатно, выставка платно. Меня больше смутило, что билет на выставку действителен 60 минут. А фестиваль длится 4 часа. Вопрос - сколько билетов надо купить, чтобы послушать весь фестиваль?))",
"а где эта инфа, Алён?",
"Осталось решить, кто что расскажет -- и можно объявлять ",
"Вопрос -- кто что расскажет? Пчёлы, лица, россман, деллойт -- я ничего из конкурсов не забыл?",
"это ребята типа имели в виду, что паника у меня. 
ну на самом деле, это я так предложил помощь :simple_smile:. которой, как выяснилось, не требуется.",
"какой формат рассказа на тренировке, 45мин + слайды или попроще?",
"хотя самому было бы любопытно послушать тех,  кто выше поднялся на россмане, там были наши",
а какого рода сегментация интересует? Что есть объект?,
"в общем случае я могу применить довольно простые методы, но вот для случая когда один объект частично перекрывает другой интересно было бы попробовать NN",
"Такой вопрос возник по предсказаниям (типа как было в Rossman), что лучше - просто выкидывать странные данные, или обходить их хитрыми регрессорами? Имеются в виду нетипичные периоды типа новогодних распродаж и прочих флешмобов.",
"ох, когда же время найти... :sleepy:",
"Коллеги, вопрос, граничный с DS, есть задача - транзакционный отток клиентов банка (именно транзакционный - падение транзакций, оплат картой), банк сам не знает, что считать таким оттоком, а что нет, это предлагается придумать нам. Вопрос - как бы вы измеряли падение транзакционной активности клиента в банке? Приветствуются все идеи.",
То есть какие бы вы меры использовали для замера этого падение.,
как раз мутим проект на эту тему,
"ситуация, когда банк сам не понимает, что они считают оттоком, а что нет, нормальна?",
"допустим, у нас есть только месячные суммы транзакций и их количество, как бы вы поступили в таком случае?",
"не, взять последние N отсчетов (AR ) как вектор",
для многих траекторная матрица звучит также непривычно как и гусеницизация (сленг) или Ганкелизация,
"ну да, мало кто слышал про SSA за пределами мат-меха",
"поэтому проще всего кластеризовать по двумерному распределению (количество и лог-сумма) и посмотреть, кто куда прыгает",
"<@U0FL6RNHM>: если построите, расскажите, плиз - как говорил, сами начинаем что-то похожее делать. надо дружить",
как весь этот календарь посмотреть?,
Это где Bengio такое постит? В G+?,
и как это пофиксить без переписывания базовой либы - я не понял,
Так кто то на Билайн Хакатон пошел?,
" <@U09S9916X> разумею, что лучший (и единственный) предиктор тут -- число общих контактов и проблема только в том, как это эффективно посчитать. ",
а почему сходу <http://sigmajs.org/> не посоветовал?,
"щас, как получится на место проскочить на своё",
"есть y=f(x) где y и x - временные ряды, x я могу делать почти произвольным в определенных пределах, y меряю, f()  - это типа черный ящик, динамическая система с задержками. Как, имея x и y получить s-plane poles этой f() ?",
"<@U040M0W0S> треня не поняла где
<@U040HKJE7>: в мейлру или яндексе или делойте? и во сколько? в 12?",
<@U04422XJL>: а как на тренировку в Яндексе 26-го попасть?,
"вроде да
<https://opendatascience.slack.com/archives/_meetings/p1450707830001156>

как регаться? <@U040HKJE7>",
"какой там адрес, это офис на Парке Культуры, куда заходить, куда внутри идти?",
"Мужики, а не находил-соображал ли кто код, который конвертирует какую-нибудь Pretrained CNN в theano/lasagne?",
Почему это не гуглится все!,
"Короче, long story short - похоже, Амазон не дает GPUs, где поддерживается cuDNN 3.0",
облако тоже есть с GPU вроде как,
"<@U04423D74>: смотря что считать бейслайном
если есть куча однотипных рядов с одного процесса (например датчики температуры), то можно для выделения их общего паттерна брать среднее\медиану\""фильтр по вкусу"" как по текущим отсчетам, так и по приращениям

если ряды не однотипны, то надо их сперва разделить, а потом провернуть все то же самое

а если хочется какой-нибудь тренд, то это другая история :simple_smile:",
"если событие бинарное (есть/нет), то можно как в A/B тестах посравнивать распределения с ним и без него",
"у меня события сами неоднородны могут быть. то есть, это ивенты на праздники, которые могут работать, могут не работать хорошо, и тому подобное. и идти могут по 15-20 дней, с разной динамикой внутри. поэтому я их как многоуровневый фактор запилю, видимо.

эм. нормальные и логнормальные таки разные распределения в моем понимании... конечно, все можно трансформировать до нормального, но это ж отдельные игрища...",
я почему люблю генеративные модели - что всегда есть доверительные интервалы,
"Когда кто-то пишет про куртозис, мне приходит оповещение :smiley:",
"mann-whitney. еще забыл как называется, очень простой (не требует никакого другого нормирования) - сгенерировать столько же нормальных значений и построить график из двух измерений - тестируемого и сгенерированного. должна получиться прямая",
"а с какими логарифмизация плохо работала? обычно, если распределение не очень нормальное, то могут быть латентные факторы, тогда можно mixture делать (например кластеризовать с помощью sklearn.mixture.gmm и оценивать число кластеров по bic - чем меньше, тем лучше, конечно)",
"так а что для многомерных данных лучше? кластеры, как говорю, по bic отлично. а уменьшить число измерений можно - у меня часто они сильно коррелируют между собой",
"уменьшить размерность а потом натягивать смесь - инженерно возможный подход, но если до него доходит то скорее всего либо странно поставили задачу, либо у реализующего много времени на поиграться

\\ занятный момент, что  часть моего русского диссера - как раз про GMM-ные смеси регрессий, но по ходу работы я в подходе полностью разочаровался и на практике его применять не собираюсь так как есть тулы лучше",
"*и тут я вылезу с вопросом про ""а что если мне надо интерпретирвоать вклады предикторов, а не просто предсказывать хорошо"" :))))

<@U0G29N5U4>: а ты с чем именно в баесовщине работаешь? и в чем именно? 
у меня есть вопросы про монте-карло, иерархические модели и проверку гипотез. плюс кастомные распределения... и я не знаю, кого можно спрашивать %)",
"аналогично кстати можно натянуть деревья глубины 1 (одномерные пни) и глубины которая требуется (допустим 5). сравниваешь значимости ""шатания черного ящика"", которые встроены во все xgboost\gbm\rf\... и там где будут расхождения намекает на важные интерекшены между переменными",
"<@U0G29N5U4>: я как раз с stan развлекаюсь, так как он единственный позволяет кастомные функции задавать

<@U040HKJE7>: чорд. это бездны, которые я еще не грокаю %(",
"<@U040HKJE7>: в качестве альтернативы байесовскому графу интересно попробовать методу, которую ты описал, но придется обратиться за пояснениями или помощью кого-нибудь, кто уже делал!",
есть кто в курсе и с опытом?,
"не знаю, как без программирования обойтись -- есть варианты? :simple_smile:",
"речь же о DNA? а оно работает, как вы видите.",
"там еще не очевидно, что ползунок надо уменьшить, чтобы увидеть собственно динамику. иначе он как бы на 100%.",
"прикольно. а что из этого у тебя не получается получить? :simple_smile: я, в общем не шарю, но все равно что-то никто не торопится))) 
множество объектов  -- просто слова? какая иерархия слов в тексте может быть? признаки заранее заданнные же выходит? и иерархии на них тоже. (ну как в ворднете -- гипонимы/гиперонимы). 
а вот самое сложное и интересное -- это ведь между объектами и признаками отношения найти",
"Просто как я понимаю, онтологии -- это красивая идея, но строить их никто не умеет",
"коллеги я тут обновил на тачке куду до 7.5 и драйвер v3, прописал пути как тут <http://deeplearning.net/software/theano/library/sandbox/cuda/dnn.html> попросил что бы вылетало если куднн не найден optimizer_including=cudnn, ну и таки вылетает -) 

причем эксепшн не самый информативный

ImportError: ('The following error happened while compiling the node', &lt;theano.sandbox.cuda.dnn.DnnVersion object at 0x7f09b6b9ead0&gt;(), '\n', '/home/p.nesterov/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-centos-6.6-Final-x86_64-2.7.8-64/tmpxWPssG/f90a7a397ede0965c66895e171db1485.so: undefined symbol: cudnnGetVersion', '[&lt;theano.sandbox.cuda.dnn.DnnVersion object at 0x7f09b6b9ead0&gt;()]’)

может есть какой то секретный трюк?-)",
Как будто не находит libcudnn.static.a. Ты её точно положил в /usr/local/cuda/lib64?,
"а еще кстати, с такой фигней сталкивался кто нибудь?

In [5]: layers_caffe['fc6'].blobs[0]
Segmentation fault (core dumped)",
"привет всем от Лемпитского:
<http://sites.skoltech.ru/compvision/events/cccv2015/> 

""Доброй ночи,

всех, кто 28го еще в Москве, приглашаем на коллоквиум по компьютерному зрению в Сколтех (начало в два дня). Шесть докладчиков из трёх стран представят свои последние разработки, в основном опубликованные в ведущих конференциях по зрению и обучению. 

Вся информация здесь:
<http://sites.skoltech.ru/compvision/events/cccv2015/>

Вход, разумеется, свободный, милости просим. Пожалуйста, распространите всем, кому может быть интересно. 

Прошу прощения за ""спонтанность"", надеюсь, у кого-то из вас всё же получится выбраться.

С наступающими,
Виктор""",
<@U040M0W0S> а как .csv импортнуть в проект,
"коллеги, сегодня произошло нечто невероятное; я скачал куду7.5, качнул куднн4 и скопировал в папку от куды, куда она отинсталилась; прописал пути в башрц; затем поставил теано пипом из репы, в конфиге написал что бы выдавала ошибку если нет гпу или нету куднн

и все заработало с первого раза!!!! на маке!!!!!",
в общем наверное языческое рождество как то влияет на релизы,
"интересно, где взяли. будут теперь как на ф 0 р у м  ах писать.",
"

Евгений Гордеев 2 ч · 
Мы тут как раз запустили новую публичную версию Купола – она собирает новости по темам, индексируя в день по 150 тысяч материалов Рунета. Покопайтесь внутри, там много уже готовых группы и тем, но готовы добавить и ваши.
Главный вопрос – что за цифра на картинке? Это количество людей, увидевших новость в социальных сетях. То есть только в социальных сетях. Не посещаемость в целом, не переходы, а только охват в социальных сетях. Если цифры нет, то мы либо просто не успели ее посчитать, либо недостаточно данных.",
"<@U041SH27M>: шотландский чай если будет прямо сильное желание у кого - будем на месте соображать)
<@U040HKJE7>: главное чтобы это дальше в вегетарианство не спрогрессировало!",
 А кто знает в самом Яндексе куда идти? Он большой…,
у кого какие планы на промежуток между тренировкой и Лешой?,
"Что уже  рассказали? Я дико про megaface хотел послушать, но все прошляпил. Как закончите, то сразу на др или промежуточные фазы будут?",
"у чувака есть еще куда улучшать решение. верхняя модель-сшиватель не xgboost, не порядок",
"нужен конкурс где запрещено слово ""хгбуст""",
"<@U041SH27M>:  а где есть изя?
и долго ли вы там?",
а когда вы выдвигаетесь? С кем можно пересечься на Белорусской и во сколько?,
"<@U09JEC7V0>: думаю, со мной %)
во сколько - не знаю, так как я сам дома еще
а господа и дамы, подозреваю, еще где-то в районе яндекса",
"ну вроде как в 6-7 собирались, поправьте меня?",
<@U04CH4QBD>: а вы когда все доберетесь до <@U040HKJE7> ?,
А кто с Белорусской пойдёт?,
"Кому-то не всё равно - взяли, перечитали, сопоставили и сделали выводы. Полный каталог притиворечий в Библии с указаничем конкретных мест и описанием, где что с чем не совпадает и чему противоречит. Хотя таки совершенно нечитабельно в таком формате <http://atei.st/src/1326784181948557.gif>",
а где водятся специалисты по блокчейну в мск и какие ивенты ближайшие кто-то в курсе?,
"```Сейчас идет разметка обучающего и тестового корпусов.
Если Вы оставляли заявку, то мы обязательно напишем, когда корпус будет готов.

С уважением,
Анна Гусева
Координатор тестирований Dialogue Evaluation```",
"Это вот как раз интересно, viable ли",
"Сегодня, кстати, про VGG говорили примерно в этом контексте, только про ускорение. Миша Фигурнов рассказывал про Perforated CNN, где он выкидывал большую часть *элементов* свёрточных фильтров, не сильно теряя в точности",
Это как - выкидывал большую часть фильтров? :simple_smile:,
"коллеги я тут столкнулся с необъяснимым явлением (или тупым багом в своем коде :smiley: может кто сразу увидит в чем косяк <https://gist.github.com/mephistopheies/706a9e79ec16e3b89aca>

вгг-19 в памяти карты занимает около 460 мб

в опщем я беру вгг-19 и заменяю у нее FC слои и другие, затем начинаю трейн, там видно что батч размером (76, 3, 224, 224) float32 что есть 43 Mb

но когда я запускаю прогон всего на одного батча то у меня потребление памяти растет до каких то адских пределов, от 460 мб до 10 гб и потом падает до начальных 460 мб

очевидно это аномальное поведение -) но чот туплю и не пойму где ошибка",
"Когда тренируется батч, то весь нетворк считай умножается на количество картинок в батче",
"Т.е. все выходы слоя будут в одной матрице, где одно из измерений - размер батча",
4667 вся матрица активаций  и если их две и добавить сами матрицы весов то как раз пиковая которую я наблюдал 9794,
<@U06K9ELB1>:  как ты успеваешь все это читать! Спасибо!,
"а где дата-сауна, можно ссылку?",
а походная баня где территориально?,
а сооружения - ну может костер большой если народ наберется. Есть вариант с ночевкой ну это кто турист.,
"У него там слайд, где он показывает разницу в тренировке Inception architecture с BN и нет",
<@U06K9ELB1>: как раз сейчас изучаю и адаптирую твои prototxt для ResNet из группы :+1:,
"И я правильно понимаю, что так как предлагается нормализовать до нелинейности...",
"Просто в оригинальном выступлении автор говорит, что у них были примеры, когда bn без dropout лучше, чем с ним",
"Есть тут санкт-петербургские жители или гости города желающие устроить небольшое pre-party перед data sauna ? Ну или кто может раздать ценные советы о заведениях, которые явно стоит посетить :simple_smile:",
кстати вопрос о препати в петербурге актуален как сегодня(чуть меньше) так и завтра,
"Кто про что, а я напоминаю о событии для настоящий Big Data miner'ов, являющем собой супер тимбилдинг и закаляющее во всех смыслах. Хотел бы видеть в группе <https://vk.com/event110489151> сознательных коллег , способных на выход за пределы зоны комфорта :grinning:",
"<@U04CH4QBD>, кстати, тоже сейчас в петтингбурге, как я понял.",
"так, <@U04423D74> а ты как в спб добираешься?",
<@U0DA4J82H>: а как результат KL-дивергенции потом интерпретировать?,
"просто у однородного распределения максимальная энтропия. соответственно, чем ближе к максимуму (а его стоит сделать одинаковым для разного числа уникальных значений как раз через основание логарифма) - тем ближе к однородному",
"ну, это как с сигмоидой при классификации - тоже не вероятность, однако все юзают",
"Для сигмоиды есть такая штука как Platt scaling, но в её обоснованности я не уверен",
"Как KS, только в дискретном случае",
а как теперь на блог попасть то?,
"а есть, интересно, многопользовательское редактирование jupyter - как wiki?",
а не встречали где нибудь готовых классов для аугментации изображений? а то чот велосипед писать не хочется -),
"<@U06J1LG1M>: язык/библиотека? Есть велосипед для кафе, хотя я не знаю какое актуальное состояние master.",
"А может кто-нибудь из практиков подсказать, рассматриваю два абстрактных варианта, вопрос возник из интереса.
 1. Обучаю некоторую модель, для обучения использую весь объем train данных, кроссвалидирую по методу, допустим, repeated k-fold (кажется это не принципиально). По результатам этой кроссвалидации получаю набор оптимальных параметров модели. С этими параметрами обучаю модель *не на всех train данных, а на их части* (допустим 80%). 
 2. Произвожу все те же манипуляции, только кроссвалидацию провожу сразу на кусочке данных (80%) и модель строю, используя данные только из этого куска.
 Вопрос, где качество предсказания (on test dataset) получится лучше и почему ? 
Если знаете, что можно почитать по этому поводу, было бы вообще круто  :simple_smile:",
"Не пойму вопрос.  Сначала по CV на всех данных подбираются параметры, потом модель с подобранными параметрами учится на части данных. Другой вариант, когда подбор параметров по CV делается только по части данных и на них же модель учится. В обоих случаях итоговая модель учится на одной и той же части данных (а зачем, если есть все?), только в первом параметры подобраны по всем данным, а во втором параметры подобраны по части.",
"&gt; а упоминаемый test dataset - это оставшиеся 20%, или вообще другой набор данных?
Вообще другой набор данных. 
&gt; а зачем, если есть все?
Вообще незачем, но вопрос интересен был сам по себе.  Вопрос понят абсолютно правильно.  Можно ли получить прирост в производительности на unseen data, если выбирать параметры модели на всем train  dataset, при прочих равных (итоговая модель будет обучаться только на части данных)? Влияет ли такой подход на качество, и если да, то как ?",
"интуитивно, параметры, которые видели больше данных, должны быть лучше, но где этому формальное доказательство найти, не знаю",
"мне кажется, если рассматривать кросс-валидацию как аппроксимацию обучения байесовских priors (empirical bayes), то это правильно, т.к. priors не должны зависеть от posteriors",
"ну он это, если я правильно помню, именно как противоположность k-fold упоминал",
А bootstrap ? Как он на практике ?,
"А напомни, для как работает бутстреп? Я помню, что это способ генерации выборок -- но как его используют в качесвте CV?",
"<@U0FL6RNHM>: сам был в подобной ситуации, поделюсь. В общем, для ознакомления — подойдет любой из дистрибутивов, от cloudera или hortonworks, они одинаковые по сути, у клоудеры есть impala (их собственная разработка), например, а у хортона — нет. Все зависит от твоей роли,  если тебе это нужно как аналитику, я бы посмотрел просто как это выглядит, поработал с hadoop fs в консоли и все. Просто в каждой компании свой workflow по работе с данными — мы, например, hive, pig  и прочее не используем вообще, а ходим в hdfs с помощью luigi (python) — <https://github.com/spotify/luigi>, или crunch (java) — <https://crunch.apache.org/>. Все завязано на MapReduce, так что очень полезно разобраться с парадигмой, а вот эти все реализации под кучей разной имен — от лукавого.",
"А ручной k-fold — имеется в виду одиночный (unrepeated) ?   А как же повторения, которые советуют на каждом шагу (например, в ссылке выше) ? Там же тоже элемент случайности есть.",
"Как мне кажется, большой разницы между вариациями к-фолд и бустрепом -- нет.
Куда важнее в самом процессе разбивки не налажать",
"<@U04422XJL>: а подскажи по такому вопросу, связанному как раз не с iid, а если в данных есть какая-то динамика по времени (ну, экономическая ситуация ухудшилась, например). как это на практике лучше обрабатывать? у меня есть только одна идея пока, которую имплементировал: поскольку все равно надо учитывать сезонность, я вкладываю эти изменения в общий с нею коэффициент. т.е. получается не чистая сезонность по месяцам, а что-то совмещенное с долгосрочным трендом, который тоже обучаю (благо, по коэффициенту производную посчитать несложно). потом уже из него можно сезонность получить отдельно. а у тебя какой-то такой опыт был? что делал?",
"скорее, про то, как этот латентный параметр лучше вводить в модель",
"И есть опасность, что мы видели множество, на котором тестируемся -- и знаем, какие предположения там справедливы (например, что в среднем все падает)",
"да, вот думаю, как бы кросс-валидацию сейчас прикрутить, блин, чтобы оптимизировать коэффициенты регуляризации",
"проблема с таким разбиением как раз в том, что тренд не учтется...",
"ну, мне не надо, мне надо обучить инвариантные к тренду показатели. по данным ""до"" - это будет по вчерашней погоде. сезонность так можно, а долгосрочные - неправильно. волевым решением назначить или подстроить под будущее - значит, перестроить модель, как <@U04422XJL> предлагает, да",
"<@U0FEJNBGQ>: спасибо. кажется, здесь описано то, как <@U04422XJL> писал про ""шатать эту точку разбиения""",
"это, конечно, решит проблему только краткосрочных прогнозов относительно тренда, но, как говорил, прогнозирование тренда и не интересует",
"<@U0FL6RNHM> кстати luigi действительно тема, мы тоже используем. Но для начала мб он не особо нужен, только когда появляется пайплайн в котором в кучу смешиваются и map reduce и pig и другие вещи",
"<!here> чат, а как определять сколько нужно будет данных для построения МЛ модели? У меня есть возможность использовать что-то типа mechanical turks, но ресурсы ограниченны. Может быть есть какой-нибудь rule of thumb для определения сколько семплов понадобиться для тренировки более-менее хорошей модели? Скажем, лог регрессии или свм. Предполагается два возможных класса, около 10 dense фич  (т.е. все строчки будет иметь эти фичи) и куча sparse фич типа bag of words. ",
"power analysis больше про такие вещи, как размер эффекта, ошибка измерения и всякие p-value
я бы исходил из общих соображений, правда, тоже, больше статистических - по 10 минимум наблюдений на переменную для регрессии",
"ух ты какой он интересный, однако %)
&gt;100,000 semantic similarity jugements from at least three human raters.",
"а при тренировке сети когда количество экземпляров в каждом классе варируется в несколько раз, как вы обычно делаете батч? состоит из каждой категории в той пропорции как и априорное распределение в трейнсете или же выравниваете что бы в каждом батче все были представленны одинаково?",
"<@U070Y25AS>: А можно подбронее? Для меня не совсем очевидно, как это может помочь...

Вот в статистике учат, что если хочешь, чтобы тест на присутствие какого-нибудь эффекта имел смысл, то нужно как минимум N сэмплов, и дается формула, как это N посчитать. А вот в МЛ про это ни слова - посмотрел несколько учебников, про размер выборки ничего не говорится. Самое близкое, что нашел, это learning curves, но они, как правило, делаются уже на существующих данных.

Меня это интересует скорее общий случай, вот например, хочу я построить классификатор, у которого аук будет как минимум 0.9. Сколько мне нужно будет данных? Я понимаю что в общем случае у этой проблемы нет решения, т.к. заранее нельзя знать, какого качества будут фичи и будет ли вообще сигнал в данных. Но всякие эмпирические правила типа ""10 наблюдений на переменную"" откуда-то взялись. Интересно, почему 10, а не 13 или 25?",
"Конкретно моя задача скорее information retrieval - дается тема, и нужно найти по этой теме наиболее релевантные термины. Сейчас используем TF-IDF, в целом работает нормально, но хотелось бы лучше. Фичи это запрос + TF, IDF, TF-IDF и подобное (т.е. фичи не распределены нормально). Опытный коллега говорит, что надо минимум 5 тыс. Почему 5 тыс он объяснить так и не смог, говорит, что по опыту если меньше 5 тыс, то модели получаются так себе. Другой говорит что 500 хватит.",
AUC 0.9 у тебя будет когда у тебя будет точность соотвествовать AUC 0.9,
т.е. когда 10 элементов есть минимум),
"А что если будет регуляризация, кросс-валидация и прочее? Кажется, что 10 наблюдений это мало, но почему - фиг его знает",
"это, скорее, просто общее наблюдение за линейными поделями, точнее, простейшей регрессией
самый минимум, который я видел по рекомендациям - 5 наблюдений на переменную
на меньшем объеме считать мнк/невязку совсем уж бессмысленно

а дальше и больше - подозреваю, это просто традиция из социо-экономических наук, где исследование ограничено объемом доступной выборки",
не знаю почему у меня была эта ссылка тут в кэше,
"<@U06J1LG1M>: Как сказал <@U04DXFZ2G> - это называется ""Cost-Sensitive Boosting” Есть крутой Тезис про это <https://uwspace.uwaterloo.ca/bitstream/handle/10012/3000/thesis.pdf?sequence=1>",
"вот кто-то говорил, log не помогает. кручу еще данные, два показателя, 666 измерений - не так уж и много. выглядят как какие-то гаммы на первый взгляд. беру логарифм и получаю очень симпатично, если, конечно, убрать аутлайеры (с ними, к сожалению, ничего другого не сделать, потому что в процессе присутствует небольшая дискретность, и иногда появляются чистые нули вместо просто маленьких значений)",
вот где я находил :smiley:,
кинь кусочек кода где есть проблемы?,
"Привет
Какие есть текущие проекты по датасаенс в лингвистике? Есть знакомый, который занимается этой темой, куда лучше обратиться?",
"Народ, а кто нибудь ковырял  исходники вот этого  <http://www.icsi.berkeley.edu/icsi/projects/ai/ntl> и в целом Embodied Theory of Language ? Рабочее или устарело?",
"Гайз, вот читаешь всякие статьи по рекомендациям, и все в основном пытаются всякие рейтинги предиктить (нетфликсы и иже с ними). 

А кто посоветует какую-нить дельную статью по рекомендациям, которые строятся только на факте покупок айтема? 

Когда тебя по факту в user-item matrix есть только единички - факты покупки, а остальные значения неизвестны и всякие user_bias и item_bias не имеют особого смысла.

Или мб я не так понимаю задачу, подскажите плиз!",
"не знаю куда это запостить, но вместо работы я сижу и залипаю на реддит:
<http://imgur.com/a/vRYsQ>",
"ты даешь сети начальное состояние в виде фразы какой то, и она начинает семплировать слова",
"Разреженная матрица, много фич
Деревья в такой постановке редко когда хорошо работают",
"а линейная модель разве не разрядится в вид  ""если есть фича А или B, то класс C"" , где коэффициенты просто наличие фич будут проверять? и чем это от дерева отличается? я не в теме классификации, поэтому может что-то элементарное спрашиваю, можно кинуть ссылкой в учебник.",
"такой вопрос: а какие можете посоветовать гуи для разработки моделей и анализа их перформанса? лучше для питона. а то застал себя за написанием оного поверх ноутбука, но наверняка же велосипед",
"я тут недавно узнал как можно ипитон прокинуть через ссш, что бы из браузера запускать на удаленной тачке, если там нет ноутбук сервера, не знаю как я жил без этого",
Чот какие-то вы все не очень...,
"ну, ноутбуком это сложно назвать. если честно, h2o куда удобнее просто будучи встроенным в проекты, как библиотека которую можно дергать",
"а что посоветуешь в качестве flow? типа накидать n моделей, под каждую потенциально свою предобработку данных, потом обучение, визуализация fit, roc, etc. кросс-валидация и бутстрапппинг. ну, в общем, все как обычно, но стандартизовано...",
я не до конца понимаю что ты хочешь :simple_smile: если тебе не хватает своих готовых процедур +- хелперов с CV да метриками - ты пишешь их сам. и используешь так как тебе надо,
<@U0G29N5U4>:  было бы круто тогда <http://worrydream.com/Tangle/> в виде виджита в сгенерированное приложение запилить. Можно будет тогда NB использовать как редактор интерактивных статей,
"Какие есть практики annealing кроме тривиальной ""делить пополам каждые N эпох""?",
"С ней проблема в том, что так как процессе тренировки прогресс замедляется, непонятно, надо ли уменьшать с той же скоростью ",
"Сколько-то близкий вопрос - в какой момент останавливать тренировку. Когда ошибка на train дошла до 0 - понятно. Но вот она не 0, продолжает замедляться и замедляться",
"я часто встречаю следующую логику (и сам использую): 1) когда обучение прекращается - уменьшить lr в 2-10 раз, пронаблюдать как кривая снова вниз идет и go to 1), если не идет - значит уже все, приехали, минимум",
"<@U06J1LG1M> , <@U04JUF21N> , <@U041P485A> выше инфа как найти тусу",
"поддерживаю технику Федора, единственно, что на глаз (хотя я оперирую численным значением ошибки на валидейшене) можно переформулировать как есть ли гейн от следующего сейва модели с учетом скорости снижения ошибки
т.к. на больших объемах текста и больших рекуррентных сетях - получается, что чек валидйшена занимает несколько часов + полностью весь титан. Т.е. думаешь, стоит изменение на 0.0001 ждать и тренить еще около дня или уже хорош)",
"Но мне тоже интересно, какие ещё бывают стратегии, что думают вышеупомянутые господа",
"<@U0AD1L5NC>: а какой вариант sgd ты используешь? AdaGrad, например, похоже более-менее нечувствителен к лёрнинг рейту",
"А кто планирует подойти? Нет ли желания чуть раньше собраться, где то в ~17.30?",
"я скорее даже опоздаю, в 18-30 где то приду",
"<@U0AD1L5NC>: тут анонс звучит, скорее, как оконечное решение уже",
"Кстати, может кто-нибудь пояснить за AdaGrad, AdaDelta, AdaM? Когда имеет смысл их использовать вместо нестерова?",
Может когда идеи получше закончатся,
"Ну заменить какой-нибудь хитрый метод на SGD можно, нужно только шаг подобрать (как, собственно, и всегда в случае sgd). А вот наоборт — не знаю, все эти ada* накапливают информацию, без которой они в первые пары итераций могут просто вылететь из той области, куда предыдущий метод так долго сходился",
"Ну, как видно, похожих значений на train они все же достигают",
"я тоже не вижу, поэтому задал вопрос ходил ли кто в прошлом году",
"как много оказывается в питоне помимо nltk и gensimа тулзов
<http://www.datasciencecentral.com/profiles/blogs/python-nlp-tools>",
"Кстати, рассказываю как попробовал convnets в живой задаче на работе - <https://closedcircles.com/chat?circle=14&amp;msg=5804294>",
"Я так и не понял, куда за грибами ехать...",
"В ленобласть, куда ж еще.",
Чота все попсу какую-то постите.,
"Так кто завтра на завтрак хочет прийти? Отпишитесь пожалуйста, что бы мы не в 3 слоя сидели за столом )",
"Из тех кто подался на Google Brain Residency, как смотрите на то, чтобы создать канал и делиться опытом прохождения по процессу найма?
<@U0AD1L5NC>: <@U04T21X7C> <@U092AGWUQ>  кто ещё?",
"о, удачи! рассказывайте что там и как",
"<@U042UQC96>: когда создадите, добавьте и меня",
"добрый день, канал.
У меня вопрос, как делаются подобные сервисы <http://kuznech.com/ru/products/mobile-recognition/> ?",
"народ, а кто-то сталкивался с хорошими статьями про то, как обучать модели, чтобы выдавали не только map, но и какие-то параметры распределения, например, квантили. или mixture distributions. я пока разбираюсь с классикой (<http://arxiv.org/pdf/1308.0850v5.pdf>), но, возможно, что-то есть новее?",
"понимаю, как это сделать через нейронные сети - построить и в A, и в B так, чтобы выход с A был частью входа в B",
"еще есть прикольный новый метод, когда сети нужно сгенерировать распределение",
"я не уверен, что с большой рамерностью x при агрегации не будет выброшена часть ценных данных. вот не рекомендуют люди, например, делать регрессию поверх pca, что тоже можно воспринимать как хеширование",
"они используют готовый сервис, который я как раз бы хотел научиться делать",
<@U0K1A87FE>: а тебе принципильно с какими именно данными он работать будет? у тебя первичный интерес сделать сервис или обучить модель?,
"<@U0G29N5U4>  а тупой способ передать y в A, посчитать модель только по A, результат отправить в B и использовать как еще одну фичу вместе с B насколько плохо работает?",
<@U0K1A87FE>:  когда скрапишь а потом руками фильтруешь,
"Вопрос: какой самый “легальный” способ добавлять js/css в ipython/jupyter notebook, что бы потом ipython.display.HTML можно было вызывать с функциями из js ?",
кто хочет сегодня сгонять на пятую волну в атриум?,
"народ, а вот разыскивается статья, я линк потерял,  может кто смотрел.  там предлагается архитектура из двух сетей одна предсказывающая другая действующая. действующей разрешается менять внутренний стейт планирующей что бы заглядывать в будущее. не встречали ?",
<@U0G29N5U4>: проскакивали клевые картинки как сеть генерирует изображения ,
"вообще говоря, по этому теме много в третьей части книги беджио написано, но мне не нравится, честно говоря, как он ее написал",
<@U0G29N5U4>: а почему именно xgboost?,
"да и xgboost это же ансамбль каких-то моделей (линейных/деревьев), вопрос про то, можно ли с ними это провернуть. 
С линейными, как выше писал, похоже что можно.",
"xgboost, похоже, самый эффективный, когда есть много измерений и все разнородные (структура в фичах присутствует). я видел твой коммент, но вот не уверен до конца на предмет линейности в твоём понимании. например, вес той или иной слабой модели - это параметр, но он ведь обучается на полном векторе фич?",
почему-то все стали называть gbm как xgboost :grinning:,
"опознает где люди по старой традиции пишут /sarcasm, srcasmmode=on  .etc?",
"коллеги, я тут потыкал в разные оптимайзеры, на подмоножестве имаджнета (а то на 1000 классов как то болго), и чот у меня NAG пока дает самые лучшие результаты, а из вас юзает кто нибудь все эти адаграды, ададельты и тому подобное?",
<@U06J1LG1M>: А на каком классификаторе тестировал?,
Есть тонкий момент как его с pretraining делать,
"читаю вот ща статейку про БН, а вообще его куда вставлять? ну я имею в виду тупо после каждого слоя или есть какие то рекомендации?",
ну типа смотря какая нелинейность конечно,
Поэтому эффективнее всего как раз перед нелинейностью,
<@U06J1LG1M>: и на какой тачке?,
"как <@U040HKJE7> рассказал про свой зоопарк с 2тб рам,  я  теперь к своим отношусь снисходительно",
"<@U0JTMFHDE>: глянь, на каких ""ведрах"" посоны в мейле и делойте гоняют. Даже жалко их стало чота.",
"Когда тренируют с нуля, то начинают с (0,1) и потом куда уже дотренирует",
"Хм, а почему он лучше параллелится?",
сейчас попробую поискать где я это слышал,
"ну я тут сижу на 8 ядрах, что считай почти как на домашнем компе",
"Какой там код у смайлика ""как лох""?",
а потом припивать чо как и где?,
"чот там дороговато кстати, через дорогу в 1morepub все тоже самое но дешевле",
И когда самый первый придёт?),
"<@U09JEC7V0>: просто кросаучек, расскажи только как тебя опознать:)",
"Позавчера на лекции Karpathy рассказывал как BN использовать, в том числе про лернинг его параметров: <https://youtu.be/gYpoJMlgyXA?t=51m23s>",
"Но я новых инсайтов про использование BN не увидел. Overview, впрочем, отличное как всегда",
"как порнотеги, только ишу лейблы: <http://atlanmod.github.io/gila/>",
"коллеги, а кто может помочь с темой марковских цепей? и байесовской статистикой (ожидания, вероятности, приоры вот это вот все)
у меня есть определенный феномен, но я не очень понимаю, можно ли его операционализировать в терминах марковского процесса/байесовских вероятностей.",
"ну это когнитивная психология, так-то
я даю стимул (линия с определенным углом наклона к горизонтали). получаю время реакции. потом даю еще линию, тоже с углом наклона. 
чем меньше различие угла наклона между линий разных проб, то тем меньше время реакции. то есть, если серия из одинаковых углов, то в какой-то момент время реакции дойдет до своего возможного минимума.
и наоборот, если постоянно давать разные углы, то время реакции будет достаточно высоким.

я хочу это дело интерпретировать как ожидания и вероятности. то есть, при пробе формируется ожидания, каким будет угол в следующей пробе. потом, при следующей пробе, эта оценка каким-то образом корректируется. и я хочу понять/проверить, если память процесса, и происходит ли реально переоценка приорной вероятности.

это если в общем. на деле структура пробы чуть-чуть сложнее.",
"углы же непрерывная переменная получается, а в марковской модели дискретные состояния, любопытно, как с ними подступаться к таким задачам. Видел, как народ непрерывное пространство на сегменты резал, чтобы принадлежность к сегменту использовать как дискретную величину, но там всё очень странно и за уши притянуто было , так что будет весьма любопытно узнать про решения этой задачи.",
"можно закодировать состояния как разницу между углами
-разница незаметна ~ меньше чем threshold (перебирается, переменная эксперимента)
-угол меньше чем предыдущий ~ разница отрицательная
-угол больше чем предыдущий ~ разница положительня",
"ну, 1/90 тоже вроде как можно принять как дискретную... угол-то другой не будет %)

по-настоящему, задача немного сложнее - у меня в каждой пробе 35 углов. и эти 35 углов берутся из нормально распределения, где mu - угол, а сигма задает вариативность углов в пробе. при новой пробе меняется mu распределения, и выбираются новые 35 углов.

я больше хочу понять, как тут переоценку приора вкрутить... но пока даже не могу понять, вероятность чего мы оцениваем. видимо, вероятность того, что базовый угол, mu, для этих 35 углов будет, допустим, 90 градусов %)",
"<@U04F2H8FM>: ага, спасибо! по ходу, это корни той статьи грейвса, где он пишет буковки",
"ну, марковский процесс - это если p(r_i | s_i, s_i-1, ...) = p(r_i | s_i, r_i-1) - т.е. ничего не зависит от истории, кроме как на один шаг назад. а тут наоборот?",
"а тут проверять надо. потому что есть вероятность, что каждый раз испытуемые принимают решение из какого-то общего опыта  (в первой пробе так точно будет, думаю)
а может быть, при решении конкретной задачи - уже на прошлый релевантный опыт.

ровно один шаг? или все же вся цепочка событий, когда каждое последующее учтывает предыдущее, пусть даже и одно, и все равно получается кумулята.?",
"вся цепочка - это как раз немарковский процесс. можно не один, а x шагов засунуть в прошлый state, но не бесконечность",
"дак, для меня это проверка поведения феномена, что лучше, марковская модель, или не марковская
привыкание, да, я уже думал об этом.

скажи, а как моделируется, я имею ввиду, технически, вот такой вот процесс? у меня есть строки, где по строкам проба, порядок пробы, данные угла и времени
я просто никогда с подобными задачами дела не имел :disappointed:",
"марковские модели моделируются очень просто - хоть регрессией/классификацией, хоть графической моделью. у тебя же тогда получается, что сколько есть временных меток, столько и iid переменных - они же независимы. только надо агрегаты предварительно рассчитать и добавить в строки. а как моделируются немарковские я не знаю",
"угу, примерно понял, спасибо. про нейросетку вообе прекрасная мысль. буду ковырять.

а вот если процесс рассматривать как байсовскую оценку вероятности, то как оно будет выглядеть, может, подскажешь?
на входе у нас есть равновероятное предположение, что угол равен какому-то из диапазона 0-90, то есть, вероятность исхода 1/90.
на первой пробе у меня есть 35 углов, в которые выбраны з нормального распределения, где мю - как раз целевой угол. 
как можно здесь посчитать правдоподобие, likelihood, чтобы утверждать, что к моменту второй пробы у успытуемого уже измененные вероятности/ожидания в голове, а не равномерное р=1/90?",
Как будто это что-то плохое.,
да я ему за пивко должен вроде как,
"&gt; В докладе также будет рассказано о возможностях и отличительных особенностях платформы VELES (например, использование GPU карт, параллельное обучение), и как с ее помощью осуществить процесс перехода от набора сырых данных к готовому приложению на устройстве.",
<@U041LH06L>: а где про порнотеги почитать?,
я тут теги апворка визуализирую -- хочу подсмотреть как вы делали,
"<@U040HKJE7>: как я это понимаю. дистрибутивная модель текста -- это когда слово представляется вектором, элементы которого зависят от окружающих слов. так?",
"тарантина кстати очень славный, но радикально советую смотреть его в англ версии, так как слушать как сэмюэль л джексон матерится можно бесконечно",
"а мне бы вот хотелось adverbial clauses и какие там еще ""клаузы"" бывают",
"Я готов помочь, но скажи чуть более конкретно - что за задача, почему не nltk и тп",
NN как видно хорошо выделяет,
"а вот как бы мне получить кусок, который я выделил красный прямоугольником :simple_smile:",
"о круто, я чот код не видел что опубликован",
"Кстати, видимо, благодаря тебе я смог собрать торч после того как он упал с ошибкой вроде string.h not found – ты (ну или кто-то с очень похожей аватаркой) апнул соответствующий issue на гитхабе. Так что спасибо :simple_smile:",
"Всем привет. Кто нибудь пытался собрать tensorflow из исходников, и на такую ошибку натыкался?",
<@U04ELQZAU>: а щас какая ошибка?,
у меня торч как раз ваще гладенько вставал,
"Все норм с tensorflow, я когда клонил репу, я protobuf из субмодулей не загрузил",
"TWIMC
Я когда-то заводил табличку для сравнения разных чипов от Нвидии, когда выбирал себе железки. Может кому-то ещё пригодится. Можете обновить до актуальных данных (особенно по ценам).

<https://docs.google.com/spreadsheets/d/1xAo6TcSgHdd25EdQ-6GqM0VKbTYu8cWyycgJhHRVIgY/edit?usp=sharing>",
где именно на вики такая же?,
Я уж не помню почему train и val разный был,
Вопрос: кто нибудь по урлам переходам пробовал применить w2v/glove (any word embedding ) ? Что получалось?,
А как ты их определяешь? Если на train ошибка большая?,
"А как отличаешь ""не падает"" от плато?",
"Урл как слово, причем очищенный урл от параметров",
<@U06LWKFMM> подскажи плиз как выглядел один документ? ,
"сам вчера с похожей задачей разбирался. похоже проблема все-таки в сходимости.
можно попробовать поменять annealing schedule, т.е. оставить до 100-150 все как есть, а потом попробовать сильно увеличить или уменьшить lr
при слишком большом lr должно расходиться, а резкое уменьшение может помочь сойтись (вдруг)
ну и аугментацию выключить",
А кто-нибудь здесь гоняет модельки на гпу на маке? Как вы на gpu utilization смотрите?,
"чувак пытался сделать аналог nvidia-smi для мака, как вышло не знаю, сам не пробовал
<https://github.com/phvu/cuda-smi>",
ага нвидия какая то на 1гб,
"они в своем слаке писали в ноябре, что запустят поддержку cuda как только получат gpu",
"Как ни крутись, а без js не обойтись.",
интересно какой толщины бы вышел справочник по R,
"Всем кто хотел заглянуть сегодня на завтрак. Я скорее даже раньше приеду к 9.00, если кому удобнее, подтягивайтесь. Всем рады.",
"на основе этого вопрос. как определить, что регуляризации достаточно? т.к. на трейнинге точность все равно почти в 100 уходит, то есть это оверфит на training, но на validation точность продолжает расти после 100 эпохи. то есть даже если просто делать early stopping и стоппить тренинг после того как устаканивается validation, то будет неясно это точка предела точности на validation или нет при текущей регуляризации. просто поиском гиперпараметров искать, или тут тоже какие-то эвристики есть?",
"<@U070Y25AS>: чем тебя latex обидел? Я наоборот в последнее время прусь, какой он классный.",
но зачем так шутку-то убивать?!,
"<@U070Y25AS> Получается, надо писать ""гореть в аду тем, кто проебался"" :D",
"Я, когда свой писал, пользовался папирией – она библиографию за меня сделала",
(зачем я это написал когда вопрос про визуализацию и скалу :mask:),
"<@U0AD1L5NC>: ну то есть 32 числа == 32 нейрона. подумалось, может, сделать выход сети как у классификатора, указывающего на небольшой сегмент, в котором находится ключевая точка. хотя, наверное, гемора это добавит, а не убавит. тем более, что у этих, с лицами, более-менее получилось выдавать координаты точек.",
"Мистика в том, что в целом на датасете оно сходится до значения куда лучше",
"Непонятно, почему бы это так адски помогало",
"Кажется, я знаю, кто придумал таблички и Excel :simple_smile: <http://www.moma.org/collection/artists/3787>",
"<@U040M0W0S> не этично сначала отказывать в любой поддержке хакатону по причине участия ""недружественной компании Facebook"" в академических партнерах, а потом когда лектор недружественной компании полностью организован для личного приезда (виза, проезд, проживание, тема - за счет дипхак) - тащить его к себе, не уведомляя об этом нас и принципиально отказывая в возмещении части расходов
Я искренне стараюсь верить, что Миколов сам постучался в Яндекс - но не получается)
Для сообщества в целом новость отменная, а с профессионально этической точки зрения - просос",
"еще есть надежда, что те люди, которые отказали вам, и те, кто позвал его выступить, просто друг с другом не переговорили",
"бугага, они и в правду такую телегу толкнули, что не поддержат по причине недружественного лектора, а потом сами его у себя выступать поставили? кто там так отжег, интересно",
"а никто не знает ли, где взять датасет с русскоязычными текстами? типа вот этого или reddit? не важно, что написано, лишь бы на общие темы и не только формальным языком (поэтому wikipedia не очень подходит)",
"выкачать даст, там безумные лимиты на число запросов. рейтинг - как минимум если учинить поиск по ключевому слову, то результаты отсортированы почти по числу участников в группе. общего списка не вижу, но он где-то публиковался",
"<@U0G29N5U4>: поищи группы где что-нибудь политическое обсуждается, там много опечаток :simple_smile: можно тянуть каменты так же к каким-нибудь новостям...",
да я хотел как раз поменьше опечаток откуда-нибудь набрать,
"готовое только в рукорпоре
но они вроде как закрыты",
"<@U0AD1L5NC> О! А ты не оценивал дисперсию разметки людьми? Для этого надо, правда, ещё потратиться. Но это как раз самый надёжный способ, хотя и косвенный, для борьбы с кривой обучающей выборкой: если у тебя l2 loss не хуже, значит, задача решена, и для юзера, в среднем, машина будет давать абсолютно адекватный результат (т.к. сам он тоже ошибается).",
но вроде как никто не мешает написать свой objective,
"Это про то, при какой ошибке на CV уже нет необходимости улучшать алгоритм.",
"Как раз потому, что данные шумные и не всегда однозначные",
надо призвать <@U040HKJE7>  и попросить его рассказать про то как мы на пчёлках подошли к пределу разметки,
"У меня есть картинки, как сеть уверенно предсказывает ""более правильный"" результат на неоднозначных данных",
"Но я бы послушал, как выявить предел разметки",
"люди косячат. можно запустить 100+ быстрых обучений на забутстраппленных данных и посмотреть, в каких точках какая ошибка. причем можно смотреть как абсолютную ошибку, так и дисперсию того, что на этих точках выдает модель

так можно будет выделить
а) сложные примеры, которые ""почемуто"" вообще не удается обучить
б) косые примеры, на которых сеть ведет себя странно",
есть тема как это хакнуть,
можно всю многомерную хрень сбагрить в группы и давать как набор векторов. и objective тоже сделать над группой аля ndcg/map,
"Ну, мой пойнт был немного в другом: если спросить себя, когда перформанс AI в аппке какой-нибудь адекватен с т.зр. пользователя - наверное, когда он просто в среднем не хуже какой-то разумной части самих пользователей.",
"<@U0G29N5U4>: зачем останавливаться, если можно сделать лучше среднего пользователя?",
"можно найти криво размеченные данные, в том числе совершенно непонятные (где на картинке нет ни осы ни пчелы). можно найти прекрасное, где две картинки  почти идентичны по этим фичам (да и внешне), но разных классов.",
"To All, кто в теме : . объясните пожалуйста what the buzz вокруг Generative Adversarial Nets <http://arxiv.org/abs/1506.05751> если сама суть методики обучения denoising autoencoders и так включала в себя шаг по восстановлению изображения . Что нового то ? Или autoencoders это делают плохо ? Или их внутреннее представление чем то не катит ? :simple_smile:",
"Ну, типа, хуже получается, менее похоже. Видел картинки, где хаотично перемешаны фрагментики, типа как под кислотой, а нейронная сеть их уверенно классифицирует, как какой-то объект? Такое вот у неё представление...",
"представление-то вариационный автоэнкодер например как раз выучивает лучше, чем обычная нейронная сеть, проблема в том, что он очень усредняет картинку, не рисует четкие контуры",
"сейчас как раз лидирует смешанный подход, когда VAE соединяют с GAN",
"Ну w2v считает расстояния как угол между вееторами
Если хочется, чтобы визуальное расстояние соотносилось с в2в -- лучше отнормировать",
"но лайтово, не так как тогда хехе",
"Вы можете и лайтово, а я -- как обычно, а может даже и задорнее.",
"А вообще любопытно, кто как часто как часто использует нестандартные лоссы в DL?",
Т.е. не будет предсказывать с какой стороны точка :simple_smile:,
"Ну да. Ты же пытаешься аппроксимировать функцию распределения наблюдаемых значений как mu = W * X + epsilon, который и распределён каким-то образом. Для распределений типа лог-нормального в GLM используют link function. Получается log(mu) = W * X + epsilon, и вот здесь уже epsilon распределен нормально.",
Как из него следует L2 loss?,
"Типа, y = NN(x) + e, где e распределена по гауссу...",
"Ну да, если предположить гауссиану. А так, для l1 легко доказать, что он минимален на медиане любого распределения, поэтому меньше зависит от аутлайеров. С т.зр. теории вероятности это так себе показатель, всё-таки не матожидание, но хотя бы близко. А l2, как частный случай mle, минимален вообще на моде, поэтому для асимметричных распределений в принципе мало смысла имеет.",
"А кстати, какая у МФТИ в целом политика про прослушивание курсов внешними людьми?",
Я значит из любопытства как-то поинтересовался какая у Стенфорда политика не этот счет,
"Очень, очень круто. Почему там очереди не стоят!",
"Потому что мало кто готов вкалывать, я полагаю)",
"&gt;А l2, как частный случай mle, минимален вообще на моде, поэтому для асимметричных распределений в принципе мало смысла имеет
L2 же на матожидании минимален",
"Собственно, если у нас есть нейросеть, являющаяся просто страшной функцией вроде NN(x) = ReLU(W_2 * ReLU(W_1 x + b_1) + b_2), то нейросеть с L2 лоссом означает как раз те самые гауссовские ошибки в модели y = NN(x) + ξ, т.е. y|x ~ Gaussian(NN(x), I).
Если мы делаем классификацию, то cross-entropy вылезает из предположения  y|x ~ Categorical(NN(x))",
"Максимизация правдоподобия — поиск моды, да. Но говоря про ""l2 как частный случай mle"" ты уже завязываешься на нормальное распределение. Собственно можно аналитически показать, что минимайзером для sum (a_i - x)² будет среднее арифметическое этих a_i",
"<@U04ELQZAU>: кажется, понял, в чём запутался. Когда мы оптимизируем l2 loss (случай не нормального распределения), мы действительно оптимизируем какую-то оценку матожидания, но эмпирического распределения, т.е. мы не добавляем никаких assumptions, не взвешивая ошибки по их значимости, не моделируя дисперсию, etc. Как результат оверфиттинг, неустойчивость к аутлайерам и проч.",
как вы такими диаграммами пользуетесь для извлечения практического смысла?,
"chord подходит для интерактивных визуализаций, и как верно заметил <@U0FEJNBGQ> , когда число узлов достаточно мало чтобы каждый можно было глазами отделить",
"Не, я имею в виду как раз негауссовский случай, что и обсуждаем. Действительно ведь часто используется l2, хотя residuals и кривые. И действительно он минимизирует их эмпирическое матожидание независимо от распределения, как ты и написал. Наверное, это связано?",
"Ну это проблемы model misspecification, когда модель не соответствует действительности. Честно говоря, ничего не знаю про это",
"Ну вот мне и любопытно, почему так редко в ML можно увидеть какой-нибудь log-normal или negative binomial loss, а в DL даже poisson ни разу не видел",
"Про дисперсию как параметр я говорил для распределений, где она привязана к матожиданию",
"похоже, этот Белан будет продвигать свой сайт с играми для ""развития мозга"". кто будет на лекции, спросите, что он думает об истории с lumosity :trollface:",
Есть кто хорошо разбирается в theano?,
<@U041SH27M>: агонь <@U040HKJE7> как Аллах смолвил!:point_up::skin-tone-2:,
"У кого нибудь есть небольшой набор данных с короткими отечественными новостными сообщениями (вроде 20newsgroups)? Мне нужно студентам погонять, на русском — для наглядности.",
"всем привет , может кто нибудь скачать и  кинуть мне cudnn-7.0 v 3 обязательно НЕ  rc",
"theano на test’ах ругается на то что cudnn какой то неправильный 
RuntimeError: You have an old release of CuDNN (or a release candidate) that isn't supported.  Please update to at least v3 final version.

Вот рабочая версия <https://www.dropbox.com/s/48y2u6ee3rwdc6d/cudnn-7.0-linux-x64-v3.0-prod.tgz>",
"Котаны, вопрос за 300.

Есть каталог айтемов, 2кк штук. Есть 600 категорий (несбалансированных), в которых эти айтемы лежат, причем айтемы могут лежать в нескольких категориях сразу. 

Известно, что айтемы внутри категории могут быть как достаточно гомогенны (а ля  ""Носки""), так и гетерогенны ( а ля ""Аксессуары для компьютеров"" и там могут быть как мышки, так и шнуры какие).

Корректность проставленных меток у айтемов неизвестна, так как это user-generated метки, но скорее всего достаточно высокая.

Каким-то образом я выделяю из датасета айтемы, которые я считаю ""наиболее вероятно правильными"", че то обучаю и так далее.

Теперь собственно вопрос:
""Каким образом мне насемплить данных, чтобы их отправить на ручную разметку, чтобы потом отвалидировать нормально работу алгоритма и как понять, сколько мне надо семплить""",
"прикольная тема, мне тоже любопытно про то, как сделать semi-supervised классификацию",
"<@U041SH27M>: как вариант, отправлять на разметку объекты, наиболее близкие к разделяющей плоскости",
"когда одно из них закончится :simple_smile:
так-то я теоретизирую, на практике не сталкивался",
"А остановиться - когда твоя метрика предсказаний достаточно хороша, очевидно!",
"Ну или когда ты видишь, что дальнейшее увеличение датасета ее значительно не увеличивает",
Который вообще непонятно как интерпретировать,
"<@U09BY2N3X>: звучит интересно, я бы записался, особенно если еще и на условиях, как написал <@U04BFDYPV>",
<@U09BY2N3X>: какие требования к тестирующим?,
"А разбить на много кластеров и вытаскивать из них по одному сначала, потом отклассифицировать все айиемы, но старые кластеры сохранить и повытаскивать побольше из тех, которые отклассифицировались с наибольшей энтропией? Потом переклассифицировать снова, и ещё вытаскивать образцы, но уже из групп, образованных предыдущей классификацией - тех, кто сильнее всего опять разошёлся? И т.д.",
"народ, а если говорить о теоретических основах ML, какие книги на ваш взгляд, стоит почитать? Я вот слышал, что нахваливают Бишопа, а вы как думаете?",
"коллеги, может видел кто такую вот аномалию, или это баг и я его еще не нашел -) в общем обучил на гпу сетку на имаджнете, запускаю и все ок; далее запускаю ее у себя на ноуте но на спу только для прогноза, и тоже таки работает; но вот цифры слегка отличаются, на некоторых картинках не значительно, сохраняя порядок, а на некоторых меняется порядок топ-5",
"Как бы мне сгенерировать еще точек предположительно из этого же распределения, которые бы не совпадали с этими?",
А как строится CDF по набору значений?,
А как оно это делает? :simple_smile:,
"Всем привет.
Не смотря на то что я и многие другие из сообщества эту неделю на хакатоне DeepHack в долгопрудном, завтра мы соберемся в москве как обычно на Data Science завтрак.
Если соберетесь, напишите здесь об этом, таким образом не придется за небольшим столом 15-ти человекам тесниться)",
"вероятно это делается через probability integral transform, но вроде как это и имелось в виду выше где CDF упоминается",
"а вообще это все прошлый век -) я как делал такое через RBM, которое по сути выучивает просто совместное распределение видимых и скрытых переменных, после того как РБМ обучена, ты можешь начать гибс семплинг начиная с какого то семпла данных, и в процессе этот семпл будет меняться, теоретически оставаясь семплом из распределения которое выучено",
"операции вроде все простые, надо теорию почитать, почему именно так",
"Насколько я понимаю, там по-сути в каждую точку помещается по гауссиане, а потом семплируется как из смеси",
"привет! я инструктор в этой специализации: <https://www.coursera.org/specializations/machine-learning-data-analysis> 
В целом она для тех,  для тех, кто хочет стать дата сайентистами. Будет Питон, машинное обучение, статистика и много примеров настоящих аналитических задач (временные ряды, тексты, churn, sentiment и так далее). 
На следующей неделе запускается первый курс по вводной математике и Питону. Сейчас он уже практически готов, и нам нужны бета-тестеры — люди, которые смогут всё посмотреть, порешать и написать нам как можно больше своих комментариев. Если вы не пользуетесь Питоном и/или не знаете/забыли линейную алгебру, оптимизацию и базовую статистику, то этот вводный курс вам может быть полезен.
Текущая политика курсеры по умолчанию даёт возможность бесплатно только смотреть видео и проходить неоцениваемые тесты, а все домашние работы могут делать только те, кто заплатил за получение сертификата (есть ещё возможность apply for financial aid и получить полный доступ, но я лично пока не очень понимаю, кому дают for financial aid). А вот бета-тестеры увидят все материалы курса бесплатно. Но недолго. К концу недели нам уже нужно будет закрыть доступ. 
Если вдруг вам интересно и есть время побыть бета-тестером, напишите мне ваш имэйл, на который у вас заведён аккаунт на курсере.",
<@U0L2F9PJT>: хочу вписаться - куда заявку оставлять?,
"Коллеги, у нас тут задачка, вроде бы простая, но чота мы призадумались. Какова вероятность того, что первый рабочий день месяца -- понедельник?",
"я на тачке где обучаю использую lasagne.layers.dnn. Conv2DDNNLayer, а тестировал утром на маке на ЦПУ, и юзаю тут lasagne.layers.Conv2DLayer",
может юзал кто или слышал об историях успеха,
"<@U06J1LG1M>: а почему выпадение рабочих дней равновероятно? Скажем, 13-ый день месяца наиболее часто выпадает на пятницу. Тут надо уточнить на каком временном промежутке мы считаем вероятность.",
почему 13 день наиболее часто на пятницу падает?,
"ответ 1/5 следует из постановки вопроса, там не оговорено ни о каких промежутках, значит имеем право рассматривать все вообще",
есть местячковый bias еще - как часто первый рабочий день января - понедельник?,
"ох, как там непросто будет.  Праздники разные бывают, в какие-то только банки и госструктуры не работают, в какие-то все бухают на работе, в какие-то все не работают",
"вот даже интересно стало, как это байесом пытались посчитать",
"<@U070Y25AS> <@U0L4DGZU0> <@U0L4KM9R9> коллеги, у нас случилось переполнение, так что мы пока останавливаем набор бета-тестеров. спасибо за отклик! я напишу через месяц, когда будет следующий курс запускаться, может быть, вам там тоже будет интересно поучаствовать.",
"я хочу понять, имеет ли вообще это какие-то преимущества перед sklearn’овским, потому что xgboost вроде как не жрёт столько памяти деревьями, но у меня почему-то они с одинаковыми параметрами дают разные результаты и xgb работает дольше",
"то есть это конечно круто, что он точнее и памяти меньше жрёт, как memit говорит, но у меня ощущение, что это я что-то не так готовлю, а не жизнь такая хорошая",
а ходят слухи что h2o годен как раз изза реализации РФ,
"""This module provides a simple way to time small bits of Python code. It has both a Command-Line Interface as well as a callable one. It avoids a number of common traps for measuring execution times. "" - какая-то ирония в мануале timeit, который как раз попадает в common trap, измеряя  min time",
"<@U0FEJNBGQ>: <http://2015.recsyschallenge.com/challenge.html> — максимум того что я видел в открытом виде, обфуцированные клики и ордеры анонимного магазина. В основном все у кого такие данные есть парятся по поводу их раскрытия.",
"А вот что делают, когда датасет перестает влезать в память?",
Кто как :-) neon делает чанками которые потом на минибатчи бьются,
"чат, а какой щас корректный transfer art style для джентельменов? вот прям который для благородных донов?",
надо будет зайти в гости -- ты сам когда будешь?,
"<@U0G29N5U4>, если интересует мощная статистика и понимание того, как там всё выводится, рекомендую мини-курс Спокойного: <https://vk.com/wall-1694_24665>
этот курс проводится в НМУ более-менее каждый год с разными вариациями (разными темами), на ютубе можно найти видео с прошлогодними лекциями
вот, например, 2013: <https://www.youtube.com/watch?v=P0xJbOxLdVs>
2015: (1-я лекция из 6-ти) <https://www.youtube.com/watch?v=0ktqq7ILySc>",
"<@U0K4F4DGW>: книжку читал как раз за этим и разочаровала тем, что это, скорее, этакий обзор того, что есть. минимум объяснений, *почему* что-то работает, а что - нет... курсы знаю эти, да, конечно",
"ревнуют! я заценил, как они с го облажались!",
не лобов а кого он вдохновил,
"как так, я не понял",
"если раскрыть скобки, поделить а потом схлопнуть обратно, как раз (e^-x + 1)^2 и получится",
кто использует ansible для своих aws-ок?,
покажешь? (я как допишу своим поделюсь) вроде секретных данных там нет,
Я не заморачивался как обычно и сделал просто single-file <http://pastebin.com/5MB8P2Ss>,
какой там pandas в репе? я тут <http://packages.ubuntu.com/search?suite=all&amp;section=all&amp;arch=any&amp;keywords=python-pandas&amp;searchon=names> смотрел,
А кто нить есть уже в беверли?,
"кто-нибудь тут занимался семантической сегментацией изображений? интересует, какие есть датасеты, может я чего-то не нашел. Из того, что очевидно - MS COCO, KITTI, LabelMe, CamVid",
"не знаешь, как он получен?",
"в статье про bitwise вообще сравнивают (между строк) как эффективно используются биты во float'ах и структурах (сама bitwise, собственно)",
"А семинар Ветрова все еще на ВМК происходит? Если да, то кто знает кому нужно писать для заказа пропуска?",
Вот-вот. Как раз на оффтоп-то меня и ввёл в замешательство.,
"<@U041P485A>: почитал статью про nn как стек glm, но не очень проникся. никак не обосновано, почему такая многоуровневая регрессия должна вообще работать, как именно выбирать типы activation function (сейчас это, по сути, перебор), почему в качестве loss используют либо quadratic, либо softmax, а все остальное не работает... либо я что-то не увидел?",
"только что обнаружил, что там оказывается уже 5 статей 0___о

другие loss'ы скорее всего не смотрели, так как они реже используются на практике (люди которые даже знают, не говоря уже о юзают, huber, quantile, cox и т.п. и так встречаются не часто)

выбор activation - на удачу :simple_smile: design choice так сказать",
что такое куе?) какого черта в смысле?,
"народ, а когда есть очень много фич, что вы делаете? проверяете на коррелированность, линейную зависимость, выкидываете коррелирующие фичи? что еще делаете?",
"Факторный, как я понимаю, больше для интерпретации нужен в социологии.",
"Я к тому, что факторный анализ -- это тот же PCA, но с допущениями относительно общей/частной дисперсии и используется в основном в социальных науках, когда снижение размерности является лишь техникой для получения латентных переменных.",
"<@U06J1LG1M>: а такая регрессия не даст ли нули в коэффициентах для случайных факторов, как l1?",
"&gt;&gt;а такая регрессия не даст ли нули в коэффициентах для случайных факторов, как l1?
даст у каких то, но почему у случайных?",
"<@U0G29N5U4>: да, это так, с лассо будет  также, как я понимаю, в таком случае надо определять коррелирующие и выкидывать их",
"&gt;почему эластик? все лассо советуют
вообще ответа точного нет, нужно пробовать, если бы все всегда советовали л1 и доказывали бы это, то не было бы л2 и эластикнета

я встречал задачи где именно оно работало",
"<@U0FL6RNHM>: вот хочется узнать, какой правильный подход к этой задаче",
"вроде как “задача классификации (2 класса)""",
"ну а как правильно сокращать размерность, не теряя в интерпретации? pca все засирает...",
и то и другое. нужна работающая интерпретируемая модель. ну как всегда в статистике...,
"А какая теорема говорит, что рано или поздно все распределения сходятся к нормальному?",
"о! а такая тема: насколько я помню, любое непрерывное распределение можно аппроксимировать как gaussian mixture, а любую функцию, как гауссовский процесс. это как-то к дискретным может приблизить?",
"А как ты хочешь приближать? Если ты дискретными же хочешь приближать, то ты просто выбираешь, какие элементы из носителя ты берёшь",
"а не подскажете какую-нибудь ссылку на то, как пытаются интерпретировать gbm?",
"а как вообще стоит задача?
компромисс между интепретируемостью\точностью достигаетс ручным структурным моделированием, простейший пример которого - побить фичи на группы, извлекать из каждой группы метафичу-аггрегат, после чего уже учить elasticnet",
"просто я провалился в контексте, как от этого вопроса перешли к приближению дискретных распределений смесью гауссиан :simple_smile:",
"ok. в общей постановке задача сводится к такой: есть M процессов, плодящих кучу разнообразной статистики (O(N) переменных). O(MN) достаточно велико для того, чтобы можно было вникать в семантику фич, вообще что-либо руками делать, кроме определения одной переменной для каждого процесса, значение которой хотим предсказывать. но при этом хотим иметь возможность drill-down: если значение целевой переменной изменилось, то почему",
"объяснение не должно быть на 100% точным. но достаточно убедительным. т.е. на уменьшение целевой переменной надо вывести некоторое подмножество остальных, которые тоже изменились за это время, и именно они, в основном, должны были дать такой результат - с какой-то мерой вклада, как если бы это была регрессия без регуляризации. но не обязательно численная мера. в общем, на пальцах, но релевантно",
"но я туда не лазил подробно,  случайно занесло, когда по цифровым фильтрам про impulse response искал",
"<@U04422XJL>: да тут были разные способы, несколько попробовал; я ставил такой эксперимент: брал 100 классов из имаджнета, искривлял распределение, что бы было от 50 (мин) до 1000 (макс) семплов в них, и в итоге рулило следующее, никакие там штрафы в функции стоимости не бьют такой способ

беру из каждого по n &lt;= 1000 семплов, в некоторых классах нету n семплов и я остаток досепплировал и помечал как “подлежит жесткой аугментации”, затем все картинки которые получились без такой метки с некоторой  вероятностью тоже помечал как аугментировать но лайтово

и полученный сет использовал как эпоху, в итоге разницы в качестве почти не было по сравнению если не искажать распределение и тупо обучить",
То есть типа у тебя разницы почти не было между выравниванием и как на душу положит?,
"я делал холд аут этих же ста классов, где было сбалансировано распределение, и тестил на нем модель обученную на сбалансированном (плюс лайтовая аугментация), и модель обученную на не сбалансированном сете (с выравнивание, лайтовой и суровой аугментацией), ну и там аккураси в процент что ли отличалась",
"А не смотрел ли, какая разница в качестве, если не оверсэмплить?",
зависит от того какие 100 классов для тестирования выбрать,
А какого порядка получались цифры с/без? Хочется примерно понять масштаб -- речь о 1-2%  или о десятке,
"Но в любом случае, <@U040HKJE7>, правильно я понимаю твой алгоритм, что, по сути, получается стек - на нижнем уровне простая регрессия из исходных данных в мета-фичи (раздельно по каждому кластеру, а по какой мере он уже получен - отдельный вопрос), а на втором - хорошо интерпретируемый GLM, например? Вопрос с регуляризацией: её на обоих уровнях надо одинаково сильную делать? Или на нижнем можно не делать вообще? Если можно не делать на нижнем, то веса распределятся у сильно коррелированных показателей практически пропорционально, что, вообще говоря, и нужно.",
"про структурный подход я имел ввиду, что ты, допустим, выделил подмножество избыточных коррелированных фич, одинаково связанных как между собой, так и в целом с остальными фичами. и из этого подмножества вытащил, скажем, среднее

после чего ты в интерпретируемом glm (с лассо\ридж по своему усмотрению) используешь это среднее вместо всех тех фич. и интерпретация происходит над средними в этих группах (среднее как прокси-фича, интерпретация над такими фичами). в случае если модель сильно зависит от именно этих фич, можно ее постепенно начинать дробить на более мелкие составляющие

вариант со стекингом кстати тоже возможен, но насчет регрессии над обученными мета-фичами, я бы не сказал что полученная модель будет явно интерпретируема (хотя в голове есть мысли, как такую модель трактовать потом)",
"т.е. мы считаем среднее, потом после glm получаем градиент на него и пропагируем его в кластер, где пересчитываем регрессию, но без регуляризации? тогда возвращаемся к полноценному стеку с разными лоссами и регуляризацией на разных уровнях. состав самих кластеров мы же не можем поменять в связи с тем, что корреляции никуда не меняются",
"<@U04URBM8V>: Саш, а где про этот семинар анонсируют? :simple_smile: а то на ФБ только 2 часа назад от Бурнаева запись)",
"а вот интересный вопрос такой. Есть, например, регрессия 1, которая по параметрам X и Y выдает вероятность P с некоторым AUC. Если построить новую регрессию по этой вероятности P, то будет новая вероятность P', но AUC будет уже меньше за счет потери информации (?). Хочется добавить в регресиию новый предиктор Z, но чтобы не пересчитывать регрессию 1, то есть построить модель на основании параметром P и Z.  Но если это будет регрессия, то см. выше - это будет заведомо хуже модель, чем регрессия по X, Y и Z. Вопрос - как построить модель по P и Z, чтобы она была такой же хорошей, как и модель по X, Y и Z?",
А где господа саентисты проводят пятничный вечер!,
"Дима, если фичи сильно коррелированы, то какая разница (даже с точки зрения интерпретации) какие конкретно фичи будут выброшены регуляризацией?
Ведь такое выбрасывание интерпретируемость в математическом смысле не улучшает и не ухудшает.
А в психологическом все равно надо выбирать под конкретного пользователея анализа: финдиректору кажется, что прибыль снизилась из-за высоких расходов, в руководителю сети - из-за большой численности персонала.
А из-за чего она на самом деле снизилась так никто и не узнал. :simple_smile:",
"кстати, пока придумываешь шутку, когда там митап у вас?",
"народ, нид хэлп: есть таблица (userID, time, domainID), таблица большая, юзеров много, сайтов очень много, надо кластеризовать юзеров, что бы вы стали делать?
то, что пока что я придумал:
1. кластеризовать их по посещаемым сайтам (но пока что не уверен как это сделать, данных многовато, в память не лезут, надо работать со спарс-матрицами)
2. посчитать для каждого юзера показатели типа ""среднее количество сайтов в день"", ""дни недели, в которые он посещает сайты"" и кластеризовать по таким показателям...
есть еще какие-то идеи? может быть где-то есть что-то похожее с решением? было бы интересно почитать..",
"а зачем SVD, можно ведь сразу PAC/tSNE?",
"если честно, я не особо подружился пока с tSNE, поэтому, как я понимаю, или, но поэтому и спрашиваю совета",
Нужна именно hard кластеризация? И какой порядок числа пользрвателей и сайтов?,
"нужна просто кластеризация, какая получится, с какой-то интерпретацией, пользователей около 30к, сайтов - 200к.",
"Можно LDA применить, выделить темы сайтов, а затем для каждого пользователя по каждой теме посчитать статистику по времени: такая тема преваилируют вечером, такая утром и т.п. В этом пространстве уже клстеризовать обычным kmeans.

Ну и логичный вопрос - зачем, как кластера будут использоваться?",
"на сколько я знаю это аналог fullconnected, преимущество в том что считется быстрее или вся сеть едино выглядит как сверточная",
"Народ, а как сейчас обстоят дела с инвариантностью к вращению в CNN.  Out of thу box или помучаться придется ?",
А есть кто на месте уже?,
"Котаны-котанчики, а какой сейчас самый современный/чоткий подход для association rules learning они же в быту е-коммерса называются cross-selling ?",
это чот не из той пьесы,
"На это я уже натыкался (хотя не пробовал). Хочется что-то, чтобы учитывало какую-то метаинформацию  айтемов - например, если чувак покупает айфончик - то предлагать не просто розовый чехол так как он часто покупается, а рекомендовать все чехлы к этому айфончику (но не все чехлы к телефонам вообще).

Или если у нас были покупки кучи разных чехлов для айфона (но мало каждого по отдельности) -&gt; то я бы понимал это,  группировал их все в чехлы для айфона  и создавал правило.",
звучит как какая-то графовая фигня на самом деле,
ну там покластеризовать как то хитро - можно получить группы айтемов :simple_smile:,
"вот такой вопрос еще про скоррелированные фичи: мне надо сделать по ним регрессию, но хотелось бы получить не абы какие коэффициенты, а осмысленные, при этом, понятно, минимизировать количество свободных параметров",
"вопрос: как это делать универсально? т.е. есть некоторая кривая, которую хочется определить, и она не похожа на полиномиальную. что делать?",
"примерно. там есть примесь и других параметров, т.е. всего их в модели довольно много. меня сейчас временной разрез интересует - как бы его минимизировать. проблема, что таких систем много, и форму кривой предсказать сложно",
"очень хорошо работают три коэффициента - для x = 0, x = 1 и остальных. все здорово. но есть другой датасет, где кривая несколько более пологая...",
"в общем, есть кусочная функция, как бы ее аппроксимировать наименьшим числом параметров?",
а где про сплайны и их использование в линейной регрессии лучше почитать?,
А вот сюда идет кто <http://www.osp.ru/iz/bigdata/event/about> ?,
<@U0K9XLC04>: а почему 2 раза Big? Это опечатка составителя афиши или так и задумано? Быстрый гугл не дал ответа),
"народ, а как вы боретесь с проблемой определения количества кластеров при кластеризации (когда совершенно не ясно, сколько их)?
может быть есть что полезное почитать на эту тему?",
и как объясняете заказчику свой выбор?,
"кстати, а в какой ситуации заказчику нужны именно кластера, а не что-то иное? (например группа пользователей, которые лучше отреагируют на рекламные акции определенного вида)",
"У кого телефоны на андройде, те возможно чекинятся не зная того. ",
"<!here|@here> у нас же кто-то был, кто с курсом Воронцова на Курсере связан?",
"ребят, а кто сегодня был на завтраке, чтобы я представление имел какое-то",
Кто-нить может подсказать какие есть нелинейные методы обучения без учителя кроме нейросетей?,
"Извольте, но как же kNN и прочие метрические штуки?",
как это KNN без учителя?) а метки откуда?),
KNN можно считать предельным случаем SVM где все точки имеют по опорному вектору,
хорошо бы точно какие из низ никак не обобщенно-линейные,
"Вообще говоря, SVM реализует линейную разделяющую поверхность в некотором спрямляющем пространстве, задаваемым ядром. 
По существу, нейросеть тоже на N-1 слое делает отображение в спрямляющее пространство -- и на последнем слое -- линейная комбинация

Поэтому я не понимаю, как можно kernel SVM называть обобщенно линейным, а нейросеть -- нелинейным методом",
"исходная постановка svm (двойственная к ядерной) вроде напоминала ridge-regression, только margin - это как регуляризация, а штрафы - как loglikelihood",
"А в нейросети зависимость от неизвестных параметров на нижних слоях по-любому нелинейна, тут уж как не крути. Вот тут типа любую зависимость можно представить зато. Отсюда и вопрос, кто еще не ограничен линейностью",
А почему тогда переход в спрямляющее пространство не считается нелинейностью?,
"вопрос 3 - какая польза от определения, которое предлагает линейно-нелинейное разделение функций?",
"меня все еще интересует польза и смысл от такого не-прикладного разделения методов, когда svm с квадратичным ядром позволяет делать нелинейное разбиение классов",
"А вопрос возник из собеседования у ""красного мобильного оператора""):
Они: Чем плоха логистическая регрессия?
Я: Ну форма зависимости ограниченная, ну хотя работает с большими размерностями, популярна в бинарной классификации, дает вероятность
Они: Ну она же обобщенно линейная! Это же даже в Википедии написано!
Я: Ну ок
Они: А какие нелинейные модели вы знаете?
Я: Ну нейросети гарантировано нелинейные.
Они: Ну зачем сразу нейросети. Вот знаете ... (и забыл)?",
"Но вообще чтобы понимать у кого больше ""степеней свободы"", кто не ограничен ""линейностью""",
"А я недавно случайно узнал, как shuttering переводится на русский",
"Хм... Если совсем в двух - то это такой способ теоретически обосновать, почему модели оверфиттят",
"народ, есть такая задачка:
есть база товаров из самых различных областей (товары интернет-магазинов), она размечена на группы, приходит новый список товаров, необходимо отклассифицировать их по имеющимся группам, найти похожие между собой товары (первично - по названию, возможно, можно будет учесть еще что-то)
Кто решал подобного рода задачи? Насколько я понимаю, это LSA, что-то еще?
Ну и главный вопрос (к тем, кто решал подобную задачу) - адекватной ли точности можно добиться?",
"согласен, но если, допустим, речь идет про существование размеченного датасета, после чего на вход подается ""чехол для айфона"", далее, с помощью LSA ищутся похожие товары, берем, допустим, топ-10 самых близких товаров и относим его к тому классу, которого больше в топ-10?.. да, самому кажется, что точность будет так себе.
В общем, очень хочется услышать людей, кто такое делал на практике...",
"<@U0FF52P7D>: в целом понятно, почитаю на эту тему, спасибо. Просто если говорить только о названии товара, то, как я понимаю, это LSA и термы, термы и будут являться переменными",
"<@U0FF52P7D>: вроде как понял, что имеется в виду",
Так главный вопрос как от карточки товара перейти к фичам. Чем фичи раскидывать - дело десятое.,
"<@U0FF52P7D>:  
&gt; Можно придумать ядро, выполнять преобразование каждого товара в векторное пространство и дальше пользоваться любым softmax.

О а можно для дурачков рассказать как этот метод ин инглиш называется, чтобы погуглить ?",
"gbdt не порадовали на большом кол-ве классов (~700) из-за того, тчо написал миша, так как категории размыты, обучалось долго и я делал не мультилейбел (хз как в хгбусте делать мультилейбел если только не 700 отдельных классификаторов обучать)",
"<@U0FL6RNHM>: была какая то изначальная иерархия, но в айтемах были ошибки",
"не, я обучал один softmax на 700 и неочень удачненько (около ~83-85% точность была ) и на датасете, который я из плохо размеченных данных выделял какими-то эвристиками.

Надо переобучить на мультилейбл случай и с какими нить логистическими регрессиями, но пока руки не дошли",
"на дофига классов и когда к нескольким классам могут принадлежать  шмотки я кроме как логрегрессий и нейронок ничо адекватно не могу придумать,  что за разумное время обучилось бы  (и то насчет нейронок не уверен :))",
"ну и скопус, брожу по авторам и тем, кто их цитирует - для общего обзора",
<@U0JJ8S5GQ> в применении к какой области? ,
А как связаны временные ряды с уходом пользователя? И реклама? Просто первое хорошо решается стандартной классификацией с временными фичами (можно попробовать кривые дожития и Cox regression прикрутить для большей гибкости).  Для второго обычно просто регрессия на ctr,
А как бы так картинки из текстов делать)) и CNN потом ,
<@U0KQ5M6KX> а как такой разворачиватель построить? С учетом того что 'чхл йафн' похож на много что? ,
"Решение, в принципе, очень интересное, только неясно как его в realtime применять.",
"А где есть ссылка на календарь, чтобы добавить его в свой?",
на розалинде самое интересное - читать как народ решал те ли иные задачи,
"Псс, а кто нибудь работал с habidatum? Я просто сижу на презентации и мне интересно что как у них по факту с nlp ",
"коллеги, подскажите какой-нибудь корпус текстов, на котором можно было бы корректно оценивать ассоциативные/синонимические связи слов, степень близости и все такое. 
хочу его использовать для оценки качества w2v-модели близких слов, но уже на тематическом копусе текстов

я так понимаю. что рукорпора была бы идеальным вариантом, но ее не достатать. может, стоит просто выкачать какую библиотеку? у меня есть сомнения, что вики или хабр будут полезны в этом контексте...",
"Только как по нему оценить то что ты хочешь? Там размеченные тексты вот примерно в таком виде
```
&lt;p&gt;&lt;se&gt;
&lt;w&gt;&lt;ana lex=""научный"" gr=""A=f,sg,nom,plen""&gt;&lt;/ana&gt;На`учная&lt;/w&gt;
&lt;w&gt;&lt;ana lex=""биография"" gr=""S,f,inan=sg,nom""&gt;&lt;/ana&gt;биогр`афия&lt;/w&gt;
&lt;w&gt;&lt;ana lex=""Святослав"" gr=""S,persn,m,anim=sg,gen""&gt;&lt;/ana&gt;Святослава&lt;/w&gt; 
&lt;w&gt;&lt;ana lex=""Сергеевич"" gr=""S,patrn,m,anim=sg,gen""&gt;&lt;/ana&gt;Серг`еевича&lt;/w&gt; 
&lt;w&gt;&lt;ana lex=""Лавров"" gr=""S,famn,m,anim=sg,gen""&gt;&lt;/ana&gt;Лаврова&lt;/w&gt; 
&lt;w&gt;&lt;ana lex=""во"" gr=""PR""&gt;&lt;/ana&gt;во&lt;/w&gt; 
&lt;w&gt;&lt;ana lex=""многое"" gr=""S-PRO,n,sg=loc""&gt;&lt;/ana&gt;мн`огом&lt;/w&gt;
&lt;w&gt;&lt;ana lex=""неординарный"" gr=""A=f,sg,brev""&gt;&lt;/ana&gt;неординарна&lt;/w&gt;.&lt;/se&gt;
&lt;se&gt;
```",
"Смесь языков и искажения слов решаются примерно одним и тем же способом. Надо построить функцию отображения из слов наименования в ваш внутренний правильный словарь. Если слово из названия есть в правильном словаре, то оставляем без изменения, иначе ищем правильное слово для данного исходного.
Тут во всю может вылезти проблема многозначности слов (например, рукав как часть одежды и рукав как шланг) . Она отчасти обходится с помощью условной вероятности (слово А0 отображается в В при условии других слов из названия {А1,...,Аn].
Очень многое здесь зависит от темы наименований (чем ближе она к естественной речи, тем легче). А если вам надо нормализовать слова из темы ""задвижек высокого давления"", то может быть очень тяжело, просто потому что учиться не на чем (нет ни словарей, ни корпуса текстов хорошего).

Но весьма вероятно, что вам повезет и проблема многозначности вас вообще не коснется, то есть, переводя слово А в первое попавшееся слово В (которое по смыслу может вообще и не подходит к исходному смыслу А), вы тем не менее получите уникальный вектор для всего названия, которого будет достаточно для последующей классификации.

А потом уже из вектора правильных слов можно действительно сделать bag of n-grams - возможно это сработает очень хорошо и реализация простая.

Другим вариантом восстановления может быть построение более сложных n-gram. Обычно они делаются по соседним буквам, но проблема с искажениями слов в названиях заключается в том, что там как раз соседние буквы и выбрасывают (молоко - млк). Так что обычными n-gram'ами их не поймать (и edit distance тут тоже не сработает).
Поэтому можно строить, например, n-gram'ы без гласных (поскольку часто сокращают именно выбрасыванием гласных).  Эти n-gram'ы можно использовать при переводе (Слово ""здвжка"" может вообще раньше не встречалось, но [здв, двж, жвк] соответствует слово ""задвижка""; и англоязычному набору [stp, tpc, pck] тоже соответствует ""задвижка"", если мы решили переводить именно на русский (а ведь можно и в выдуманный язык переводить)). 
Но и тут расслабляться нельзя потому что краснодарское молоко все равно можно перепутать с красноярским муляжом. Хотя  без ошибок все равно не обойтись.

В вашем случае возможно есть и другие правила формирования n-gram, которые вы можете быстро выявить и они дадут заметный эффект.",
"<@U041LH06L>: ОК, если баян, то вот вам, Александр, загадка -- попали Вы на зону, а в камере Вам показывают на батарею четырехрядную и говорят: ""Сыграй как на баяне."" Что нужно ответить? ",
А кто ещё курс Воронцова проходит?),
"Возможно, стоит спросить, кто его не проходит :). Я, к примеру, прохожу. В режиме неспешного повторения, матери повторения. ",
"я не могу понять, почему не принимает ответ",
<@U070Y25AS>:  без регуляризации на каком шаге цикл завершился?,
<@U070Y25AS> и какой roc получился?,
"А вот если нужны именно красивые интерактивные графики, типа как у highcharts, какие альтернативы рекомендуете? Если можно вставлять в ipython notebook, особенно приятно было бы. Bokeh субъективно страшненький.",
"<@U040HKJE7>: я что-то пропустил, где можно узнать подробности про хакатон?",
работал + мог сфокусироваться даж когда вокруг там музычка играла,
"для каких задач всё еще имеет смысл применять SVM (как одну модель, без ансамблей), или xgboost отовсюду вытеснил?",
в смысле на каких задачах linear svm работает лучше других методов?,
"классификация по длинным разреженным признакам: когда данных мало как правило работает устойчивее лог-регрессии, ну и плюс у линейного svm обычно чуть-чуть выше чем у лог-регрессии auc и precision/recall@k",
"Котаны, а какой кошерный способ предсказывать в undirected weighted graphs новые/пропущенные связи ?",
Как тебе ФБ друзей подсовывает - по кол-ву общих друзей. ,
а как веса тут учитывать?,
а чиселками или алгоритмами это где нить описано?,
"тема с пересекающимися кластерами интересна (ее просто реализовать и сама идея мощная). Но как именно ее использовать?

Допустим ктото пересекается с другим юзером по паре человек в одном из кластеров эгосети. Значит ли это, что этим людям имеет больший смысл сдружится, нежели с кемто с пересечением по случайной паре друзей?",
"а если значит, то как? всмысле, какие из этого делать фичи?",
<@U070Y25AS>: а это не тебя я видел в кафешке Нью-Йорк в здании где ФРИИ?,
почему у дипсеток огромная vc размерность а они все равно рвут всех?,
А почему огромная VC-размерность -- это плохо?,
<@U0873FY94>: выглядит как гуру пикапа или бизнес тренер из БМ  на фоточке :simple_smile:,
"Ну потому что VC размерность это как мера склонности к переобучению. Если классификатор может объяснить что угодно, это не очень хорошо. ",
Ну т.е. там есть верхняя оценка на ошибку и при фиксированном размере трейна она растёт по VC размерности. Поэтому вроде как лучше взять модель у которой поменьше ,
"&gt;&gt;&gt; Кароч, ребята, тут такое дело, к нам приходит чел, который получал 300 000$ в год, и говорит, что ему мало и хочет лям И в месяц!
Сделал стартап связанный с Big Data и расскажет, как можно все круто организовать и привлечь инвестиционный бабос и вырасти в N-раз.
Между прочим, Паша продает свои проекты Тинькову.",
но ценник то у них как будто на продукт,
"А вот кто-то может подсказать, где с минимальным геморроем собрать дашборд? Лучше в облаке. Из специфических требований только желательно иметь role-based auth, чтобы разным юзерам давать доступы на разные страницы, как минимум.",
"Спасибо, но я именно в виду страничку, куда диаграммы plotly можно повставлять",
"Bokeh очень печальный, да. Страшный ещё как не знаю что...",
"С plotly прикольно, что с красотой можно из админки поиграться, хотя она тоже неполная (сплайны не понял как сделать, хотя в питоне это две строчки). А потом перенести в код (хотя тоже не без особенностей). Ну и большая проблема plotly в интеграции с matplotlib - легенды. Но они этому даже отдельный раздел посвятили в доках, слова Богу.",
"а возможности оффлайн sharing'а нет, как у bokeh?",
"Не увидел пока. Такое впечатление, что официальные API работают только с облаком. А вот как выгрузить оттуда диаграмму... Надо поискать.",
"Всем привет, кто завтра на завтрак собирается заглянуть?",
"левые отзывы много, где используются. как и подобное ботоводство.",
Ну свм найдет отзывы которые отличаются от остальных по каким то признакам. Не обязательно эти отзывы будут заказными,
"<@U040M0W0S>: работал в Тиньков, ни о каком Павле Ершове не слышал, странно это очень",
"но обычно по паспорту могут пропустить, если знаешь к кому идешь и зачем (хотя есть риск, что не пустят), нужен аля сопровождающий",
"в курсе ли кто-нибудь, семинары в ИППИ (типа как в след пятницу будет) записываются?",
"непонятно пока, какие на ограничения на offline: более скудный выбор графиков, т.д.",
"так как новая версия плотли содержит в себе локальный plotly.js, на котором все и отрисовывает",
Встречал ли кто в литературе задачу решения ПДД тестов с картинками нейронками?,
у меня несколько осей через `ax2 = ax1.twinx()` не отображает как надо,
"Коллеги, как достать базу ЕГРЮЛ (юр лица) по всей РФ. Еще Беларусь и Казахстан нужно, но это дело второе",
<@U0FEJNBGQ>: А как оно примерно проходит?,
"Ребят. а какие еще есть датасеты/задачи для демонстрации классического ML (как титаник, но чуть менее попсовые)",
"кажется, уже обсуждали, как в ИППИ попадать, но поиском найти не получилось. Туда человеку со стороны реально придти и сколько это забот создает организаторам?",
"народ, как вы кластеризуете данные с признаками разной природы? ну типа пол, возраст, вес кошки, диаметр шин автомобиля, сексуальная ориентация бабушки, ну и тд; если вектор фич состоит из такого вот разносортного",
"как вчера
я думала вчера только бухали)",
"когда у меня возникла такая потребность, я и открыл для себя все прелести распределения дирихле: на каждом подпространстве натягивал типовые для них кластеризации (обязательно вероятностные, чтобы была непрерывная принадлежность кластерам). делал число кластеров с запасом (штук 20). после чего натягивал смесь дирихле поверх полученных кластеризаций подпространств.

например, было 1300 фич, из них первые 50 - непрерывные, следующие 750 - бинарные bag of words подобные, и еще 500 сгенерены типа непрерывными. после кластеризаций на подпространствах я получаю вектор из 60 фич (по 20 на каждый), сумма по строкам соответственно ровно 3 (3 подпространства, суммируются к 1 каждое).

а потом смесь дирихле. вообще возможны и другие варианты кластеризации, просто именно смесь дирихле лучше всего математически подходит под такую задачу",
"а ты какой то фреймворк юзал для этого, где можно задать распределение и каким нибудь емом найти кластера, или ты писал все вручную (боже упаси)?",
<@U04422XJL>: когда там семинар годный?),
"вообще уже потом я узнал про преобразования над compositional data чтобы потом можно было работать с ней как с обычными аля нормально распределенными данными, но дирихле мне хватило",
"понимаю что визуализационно сам проект простой как пробка, но круто же:
<http://explorer.opensyllabusproject.org/graph>

добраться бы до данных - какие книги брали вузы для обучения студентов разных специальностей.",
прям как в школе хвастаешься!,
а как ты их объединял в подпространства?,
а где анонсировать в онлайне или офлайне?,
"разбивал мега-матрицу ""доменной экспертизой"" :simple_smile: тоесть я знал что там лежит и уже посмотрев на названия фич и гистограммки, понимал где кончаются одни фичи и начинаются другие. если б мне их перемешали, я думаю все равно бы получилось их так разбить, как минимум по числу уникальных значений и по дисперсиям",
"те кто в прошлый раз был ан DF - скажите, вам письмо с аннонсом еще не приходило? стоит писать ручную расслыку всем прошлым участникам?",
трансляции то будут не? Для тех кто не из мск,
а про преобразования над compositional data где лучше почитать?,
"ну, очень по уму можно как раз переформулировать тогда PCA над ними либо в DCA либо в Acomp (выше) :simple_smile:
просто из-за ограничений на суммы по строкам возникнут пакости: на практике будут видны длинные линии и полосы в проекциях",
"она ни у кого не работает, так задумано",
что я делаю не так? и вообще какая нормальная скорость такой обработки?,
"я все-же не пойму как считать probability для LDA , и у него нет метода predict в отличии от Kmean
<http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html>",
А как в LDA выбирать количество топиков?,
"Для kmeans вот есть силуэт плоты, а в LDA как на глаз?",
"Как страшно жить, спасибо :simple_smile:",
"если короткие то просто не делайте :simple_smile:
а если это reuters20, то там как раз 20 тем и найдется",
"&gt;а если это reuters20, то там как раз 20 тем и найдется
в рейтерсе простым ЛСА найдутся все 20 -)",
transform это какаято питоновская команда? предикшены делать как предложил <@U0JKYTE4B> - считаешь евклидовы расстояния до центров кластеров. кто ближе - того и тапки,
"Лайтово, без энтузиазма. Не как в _тот раз_.",
И если да - то какие есть грабли?,
"ну они платы под этим брендом тоже вроде как продают, но похоже это та же тесла",
"у меня какие-то такие смешанные впечатления по поводу кавасаки всегда были)

ну то есть он безумно крутой спикер, а потом он пишет книжку и рассказал как быть крутым спикером и как в себя всех влюбить, список примеров и уловок которые использует) и потом начинаешь замечать что от лекции к лекции он использует все те же шутки про микрософт и истории про эппл
и вроде сам ничего супер-значительного не аккомплишд, ну кроме евангелирования аплла в начале..

но человек очаровательный и говорит как мед в уши)",
"у любого, кто делает больше одной лекции в полгода, будут одни и те же шутки везде %))))",
Кто нибудь тут занимается распознаванием речи?,
"Хотя, когда слышишь от лектора одну и ту же шутку дважды, поначалу сильно печалишься.  Я такое от многих слышал, ну и сам, естественно, испытывал.",
"хуже, когда от профессора в универе слышшишь одну и ту же лекцию пять раз за время обучения %))))",
"Что вполне может быть оправданно в ситуации, когда образование стандартизовано (математики, инжееры), но в гумбольдтовской модели университета это неприемлемо.",
интересно для кого большая размерность будет очень :simple_smile:,
линейные модели как раз очень хорошо переносят большую размерность,
и вдруг у кого завалялись базы с конкурса ЦРТ,
а как же double-precision &amp; ecc /s,
это ты как мастерски так сделал?,
ну свожу вас как диплом получу,
"Ребят, для тех кто в танке. Где тренировка проходит?)",
куда есть-пить пойдем после тренировки? ,
я как раз оттуда и нашел ее,
"а ни у кого нет такой экзотики как натренированная флекснет на ILSVRC 2010, а то везде натрененная на 2012",
"я тут готовлю макеты стикеров и возник вопрос. с одной стороны, <#C047H3DP4> самый популярный по голосам. но я понятия не имею что ставить на стикер как фон. есть идеи, с чем у вас ассоциируется этот канал?",
"была гифка, где на веревках висит ряд горшков, крайний горшок отводят и отпускают, в надежде, что он стукнет ряд и с другой стороны ряда отскочит другой горшок, но он разбивается и его содержимое высыпается, вот физиономию проводивших опыт после этого надо запечатлеть",
"у нас шуточка была - так как студенты дико овощат над простыми задачами, их суммарный сабмит представлял собой случайную грядку",
"спасибо, и такое тоже интересно - тоже кидайте
но gate - это как конструктор - самому нужно подбираться плагины и тп, писать правила в некоторых случаях.
а что-то еще проще. Поставил и профит (для любых задач по анализу текста)",
"Я недавно его пробовал, он как раз такой - поставил и все, делаешь `nlp(text)` и готово. Правда для задачи, которая у меня была,  результаты не устроили. И еще там только английский",
"а главное - про то, зачем оно может быть нужно",
<@U04DXFZ2G> а туда как попасть? кому написать что б в какой нибудь список включили?,
"<@U06J1LG1M>: можно просто позвонить мне например или Антону Слесареву (в фб телефон), как подойдешь - мы тебя пропустим, подходить со стороны ШАДа, там где шлагбаум и проходная такая как на завод",
"блин, самые интересные темы когда я уезжаю :disappointed:",
"Вообще, неясно, какие у них проблемы с записью: ведь сразу после семинара идут пары в ШАДе, которые записываются",
"___________________________________________________________
котаны, а кто может помочь нам поволонтерствовать на datafest'е? надо утром помочь регистрировать людей, потом во время обеда - раскидать пиццы, ну и еще по мелочи помощь в течение дня",
"Гайз, а тут как-то мелькало давно, но мою память чутка отшибло, какие есть годные способы определения количества кластеров, если их неочень много (&lt; 10), но не на глаз, так как процедуру надо повторять много раз для разных множеств.

Датасет - тексты, которые как то кластеризованы в виде иерархии, хочу понять, можно ли их еще расщепить сильнее., чтобы прям совсем близки по смыслу были.",
"Там, видимо, проблемы начинаются, когда датасет большой, в маленьком то реально можно тупо перебирать просто разное число кластеров.",
"ну у меня есть уже какое то глобальное разбиение большого датасета на мелкие части, я хочу мелкие части еще подробить",
"Нам же неясно, что для тебя мелкие части :simple_smile: А так kmeans + BIC и там тупо берешь максимум, почему бы и нет",
"сталкивался, правда, я с такой засадой, как фрактальные процессы...",
"ну, самоподобные. в общем, когда кластеры состоят из кластеров, те еще из кластеров и т.д. и нет оптимального разбиения по определению, если не делать агрегацию на каком-то уровне",
"это когда сколько ни режь на кластера, внутри еще столько же остается?",
У кого есть Intel MKL для Linux для некомерческих целей,
Смотрите как чудесно <https://vk.com/doc-44016343_437309027?hash=8c87dc3a28130fac4b&amp;dl=06604259345414a0d6> Ребята кортекс моделируют,
Какие впечатления от Machine Learning: A Bayesian and Optimization Perspective ?,
Всем привет. У кого желание есть на завтрак завтра зайти ?,
"<@U06J1LG1M>: а ты всё равно приходи, мы там до 12 сидим как минимум",
будем как белые люди собираться на завтрак в 11,
"<@U04423D74>: хороший вопрос, но почему-то молачат все :simple_smile: А для кого визуализация? Для бизнес-пользователей, для спонсоров, для дата саентистов и инженеров? Предполагаю, разные тулчейны для разных целей.",
"Есть ли здесь те, кто работает удаленно? А еще лучше, если не на русскую компанию",
"я жадный, я все хочу. потому что лично я предпочитаю смотреть в таблицы, и графики не очень люблю...
но хочу понимать, что и как надо визуализировать",
"Смотря для каких целей и данных. Если динамическая визуализация, то библиотеки типа d3js <https://d3js.org/>. Если инфографики разные, то там целая куча тулов, мы пикточарт используем <http://piktochart.com/>, ну или дизайнера в ручном режиме.",
"Есть еще Tableau. Комбайн, который когда-то давали попробовать на каком-то из курсеровских курсов. Платный. Впечатления у меня от него как от экселя.",
"*ну и тут как бы я делаю акцент, что мы триалили определенный набор функций для нас. Я не ставлю знак равенства между этими системами отнюдь.",
"опять филиал флудильни развели, что за аналитики...

<@U0BLCTAJK>: а как ты плотли вставляшь в rmarkdown?",
"ага, ага. кажется, у меня были пробелмы, когда я вот так рендерил
```
knit('report.Rmd', 'report.md', quiet = TRUE) # creates md file
markdownToHTML('report.md', 'report_sdk.html', stylesheet = ""custom.css"") # creates html file
```

кстати. ты всякие js-прибамбасы не пилил туда? типа слайдеров/выпадающих менюшек на одном графике?",
Это как мастер-класс с призами,
"Вы как будто вчера родились, ей богу.",
"хакатон если с рекламой это ок; а вот как IBM делал из R митапа просто тупо рекламу себя это как то не очень; ну в принципе R митапы померли, а мс хакатоны живы",
"ну либо попоказывать дичи, как из изоленты,  data.table и Reduce'ов делать из предпоследнего почти матрицу",
"надо понять, на кого целиться, тех, кто еще не в теме, вводное всякое для них читать,  или для бывалых в жилетку друг другу поплакаться",
А как же ~свальный грех~ pop-porn игры после Секспросвета?,
"есть еще такой интересный проект, где можно код подсмотреть: <https://github.com/cmusatyalab/openface>",
"ну достаточно глянуть на проект гуглакара который ведет Себастьян Трун (ну или вел, хз), там вообще напичкано всего, и дл там года полтора назад почти не было, щас если появился, то уверен  не как основной модуль, а вспомогательная часть",
"чят, есть нупский вопрос по статистике. Имеются две популяции, у каждой из которой меряется определенная характеристика (длина хвоста). Какой применить тест для проверки гипотезы средняя_длина_хвоста(A) = alpha * средняя_длина_хвоста(B), где alpha &gt; 1 задана?",
"что то мне подсказывает что стат теста тут нет, ибо как я понял сама популяция и есть семпл (длинный  хвост распределения), и получается всего два семпла",
"формулировка корявая
как минимум потому что нельзя говорить о стат.достоверности - достоверносе событие это событие с вероятностью 1
обычно говорят стат.значимо %)",
"я бы решал задачу двумя путями
первый - проверил бы значимость различия длин хвостов в А и Б.  потом посчитал бы, как говорит <@U0FEJNBGQ> какой-нибудь размер эффекта (пришлось бы повозиться с выбором, какой именно индекс взять). и сравнил бы значение размера эффекта с 2.
второй вариант, и он для меня проще - сделал бы два сравнения. по чистым выборкам, и по домноженной Б

формулировку
&gt;Могу ли я использовать имеющиеся у меня данные для того, чтобы с большой (90%) степенью уверенности выяснить, является ли мое подозрение неверным?
 я бы изменил на 
&gt; Могу ли я на основании имеющихся данных выборок утверждать, что хвосты особей популяции А в два раза больше хвостов особей популяции Б?",
"да зачем такие сложности, делай анову - она, как утверждается, робастна к отклонениям от нормальности, и вполне работает на выборках разного размера
ну, разве что, не очень хорошо жрет разные дисперсии",
"там выборки меньше 100 рыл
так что даже непараметричсекий бутстрап - как мертвяка пинать, имхо",
"Как раз только так и наскрести распределение :) когда точек мало то либо опускать лапки, либо яростно бутстрапить ",
про бутстрап как раз понятно,
"<@U06MTEXQQ> это про то чтобы нарисовать гистограммы каждой статистики, убедиться что распределение не какое будь powerlaw.  А дальше есть как раз тесты на распределения (колмогоров смирнов самый попсовый)",
"а анова, как считается, робастна к отклонениям от нормальности",
Так что там не просто сел и поехал куда угодно,
"<@U04423D74>: а вот как тогда насчет критерия вилкоксона, который mabrek предлагает? он тоже для двух переменных вроде, и как раз для ненормальных распределений",
"Котаны, а есть у кого саксесс стори использования NMF и в каких доменных областях/задачах ?",
разве что тем кто на белорусской как вариант),
"Сегодня в ШАДе кто будет Федора перескопить?
не могу приехать но хотел бы послушать",
"Кстати, как туда попасть, если пропуска ещё нет) вдруг из-за снега опоздаю",
ютуб как раз умеет запись хранить,
"Всем спасибо, кто был, смотрел и задавал вопросы. Есть конечно проблема с разбором статей, то что люди думают, что ты - это статья, ну или по крайней мере автор. Немного мямлил, my fault. Будем ботать глубже и до самого дна. Рад, что обсуждение, потому как это не лекция все же, а семинар",
"Ага. Интересны как-раз примеры, когда nmf дает заметно лучшие чем svd результаты. А то я пока что видел в основном обоснования с точки зрения области применения.",
какие есть публичные решения для этого? типа сделать себе свою сири,
"Скажите мне, какой фреймворк лучше взять для рекуррентной lstm-based сети?",
"Поняла, спасибо, как раз ожидаю боли и граблей, да :smiley:",
"Keras похож по интерфейсам на Torch
Вообще фишка Torch именно в низкоуровневости, как следствие -- возможны хаки с памятью и скорость

Если скорость/память не являются критичными, то я бы брал TF",
"простые можно в keras накидать же, он поддерживает tf как бэкенд",
"Для TF, как и для Theano, можно использовать разные уровни абстракции
Можно прям на TF писат, можно обертки разные использовать",
"а имеет смысл использовать такую обертку, как keras, вообще? не приведет к тому, что придется переписывать, как какая-то кастомизация понадобится?",
"спасибо. подумываю просто не только о deep learning, но и где писать что-то, чего нет в sklearn. например, glm там только как название, без каких-либо link funtions. вот и думаю, копаться в его низкоуровневых api и писать поверх них или разбираться в чем-то совсем новом. это перспективнее с твоей т.зр.?",
но смысл есть только на больших данных когда в оперативе ты не сможешь обращение матрицы сделать,
"подожди, poisson regression - это тоже glm. и какой же там мнк?",
про первую и не знал. это когда преобразования фич что ли? полиномы всякие?,
"Нет, это когда Y - не число, а вектор, т.е. вектор исходных фич линейно преобразуется в вектор результирующих фич",
"а в какую сторону вообще сейчас tf развивается? типа theano или все-таки torch, т.е. заодно включить в себя большинство слоев и библиотеку моделей - т.е. заместить лазанью и керас?",
"я вот как это вижу: theano - это низкоуровневая библиотека вычислений общего плана; модели и слои либо кодируются, либо берутся готовые в лазанье или керасе и др. torch объединяет и то, и другое - и низкоуровневое api и кучу моделей безо всяких оберток (точнее, со стандартной библиотекой nn и т.п.)",
"вопрос: в какую сторону движется tf - неважно чьими усилиями? стоит ли изучить тот же керас поверх него или можно ожидать, что все распространенные модели и кубики появятся в самом tf? ну и если все-таки керас, то надо ли будет переписывать, если потребуется кастом, или там достаточно просто все расширяется?",
можешь посмотреть как там бекенд сделан <https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py>,
"Всем привет! Ищу словарь словарь цветов (англ + рус) с различными извратами. Например, налчием заимствованых слов из других языков (BIANCO) и транслитерацией (бьянко). Не завалялся ли у кого такой?",
"Интересны как раз не сами цвета, а их написания",
<@U0FEJNBGQ> а есть более подробное описание семианара? на какой уровень расчитан материал?,
"&gt;Название: Еще раз о рекуррентных сетях, или что уже можно и чего пока не получается предсказать, глядя на сигнал изменчивости сердечного ритма.
&gt;
&gt;Докладчики: Ольга Малюгина, Семен Карпенко
&gt;
&gt;Аннотация: Последовательность расстояний между R пиками кардиограммы называется сигналом изменчивости сердечного ритма (heart rate variability, HRV). Его получают либо как выходной сигнал пульсометра, либо как результат резкого сокращения размерности достаточно длительного сигнала ЭКГ. В известных (благодаря К.Воронцову) работах В.Успенского использовался более богатый сигнал - с учетом амплитуды пика. Однако, на сегодняшний день существует достаточно большой по медицинским меркам массив доступных именно HRV сигналов. Насколько нам известно, факт наличия/отсутствия пика считается гораздо более устойчивым признаком, нежели его высота. Для анализа HRV обычно используется множество нетривиальных характеристик сигнала, вплоть до оценок $\epsilon$-энтропии и фрактальной размерности. Оказывается, многообщающие результаты на массиве HRV данных удается получить, вычисляя исключительно простые базовые признаки - но с использованием рекуррентных сетей в качестве инструмента. Попутно планируется еще раз обсудить современные архитектуры RNN и проблемы их обучения.
&gt;
&gt;Место проведения: ИППИ, 6 этаж, 615 аудитория.",
"Парни, а как со входом на семинар иппи, заранее надо регится? паспорт захватить или ещё что? Спасибо.",
"о, спасибо, крутяк. А FFT тогда каким боком можно применить? Не до конца въехал",
"Черт, как все сложно :simple_smile:",
"это точно
есть некоторым образом релевантная лекция на полтора часа про цвета <https://www.youtube.com/watch?v=SUCVj3qBmNQ>
рассказывают, что может произойти с цветом изображения/сцены
в частности, объясняется, как именно не работает выделение объектов на изображениях сегментацией по цвету",
"<@U0BLCTAJK>: спасибо, попробую.
<@U041LH06L>: это не пакетик пробегал, это серия статей про то, как использовать пакет RDCOMClient для создания слайдов в PowerPoint через R. Вот тут все основные ссылки: <http://blog.revolutionanalytics.com/2015/10/programmatically-create-interactive-powerpoint-slides-with-r.html>",
"Keras кто-нибудь использует?
Как поглядеть размерности аргументов приходящих в loss?",
<@U041LH06L>: вы не знаете где можно посмотреть результаты хакатона?,
"<@U0KPCJWAC>: <@U0KPB45TK> привет, где можно посмотреть результаты хакатона? Этот лидерборд кажется поломан  <http://alchemist.dotascience.com/leaderboard/>",
вопрос <@U06MTEXQQ> <@U06J1LG1M> - как раздобыть видео с DS митапов в сети?,
а с датафеста когда будет видео?,
"звучит как ""как пропатчить kde под freebsd""",
h2o в standalone режиме как библиотека будет работать?,
"хотя у нас в проде вроде как это все в докере, наверное сборку xbg можно будет на крайний случай прикрутить",
"куль, а как проще всего найти все видео с data science meetup'ов?",
"Стас Семенов в первый день недавнего феста рассказывал о решении одной из задач Kaggle. Он упомянул, что использовал ансамбли из 5XGB и 2ET. Как сделать разные деревья - понятно: по критерию ветвления. А вот как сделать 5 разных гбустов? Ваши советы.",
"Где были априорные вероятности и семи-супервайзд - менялись и гиперпараметры, ставил больше регуляризации для деревьев",
Всем привет. Кто завтра на завтрак заглянет?,
а почему это “semi supervised объекты” называется?,
ну или ты посоветуешь где достаточно тихо и можно будет до вечера зависнуть,
А ни у кого полностью поскарпленного пабмеда нет?,
"Господа, а кто читал The Elements of 
Statistical Learning (Trevor Hastie, Robert Tibshirani, Jerome Friedman) и Pattern Recognition and Machine Learning (Bishop)? Какая из книг на ваш взгляд предпочтительнее для самостоятельного изучения (при наличии определенного бекграунда в статистике и лин. алгебре)? Какие у них плюсы и минусы?",
"<@U0QB98BB9>: Спасибо! Это будет не первая книжка по ML, до этого проходил курсы Andrew Ng и вот этот (который, на мой взгляд, намного круче. Жаль, что про него мало кто знает) - <https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about>. Плюсом, читал Introduction To Statistical Learning от тех же авторов, что писали The Elements of 
Statistical Learning",
"np. Тут учитывайте, что Бишоп больше в байесовскую статистику клонит. Может непривычным показаться сначала, что на любой параметр хочет навесить распределения. Но потом привыкаешь и начинаешь понимать, как это работает )",
А кто из них двоих готовит обновлённую версию своей книги: Мёрфи или Бишоп?,
"&gt;  учитывайте, что Бишоп больше в байесовскую статистику клонит
вроде как раз Мерфи - это баисианщина, по крайней мере мне так рассказывали",
это как бы вопрос - разве я что-то путаю?,
"<@U041LH06L>: угу, угу. старина филд. я вот ее как раз читаю-вспоминаю.
*чорт дернул согласиться провести мастер-класс для психологов...",
"если я правильно понял, он работает чуть быстрее теано, но при этом там нельзя писать свои хитрые слои и лоссы удобно как на теано, только руками всё набивать, как тут - <http://mxnet.readthedocs.org/en/latest/python/model.html>",
"Еще duda &amp; hart хороший, нам по нему в универе часть лекций по МЛ читали. А Бишоп мне нравится использовать как reference, читать его от корки до корки - занятие не для слабонервных ",
пора запилить опросик кто чем пользуется,
какой мультимодальный опрос получается с помощью этих штучек,
А почему Theano/Lasagne и Keras не отдельно?,
"мерфи дает отличную базу - из чего что как выводится. бишоп, на мой вкус, похоже, но посложнее. зато у мерфи, хотя и кратко, почти все современное упомянуто",
"да не, в шаде ценность вся в домашках - а их как раз примерно на выходные, даже меньше, так что совмещать вполне реально",
Я бы тоже не рекомендовала совмещать. Легко и за выходные -- это на зачёт и без глубокого погружения имхо. А зачем тогда вообще топать в ШАД с таким подходом.,
"вот прям интересно, где никак нельзя обойтись без p-values...",
"коллеги, а никто не имел опыта замены в DCGAN генератора на автоенкодер? или есть менее извратный способ как с помощью всех этих генеративных штук получить скрытое представление входных данных?",
"Проблема на самом деле гораздо глубже. Ученые живут на гранты, которые по сути выдаются на исследования, которые дают какой-то определенный результат, типа, накормили таблетками - они начали быстрее бегать, подвесили гирю к ногам - стали медленнее бегать, ага - таблетки лучше гирь.

Целые (псевдо) научные индустрии на этом основаны, в частности, фарма, мед и спортивная физиология (вкупе с оборудованием).
Если они (ученые) вместо четко определенных ответов начнут давать неопределенные, типа, ""ну мы протестировали и что-то ни фига непонятно дают новые педали прирост скорости велосипедистам или не дают"", то их быстро отлучат от всех грантов. Потому что ответ ""не дают"" опять же для всех лучше чем ""непонятно"". Поскольку завтра можно выпустить усовершенствованные педали. И по новой.

В результате нескольких таких циклов все друг друга быстро понимают и результаты исследования вообще всегда становятся строго положительными, например, ""обычным людям толку от педалей никакого, а вот элитным даунхильщикам на ранних этапах участков с резкими ускорениями есть незначительное, но статистически значимое улучшение..."" 
И тут вообще все в шоколаде. Потому что никто из обычных людей себя к обычным не причисляет, а в худшем случае - к почти ""элитным"", даже если они даунхилом и не занимаются вообще. В итоге, у покупателя - новые педальки, у производителя - продажи, у ученого - гранты.

Я однажды в толпе таких ученых неожиданно спросил, а почему вы в качестве доказательства используете ANOVA, если ни одно из ее условий не выполняется? Я вам скажу, что по уровню агрессии даже окраины Челябинска не сравнятся с толпой этих отморозков, которые меня как только не обозвали, так и не ответив ничего по существу.",
"Ни найдешь ни одного спорт.исследования, в котором тестировалось бы менее 6 параметров - и ведь это только то, о чем они рапортуют, а по факту может тестировали вообще 20. Так что всегда есть гарантия, что что-то да выстрелит. Просто по вероятности хоть один параметр, а то и два-три сразу покажут статистическую (псевдо) значимость (там где ее и подавно нет).
И ровно по этому почти не бывает полностью идентичных повторных исследований. Всегда что-то да меняют, то уровень способностей тестируемых (обычные люди, немного тренированные, много, очень много, элитные спортсмены), то уровень нагрузки, причем измеряют специально по разному (80% от ПАНО, 100% МПК и т.п.), то длительность нагрузки, то еще что-нибудь. 
Рано или поздно какое-нибудь p-value все равно выстреливает - и можно закрывать исследование, чтобы вскоре открыть новое.",
"по второй части: тут уж не важно, p-values или не p-values используются, если на одних данных тестировать много маловероятных гипотез, что-нибудь да выстрелит, как ни меряй",
"ну, когда-то придумали поправки на множественные сравнения... мутная тема, но что есть.

а, вообще, проблема даже не в грантах. а в том, что анализ данных у рисечеров - важный этап, но не единственный и даже не основной. регулярна ситуация, когда к анализу приступают раз в полгода. соответственно, народ берет то, что помнит, то, что пишут в похожих статьях и так далее.",
"а по поводу ановы - почти в каждом учебнике пишется, что она робастна к отклонениям от нормальности. и все это отвечают на автопилоте. но я вот ни разу еще не видел исследования робастности, откуда, как и почему. %(",
"Так что да, стоит сделать как в статье",
а в VAE обратный инференс как я понимаю должен делаться просто? (я пока еще хз как но наверное должен),
"в VAE у тебя же уже сразу после обучения есть кусок сетки, который отображает из представления в исходное пространство, как с обычным AE",
"<@U04423D74>: ""а по поводу ановы - почти в каждом учебнике пишется, что она робастна к отклонениям от нормальности"" - вот я как раз не нашел такого в учебнике, который у меня под рукой. Приводится пример непараметрического теста, а анову сравнивают с несколькими t-тестами, и написано что каждая выборка, предполагается, имеет нормальное распределение с одинаковой дисперсией",
"Так у тебя в VAE, как и в обычном AE пол сетки эмбедит, пол сетки разворачивает обратно",
"&gt;&gt;&gt; Информация о лекции №3 ""Расширения для PostgreSQL""

   - Занятие будет проходить на площадке компании Яндекс по адресу Москва, ул. Льва Толстого, 16 (м. Парк культуры).
     Подробную схему проезда можете посмотреть здесь: <https://yandex.ru/company/contacts/moscow/>
     Зал ""Экстрополис""

   - Третья лекция пройдёт 10го марта (четверг) в 19.00.
     Если вы захотите принести ноутбук, в зале есть WiFi и розетки.
     Сегодня это уже стоит сделать, так как будет много примеров кода.

   - Для первых лекций онлайн-трансляция не планируется.
     Мы обязательно будем выкладывать слайды и обработанные видеозаписи лекций в течение 3х дней.

   - Площадкой для материалов и обсуждений будет сайт <http://postgres-edu.blogspot.ru/>.
     Там вы сможете уточнить расписание, обсудить лекции, задать дополнительные вопросы и найти вопросы и задания для самопроверки.",
"Друзья, привет,
не знаю, правильный ли чат выбрала - если что - поправьте меня.

Коллеги из Роспатента хотят сделать запрос в Data Science сообщество на решение своих задач с помощью ML и DL. В частности, им интересно:
- разработка системы патентного поиска для пользователей,
- разработка надстроек над системой патентного поиска, которые могут решать отдельные задачи.

Для этого 18 марта в 15.00 мы организуем встречу с ними и теми из Вас, кто хотел бы услышать и обсудить круг задач Роспатента и, если задачи интересны и Вы можете предложить решения, будем рады содействовать получению реального заказа.

Мы планируем провести серию таких встреч с разными корпорациями, чтобы разработчики из первых уст услышали постановки задач и могли познакомиться и дальше плотнее взаимодействовать c индустрией.

Регистрация на мероприятие здесь: <https://leader-id.ru/event/1986/> Приоритет реальным командам разработчиков.
До встречи!",
"У кого какое мнение насчёт <http://deeplearningbook.org|deeplearningbook.org>?
Стоит ли читать, если это моя первая литература по глубинному обучению?",
"Или глубокому, не уверен, как правильно :grin:",
"А насколько основы?
Многослойную сеть прямого распространения я уже знаю, как делать",
"кароч чот мы моск имеем, все списки же уже давно готовы",
когда человек всю конференцию смотрит тебе в душу немигающим взглядом,
"интересно, как они здесь это пилят",
"GPU Вопрос, у кого то есть опыт работы с Nsight Eclipse Edition? Это Nvidia’овская IDE для CUDA разработки ?",
"ну да, там и третья часть почти готова была (я в январе как раз качал)",
"<@U0JKYTE4B> Иван, нас интересуют в принципе все, кто может предложить современное решение (т.е. если вы компания, которая именно этим и занимается - welcome). 
Про дальнейшие встречи с корпорациями - есть идея используя наш достаточно легкий доступ к большим российским корпорациям привить им решения на базе нейросетей. Что-то вроде акселерации развития индустрии.",
"<@U0JKYTE4B> да, корпорации именно как потребители продукта, представители Data Science сообщества - как поставщики продукта или компетенций.",
Кто знает где аналитика по использованию смартфонов в России есть? операционные системы /модели и.т.д,
"кто сегодня по пиву собирался? я с товарищем буду в 18:30 в ""бутылке, кружке и котле"" на покровке (мск), если сможем туда войти в такое насыщенное время, а дальше броуновским движением в сторону центра.",
"коллеги,  я уверен среди вас много графоводов, подскажите как мне кластеризовать граф (не взвешенный, неориентированный, ребер сильно мало, но он связный) в котором 1М вершин и всякие MCL не очень, тк хранение в памяти плотной матрица 1Мх1М как бы не очень хорошо",
"я статью не читал но рискну предположить что если это в2в и случаые блуждания, то на каждый ран можно получить последовательность вершин которые были посещены, затем интерпретировать последовательность вершин как предложение текста, ну и как бы оп оп",
"у кого ios, потыкайте и расскажите впечатления",
Как работает AlphaGo - <https://habrahabr.ru/post/279071/>,
в догонку (вдруг кто еще не видел) - есть неплохой курс от Google по deep learning (на базе tensorflow) - <https://www.udacity.com/course/deep-learning--ud730>,
"<!here|@here>: <https://sites.google.com/site/deeplearningsummerschool2016/home> выглядит, как очень крутой воркшоп по DL в монреале",
А где хорошо можно почитать про многоуровневые модели? Которые все подряд используют для топовых мест в Kaggle.,
"Добрый вечер.  Кто-нибудь знает, где можно добыть большой корпус политических или религиозных споров (кроме пабликов ВК)?",
"народ, такой вопрос: есть словарь из большого количества уникальных слов (~5 млн. слов), на вход приходит какое-то количество ""новых"" слов, необходимо понять (как можно быстрее), если ли эти слова в словаре. Как бы вы решали подобную задачу?",
"Тут ребята тоже обсуждали, как искать строки: <http://programmers.stackexchange.com/questions/118759/how-to-quickly-search-through-a-very-large-list-of-strings-records-on-a-databa>",
Есть у кого TIMIT датасет?,
"Друзья, а кто подскажет хорошие ресурсы/книги для того, чтобы подтянуть мат. оптимизацию для достойного ML уровня (знаком с ней на уровне статей по SGD, Adam и L-BGFS в блогах и стенфордских лекциях по deep learning)?",
"В ШАДе есть курс по оптам для машинки -- очень годный. Ведет его, если я не ошибаюсь, как раз Кропотов",
"<@U0QTS1LRF>: не, сам не читал. По оглавлению похоже на книжку для тех, кто с оптимизацией уже более-менее знаком, а теперь хочет на конкретные применения к МЛ посмотреть",
"28 апреля на ФКН начнутся лекции Юрия Нестерова. Просьба зарегистрироваться всем, кто планирует их посещать.
&gt; Ссылка на форму: <https://docs.google.com/forms/d/174qe9PoP23RV_W8troSUHoHXry1ICg0X_DIhxuYw2xg/viewform>",
"28 апреля на ФКН начнутся лекции Юрия Нестерова. Просьба зарегистрироваться всем, кто планирует их посещать.
&gt; Ссылка на форму: <https://docs.google.com/forms/d/174qe9PoP23RV_W8troSUHoHXry1ICg0X_DIhxuYw2xg/viewform>",
<@U0QLWLTK4>: а что за решение собственное?  можешь рассказать как оно построено архитектурно?,
"Архитектура подробно описана вот тут <http://voiceassistant.mobi/docs/ru/api-developer-guide/architecture.html>, но я так поняла вопрос, что интересует именно разбор запроса - там использовалась система паттернов, вот тут есть примеры <http://voiceassistant.mobi/docs/ru/api-developer-guide/patternsyntax.html>. При этом мы для ограничения темы использование понятия контекста и сервисов - так получалось разобрать неполные вопросы:
- Какая погода сегодня?
- такая-то
- *А завтра?*
Но понятно, что если приложение  изначально не сматчило контекст, то дальше вступал в дело обычный чат-бот, разговор становился глуповатым :simple_smile:",
"вот у них в опенсорсе как раз реализация мультиязычности. когда заглядывал в гитхаб несколько месяцев назад, много всего было, но русского нет",
"какой бы хороший не был внешний сервис, мы не можем туда ПД передавать",
"дано: матрица смежности направленного взвешенного графа. какую предложите наипростейшую библиотеку для визуализации, чтобы в узлы положить текстовые метки, а на ребра навесить веса? не хочется на js пока что-либо писать. питон - ok",
А где у нас смайлик :говно мамонта:?,
"блин, не найду в примерах или доках всех перечисленных либ, как повесить метки на ребра... только потолще-потоньше есть",
"блин, какие же они запутанные",
"Вопрос; Каким самым незатратным образом найти множество фотографий разных людей в разных позах в разной одежде на разных фонах? множество -  это от сотен тысяч до миллионов. Можно скрэпить, но что и каким образом. Можно использовать pedestrian detection для бутстрапа, но где проверять исходное множество фотографий, watermarks не смертельно, но желательно без них.",
"Было бы прикольно, если бы были сайты, где люди выкладывали бы свои фотки в разных одеждах и на разных фонах...",
"Желательно что-бы можно было разделить, те где один человек в кадре и более чем один. И чем больше разрешение тем лучше",
"Даже отзывы были 
""дота в партнерах это интересно :simple_smile:
да там какой то колхозный школосайт
А кто организовывает этот BBC?
У них странный сайт. На странице рекомендованных статей показывают работы по диабетической ретинопатии.""",
"Прям настоящий, как в machine translation?",
Какая там обычно архитектура сети?,
"еще я видел на youtube какого-то разработчика вроде бы из Нидерландов, где он гулял с лаптопом, а на лаптопе semantic image description работал в реальном времени",
Кто то завтра на завтрак зайти собирается?,
"я жду, когда уже прикрутят к ифону какой-нибудь специализированный чип для дип лернинга :simple_smile:",
"а второй когда я последний раз смотрел был поверх accelerate framework, что совсем не то же, что Metal.  Может сейчас по-другому, надо будет глянуть",
"<@U0QB98BB9>: спасибо. Там есть SELF-DRIVING CARS AND AUTOMOTIVE TRACK , как раз недавно в job пролетало обсуждение беспилотоного автобуса в Москве. Вот кому туда надо съездить :simple_smile:",
А есть тут кто из наносемантики?,
"господа, приходите завтра на завтрак в наш старый добрый Beverly Hills Diner, кто в Москве",
А как на Гараж регнуться?),
"Соундараджан рассказал, что его натолкнула на мысль о проверках «случайности» в мире простых чисел лекция японского математика Тадаши Токиеда [Tadashi Tokieda]. В ней тот приводил пример из теории вероятностей. Если Алиса будет кидать монетки до тех пор, пока не получит решку, следующую за орлом, а Боб – до тех пор, пока не получит две решки подряд, то Алисе в среднем потребуется четыре броска монеты, в то время как Бобу – шесть. При этом вероятность выпадения орлов и решек одинакова.",
"<https://habrahabr.ru/post/279337/>

Как такое может быть?",
"А как называется такой метод ML, чтобы погуглить-почитать про него? Метод: смотреть на кросс-валидации, где модели делают ошибки, а потом обучить другую модель на этих данных. Это я Santander Customer Satisfaction решаю, думаю, что придумать можно.",
"Просто идея была посмотреть, где модель ошибается, там и обучить другую",
"нид хелп от коллеги (его нет в слаке, поэтому просто его письмо):

&gt;&gt;Коллеги, если кто имел опыт общения с pymc или stan, не могли бы вы взглянуть на вопросы, которые я задавал тут <https://groups.google.com/forum/#!category-topic/stan-users/general/fQ-3K3b9_no> и тут <http://stackoverflow.com/questions/36045851/pymc3-regression-with-change-point>? Застрял с моделями.",
"приветствуются желающие выступить. тематика - data science и machine learning.  в этом году, так как мы только начинаем, то докладчикам дорогу с проживанием оплатим.",
"коллеги, если есть у кого интерес опробовать карту NVIDIA Tesla M40 для обучения нейросети - приглашаю на тест-драйв. Есть сервер оснащенный M40 в удаленном доступе специально для этих целей. Регистрируйтесь и вам будет создан аккаунт <http://www.nvidia.ru/object/m40-gpu-test-drive-ru.html>",
<@U04423D74> а почему это Андрея тут до сих пор нет? Упущение. ,
а где у вас офис?),
"о, а когда ваш семинар?",
"<@U0GMCSHR9>: а был ли уже анонс, когда стоит ожидать появления карт на Pascal на рынке?",
а так же хочу спросить - как моделировать выбросы в распределении? :simple_smile:,
"добавлять выбросы в обычные переменные в общем случае не знаю как. в изображениях понятно как это делать, в аудио - тоже, а в абстрактной матрице с фичами может произойти черте что
есть модели errors in variables, но они работают на зашитых предположениях",
"у меня процесс, который описывается четырехпараметрическим распределением
и выбросы - это длинные хвосты
единственные варианты, кторые приходят мне в голову - шатать либо параметры (домешивать то же распределение с умноженными на коэффициент апраметрами), либо рапсределения (домешивать равномерное или еще какие распределения)",
"если есть модель процесса, то почему бы не пошатать параметры действительно?)",
"да все равно лажа
непонятно, как сильно шатать, какие коэффициенты брать
ясен пень, что коэффициенты будут из равномерного распределения
и что это пойдет на тысяч 10-100 так симуляций
но все равно, может я что-то упускаю %(",
"а почему я должен его брать? :simple_smile:
скошенное t",
"какая экзотика. ну, как бы неотрицательность и overdispersion",
"у нас дурацкая ситуация сложилась - народ, когда обрабатывает такие данные, творит что хочет. кто-то логарифмирует, кто-то делает инверсию типа 1/х, кто-то еще каким-то образом развлекается. аналогично с выбросами - кто-то режет по порогам, типа ""все, что выше 2500 - режем"", кто-то по SD, кто-то по процентилям. я хочу посмотреть, как сравнить эти методы, насколкьо эффективны нормализации, насколько точно работают те или иные методы чистки выбросов.
соответственно, simulation studies во все поля",
"кстати, не очень верю, что доеду, но как там вообще регаться?)) не нашел кнопок.",
"<@U040M0W0S> спасибо)
<http://info.deephack.me/?p=391>
Вот тут, как я понимаю, у тебя нет пропуска мфти - есть риск что не успеем вписать в служебку, пропустить сегодня
Но можно рискнуть :) и позвонить <@U0H7VBQQ1>  например как пожойдешь, чтобы он по своему пропуску пустил",
Существует ли у NVIDIA технология виртуализации видоекарт и где можно про это почитать?,
"<@U0873FY94> очень хочу на семинар, но по времени совпадает с парами. Я понимаю, что это семинар и вся соль в том, чтобы присутствовать,обсуждать и задавать вопросы. И всё же, может быть, есть какая-то возможность записать семинар или присутствовать виртуально по скайпу? Если нет, то как минимум сегодня я пропущу пары и приеду. Регалась сегодня ночью, пропуска мфти у меня нет, мне тоже надо будет звонить? Я просто параноик, боюсь, что разрядится телефон/пропадет связь/связи в долгопрудном нет :simple_smile:",
"Т.е. сидеть на парах со скайпом? :thinking_face:
Да, надо будет позвонить)
Технически можно обойти здание и постучать в нужное окно. Но для того чтобы знать в какое — придется один раз посетить нормально.",
А почему бы и нет? :simple_smile:,
"Не знаю на каком ты курсе, но явно не первом-втором, пора бы уже соответствовать студенту из анекдотов
&gt; Так студенты смотрят расписание:
&gt;1 курс. Ух сколько предметов! Буду много знать
&gt;2 курс. Ух сколько предметов! Буду много уставать
&gt;3 курс. Сколько предметов… Какой бы прогулять
&gt;4 курс. Сколько предметов… на какой бы пойти…
&gt;5 курс. Бляяя, а Каникулы уже кончились?",
Может кто видел dataset по футбольной статистке?,
"Господа, а кто тут разбирался с генеративными моделями, вроде <http://arxiv.org/abs/1312.6114> , <http://arxiv.org/abs/1401.4082> ?",
как раз про это будет митап в вс,
"А то я читаю статьи, и создаётся ощущение, что они все строятся на одном математическом ""фреймворке"", который я несовсем понимаю, хотел спросить где можно почитать про него?",
а вот плотли графы вроде как не умеет,
Кто то сегодня на Ветровский семинар идет ?,
"<@U0QTS1LRF>: на питоне, куда он от зависимостей то уйдёт",
"Сразу оговорюсь, что не как в тот раз)",
У кого есть предложение по месту?,
<!here> Кто хочет - в 20:00 на Китай-Городе встречаемся,
"<@U0989QUVC> мы отпишемся, где в итоге осели) Подходи",
"тут недавно обсуждали нейросети для iOS. скажите правильно ли я понял что DeepBeliefSDK (<https://github.com/jetpacapp/DeepBeliefSDK>) поддерживает только alexnet? документации по их формату *.ntwk нет, так что и портер даже не понятно как писать",
<!here|@here> кто был на Роспатенте?,
"<@U0JJ8S5GQ>: видимо ты пытаешься обратиться как к тензору к тому, что должно быть числом",
"""Лайтово"" they said. ""Не как в тот раз"" they said...",
"теперь присказка будет ""не как в этот раз""?",
А кто может подсказать как именно выходит (4.48) из (4.47) в elements of statistical learning на стр. 132? Что то не доходит из-за чего M в 1/2beta^2 под max переходит,
"Кто-нибудь, подскажите как чистить GPU память в питоне?  А то кернел падает ",
как раз хотел что-нибудь лайтовое про deepq почитать,
<@U0TUTASNR>: А как python объекты попали в GPU память?,
"собственно, откуда? и почему там множитель 0.5?",
"и почему вообще Q-value, а не Sarsa или еще какой-нить реинфорсмент?",
понятно и зачем так делается,
"я к тому, что когда мы предполагаем квадратичный лосс, мы накладываем определенные ограничения на природу ошибки",
"а какой правильный лосс тут, думаешь?",
не знаю.. но почему бы не l1,
"<@U0U2ENJ4U>: а что ты соином делать хочешь? прост как по мне модель интересна только идеями заложенными в ней, но для практики не особо годится; точнее эти же задачи можно решить по другому и эффективнее",
"Какой примерный оверхед  при использовании  keras + theano? На сколько медленнее работает, чем просто  theano?",
"Вообще, SOINN я начал интересоваться с этого поста - <https://geektimes.ru/post/190280>, там чувак пытался приспособить его к алготрейдингу. Тогда как раз была очередная волна интереса к нейронкам в этой области.",
"ну и если кто то намекает что юзает, я почти уверен что юзают нормальные сети, а не такие вот штуки",
"<@U06J1LG1M>: ""нормальные"" - это какие в твоем понимании?",
"а принципиально ли алготрейдинг? занятие достаточно сомнительное в том плане, что выигрывает, как правило, только брокер. Если азарта хочется - добро пожаловать в <#C043ZEF6K> , там тратишь только свое время, а иногда и призы дают",
"<@U040HKJE7>: вообще забавно что если ты рассматриваешь свм как нейросеть, то получается что ты можешь в свою глубокую сеть встроить в любую ветвь flowing graph’a кусок свм",
ну как и конкурс авито сейчас - подари нам модели и накачай датасетов,
"..правда, за прошлые годы результаты интереснее, пик в был в 2011, когда деньги раздавали всем бесплатно :simple_smile:",
"это как раз тот самый, ""простой бот на Delphi""",
"ещё бы понимать на что тут смотреть, где колонка профита? :grinning:",
"чет какая-то мутная история с этими роботами с 100500% доходностями, кто нить валидировал вообще, что это не какой-то модный маркетинговый ммм?",
"<@U041SH27M>, дык, эти данные сама биржа предоставляет, кто еще более достоверно тебе их может валидировать?",
"Ну в данном случае бирже так-то выгодно расплодить чуваков, кто и будет комиссии бирже генерить (никаких предъяв, просто мысли вслух).",
"Ну я хз, какие там ""но"" могут быть в условиях и какие конкурентные преимущества используют такие боты.  А ля, бахнул самый ультрабыстрый доступ к бирже за 1кк бачей и сел разгонять бота с 50к рублей до 1кк рублей (абстрактно, опять же). Ну да ладно, это уже <#C040HKJF1>",
"<@U041SH27M>:  для этого как раз и существуют конкурсы типа ЛЧИ. но все доходности и просадки, показанные там - настоящие. другое дело, что эти конкурсы имеют ограниченный срок, что поощряет участников применять сверхрискованные стратегии, которые на долгосроке сливают.",
просто нафига чувакам кто умеет генерить такие проценты показывать их и пиариться в каких то конкурсах - непонятно,
"конкретно обсуждаемый участник торгует уже пять лет подряд, сливаться не собирается. Где ты там повышенный риск увидел - непонятно.",
"как раз наоборот, HFT демонстрирует самые минимальные риски из возможных",
"с опционами же, кто набрал декабрьских колов вне денег на бакс в 2014м, думаю, в накладе не остались, мягко говоря :wink:",
"или те, кто путов набрали на все? :simple_smile:",
"я думаю, все дело не в конкретном инструменте, а в том, как контролировать риски",
"путы тоже хороши, когда к месту :wink:",
"зачем восстановить правило, если оно уже записано в явном виде?",
"ты можешь уже дальше оптимизировать циферки, чтобы на симуляции было больше денег. зачем тут sklearn и ML - непонятно",
"а ты знаешь как поставить перед сетью задачу чтобы она восстановила именно это правило?) на пустом месте это может сойтись к задаче генерации стратегий, которая не так проста чтобы  разбрасываться ML и ожидать результата",
"а то я тоже могу спросить - ""а вы зачем MNIST все время распознаете?"" :simple_smile:",
"ну почему же - у меня задача трехклассовой классификации, по эталонному набору данных",
"в правиле этого не видно :simple_smile: к тому же, ты уверен, что то, что ты обучишь, будет хуже твоего правила? как у тебя устроена валидация твоего результата?",
"нет, речь идет про то на каких данных ты это обучаешь, на каких валидируешь?",
"<@U0QB98BB9>: я разумеется нисколько не эксперт в RL, но профессор из Беркли пояснял два момента, почему Q-learning у них хорошо работает",
"не нашел, где там layout задать, чтобы узлы были вот так упорядоченно расположены, как на у меня на картинке. т.е. хочется не звезду, не по кругу, а что-то более-менее ориентированное. самое близкое нашел вот это там: <https://graph-tool.skewed.de/static/doc/draw.html#graph_tool.draw.draw_hierarchy>, но доступные layouts:

If layout == ""radial"" radial_tree_layout() will be used. If layout == ""sfdp"", the hierarchy tree will be positioned using sfdp_layout(). If layout == ""bipartite"" a bipartite layout will be used. If instead a PropertyMap is provided, it must correspond to the position of the hierarchy tree.",
"господа юзающие vw, у меня тут обученная лда, хочу для новых данных проекцию посчитать, там есть опция —noop и -t, вы какую юзаете?",
для тех кто в Питере,
"говорят, там будет живая дема Isilon-а с хадупом, как можно такое пропустить",
"Я сам иду, просто стало интересно кто из ods тоже будет :)",
"<@U0JKYTE4B>: ну и заодно ссылку на конфу кинуть, мало ли кто ещё заинтересуется ",
"как видишь, никто особо не активизировался на твой вопрос :simple_smile:",
звучит как имя и фамилия: Макс Пулинг и его друг Филип Пулман :),
"Господа, знающие толк в функциях потерь: нужно максимизировать точность (accuracy) в мультиклассификации, какую функцию потерь использовать?
Сейчас пользую mloglos, но может есть более модные вещи :smiley:",
"Не могу использовать точность как функцию потерь, потому что ее невозможно подставить в xgb (чертов градиент)",
а как оптимизировать точность напрямую?,
"да в xgb понятно, что никак, там же градиент нужен, я просто не понимаю, как без него её оптимизировать кроме как полным перебором точек в пространстве параметров",
"Где-то пишут, что hinge даёт лучше accuracy как раз",
"<@U0G29N5U4>: каких гипотез? Как я понял, нужна обычная многоклассовая классификация",
"<@U06J1LG1M>: кросс-валиадация кросс-валидацией, а болтать-то нужно о чем-то. Так можно на любой вопрос отвечать, что возьми свой датасет да проверь :simple_smile: Этот ответ - правильный, конечно. Но тут же, надеюсь, большинство и так понимает, что все надо проверять, а вопросы - это чтоб какие-то идеи новые для себя найти, чтоб получше понять, как оно все работает.",
"А, кстати, вот вопрос - в чем подвох Platt scaling, ну когда логистическую регрессию поверх SVM тренируют? 1/0 как у SVM получается, а вероятности ""правильные"", бесплатный обед какой-то?",
"всем привет. нужен совет от хорошо знающих sklearn ну и вообще по общей концепции. в файле много данных - как текстовых, так и числовых. чтобы воспользоваться инструментами для прогноза, мне нужно выделить значимые данные, влияющие на этот прогноз. 1. в sklearn при беглом взгляде нашла инструменты для линейной регрессии, однако она вроде предсказывает, а мне нужно понять, значимая ли связь между данными или нет. что посмотреть в sklearn на эту тему? 2. как можно понять, связана ли текстовая информация с показателем? мне приходят в голову эксперименты с классификаторами типа Наивного Байеса и SVM, но мне кажется, для выявления связи это не совсем правильный подход.",
"<@U0SCWKRM1>: в этом собственно и работа аналитика данных :simple_smile:. В качестве первого подхода: 1 шаг -&gt; представить текстовые переменные в виде набора векторов, 2 шаг -&gt; в зависимости от того как распределены данные и какие есть представления о модели подлежащего процесса - корреляционный анализ, PCA, LDA и так далее",
"В качестве примера: какие векторы в данных можно рассматривать как статические, а какие - как time series.",
"<@U049HDR2Z>: вот я и думаю, как выяснить эту природу. я с большими данными раньше не работала, а на молекулярном программировании, например, мы просто брали два набора данных, смотрели линейную регрессию и коэффициент детерминации. если коэффициент детерминации близко к 1, то работали дальше с этим. В автоматической обработке текстов данные вообще скорее интуитивно подбираются.",
"да, я в курсе, конечно, много крутил. но как ты сам правильно сказал, это именно стат, а не ml. кастомного там ничего не сделаешь",
если ты хочешь заняться эконометрикой и объяснить хоть как то эффект регрессоров на целевую переменную то пока только стат тесты это дают,
"ну, я привел кучу примеров выше. и всегда можно дописать что-то. просто там больше из коробки для тех методов, какие есть",
а как корни православно перебирать?),
"на самом деле сперва стоит определиться с тем, насколько тебе дорого косячить на каких точках",
есть прям куча подходов как с такого рода радостями работать (всякие поправки на правдоподобие в тру стат моделях),
"Валентин? Полезность данного слака всем участникам прямо пропорциональна релевантности обсуждаемого контента. Пожалуйста, или объясните как ваши картинки и HTC Vive и Fibrum относятся к Data Science или перенесите контент в #random_flood",
"мы тут все будем рады ответить на вопросы, которые касаются математики, социологии,  сатаны, поклонению ему, биоинформатики, распределений, НЛП, big data, вот это вот всё. 

Даже можем организовать специальную встречу! Но как связана VR пока не очень понятно с этим вот всем. 

Спасибо. Да пребудут наши ряды чистыми.",
"<@U0AD1L5NC>: вот сейчас как раз пытаюсь этот датасет сформировать, если не поможет, то... плохо)",
"<@U0G29N5U4>: вот было бы неплохо. а то тут дали задание на таблицу, где 200 000 с хреном строк. полчаса выясняла, работают классификаторы или нет.",
"хм, прогнать с feature selection через кросс-валидацию?.. но я вот чего боюсь: когда будет больше данных, feature selection может быть другой..(",
"<@U0SCWKRM1>: Сделай поочередный вывод качества классификаторов + замер времени работы c помощью time.time() например. +сделай все на 100 объектах, а не на 40000, а потом увеличивай число объектов - чтобы прикинуть, сколько какой метод будет работать. Очень вряд ли у тебя зарулит kNN с 18 соседями - уж скорее с одним или несколькими, SVC почти наверное не будет лучше нормально настроенного леса, а работает наверняка не быстро (по умолчанию SVC использует радиальное ядро). Имеет смысл попробовать логрегрессию, леса и GBM (лучше из XGBoost), если уж говорить о выборе алгоритма.",
"<@U0UTMBJLW>: так всё равно потом тестировать на большой выборке. Когда трестируешь предварительно на 20% больше вероятности, что на всей выборке оно так же прекрасно сработает",
"<@U0SCWKRM1>: чтобы подобрать лучший, тебе надо выбрать из тех, которые ты можешь обучить. Очевидно один из классификаторов за разумное время на 20к объектах обучить не получается. У тебя два пути - узнать какой и ничего с этим не делать :simple_smile: Узнав какой, ты можешь подобрать, на какой части выборки все еще можно его обучить за разумное время",
<@U040HKJE7> а чем бутстрап из семпла тут лучше чем просто хгб на одном семпле? Как потом это использовать? Визуализировать распределение ранга каждой из фич? И как можно взаимодействия оттуда вытащить? ,
"<@U0DA4J82H>: всмысле из сэмпла? я имел в виду следующее: есть набор данных и хочется оценить важность фич xgb на них. но учитывая высокую долю случайности, хочется получить не одну единственную реализацию значимостей, а их распределение, полученное из случайных перетасовок данных. 

для этого нужно много раз строить модель и вытаскивать значимости фич, что можно сделать либо кросс-валидицией (добивая повторными запусками нужное число реализаций), либо бутстрапом (проще в реализации). после чего значимости по каждой фиче можно усреднить или взять медиану. а также можно нарисовать каждой из фич ящик с усами, чтобы увидеть насколько случайно выбираются фичи и насколько устойчива модель в целом.

вытаскивание значимых взаимодействий - тема для отдельного поста :simple_smile: есть как минимум 2 наивных способа: сравнение значимостей с xgboost глубины 1, а также раскладывание точек по конкретным листьям (и lasso-чистка)",
"А можно чуть подробнее про эти наивные способы определения взаимодействий? Или линк. Видел программку для этого, но увы там С#. Но все таки было бы интересно узнать как такое работает",
"<@U0G29N5U4>: упрощенное распределение с IQR и размахом: <http://docs.ggplot2.org/0.9.3.1/geom_boxplot-2.png>
по нему видно где основная масса точек, где медиана и до куда могут доходить выбросы. есть еще упоротый аналог - violin plot, если хочется подробностей распределения",
"Немножко еще про наивные подходы, у них, очевидно, есть свои минусы:
1. сравниваем полноценный xgb с xgb глубины 1 (tree vs stump). 
Если выяснилось, что значимости сильно отличаются по десятку и более фич, понять какие их взаимодействия кого с кем наиболее значимы сложно. Это дает подсказку о том, что что-то внутри есть, но что именно - придется искать самостоятельно (pairwise scatterplot, лол). 

2. lasso-чистка разложенного по листьям ансамбля (по мотивам <https://github.com/dmlc/xgboost/blob/master/R-package/demo/predict_leaf_indices.R> ). 
Это точно поможет упростить модель и повысить ее устойчивость, выделив наиболее значимые ""листья""(!). Но сложность опять же в выделении взаимодействий. С одной стороны, взаимодействия скорее всего будут важнее чем ""выше"" в дереве они находятся. По факту, они могут ""гулять"" по дереву (тратя первые сплиты на выбросы, или, например, меняя фичи во взаимодействии местами). А еще взаимодействия могут быть проще чем дерево полной глубины (например 3 фичи друг с другом, гдето в дереве глубины 10).

Если подитожить, волшебной пули пока что нет (сюрприз). Но если честно, она и не нужна. Эта наркомания нужна в основном в 2 случаях:
-в исследовании доменной области, где надо найти и выделить несколько наиболее важных взаимодействий и проинтерпертировать их (для медитаций и написания статей). 
-в упрощении и чистке моделей (а тут все средства хороши)
И в обоих случаях имеющиеся ""наивные"" подходы вполне годятся.",
"<@U0TCLGLJ2>: Если предложение еще актуально, то мы от нашей компании готовы сделать доклад, можем даже и не один. Мы как раз работаем в Самаре, и да в Самаре есть deep learning. Ох, уж эти московские снобы.",
"<@U0GLJUFD2>:  Предложение актуально, нужна тема, abstract и кто будет докладывать. и от какой организации",
кто-нибудь классифицировал интернет-юзеров по их истории хождения по сети? у меня есть пару миллион изеров с возрастом(6 групп) и нужно классифицировать остальных. как это лучше всего сделать?,
"использовать ссылки как BoW и классифицировать, как самый простой вариант",
"ну все равно круто, в этой сфере все очень плохо, была когда-то идея попробовать попродавать  технологии
хотя.. какая сфера, такие и технологии :grinning:",
"<@U0JHK9001>: тут такими вещами, как я понял, вообще не занимались. сейчас только первые шаги :walking::skin-tone-5:",
"А кто может разжевать простыми словами для чайника, без всяких уравнений с градиентами. Вот в регрессии бесконечно большие коэфициенты вроде как говорят о корелляции между параметрами. С другой стороны, чем больше коэффициент, тем большую значимость имеет параметр. У меня немного не укладываются в голове эти два факта, как это так происходит, откуда там деление на ноль.",
"если я делаю url через BoW, то у меня выйдет около миллиона столбцов, при том что юзеров у меня тоже больше нескольких миллионов. Я так понимаю, что мне нужно оставить только пару тысяч самых значимых сайтов. Как это делается? выделить по 1 тысяче сайтов на каждую возрастную группу, где ее часть существенно больше других?",
"<@U0JKYTE4B>: А в какой реализации получилось inf? 
Значение коэффициента не так чтобы связано со значимостью. Тут все может зависеть от шкалы признака.
Например, взять зависимость [стоимость авто (руб)] - [пробег (км)]. Там будет не то чтобы большой коэффициент.
Если пробег отнормировать по z-score, то коэффициент должен резко возрасти по абсолютному значению.",
<@U0JKYTE4B>: Тип рост и вес как предикторы?,
"а есть какая-то ссылочка, что бы сразу мне разреженную матрицу давало как в пайтоне?",
"<@U064DRUF4>: Ну не важно) Можно записать через обычную)
<@U0JKYTE4B>: Так а в каком пакете\языке inf получилось то? Может там костыль стоит если матрица обособленная получается",
"Я использовал, когда он еще был Google Refine. Если в исходных данных слабо формализуемый бардак, то руками править через Refine очень удобно. Особенно если исходный файл большой, то можно делить документ на части, и каждый человек разгребает свой вид бардака, при этом для каждого записыватся скрипт всех действий, которые потом применяются ко всем частям.
А если бардак более-менее предсказуемый, то лучше сразу прогу писать на любимом языке.",
"доброе утро всем. а я тут опять со своей задачей на приём. хочу совета от более опытных людей: мне посоветовали внимательнее быть с выбором предиктором, когда я отправила первую версию решения. но дело в том",
"не самый лучший, конечно, но какой работает +- быстро.",
"когда просматривала аналогичные задачи, видела, что люди вручную просматривали информацию, проверяли на графиках корреляцию и тп..  но мне кажется, это нереально со 119 столбцами. значит ли этот совет, что нужно подобрать более хороший feature selection?",
"<@U04422XJL>: нужно предсказать по данным, сколько людей отключится от услуг провайдера. отключение помечено 1, неотключение 0. нужно качество, я так понимаю, но ведь в любом случае  нужно отбросить те столбцы, которые более чем на 90% нулевые? и как можно отбирать аккуратнее - всё же ручками или feature selection адекватнее подобрать?",
"1) есть 90% нули -- это ещё не значит, что в оставшихся 10% нет полезной информации 
2) такого метода не знаю. Но ведь можно вытягивать кусок строк подряд и самой фильтровать как нужно",
"зачем тебе uploads туда, скачать же надо, а не залить",
"если хочется посчитать энтропию на выборке, то как лучше всего делать? Я воткнул вместо плостности ее оценку с помощью гистограммы, есть ли лучшее пути?",
"там wrapper есть, который ведёт себя как модель sklearn",
"у него интерфейс, как у моделей в sklearn, есть .fit, .predict, .predict_proba и все плюшки",
ставишь через brew clang openmp и в makefile указываешь его как C и C++ компилятор,
в общем лучше у кого получилось скиньте пдфку ),
"как альтернатива, наверное, можно попробовать поэкспериментировать с размерами интервалов дискретизации. по идее, должна быть какая-то сходимость. если данных мало, то можно зафитить какую-то mixture и нагенерить из нее уже сколько угодно",
"ну выложите пдфку, кто запустит таки! плииз",
надо воспринимать это как задачи для самостоятельного решения,
почему бустрап подойдет только для классического?,
<@U040HKJE7> а в каком канале можно попробовать собрать команду или присоединиться к существующей на какое нибудь мероприятие? Сейчас я о Ночи анализа данных в Вышке.,
"ребята, а можете подсказать ресурсы, где лежат примеры с анализом временных рядов в python? интересует случай, когда у меня множество значений помимо предсказываемого (когда в модели только предсказываемое значение и собственно сам временной ряд - такой пример у меня есть). я выделила наиболее коррелирующие через SelectKBest (func=f_regression), но что с ними делать дальше и как их вместе с предсказываемым значением скормить ARMA не очень понимаю...",
"Какой ряд, есть ли сезонность? ",
":disappointed: ну чисто на интуитивном уровне теория мне понятна, но здесь ещё вопрос, как правильно скормить данные функции. а stats model не размещает примеры использования функций(",
"я посмотрела, картинка похожая на <https://habrahabr.ru/post/207160/> при этом интегральный уровень =0, так как ряд сам по себе стационарный. так что я сделала вывод, что можно использовать ARMA. ну такой прислали мне.)) это тест для вакансии.",
"если это тест из вакансии, то данные могут быть генерированные, может быть, действительно имеет смысл порыться на тему поиска того, как они сгенерированы",
"Ребят, а кто знает, можно ли в spark mllib при использовании CV и Pipeline вытаскивать метрики с каждого фолда из evaluator'а? Что то никак не могу найти примеров как это делается.",
"Они там в целом берут не самый большой dataset + открытый набор 3d лиц  -&gt; натягивают лица как могут на разные 3d shapes -&gt;  крутят их получают кучу проекций -&gt; дополнительно игрались с выражениями - научились закрывать лицам рот.  Как результат утверждают, что получается достичь результатов распознавания, которые сравнимые с системами обучающимся на сотнях миллионах изображений и все за счет синтетически сгенерированных лиц. Вообще прикольная статья",
"ребят, а как в панде из всей таблицы выбрать два интересующих меня столбца? то ли из-за excel на входе, то ли из-за того, что столбцы у меня 1, 2, 3, а не поимённо вот так data[0][data[75]] возвращает фигню(",
а где в tsa класс ARIMA? я вижу в доке только <http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.arima_model.ARIMA.html#statsmodels.tsa.arima_model.ARIMA>,
<@U0SCWKRM1>:  тут пожелание к тебе гуглить перед тем как писать в чат,
"а знание о том, что он может как randomforest работать вообще избранным доступно",
он теперь ещё и как extratrees работать может с colsample_bylevel,
ну как в extratrees допсэмплинг на каждом сплите,
"о, надо попробовать будет прогнать как extratrees, тут даже больше опций чем в sklearn получается",
"я пытался медитировать на тему того, использовать colsample_bylevel или colsample_bytree. но не смог однозначно понять, как это повлияет на результат модели. есть какие-нибудь мысли? я фантазировал и теоретизировал, но однозначных выводов придумать не смог (надо экспериментировать)",
кто пользуется SMOTE: Synthetic Minority Over-sampling Technique и вариациями для балансировки датасетов?,
"такая штука: есть модель, которая делает классификацию на основе tf/idf. Так вот, я теперь хочу сделать прогноз для новых входящих данных, как мне правильно считать tf и idf в таком случае?",
"я их просто не буду включать :simple_smile: у меня просто xgboost, поэтому надо сохранить колонки, как я понимаю",
<@U0FF52P7D>: а какие еще есть варианты?,
"Пацаны, а кто как делает динамическую аугментацию?",
"Привет! Нужны ежедневные данные по погоде (температура max-min-avg, величина осадков) по РФ с 2010 года до текущего момента.  Можно по городам, можно по точкам измерения.
Может кто-то уже сталкивался и знает где достать? <http://meteo.ru/data|meteo.ru/data> не предлагать, там дырки в данных большие и сами данные неактуальные.",
Потому что для динамической вам еще как минимум влажность воздуха нужна,
<@U0JHK9001>: они вроде как прячут историю и с сайта ее не скачать,
"есть ли какие-нибудь туториалы по тюнингу ЕТ? Типа как по шагам какие параметры крутить, без grid search? Поиск в гугле не особо помог",
"хотя для ET это может быть не так актуально, как для RF, если он на каждом сплите семплит фичи из всего пула и деревья достаточно глубокие",
правда я не знаю как к torch прикручивать — какой там интерфейс?,
"<!here|@here> 
Есть проблема: может кто то знает специальный термин: Как делать рекомендательный движок для группы факторов.
Вот если у нас есть отдельные пользователи U = {u_i}, i=1..n с интересами X_{u_i}, а потом образуется какая то группа пользователей G \in 2^U, и нужно сделать рекомендацию для некоторой группы, ясное дело что можно искать “общее подмножество в пространстве интересов”, а что если интересы группы зависит от интересов отдельных лиц условно?",
Если группа идет бухать как найти место по нраву для все,
Здесь есть также гибкость в тех зонах где нет интересов у людей,
"чот меня переклинило, помогите расклинится. Есть временной ряд, в нем найдены выбросы. Хочется заменить выборосы поточнее, тогда это разумно сделать моделью, которая фитится временным рядом. Но модель получится хорошей, только если обучить ее когда уже выбросы удалены. Как разомкнуть круг?",
"можно взять бейслайн по тем местам, где нет выбросов",
"а ты как определяешь, что выбросы - это выбросы? На глаз?",
"кто выбежал из доверительного интервала тот выброс. доверительный интервал это проход окном и MDA,",
"в принципе, выбросы можно хоть по скользящему окну tukey outlier тестом выкинуть, типа медиану +- X*IQR  как хорошие значения, а за пределами  - выбросы, но X надо подобрать, и вверх и вниз могут быть разные X (для ассиметричных распределений)",
"звучит как традиционный ЕМ алгоритм :simple_smile: говоришь, что выбросы это пропуски, а дальше как описали в чате :simple_smile:

вообще стандартные методы заполнения ряда вроде SSA работают как раз итеративно. аналогично работают итеративные алгоритмы SVD заполнения матриц (и вот сюрприз, SSA именно его и делает)

с твоей стороны нужно только пакеты запускать",
"Коллеги, подскажите, а где можно взять корпус русских слов с сентиментом, как AFINN?",
"Ребят, такая проблема: есть обученный хгбуст на каких то данных и со временем приходят новые данные. И суть дилеммы: будет лучше обучить хгбуст заново на всех данных включая новые или взять старую модель и обучить только на новых типа online learning. Будет ли результат разный?",
"<@U061VJ5SB>: можешь пояснить  как ты будешь делать ""типа online learning""?",
"что-то странное со временем dRinkup, почему 21:00 начало? или опять путаница с timezones",
"со штатом может и не обязательно, но если даты в районе марта или октября, моя память дает сбой про то, какой сейчас действует закон",
"мало ли кто из Астрахани приехал, и часы не перевел, а тут все по-другому :simple_smile:",
"<@U041SH27M> <@U040HKJE7> Мы в свое время дообучали xgboost для конкретной задачи именно как предлагается, докидывая деревья, обученные на новых данных. У нас была проблема, что данные со временем начинали приходить из областей пространства, где их до этого не было,а из каких-то переставали. В итоге быстро все переобучалось под новые данные, а про старые забывало. Решением стало хранение небольшой части старых данных. При приходе новых данных  их объединяли с сохраненными старыми правильно перевзешивали - и это давало результат более менее ОК.",
"Коллеги, подскажите, пожалуйста, несложную статью с доказательством того, почему bootstrap работает. Во многих источниках переход от доказательства edgeworth expansion скомкан. 
Т.е. если то, что P(1/\sqrt(n) \sum\limits_{i=1}{n} (x_i) &lt; x) ассимтотически сходится к нормальному, понятно как получается, то что из этого следует?",
"btw, я тут сейчас понял, что я ебин и труъ RF это как раз с colsample_bylevel будет, а суть Extra Trees в другом - в выборе точек для сплита не жадным алгоритмом, а рандомно",
"коллеги, а кто может кто ссылку с примерами кинуть - какие переменные для анализа  изображений извлекаются алгоритами сейчас?  например, для распознавания лиц",
а вот какие фичи бывают? мне для общего развития пока.,
"А какого рода специализированных хочется? Есть куча эвристик под конкретную задачу, есть общие признаки.
Из них и собирается решение",
"В процессе обучения мы выбираем, какие именно прямоугольники и с каким весом должны быть",
"вот было бы так понятно написано в википедии, как тут мне объясняют :simple_smile: было бы проще жить",
"<@U0AD1L5NC>:  же говорил о том, как это работает, а не как это учить.
С точки зрения test-time -- да, это просто суммирование)",
"```
А еще наша мечта – создать настоящее российское Big Data сообщество.  Эра персонализированного маркетинга, где каждый клиент получает адресную рекламу, и каждый цент потрачен  не зря, уже началась. Там, на загнивающем Западе. Теперь и у нас на пороге. Не отставай. А лучше возглавь.

Добро пожаловать в клуб!
```",
"какое то описание там что типа вы тут все уебки, а мы тут приехали монстры такие и ща научим вас неандертальцев как нужно вообще жить",
какие вы критиканы все +),
"<@U040HKJE7>: непременно, я как каз иду, чтобы оценить его (Малкова) технологическое и научное превосходство над нами",
и как бы намекнуть что все ваши спикеры в этом сообществе уже состоят,
Интересно посмотреть кто придёт на встречу :simple_smile:,
"Объясните, плиз, когда категориальные переменные лучше менять просто на числа, а когда делать бинарное кодирование?",
"<@U0DA4J82H>: только когда категорий много, то деревья будут подразумевать что порядок имеет смысл",
"ну one hot как раз должен эту проблему лучше решать, чем разные порядки кодирования лейблами же, не?",
"а есть вообще смысл что-то делать с переменной, когда в нее входят много категорий? как вообще с ними принято работать?",
А какой там датасет по структуре?,
"ну категориальные фичи кодировать не тупым ohe, а разворачивать наборы категорий как конкатинированную строку и делать доп категорией",
"а если на каких-то категориях примерно одинаковое распределение целевой переменной, то можно их заменить на какую-то одну, они ведь не должны нести в себе никакой важной инфы? я имею ввиду, когда бинарная классификация и распределение примерно 50 на 50",
"<@U064DRUF4>: не знаю в чем преимущество, сплиты иногда интересные получаются и модель немного лучше работает, суть, наверное в том, что тут все значения доступны для дерева, если RF для примера брать
когда бинарные фичи и по ним семплинг маленький, то дерево не может видеть все значения, видит только часть, совсем другая модель получается",
"Такие, какие обычно в статистике делают",
"у меня работало вроде, но чаще вылетало с memory error, так что я переключился на theano. Кстати, вопрос - какой самый простой способ чистить GPU память?",
"А вот когда я скажем делаю interrupt во время выполнения, то чистить и форсить GC тоже помогает",
какой порядок arima модели получался?,
"хорошо, спасибо, попробую учесть этот совет. а моет быть еще какие-то предположения есть , почему прогноз нового месяца повторяет последний месяц обучающей выборки..:disappointed_relieved:",
чот кажется немного бесполезное занятие прогнозировать такой расколбас на малом числе точек,
"просто сейчас люди считают  в экселе по скользящему среднему,   и для агрегированных групп точек продаж суммарный прогноз у них намного сильнее отличается от суммарного факта на тестовом множестве, чем прогнозировать даже такую говнистую ариму как у меня)",
"и судя по всему, Волевое Решение здесь - это ждать , когда появятся данные за больший период?)",
а разве не эпсилон из какого распредения?,
зачем люди считают три дня а вы по месяцам?,
"просто поразительно , что для агрегированных значений по регионам - мой тупорылый прогноз дает меньшее отклонение от факта по региону (то есть схлопнутые результаты).

и это вдохновляет зачинщиков, кто это попросил делать.",
"там справа снизу окошко справки по decompose, его бы чуток вверх поскроллить и почитать на тему того, как он тренд определяет",
"вот что значит когда тян задает вопрос в ботаническом чатике, сразу начинается активная дискуссия",
"про компоненты я имела в виду, что не выходит их выделить.
а по поводу формулировок вопросов и реакции на них, простите уж, что не всем дано, что вам.
по крайней мере, мне казалось, лучше задавать вопросы профессионалам в своей области, нежели общаться с нубами моего уровня, с кем бы мы точно ни к чему вообще не пришли.",
"<@U047YP9SM> про компоненты я не понял, если честно. Не выходит выделить потому что данных более детальных нету? Если что,  попробуй как раз вот такие : продажи в рублях = цена*средние продажи в объемах на магазин* нумерическая дистрибуция - эти шиуки себя часто независимо ведут",
"<@U04422XJL>: Я пробовал, для одного классификатора на последовательности текста. Не работает. attention работает в плане текста, когда у тебя на выходе тоже последовательность, типа neural machine translation.",
"Посоветуйте, как лучше всего прикрутить регрессию к GoogLeNet",
Вот куда бы там лучше регрессию воткнуть?,
Попробую фтыкнуть как у них,
да. для этого слой должен быт описан на  lasagne. а у меня он на caffe. как бы его переписать с наименьшими усилиями)),
"<@U09BY2N3X>: а ты понимаешь, в какие даты это все происходит будет?",
"Он не так как caffe устроен, там такие слои не нужны",
<!channel>: у кого нибудь есть датасет с конкурса NIST SRE ?,
*channel* и *here* у нас только когда вопрос касается жизни и смерти. Особенно в рабочий день в рабочее время!,
вчерашний семинар в ИППИ - ссылку на видео куда скидывать?,
"<@U0XF4GAM8>: а зачем ликбез по фреймворкам? начни какой нибудь проект, пиши сюда вопросы и вперёд :simple_smile:",
"господа и дамы в Москве, на всякий случай напоминаю, если кто хочет - приходите завтра утром, начиная примерно с 10 часов утра, на Data Science завтрак в Beverly Hills Diner по адресу Сретенка ,1. Места там довольно мало, в худшем случае разобьемся по нескольким столам, но стабильно человек 7-8 приходит.",
"надо будет найти место, где есть большие круглые столы",
"<@U04422XJL>: Миш, пока подробностей нет. Концепт - дать совсем новичкам минимально необходимые знания (математика, алгоритмы, программирование) и навыки для решения хотя бы одной практической задачи, и вместе с ними эту одну задачу решить. Меня позвали туда тьютором. Сейчас идет утрясание формальностей, когда будет более менее ясно - могу рассказать подробнее. Потенциально, я могу кого-нибудь позвать в качестве тьютора, если такой интерес будет :simple_smile:",
"```В этом году в рамках образовательного проекта «Летняя школа» пройдет мастерская по анализу данных в психологии на базе R.
Это отличная возможность для студентов-психологов получить интенсивный старт для научной работы и заодно неплохо провести время. Курс включает в себя собственно занятия по R, а также работу с настоящими данными из психометрики и психофизиологии.
Даты: 24 июля — 8 августа
Преподаватели: Владимир Волохонский, Иван Иванчей, Илья Захаров, Георгий Васин, Георгий Милютин и Алмара Кулиева
Организаторы: Александр Фенин и Мика Пятницкая
Кто может участвовать: студенты-психологи 2го курса и старше
Подробная программа и подача заявки: <http://letnyayashkola.org/andan/>```",
"Чуваки, кто знает о ночи анализа данных в вышке в эту пятницу? Кому-нибудь прислали уже приглашения? Или ещё подождать надо и никому не присылали?",
"Я <https://www.reddit.com/r/MachineLearning/> просматриваю раз в пару дней, почти все интересное оттуда потом кочует в vk/dl.
 Немного грустно, когда ссылки повторяются, а контентом (в виде комментарий/опыта/мнений) не обрастают",
"Как пример, вот я рассказываю как слушал лекцию на Стенфордском эвенте - <https://closedcircles.com/chat?circle=14&amp;msg=6071170>",
как из совместного распределения нагенировать выборку векторов? если маргинальные не можется сделать из совместного :disappointed:,
"Зачем тебе маргинальные, если ты умеешь семплировать из совместного? Просто выкинь лишнее",
"Просто обычно у меня были какие-то стандартные маргинальные и семплировал первую координату по маргинальному, вторую как условную по первому и т.д.  Как выкинуть лишнее?  надо тогда найти какую-то плотность, которая всегда выше моей, как это сделать тоже не понятно :disappointed:",
"<@U0QB98BB9>: я пробовал на небольших играх(не Атари) + еще PAL и AL стр 6 <http://arxiv.org/pdf/1512.04860v1.pdf> . Получилось что эффект был тоже только от double q learning. А ты проверял на Атари играх или других каких то? Я хотел проверить на Атари еще, но пока времени не было.",
"У Мниха есть статья по этому поводу <http://arxiv.org/pdf/1602.01783.pdf> там они значительно улучшили результат за счет параллельных игр, плюс это  дало возможность использовать оn-policy методы обучения такие как актор критик",
"Я об этом еще в книжке Sutton'а читал когда-то. Может быть полезна, когда сэмплить игру слишком дорого (что обычно и происходит в реальных задачах).",
"как это было:
Вадим: я из туту.ру
Саша: не знаю такое, забей, не интересно",
лучше скажите где там <@U075SR8JX> :simple_smile:,
еще забавно как люди отличаются в живую и на аватарках которые себе выбирают :simple_smile:,
"&gt; еще забавно как люди отличаются в живую и на аватарках которые себе выбирают
<@U040M0W0S>, например, да? :simple_smile:",
Какой ты датасаентист без попугайчиков,
аватарка разноцветненькая -- прям как я ,
"&gt; еще забавно как люди отличаются в живую и на аватарках которые себе выбирают
чья бы борода об этом говорила",
"<@U040M0W0S> я как раз не справа, я слева",
"&gt; Заседания коллоквиума проходят, как правило, раз в две недели по вторникам в 18:10 в здании факультета компьютерных наук по адресу Кочновский проезд, дом 3, лекционный зал Декарт на 3 этаже (внимание: в 2014/5 году днем коллоквиума был четверг, но с сентября 2015 г. расписание и время начала коллоквиума изменились!).",
"помогите, пожалуйста, кто может. у меня задача оттока клиентов, 112 разных данных по каждому клиенту. среднее значение кросс-валидации для разных классификаторов показывает крайне высокие значения (0,96 и выше), а при проверке f-score выдаёт для тех же классификаторов крайне низкие значения - 0,2 и ниже. в чём может быть косяк? пробовала устранять коррелирующие между собой данные - стало ещё хуже. пробовала делать feature selection с помощью RFE и KBest - улучшений тоже не было. также использовала нормализацию данных с помощью standart scaler (sk learn). тоже самое. где ещё может быть проблема? спасибо заранее.",
"пропорции клиентов, которые ушли и остались какие в датасете?",
Есть ли идеи как сделать поменьше параметров?,
Но может есть идеи как можно попроще? Один вариант еще серия conv-maxpool (avg pool для detection поди будет плохо работать),
Это кстати да. Это как раз в стиле inception,
"forward pass тоже тяжелее в 100 раз, как ни странно!",
"<@U064DRUF4>: на тебя вся надежда, ни у кого больше не получалось это сделать ни до тебя, ни после.",
"ну как бы не обязательно, но желательно. и не только студентов.",
в прошлом году довольно шустро разобрали. не знаю как в этом.,
Как бы лучше чем раньше не получилось :simple_smile:,
":+1: в сиборне очень удобные штуки для рисования датафреймов, но как только нужно сделать шаг в любую сторону сразу хочется вешаться и приходится всё на обычно matplotlib рисовать",
<@U04CH4QBD>: Почему бы и не ggplot? Можно просто замапить третью координату на цвет.,
<@U0QTS1LRF>: а почему ко мне вопрос? :simple_smile: у меня практического опыта с нейросетями маловато,
А у тебя какой размер сети? (n =?),
как вариант еще в полазить внутри sklearn и там подправить чей-нибудь loss,
"мода на нейронки жжот, давайте их применять для задачи, где банальных счетчиков в real-time обратной связи достаточно",
"А какую библеотеку для этого взял, если не секрет?",
"<@U041P485A>: А где ты прото файлы нашел для resnet’а, можешь линком поделиться ?",
"&gt; Пока самая маленькая модель выдает 0.94 на трейн и 0.007 на валидации
звучит как какой-то дичайший overfitting",
"ну вроде как в тест режиме нужно --link=logistic, не? я прост давно не юзал, но вроде что то такое",
"пол часа уже бьюсь и не могу понять где ошибаюсь, сейчас попробую вариант с --link=logistic",
не вспомню как он называется,
"ну там никакой магии, как видишь, можешь в своём языке набросать",
"коллеги а может кто нибудь знает датасет для супервайзд лернинга в котором семплы - это мультимодальные данные, ну а метки что угодно?

мне это нужно для студенческого проекта, типа зачет такой; в том году я выкачивал паблик из вк с говноцитатками где семплы это были картинки+текст, и нужно было спрогнозировать количество лайков и репостов; но там датасет так себе получился, в нем данных не так много, и в принципе линейная модель на текстах давала хороший результат",
А кто всё ещё сейчас в Вышке сидит?,
"Кто может подсказать, как лучше генерировать последовательность? А именно когда для генерации следующего элемента надо подавать на вход предыдущий сгенерированный. Пока что я приноровился делать это в Lasagne.  В ней LSTM не хранит состояния между вызовами, но я вставил костыли, чтоб их получать. Таким образом для генерации каждого нового элемента последовательности приходится делать вызов функции. Всё работает, но довольно медленно. У кого-нибудь есть подобный опыт?",
а куда почтенная публика ее вставляет,
Поэтому надо это делать как раз перед ней,
"Пример сигмоиды как раз плохой, потому что нормализация приводит в линейный режим",
но там как раз параметры для этого обучаются,
"ребята из FB же недавно писали на тему этого холивара, как раз в контексте ResNet",
"<@U04F2H8FM>: Тут даже scan переопределять необязательно, там внутри уже есть всё, что нужно. Грубо говоря, я в конце функции get_output_for добавил строчку self.cell = cell_out. Ну и потом при создании функции с помощью theano.function просто оставил это в output. В принципе ещё можно взять GRU слой, тогда таких проблем вообще не возникает, но я не знаю, достаточно ли мне будет мощности GRU ячейки. А вообще где подобное сделать проще и эффективнее? Я кроме лазаньи ничего не пробовал в питоне.",
"в tensorflow попробуй посмотреть, LSTM ячейка там выдает скрытое состояние и память, ну в какой-то презентации Карпатого было написано, что вроде как tensorflow самый удобный для реккурентных сетей",
"Задачка на поиск. Есть обратный индекс, для каждого слова список документов. Нужно для каждой пары слов найти список документов, в которых они встречаются одновременно, исходя из гипотезы, что это будет разреженная матрица. Слов 100 000, простым перебором будет долго. Может знает кто какой-нибудь алгоритм для этого?",
"который 
&gt; Семинар ""Анализ данных в нейронауках"": Болезнь Гентингтона как модельное заболевание в нейронауках",
"По идее hash join сам выкинет быстро вырожденные слова, но как первая оптимизация тож покатит.",
"100к не надо, как писал Антон - будет толстый хвост который не жалко выкинуть",
"в питоне очень быстрое пересечение сетов, попробуй. и вроде как задача неплохо параллелится, если нарезать крупными кусками",
"проблема не в деньгах - а что это время в трубу)
если ты хочешь отвлечься от работы - норм вариант.
выступить - тоже норм вариант, попиариться
но вот как слушатель - пользы там 0 (ИМХО)",
"&gt; выступить - тоже норм вариант, попиариться
я как раз буду выступать, но это неблагодарное дело. Там люди из бизнеса, поэтому ничего интересного не рассказать. Я собственно и спросил, вдруг там кто-то из наших будет",
"просто надо на картинку какую нить тетку поставить и на ее примере показывать, где там логарифм",
а сейчас я покажу как логарифм правдоподобия катится в...,
"а можно узнать, почему нельзя людям про бизнес рассказать про бизнес? :simple_smile:",
Пацаны -- а кто где берет базу ImageNet? Не с оффсайта же качаете со сокростью 40кбс?,
ага от туда за несколько дней выкачивается -) могу залить куда нибудь архив ILSVRC2015_CLS-LOC.tar.gz 155гб,
"пока я решил, что это означает, как переменные по кластерам распредлились, income какой-нибудь в трех кластерах должен был быть  low, medium и high, а получилась какая-то фигня",
"коллеги, подскажите, как сейчас обычно конструируется рандомизатор? мы же ведь всегда имеем дело с псевдослучайными процессами? я вроде как видел, что оно привязываеися к сисиемному времени или кликам мыши и проч.",
"не. функцию я могу найти. а вот что в ее начинке, как создается эта случайность?",
"спасибо
а как оно работает? то есть, берется системное время, и берутся тики от какого-то события, или еще какие операции поверх наворачиваются?

хм. железеые - это интересно, но я сейчас именно про яп, как там функции рандомизаторов созданы",
"Можно время как количество миллисекунд с 1970 года, можно наверное как количество микросекунд со старта системы",
"А все яп, как я понимаю, инициализируют seed либо из системного генератора либо тем что указал пользователь",
"Кто сегодня идет на ""Physics Informed Machine Learning"" ?",
"<@U0JKYTE4B>: я думаю, твоя гипотеза про суть вопроса верна, т.е. может быть проблема в том, что какая-то переменная размазалась по кластерам равномерно. Выскажу предположение, что это может быть применимо к k-means, потому что там по умолчанию кластера образуются пересечениями гипер-сфер. Поэтому если у какой-то переменной диапазон значений уже, чем у другой, кластера образуются вдоль оси второй переменной, а по первой получим как раз равномерное участие. Соответственно, надо либо менять меру, либо сначала нормировать значения всех переменных к единой дисперсии.",
"Я буду уже в 9.30, но люди подтягиваются как им удобно.",
"<@U0TUTASNR>  я прогнозирую не выручку , а объемы продаж в тоннах и  информации по размерам пунктов продаж и их месторасположению и всему прочему нет. все, что у меня есть, это исторические данные по помесячным объемам продаж за два года (ну и фактор погоды можно запариться учесть , но я не стала. и так возни с рядами много, так как некоторые имеют всего 6 месяцев инфы, куча дырок , выбросов и тп)

<@U0FEJNBGQ> попробовала по вашему совету линейные модели. в 30% случаев отлично себя показала аддитивная модель  с зависимостью от тренда и компонент фурье. спасибо большое)",
"Первый раз слышу о таком, и у меня сразу возникает два вопроса. 1. Какова интерпретация подобного пулинга и где он может быть полезен? 2. Как считать градиент для такой операции?",
"Строго говоря, sum pool. Но это не принципиально. А разве hard sigmoid лучше, чем soft sigmoid? Веса всё равно подгонятся, так чтоб выдавать значения в области линейности сигмоиды. А вообще ""идеальная"" hard sigmoid не пропускает градиент, так как производная везде равна нулю. Всё что ниже этого слоя вообще не будет обучаться.",
"Он unrelated note: я сам статью не читал, но Бенжио сотоварищи когда исследовали сети на бинарных операциях, основное что придумали - это как градиент считать",
и как они его считают?,
"производную считают, как hard sigmoid",
"а если добавить бинаризацию, как в статье - становится прямо оно",
"<@U0BLCTAJK>: но в статье бинаризуются веса, а не активация нейрона. по мне это совершенно разные вещи, и я не понимаю, как можно переложить это к counting pooling. через веса градиент не проходит, а через активации - да. поэтому бинаризация даст нулевые градиенты, а насколько hard sigmoid поможет приблизиться к counting pooling - это тот ещё вопрос. и чем hard sigmoid лучше обычного  в данном случае я не вижу. по мне, если каунты - это действительно важная и легко выделяемая фича для результирующего выхода сети, то, действительно, бери  sigmoid + avg pooling и сеть сама подгонит остальные веса, чтоб проход через этот слой выдавал каунты. если каунты для результата неважны, то такой подгонки не произойдёт.",
Какими свойствами обладают условные экспектили? Где и чем учатся? Зачем это?,
"А, ок, понял
А то просто запись выглядит так же, как и аргументы функции",
"<@U040HKJE7>: по крайней мере, я узнал, что такое существует. Хотя пока не осознал, зачем оно :disappointed:",
"блин, вот не круто светить вопросы) а как я потом нанимать то буду)",
тогда это лукавство - мне же любопытно какие есть вопросы чтоб тоже включить их в собеседование,
"Коллеги такой вопрос. Есть набор лайков (нормализованный в [0,1]). Как лучше выбрать пороги чтобы получить три категории: много лайков, нейтрально, не понравилось/мало?",
"Но к тому моменту, когда стоило разобраться и попробовать что-то с памятью или рекурентное, начался авито конкурс и я забил :smiley:",
"После того, как у меня появилась сетка, которая уходит в плюс, я начал играть на всем уровне",
"После того как у меня появилась последовательность действий, которые генерятся сеткой я начал учить новые сетки на этой последовательности",
<@U0JJ69UB1>: А как ты сделал реворд?,
"А как ты решал, что действие правильное или не правильное?",
"уравнение Белмана использовал(TD-error), по сути подход точно такой же как в DQN только сеть не сверточная а полносвязная",
"Значения первых 35 фич в каждый момент времени на зависят от того какие были выбраны действия до этого, они как бы являются характеристиками уровня. Если проходить уровень кажый раз с разными действиями, то значения этих фич всегда будут менять одинаково",
"почему по одному, эти параметры могут влиять на награду, например.",
"Кто как понял, как генерируются уровни? Эти 35 компонент генерирует один какой-то стох.процесс? Затем Gain(state_t,  (x)_t)
И что такое 36 фича, т.е. что означает ее изменчивость? От чего она?",
"Это ты так иносказательно выясняешь, кто декомпилировал so и вскрыл уровни? ",
"подскажите, как можно выделить ключевое слово из сообщения и есть ли дата сеты для этого?",
"<@U0AS548A1>: 
&gt; разделяющая поверхность линейная, ее VC dim = n = 3
Эт как так получается? Ведь если класть все точки вдоль прямой, тогда VC-dim = 2 для любой размерности пространства, нет?",
"Честно говоря, это первый случай, когда мне потребовалось вспомнить, что такое VC",
"<@U042UQC96>: а как тогда стекаешь одну архитектуру сеток, обученную на разных фолдах?",
"А долго ли ждать? Взять какую-нибудь натренированную на ImageNet сетку, вставить перед выходом тот самый latent layer, заморозить весь низ сети, подгоняя только два последних слоя, как застопорится - разморозить и ещё немножко всю сеть потренировать. Должно быть гораздо быстрее, чем с нуля обучать.",
Девочки на психфаке могут быть куда более интровертированные,
"господи, я как раз для магистерской пыталась выявлять экстравертов и интровертов по текстам))) прям напомнило)",
"кстати да, соглашусь. сейчас среди программистов не больше интровертов, чем в любой другой области. я сама экстраверт вроде как (по крайней мере, так меня ""эксперты"" определили)",
"<@U0SCWKRM1>: сейчас просто себя программистами все подряд называют, так же как и дата-сайентистами :simple_smile:",
"<@U0U2ENJ4U>: я сужу по тем, кто со мной в бауманке, собственно, на программистов учится. ну я понимаю, куда нам до высот..., но всё же чисто с формальной точки зрения мы программисты - так в дипломе напишут.)",
"пст, давайте безумно конструктивные срачи на тему кто интроверт, а кто нет перекатывать в <#C040HKJF1>",
есть какой нибудь датасет по предсказанию оттока клиентов?,
Вопрос такой -- а не пробовал ли кто интегрированное видео на нужды человечества пустить?,
"Просто карточка-то есть. Я понимаю, что это не Titan, но когда очень нужно -- может оказаться лучше, чем cpu",
"Да, я понимаю, что это от многих факторов зависит 
Torch + OpenCL вроде еще куда ни шло",
"я razor blade stealth буду брать: это и ноутбук, и можно видяху воткнуть как для десктопа",
"Я не могу взять -- у меня уже есть)
Теперь вот думаю, как бы по максимуму заиспользовать",
"у станции для мака возможны потери производительности 10-20% в играх, для dl задач может быть повыше
не знаю как у razer и dell, может они 100% производительность гарантируют",
А где можно глянуть список ImageNet'овый классов? Быстрый Гугл не дал результатов,
"коллеги, вопрос про vw lda, обучил я значит лда, но я не индексы подавал в трейн сете, а тупо слова (только сейчас узнал про то что можно индексы подавать), кто нибудь в курсе как от слова хеширует, что бы восстановить топики (какие слова с каким весом туда входят)? я тут прочитал что вроде мурмур3 юзается, но я так и не понял его выход по модулю берется или как, ибо область значений хеш функции превышает количество слов",
"по модулю вроде
по модулю числа 2^b, где b -- гиперпараметр",
там код есть да <https://github.com/JohnLangford/vowpal_wabbit/blob/master/utl/vw-lda> но они грузят self.hash_values = “hash_values.csv” который как раз с предрасчитанными индексами (ну как я понял),
"кароче попробовал по модулю, у меня ровно 2**19 слов и b=2**19 соответственно, запустил такую штуку на свои слова в цикле по строкам tokens[mmh3.hash(line.strip()) % 2**19] = line.strip()

в итоге 
In [6]:
len(tokens)
Out[6]:
331540

как то меньше чем 2^19 -)

неужто он похерил при обучении столько слов?",
"С фиксированным размером входа все ок
Вопрос в том, как на нефиксированный развернуть",
"меня смущает не это, а то что если он хеширует слова мурмуром3 и индекс вычисляет как деление по модулю, то тупо херится куча слов",
"Да, я как раз в сторону дерева думаю.",
"сейчас юзаю  fullConv в торче, но у меня фикс.размер, не пробовал как оно будет масштабироваться",
"это странно, потому что при инициализации слоя ты же не задаешь размеры входа-выхода, на каком слое падает?",
вместо него как раз свертки с так называемым дробным шагом,
"а слой где у тебя ошибка - это анпулинг по заданным индексам, и у тебя карта индексов фиксированного размера, вот он и ругается",
"Нет, анпулинг же завязан на пулинг -- а тот знает, что ему пришло и как оно лежало",
"Привет. Скорее всего, вопрос задавали, но история тут быстро умирает. 

Может кто знает что-нибудь про магистратуры в европе по тематике? 
У нас вроде понятно - физтех и вышка с Шадовскими кафедрами, Сколтех, вроде в МГУ что-то, в Питере в ИТМО тоже , а что за бугром не ясно. Кто знает, где посмотреть почитать, личный опыт?",
там есть вопос из какого вы вуза,
"либо если придумать, как можно индексировать строки с сохранением топологии (ну фиг знает, если требуемая мера не левенштайн), то будет n log m при простом поиске. насколько я понимаю, в lucene что-то такое",
"еще один вопрос: допустим, необходимо построить классификатор на 10000 классов, в какую сторону смотреть, чтобы он обучался адекватное время?",
"&gt;  необходимо построить классификатор на 10000 классов
я бы попробовал сделать group by y, затем взять mean по всем фичам, и потом смотреть к какому центроиду ближе всего",
"Это долго тренировать без видео карты? Сейчас на каггле Home depot заканчивается, как раз такая штука может пригодиться",
"толк есть, когда данных мало?",
"Обычные методы куда эффективнее
Но там данных было реально мало, 2k",
"Что мне как Байесианину, кажется куда более осмысленным занятием, чем просто максимизация правдоподобия :simple_smile:",
"коллеги, а ResNet-152 билдер есть у кого нибудь?",
"kibana - тема, да. только не очень гибкая, абы какую диаграмму не построишь",
<@U11AVGHMF> это когда <@U04URBM8V> рассказывает как правильно стоит делать ,
"<@U040M0W0S> ну что-же ты меня так, это где мы обсуждаем анализ данных, IT, и переодически другие жизненные темы, все это наслаждаясь чашечкой чертовски вкусного кофе и вишневым пирогом.",
"Да и вообще все кто могут приходите, только отпишитесь здесь об этом, а то может быть немного перезаполнено место.",
Но мы обсудим как разумно и ненавящево резрешить эту проблему.,
"Я просто подобными задачами занимался. Они обычно делятся на два класса: ""а-а-а, это просто"" и ""трындец как сложно""
Например, один из моих алгоритмов находил такие похожие строки:
- удаление зубного камня
- снятие над- и поддесневых отложений.",
"<@U0KQ5M6KX>: ага, задача примерно такая (собственно, это record linkage): есть название продукта эталонное, например, iphone 6s, а есть название того же продукта, но вот, например, такое - iphone 6 эс. Собственно, ищем названия второго типа в названиях первого (да, может быть так, что в названиях первого типа нет названия, которое мы ищем, это необходимо также понять). В общем я думаю, что суть ясна. Может быть подскажите куда копать?",
Утра всем. Я немного проспала. Кто нибудь до 10 30 останется / приедет?,
А про учиться с нуля это где пишут?,
"Направление копания сильно зависит от того 
- насколько ""изуродованы"" неэталонные наименования
- сколько их
- сколько в них дублей
- насколько страшно пропустить дубль
- насколько страшно неправильно исправить наименование
В зависимости от ответов на эти вопросы алгоритм усложняется многократно.

В самом уродском случае наименования могут быть написаны буквами какого попало алфавита, похожие строки могут отличаться по размерам в разы, причем дублей сотни.
В самом простом случае (как раз iphone 6s и iphone 6 эс) можно обойтись поиском по обратному индексу ngram и косинусным сравнением наименований по ngram'ам.",
"Тогда как раз совсем чуть-чуть про древнюю историю, чуть больше про deepface, еще больше про facenet (у меня как раз так в посте)",
"Понимаю. Ну вот на уровне поста где-то ок, как думаешь?",
"так как с точки зрения всяких левенштайнов 

iphone 5 black и iphone 6 black - очень похожие строки, но это явно разные айтемы",
"скорее вопрос в том, какие бы вы методы использовали, чтобы научиться различать действительно близкие строки, и понимать, одно это и то же или нет
вот, например, constraint clustering никто не использовал?",
а как сейчас с распознаванием лиц дела обстоят? Запомнить кастомера (и узнать его позже) в магазине по вебкамере можно?,
"Привет, чят. Дружественный мне проект хочет погрузиться в болото МЛ и собирается делать опрос своих пользователей для сбора обучающей выборки. Кто-нибудь может порекомендовать источники, в которых описано, как правильно составлять опросы, на которые пользователи будут охотно отвечать и делать это честно. Еще желательно, чтобы опрос состоял из таких вопросов, чтобы по ответам было удобно фильтровать фрод. Есть ли у кого-то подобный опыт? Спасибо!",
"Посоны, а как же я =(",
"<@U0FL6RNHM>: ну про косинус это вы зря, он хороший.
Макс совершенно справделиво отметил, что возможно вам стоит ввести новые признаки, выявляемые по словарям (производитель, категория, модель, цвет, размер и т.п.)
При этом эталонные наименования должны иметь как можно более полный набор признаков.
По сути ваша основная задача - преобразовать строку в точку: ""iphone 6s белый"" в [""Apple"", ""iPhone"", ""6s"", ""white""].
А с точками затем уже будет легче управляться.
Еще один важный момент - научиться выделять супер-ключевые слова. ""Чехол для iPhone 6s"" очень похож на ""iPhone 6s"", однако ""чехол"" все меняет и сразу _исключает_ равенство двух наименований.",
"Фрод — это когда “типичное поведение” было получено заполнением за 3 минуты анкеты, занимающей 12 минут.",
"имелось в виду, наверное, точечное вранье по денежным вопросам, например. но здесь, как мне кажется, как раз надо отсеивать аутлайеров, если их немного, и плотно работать с кластерами, если много",
"Кстати, да. Ремесло социолога — как раз в этом.",
"это когда ты каждый вечер бухаешь и пиздишь жену, но знаешь,ч то это некошерно, и на соответствуешие вопросы отвечаешь ""пью раз в месяц или реже""",
На самом деле интересная тема как вылавливать wannabe ответы и поведение у юзеров,
кстати есть ли нормальный ресёрч на тему ml на опросниках и почему это говно?,
"тут где то есть человек из психологии, в этой профессии очень много наработок на то, как инвалидировать и составлять опросники. Не ML но серьезных стат подходов там много",
смотря какое распределение == честный рандом,
"Я давно мечтал провести такой эсперимент со студентами, но так и не придумал, как его технически реализовать.",
"<@U041LH06L>: у меня просто было представление, что все махания руками на тему «сейчас мы сделаем опрос на фрод, а потом настакаем xgboost’ов сверху» это всё от лукавого и вся суть лежит в правильности составления опроса, а не в том, какой потом сверху ml натягивать",
"Я когда в KPMG работал хотел это продемонстрировать коллегам, т.к. суть выявления фрода не в “Бигдате”, а в том, что человек тупо не умеет “в рандом”.",
"после майских праздников, раньше я физически не потяну адекватную подготовку
думаю. в первую неделю после майских
по месту - думать надо. если чисто лекционный формат, то с каким антикафе или еще кем можно попробовать договориться.",
"Или даже 10 пар, как в эксперименте Фишера с чаем по-британски.",
"ну представь: строки, одну человек придумывает, другую честным подбрасыванием монетки генерирует. а мы говорим, где что",
"вопрос, конечно, как много кидать надо... это стоит проверить",
"Когда мы лет оч много назад на чьей-то кухне упарывались по теме генерации рандома человеком, я из 100 бросков монетки на пол выкинул 3 “ребра”. Этого ваши студенты точно не нарисуют ))",
"Когда тренируют на несбалансированных классах, лоссом тоже можно играть",
А где можно про семплирование для содержимого батча почитать?,
"Я согласен, что асимптотически наверно должно +- одинаково получиться
Но блин, помножить лосс на константу кажется куда проще и эффективнее",
"Зависит от того, какая у тебя агментация",
"Моя история выглядит так:
Я использовал код, который привел выше. Но переписал его так, чтобы использовать батчи при обучении, ибо имея видюху как-то грустно учить по одному сэмплу.
Затем я играл до 20к-100к отсчетов. К этому моменту сетка уходила либо в минус, либо в плюс. В качестве базовой полиси я использова референсного бота. А в качестве рандомных действий - действия сетки.
Если такая последовательность действий уходила в плюс, то я считал, что все действия, которые она сделала, были ""хорошие"" и обучал ее делая y = [0,0,g,0] для каждого стейта, где g это число в районе 0.95 и стоит на месте того действия, которе было сделано.
Ну а если сетка уходила в минус, то я не обучал ее на этих действиях.
Так я очень много раз инициализировал сетки случайными весами и какие-то инициализации приводили к тому, что сетка на мини игре уходила в плюс.
Такие сетки я сохранял и из них составлял новую последовательность дейстий, которую уже использовал в качестве базовой полиси для обучения новых сетей.
Последовательность действий выбирал просто перебором среди сеток на промежутках по 10к отсчетов. Какая сетка давала максимальный скор в итоге на этом промежутке, ту и оставлял.
Паралельно я немного менял параметры сеток в плане количества нейронов. Но всегда использова несколько полносвязных слоев.",
"Ну и опять же, возможно, тоже поможет. После того как я дампнул уровень, то я могу не запускать саму игру. Либо предсказывать все действия модели большими батчами очень быстро. И соответственно играть не делая предсказания свой модели для каждого стейта, а просто используя дейтсвия из массива.",
А есть версии как орги сделали свое решение?,
"Когда сетки начали стабильно уходить в плюс, почти перестал.",
"да, примерно как reinforce, актор-критик, у сети 2 выхода один выдает распределение по действиям для состояния, а второй выдает value function для состояния",
в reinforce насколько я знаю использую как value function суммарную награду до конца эпизода,
"ну я не большой специалист) но делают вроде по разному, я так сделал, так как увидел это в одной из последних статей Мниха, они обучали актора критика для Atari",
"У Мейла зимой соревнование было,  расклассифицировать какую последовательность человек сделал , а какую генератор",
"И какие результаты были, accuracy между первым и остальными?",
"Приветствую всех! Если есть еще желающие выступить со своим докладом у нас на конференции AI&amp;BigData Lab 4-го июня в Одессе, пожалуйста подавайте заявки как можно скорее здесь: <http://goo.gl/forms/D4LKYzsy7B>
Для тех, кто планирует ехать в качестве участников, купить билеты можно на сайте - <http://geekslab.co/events/61-konferentsiya-aibig-data-lab> По традиции билеты быстро заканчиваются)",
"<@U0ZHQNKQS> а как сейчас из москвы к вам можно добраться?
я нашел лишь поезд 24 часа и самолет с пересадкой",
"понятно, что tractable (не знаю, как по-русски) модель будет значительно проще",
Ну вот так ещё куда ни шло.,
"<@U0JKYTE4B>: всё не так. Индивидуальная иррациональность в каких-то решениях вовсе не означает иррациональность рынка. Но по предмету есть недавняя нобелевка за то, что показали, что фондовый рынок - это random walk, т.к. вся существующая информация уже зашита в цене (гипотеза эффективного рынка). С тех пор поправились, что в HFT не так (пока, но он всё более high-frequency становится), а также на очень долгосрочных трендах тоже. Так что если что и моделировать - то только их. Но в HFT модели элементарные, а что-то более умное не работает, как я слышу из разных источников",
"а кому нобелевка? гипотезу эффективного рынка как раз и опроверг канеман, насколько я помню",
"вот вариант с кишиневом ниче так :simple_smile:
<@U041SH27M>: <@U04CH4QBD> <@U041LH06L> как вам такой АДик?",
"Я его больше с социально-психологического конца знаю, как индивидуальная иррациональность проецируется на рынки и экономику в целом — не знаю, т.к. лженауками не интересуюсь.",
<@U0G29N5U4>: на странице про гипотезу эффективного рынка как раз про Канемана и написано,
А какая у тебя цель?,
так а какая теория тогда изучает фондовый рынок в качестве основной части?,
"Основная проблема, с которой я сейчас борюсь -- это толстые хвосты реальных данных и ""худые"" для моих моделей. Поэтому и спрашиваю, какими предположениями вы пользуетесь для моделирования цены.",
"ээ, какое это к целевой функции имеет отношение?",
"но я не пытаюсь кому-то что-то доказать -- просто хочу узнать, кто этим занимался и какие есть результаты",
"Кароче если есть желание, мы с Колей и еще людьми в крафтере будем через час примерно, часа на полтора два посидеть, если кто хочет, подтягивайтесь",
"а зачем отдельную, чтобы заглядывать туда, и убегать в ужасе от того, на каком генераторе случайных чисел они в этот раз свои деньги сливают",
Там есть фронтализация как первый шаг AFAIR,
"Подозреваю, в продакшене у FB уже не как в статье",
А Фб зачем это использует? Чтоб предлагать кого отметить? ,
"какая-то неудобная штука для интеграции в продукт эта томита. Может кто пользовался  java / python библиотеками, подходящими для такой задачи ?",
"В NLTK есть RegexpParser; его можно (через хаки) заставить работать как что-то похожее на Томиту. См. <http://www.nltk.org/book/ch07.html>. Там не будет грамматики, но будет каскад регекспов, что не сильно хуже. Еще согласования слов по падежам-числам не будет. Но это все коряво.",
"у меня данные абонентов: количество и продолжительность звонков (в среднем  100 и 200 соответственно), и количество и продолжительность международных звонков (в среднем  4 и 10 соответственно). Как анализировать такие разные показатели?",
"что-нибудь типа анализа выживаемости?
он как раз на малых данных  работает обычно у медиков",
"выявить отток - это предсказать кто из абонентов скоро покинет или выяснить, какие причины влияют на вероятность покидания?",
"<@U0U2ENJ4U>: мне тоже интересно послушать) давно ли занимаешься? сколько примерно стратегий в пуле? Какого типа стратегии (теханализ, графические паттерны, статистические паттерны, новости и т.д.)? Как часто ротируешь набор стратегий, по каким их признакам? Это для начала)",
"""паттерны"" все из ордерлога, не знаю - к какой классификации их отнести",
"немного не то. Хочется статью которая объясняет что такое свертка, потом показывает всякие картинки - вот low-level признаки - какие-то линии, затем переходит к high-level - где уже какие-нибудь черты лица и т.д.",
HFT в вашем понимании - это какое latency от принятия решения до исполнения заявки?,
"Ага, все так. Подтверждение в этот раз не должно было приходить. Сейчас будет рассылка-напоминание, для тех кто поставил галку при регистрации.",
"нужно еще указать дедлайн , а то вдруг кто будет за 10 минут до начала регаться - а списки уже поданы",
"Как правильно  разбивать слова на подслова при проверке орфографии? Вот например, есть строка  ""aircondiioner"", каким образом можно получить ""air conditioner”? Если при этом у нас в словаре нет строки ""air conditioner"", только air и conditioner по отдельности?",
Как установить xgboost под Windows?,
"о, а расскажите еще, как такие косяки возникают (как их избегать)?",
"алгоритм, в принципе, несложный, искать в словаре префикс побуквенно, как только нашли, откладывать и повторять процесс с остатком строки",
"надо поискать, как опечаточники устроены, т.к. это обычная фишка при поиске",
"как возникают - понятно: криво html в текст преобразуют) например, в `&lt;span&gt;Н&lt;/span&gt;ачало` пробел добавлять не надо, а в `&lt;p&gt;параграф1&lt;/p&gt;&lt;p&gt;параграф2&lt;/p&gt;`  - надо (а лучше даже перевод строки). А потом страница может для `&lt;span&gt;` CSS дописать с пробелами, и все летит к черту. Но и без этого по дефолту библиотеки недостаточно стараются и разбивают как было удобнее реализовывать, а не как правильнее.",
"фига се ничего, у них это как раз большая проблема для процессинга текстов",
"обычно как раз два соседних слова слипливаются, а не три",
"aspell - это весьма старая вещь,  ставится на линуксах как зависимость от чего-нибудь, но я им не пользовался",
"<@U0DA4J82H>: если у тебя короткие запросы то можно, почему нет. склеивай всё из словаря и вперёд проверять. но если там ещё и опечатки то уже надо устраивать перебор, это будет вычислительно дорого",
"наверняка можно ещё намутить вероятностые проверки, но я не знаю как",
"или просто как для китайского, crf взять. мы так хотели сделать, когда нам надо было юзернеймы токенизировать, но руки не дошли",
"Недавно был пост
<https://habrahabr.ru/company/yandex/blog/281777/>
Мне вот о нём больше всего интересно, как потом применяли всё-таки эту модель оттока:) Потому что о том, как собрать данные и получить офигенно точный чёрный ящик, говорят многие. А что потом-то с ним делать?:)",
"Коллеги, кто-нибудь работал с EEG-данными? Поделитесь опытом. Есть задача классификации коротких серий (5 сек) снятых с испытуемых которые запомнили/не запомнили картинку. Kaggle Grasp and lift разбирал, немного не то. Хотя совершенно непонятно почему там на временных рядах так хорошо работают сверточные сетки.",
"<@U0ZSV3Y83>: тебе может помочь, думаю, <@U0JVCAA90> 
или еще кто из профессиональных психофизиологов",
"&gt; Хотя совершенно непонятно почему там на временных рядах так хорошо работают сверточные сетки.
Совсем непонятно или в принципе понятно, но хочется бОльшей прозрачности/интерпретируемости/физичности?",
"ребят, очередной вопрос от меня :simple_smile: Пусть у нас есть случайный процесс с хорошими свойствами (типа Гёльдера) {Y_t, t&gt;0}. Мы наблюдаем случайные величины из условных распределений p(X | Y_t). Вопрос: как лучше всего оценить распределение p(X | Y_t) при каждом t?",
"запись не нашел (может кто другой найдет, отзовется), но вот слайды нашлись в рассылке",
Сегодня там же где и обычно?,
"(опаздываю, хочу понять куда идти)",
"Это другое место, там где основная куча зданий яндекса",
"Не знаю, где мулен руж",
а кто из присутствующих уже начал expedia_hotel_recommendations?,
расскажешь как раз почему странный,
"Т.е. неизвестны ни Y_t, ни процесс, который его генерирует. А почему тогда интересует именно P(X|Y), а не P(X|t)? Зачем Y тут вообще, если мы о нём ничего не знаем?",
"на самом деле интересует как раз p(X_t) :simple_smile: Y_t здесь выступает фиктивной (latent) переменной, как, например, в скрытой Марковской модели. Кстати, под оценкой распределения p(X | Y_t) я имел ввиду оценивание Y_t, поскольку он полностью определяет распределение.",
"<@U0AD1L5NC> , я правильно понимаю, что успехи в deep_learning напрямую зависят от обучения на активациях нейронов? То есть как только мы сможем снимать сигналы со всех или большей части нейронов в  исследуемых долях мозга, то выйдем на качественно новый уровень распознавания и классификации изображений? Почему ""обучение на выделении объектов — это значительно более мощный constraint""?",
"<@U0FEJNBGQ>: я так понимаю, когда снимают интенсивность, это как раз частота импульсов или что-то такое",
"Почему не сделать Байесовскую модель, где Y_T зависит от t и, возможно, Y_(t-1) или глубже, а X_t - от Y_t? Если можем принять распределение X нормальным, то получается, что можно будет просто зафитить несколько различных распределений Y и выбрать оптимальное. Проще всего с MCMC стандартным пакетом типа JAGS",
"На тему того, как транслировать активации нейронов мозга в сигнал можно глянуть книжку к курсу на Курсере Computational Neuroscience. Там совмещение с signal processing. Общая идея <@U0AD1L5NC>  передана верно - оценивается частота",
"Коллеги, а есть какие-нибудь хорошие туториалы по архитектуре Encoder-Decoder, чтобы детально все разбиралось? Если с кодом, то вообще отлично. 
Прочитал статьи Bengio, Graves, Sutskever и еще пачку, на которые они ссылаются, там везде описывается только общая идея - когда начинаю кодить, возникает много вопросов.",
"TF - это, наверное, way to go, но я хотел на theano, чтобы понять вообще все что внутри :)
Вопрос элементарный - есть последовательность длины N, EncDec ее принимает и выдает последовательность длины M.  Как он понимает, когда надо остановить генерацию новых векторов?",
"<@U0DA4J82H>: metric learning это про то, как выучить метрику, в которой похожие классы ближе, чем непохожие",
А с практической точки зрения насколько это полезно? Есть какие-нибудь проблемы где без этого никак? ,
"Да, как я понимаю, но я не спец в этих делах",
"кто как делает finetuning сети в torch? У меня есть сеть построенная на nngraph хочу заморозить ее веса, прикрутить еще один слой и учить только его. Кто такие штуки уже делал с сетями в torch?",
где  module -  это твой надстроенный слой,
"йоу! не подскажете, в каких группах в фб\вк стоит пиарить мероприятия и наши DS митапы? чтобы была релевантная аудитория

в голову приходит следующее:

vk: 
DMLabs
Deep Learning
Data Science
Model Overfit

fb: 
Тренировки ML
Хакатон DeepHack
Deep learning Moscow",
"<@U04DXFZ2G>: спасиб, я правда всеми способами хотел этого избежать. Мне еще непонятно, как к загруженной сети прикручивать что-то новое",
где еще много клевых живых людей?),
"точнее, так может не выйдет, потому что когда ты делаешь backward от верхнего слоя, он автоматом по всему пробегается. А если backward не вызывать, так это почти все переписывать",
"коллеги, абстрактный вопрос про временные ряды.
допустим, у меня есть данные по сессиям пользователей. есть данные по времени, определенных действиях и т.д. в этих сессиях. хочу на этих данных предсказать что-нибудь про поведение пользователей. и тут возникает вопрос, как организовать структуру данных.

вариант 1. агрегирую все по пользователям, и делаю сводные статистики в виде фичей - например, количество сессий на пользователя, средняя длина сессии, и так далее. плюс какая-то переменная, которую хочу предсказать.

вариант 2. не хочу делать агрегацию, хочу сохранить временную структуру сессий - например,  я знаю, что пользователь делал платежи в первые сессии, и потом миллион сессий был без платежей. если я агрегирую данные, будет размазанный фарш. 
собственно, как должна выглядеть таблица данных, и как работать с такими данными, где по пользователям есть последовательности сессий, каждая со своими фичами? и я на этом хочу что-то предсказать из поведения пользователя?",
"а эта задача не похожа на bnp? там нужно было предсказать удовлетворенность пользователя от продукта, но правда там нужно было сделать прогноз как на будущее, так и на прошлое",
"о, <@U0JKYTE4B> расскажи нам, как туда добавить ассоциативные правила, и как анализ временных рядов?",
"Аримы, хм, не знаю. Но вдруг, например, может и помочь разложение в ряд фурье - поможет выявить несколько периодичных сингалов сильных и выкинуть остальные как шум. Если имеет смысл такой сигнал как некоторое периодичное поведение пользователя. В ариме может тоже найдется физический смысл.",
"очень интересно, кто какие фичи из event log извлекает, кроме счетчиков по разным образом фильтрованным событиям по разным временным окнам",
"Вот статья про то, как из кликстрим (тот же по сути event log) анализировать при помощи association rules - <http://facweb.cs.depaul.edu/mobasher/research/papers/widm01.pdf>",
"Друзья, есть не самый умный вопрос по вроде бы статистике, в которой я не то, чтобы мастер.

Предположим, у нас есть 5 непересекающихся групп людей, для которых мы измеряем некий вещественный показатель. Предположим, нас также интересует, как меняется этот показатель по времени в рамках конкретной группы (то есть, его среднее), а также сравнение этого показателя между всеми группами в том числе и по временной оси.

Проблема вот в чем — чем больше проходит времени, тем больше людей отваливается и не достигает конечного этапа. Таким образом, у нас получается что-то вроде воронки типа: вначале было 400 людей, потом 200, потом 100, а под конец может и вовсе 1. И тут возникает вопрос — если в начале мы можем без особых проблем усреднить этот показатель, то чем ближе к концу, тем ненадежнее оказывается усредненный результат.

Моя первая идея — оценить дисперсию распределения по выборке в самом начале и использовать ее для оценки границ показателя на последних этапах (возможно, как-то экстраполировав на кол-во людей). Но я не очень силен в подобном и хотел спросить — это адекватный подход или есть что-то получше?",
"Можно попробовать использовать многомерное нормальное распределение с фиксированной дисперсией, где каждая координата будет соответствовать группе. Затем использовать байесовский подход (в этом случае conjugate prior будет тоже нормальным).",
"<@U0AGVBP6U>: Почему нельзя просто доверительные интервалы строить? Хоть через бутстреп, хоть по формулкам?",
"А как его строить, если в выборке на момент времени всего один-два элемента? Надо как-то использовать инфу с предыдущих этапов видимо.",
анализ выживаемости вроде как рассматривает цепочки этапов,
"Ага. Я бы просто нарисовал график с линиями этого показателя, с начала и до последнего выжившего, как это часто с чёрном делают. И ggplot facet по пяти группам разумеется, чтобы на них глазами посмотреть. А дальше уже думал как их сравнивать. Например каждую группу аппроксимировать кривой дожития и соотнести её с измеряемым параметром (например сколько денег заносили юзеры)",
"когда alpha идет к нулю, то прогноз будет идти к константе c,  то есть получается covariance между константой и константой + шум",
<@U0AD1L5NC> я не совсем понял твою идею... Spatial transformer как в spatial transformer networks? ,
"<@U042UQC96>: нет, это в смысле как facial keypoint",
"а сейчас как решается эта задача на CNN? Регрессия смещения (dx, dy) координат точки до центра изображения?",
"Если у кого то есть вопросы по “data science завтраках” спрашивайте меня, и если хотите заглянуть, отпишитесь, что бы если будет слишком много людей, мы могли что то придумать",
"хорошая практика ставить ‘+’ под сообщением бота (тот кто пойдет конечно)
не только про завтраки речь",
"Помните кто то обсуждал, можно ли end2end обучение сделать для автопилота автомобиля? Вот есть статья очень свежая на эту тему <http://arxiv.org/abs/1604.07316>",
"Также вопрос: какой алгоритм генерации архитектур сеток вы используете ? Я имею ввиду, есть эвристики что при уменьшении размера пространственного в сети в K раз нужно кол-во фильтров увеличить в K^2 раз и тому подобные вещи, что последовательность 3-х слоев сверток 2 эффективнее о двум по 3 и.т.п.
Если осущевстлять гипер-параметрическую оптимизацию архитектур сети то много параметров условные, какими методами из фиксированного кол-ва патаметров генерировать модели?ё",
"а как тогда, вход свободный?",
"подскажите, как попасть на этот семинар",
Видеозаписи для тех кто не дошел до субботней тренировки: <https://www.youtube.com/channel/UCeq6ZIlvC9SVsfhfKnSvM9w>,
"коллеги, еще один полу-абстрактный вопрос
я продолжаю ковырять churn prediction
и везде вижу сначала длинное и не всегда внятно аргументированное обсуждение, когда же начинается churn. то есть, проблема создания разметки 0/1 в полный рост.
в то же время можно ведь попробовать просто предсказывать метрическое значение, через сколько дней вернется кастомер, и только потом думать, сколько дней можно терпеть, чтобы потом начинать ретаргет

соответственно, вопрос. почему народ выбирает классификации, а не, условно, ""регрессии""? есть ли какие-то аргументы, например, про машинное время, стоимость алгоритма и прочее, или это просто вкусовщина? или что-то еще есть?",
"В принципе с churn на моем опыте такой прикол, что он в принципе определяется как что-то бинарное — отвалился/не отвалился. Связано это с тем, что работа с людьми, которые вроде как уже отвалились, но вернутся через месяц — это совсем отдельная работа и группа этих пользователей требует своего подхода.

Регрессиоными штуками хорошо оценивать ”заинтересованность” пользователя — то есть, насколько он активный на основе того, сколько он времени провел в продукте и сколько времени он ”мог” провести в продукте — что-то типа &lt;время, когда его последний раз видели&gt; — &lt;время, когда он начал пользоваться продуктом&gt;.

По теме могу сказать две вещи:
1) Churn круто считать по примерно следующей схеме: выбираем начальный период активности пользователя (зависит от продукта, может быть день, может быть неделя). Высчитываем его engagement за это время по формуле, опять же зависящей от продукта (по количеству ключевых действий, например или просто по времени в продукте) и принимаем этот показатель за 100%. Дальше скользящим окном фигачим по его логам и если его показатель активности снизился до определенного уровня от начального (скажем, 10%), значит он пользуется меньше и меньше, и скоро отвалится.

Само собой, такая схема годится не для всех продуктов, а для тех, которыми люди должны по идее пользоваться регулярно.

2) Недавно у Amplitude был крутой семинар, где как раз обсуждались вещи типа ретеншна и черна, и как их можно посчитать. Мне понравилось, рекомендую глянуть:
<http://www.appcues.com/blog/a-360-view-of-user-retention-appcues-amplitude-and-customerio/?utm_campaign=Webinar+promotion&amp;utm_source=hs_email&amp;utm_medium=email&amp;utm_content=28603934&amp;_hsenc=p2ANqtz-__aREGzgMRk8gEmqgnce-Feag4wJWiiEEui0iKyR3Jf0ClFXuwLNnO05YBTzmOapJWG6GWa3XkSBAUe60JbXNiCNCArw&amp;_hsmi=28603934>",
"2. как правило, надо предсказывать склонность к оттоку, а не сам факт, что юзер уже утек. а если он не утек, то непонятно, на что регрессию строить - он ведь продолжает юзать сервис пока",
"присоединяюсь к вопросу, но почему бы не предсказывать самое главное что ни на есть - сколько на нем можно заработать денег?",
остальные параметры как таймаут и степень активности это все второстепенные вещи,
"о, правильный вопрос! я как раз занимаюсь этим - ltv называется",
"но непосредственно на ltv регрессию трудновато построить, поэтому надо еще и предсказывать лояльность, хотя бы как proxy",
а как обычно работаю с несколькими файлами? Объединяют в один большой df?,
ну то то есть как я и думал: объединяю все в одну разряженную (так сказать) таблицу,
"там же вся фишка в том, как сделать фичи (не обязательно разряженные) из действий и взаимодействий",
"Товарищи, давеча прочитал тут у нас, что какие-то ребята успешно учили нейросеть бутылки на витринах детектить: <http://opendatascience.ru/%D1%87%D1%82%D0%BE-%D1%82%D0%B0%D0%BA%D0%BE%D0%B5-batch-normalization-%D0%BD%D0%B0-%D0%BF%D0%B0%D0%BB%D1%8C%D1%86%D0%B0%D1%85/>

&gt; Датасет следующий — фотографии витрин из алкогольных отделов магазинов (делали детектор некоторых видов пива для одного клиента). Бутылки на фотках были размечены bounding box-ами, детектор обучался в стиле «MultiBox + Fast R-CNN» , в результате мог находить на фотках бутылки и размечать прямоугольными окошками.

&gt; Дообучали на основе обученного GoogLeNet, плюс много адового augmentation-а, так что хватило. Работало хорошо (с высокой точностью, но точных цифр не помню), особенно если качественные фотографии на вход подавать.

Кто натыкался на эту статью или какие-то подобные (локализация объектов на фотках в условиях маленького датасета), понакидайте ссылок, пожалуйста.",
"Ммм, мы учим среднее и дисперсию, чтобы они были такими, как на всем датасете, но счастье BN не в этом",
Если для этого нужно с ними ничего не делать - то выучим среднее и дисперсию такими как и были раньше,
"А почему мы не можем просто прогнать весь датасет через сеть и посчитать на всём эти средние и дисперсию?
Он слишком большой?",
"А как они получатся отличными от среднего и дисперсии на всём датасете? Как, кстати, мы обучаем их?
Правильные средние и дисперсии мы берем как вычисленные на последнем minibatch?",
Тренируем сеть и оптимизируем их как все остальные lernable parameters - градиентным спуском,
"То, какими будут среднее и дисперсия _после_ BN",
Надо теперь объяснять зачем накапливать среднее именно активаций во время тренировки?,
"А у тебя один сэмпл - как его туда нормализировать, когда других нет?",
"Еще можно по-байесовски предположить, как группы между собой связаны, и сделать вывод.",
"Появился ещё один вопрос.
Получается, что мы по сути вектор активаций умножаем поэлементно на learnable вектор и прибавляет другой learnable вектор? А средним и стд.отклонением их называют, чтобы было удобнее понимать, зачем это?",
"Ребят, кто-нибудь занимался predictive maintenance? Конкретнее, задача такая: есть устройство, оно состоит из множества узлов, которые могут ломаться независимо друг от друга. Есть исторические данные по тому какие узлы когда ломались, но нет исторических данных о причинах поломки (то есть нет никаких счетчиков оборотов, температур и тд, удастся ли их достать и существуют ли они вообще пока неизвестно)
Пока что это получается задача прогнозирования временного ряда, поскольку в наличии есть только прецеденты и время когда они случились, я правильно понимаю?",
"ээ, а как тут прогнозировать то? (узлы, кстати, ломаются зависимо друг от друга). Единственное, что можно сделать, это по исторической частоте поломок определить примерно время жизни узла и по нему его менять",
"мне пока приходит такой пример: ""если какой то узел сломался из-за превышения напряжения, то другие тоже могли в это время попортиться, поэтому в дальнейшем сломаются быстрее""",
"для тех кто интересовался, в каких задачах применяется эта экзотическая штука. условные кривые дожития, какие фичи влияют на то, когда откажет оборудование",
"А разве на бесконечной дистанции выученные средние и std не будут равны настоящим среди всего датасета?
Получается, мы обучаем std, mean с помощью сдвига их к значениям в batch,  а затем делаем как раз умножение на вектор и прибавление вектора, которые мы тоже обучаем?",
"Мы обучаем их как часть градиентного спуска, как и все остальные параметры",
"Двигаются они туда, куда толкает их градиент, привязанный к loss function",
"xgboost лучше работает на неглубоких деревьях, а как на счет количества признаков? лучше больше нагенерить?",
"Есть и другая статья -- о том, что лучше деревья делать как раз глубокими и усреднять несколько бустингов",
как правило меньше чем у RF раза в два,
"вся суть во взаимодействиях и том, как их вытащить. если с глубиной в 20 в листьях будет 100+ точек, то можно и такую глубину оставлять. основная проблема с глубокими деревьями в том, что они углубляются, ""зарываясь"" в данные и оставляя все меньше точек для нормальной оценки целевой переменной в листе",
"как хинт, на infinite mnist (аугментированный до миллионов точек) и rf и xgboost работают сравнимо с глубокими нейронками",
"Так.... :sweat:
Этот момент я тогда не понял. 
Во время тренировки мы берем значения в batch, вычисляем средние и std. Затем сдвигаем и растягиваем на параметры, равные разности между реальными и выученными. Почему на разницу? Какая в этом логика? Почему это сработает на тесте, где средняя равна одному этому значению, а дисперсия нулю?",
"А, ну в тесте мы не пользуемся самим значением как средним, да",
"Да, именно)
Я это и хотел узнать, когда писал первое сообщение сегодня. ",
как вы думаете такие онлайн мероприятия стоит добавлять в календарик?),
"Может даже лучше до фрунзенской. Вчера там какой-то ад был, очередь на вход была как в первом Макдоналдсе в СССР.",
"&gt; как в первом Макдоналдсе в СССР
Почувствовал себя молодым)",
"мда. одни знакомые лица, как жыть",
как композицию сетей с одинаковой архитектурой dropout можно представить в  качестве мысленного эксперимента. Но при этом у них shared веса,
"а маякните пожалуйста когда все перейдет к части `21:00	Общение`, а то я не успеваю на основную часть. Но если после что-то будет то присоединился бы",
"Получается, когда мы тренируем, мы можем, например, при каждом минибатче случайные веса выключать?",
"<@U042UQC96>: а ты как планируешь участвовать, один или с командой?",
"Если кто-то не удосужился посмотреть, как Google DeepDream работает, есть шанс узнать за 5 минут! <https://closedcircles.com/chat?circle=14&amp;msg=6176388>",
"Уж больно интересно, как орги учили свою модель",
"<@U041P485A>: вот кстати, получается по таким коротким описаниям понимать как оно работает примерно?",
на самом деле я щас в офисе -) пришел специально когда никого нет,
я почему то не удивлен что семенов стал уже стаканы и сюда транслировать,
Саша я я видел твой акт агрессии и то как ты его удалил :putin:,
"как я понял, если можно так сказать, сеть преднамеренно ""перевозбуждается""",
"Привет, всем... если Train и Test сеты (на компетишене аналогичном кагглу) имеют разные распределения определённых параметров, что может влиять на оценку на Test сете. Имеет ли смысл корректировать веса обучающих примеров, чтобы получить лучший результат на Test сете? и какие вообще методы используются в таких экзотических случаях?",
в более общем виде как осуществляеются подгонка к распределению на LB на Kaggle?,
"вот когда то спрашивал у человека который попал в 10ку лучших на одном компетишене ""у всех был одинаковый подход - получить распределение из лидерборда... код выложат посмотришь как делали победители"" ждал кода, и так и не увидел",
"Что за метрика, какие классы?",
"а если первоначальный кейс... когда данные не идентично разбиты на train и test сеты... видно что некоторые параметры сильно влияют на результат, но имеют разное распределение на train и test сетах? можно ли использовать веса примеров чтобы улучшить результат на test сете? или как это лучше сделать?",
как наиболее правильно рассчитать вес?,
"А что именно? Берем весь датасет -- проецируем его на 2д плоскость
Раскрашиваем точки при знаку трейн или тест
Смотрим, как они между собой соотносятся",
"""Другой способ -- строить kNN для меток train/test"" это будет некоторый аналог LSH? оно кстати не будет работаь если есть взаимодействия между фичами (или есть какие то ухищрения)?",
"а уж как оно потом быстро или медленно работает, дело десятое - хотя скала и быстрее, и удобнее получается",
"отчасти их использовали, потому что не умели в back-propagation, это как бы жадный алгоритм тренировки глубокой сети. В остальном да,  representation learning делается сейчас автоэнкодерами и всякими еще извратами",
Тогда какие еще варианты scala IDE кроме IntelliJ?,
"в общем-то получается - IDE это чистая Java, Spark - чистая Java, Scala - чистая Java - за каким боком сюда питон вообще? :simple_smile:",
"Ну скала это тоже джава, как бы. Расширение",
И кто вообще у нас такого рода курсы преподает?,
"кажись гугл продавил, как только по скорости сравняли
<https://twitter.com/demishassabis/status/726079159051649024>",
"Пс, кто по алко в столице? Необходима тренировочная выборка ",
где ж ты вчера то был,
"Кто знает почему VW(Vowpal Wabbit) c ключём —bfgs даёт хуже точность (как не выбирая регуляризацию) чем с —sgd?
```sgd
average loss = 0.203566 h
bfgs
average loss = 0.352478 h
```
есть правда странное сообщение “Early termination reached w.r.t. holdout set error” к котрому объяснение слабо гуглится",
ок… если такой сценарий почему точность хуже чему SGD?,
"Кстати, проверили, что архитектура CNN в изначальной статье DeepMind учится как положено?",
"Пишу про SVD и про то, почему вообще работает deep learning - <https://habrahabr.ru/post/282826/>",
"Кто-то, кто что-то понимает в deep learning, но не знает, как работает neural style transfer",
"<@U0AD1L5NC>: если что, то я не совсем в неведении. Я смотрел курс Karpathy, где он довольно детально neural style описывал :simple_smile:. Но я, действительно, мало что понял",
Как я понял и как бы выразил:,
Потому как я про этап обучения,
"&gt; Вот возьмем какой-то натренированный на imagenet CNN и пропустим через него картинку, посмотрим какие это вызовет activations
&gt; Оказывается, стиль неплохо передается следующей статистикой активаций",
Я там половину текста как раз ее и объясняю :simple_smile: Приведи как бы ты объяснил?,
"Минус может быть для тех, кто обычно ходит, но в этот конкретный раз не сможет",
"&gt; И вот теперь чтобы перенести стиль на входную картинку, *ищем изображение, которое максимально близко к изначальному*, но у которой *вот такие фичи активаций наиболее близки к вот такой статистике стиля*
У меня как и у Егора затык произошёл на этом утверждении. Жирным выделил то что не могу осознать. Статью прочитаю, пожалуй.",
а как они это провернули? на ICLR же уже полгода известны статьи,
"Я не знаю, как эта кухня работает :simple_smile:",
Почему бы благородным донам не бэкпропить через корреляционные матрицы,
"<https://twitter.com/karpathy/status/727618058471112704>
никто не выкачивал случайно? хотел посмотреть, но не успел
может кто в закрытых сообществах дамп увидит",
Хотя это хороший вопрос - как с размером разбираются,
"Весь этот инцедент запишем в список ""зачем нужен децентрализованные сети распространения"", такая хрень уже была с курсерой, когда они делали ролики недоступными, начали разрабатывать 'coursera-dl'",
"А в чем суть проблемы,  почему видосы удалили? ",
"Товарищи, я тут пишу диссер, где первой глава это фактически обзор современных методов байесовского вывода. Если кто-нибудь из вас изучает вариационный вывод и чувствует, что подобный обзор мог бы быть для него полезен, то я бы с удовольствием поделился текстом для с целью получения обратной связи",
"коллеги, не сочтите мой вопрос за оскорбление, но вопрос касается винды -) теана выдает разный результат на винде и на линуксах, причем результат существенно разные, а не в 5-6 знаке, как при ЦПУ и ГПУ",
"У  Kyunghyun Cho в FB нашел пост (кто знает как его имя произносить правильно, фонетически) интересные размышления про обучение концепциям в DL, обсуждает классиков <https://www.facebook.com/photo.php?fbid=10205239062969707&amp;set=a.1082089426039.12448.1640456245&amp;type=3>",
"&gt; кто знает как его имя произносить правильно, фонетически
На дипхаке <@U0748JETV> его имя произносил как ""Кунъюн Чо"" (<https://youtu.be/zwYKaq9RG9w?t=17>)",
"нуууу в данном случае я бы не назвал это  ""междисциплинарностью"". Наверное, графмодели захотелось включить в университетский курс к остальным методам, а там так как в социологии-экономике все линейное и источник случайности нормальный, то это можно было переупаковать, чтобы рассказать в привязке к остальному, убрав ""лишнее""",
а как это противоречит первому? ей так и рассказали убрав все лишнее,
Ты говоришь как не родной.,
ага все выложим когда будут утверждены доклады,
"из закромов, что сейчас у меня открыто и читается:
Shalizi - Advanced Data Analysis from an Elementary Point of View
<http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf>
Goodfellow - Deep Learning
<http://www.deeplearningbook.org/>
Sutton - Reinforcement Learning: an Introduction 2015 
ссылку не помню где подрезал",
"это древний баян про то, как дизайнера попросили сделать постер (объявление по-нашему) про пропавшую кошку",
"гуглотаблица тогда тоже не подходит. нужен ресурс, где можно оставлять отзывы",
Таких списков — как грязи. Нужно мнение коллективного бессознательного относительно перечисленных там книг.,
"Почему в pca отношение обрезанного трейса ко всему называется ""variance explained ratio""? Это же не variance. И кстати, что это?",
"Ну т.е. понятно что это  1- норма ошибки подгона / трейс , но вот когда это словами пишут про variance of data .etc я не очень понимаю что имеется ввиду ",
"Ну, во-первых, pca это не задача максимизации дисперсии. А решение pca, это векторы вдоль которых дисперсия максимальна. Это не одно и тоже. Во-вторых, почему рассматривается вдруг дисперсия суммы компонент? ",
"в результате получаются данные с диагональной ковариационной матрицей, столбцы которой как правило упорядочены по дисперсии в их единственном ненулевом элементе (самый большой в [1,1], в [2,2] меньше и т.д.). так что с точки зрения общей дисперсии, полезно знать какой ее процент объясняется каким числом первых компонент",
"можно рассмотреть эту задачу, как приближение данных плоскостью. Тогда ""остаточная"" дисперсия будет отвечать за ошибки приближения. Таким образом полезно знать ошибку, которую мы внесем в данные, если спроектируем часть компонент.",
"Спасибо, это мне известно. С точки зрения линейно алгебры я понимаю задачу, и чем это метрика полезна, как я и написал в стартовом посте и <@U0ZHHV83C> тоже сейчас написал. А если решать задачу с вероятностной точки зрения, я так и не понял, что это метрика измеряет. И что такое ""общая дисперсия""",
""" При условии, что признаки независимы -- дисперсия суммы = сумме дисперсий."" -- это верное утверждение. Но почему мы заботимся о дисперсии суммы?",
"<@U0ZHHV83C>:  спасибо, там предложена модель и указано на какую ""дисперсию"" мы получаем  оценку таким образом, это все тогда имеет смысл. Такой же подход примерно в статье которую линкнул <@U0UG04H3M> : метрика дисперсия_сигнала/дисперсия шума.  Еще мне кажется хороший подход, это сказать, что мы ищем деколлерирующие преобразование. Но от этих слов вроде ""объяснение данных"" ""сумма дисперсий"" это все немного далеко) ну по крайней мере с моей точки зрения, это не очень правильно так понимать",
"What if I told you, что есть ещё Correspondence Analysis где та же фигня и вовсе зовётся инерцией (inertia).",
<@U06MTEXQQ>: Ну или когда будут известны темы докладов?,
Угу. А пин может отпинить кто угодно.,
"а когда все начнут их использовать, и накопиться 10к звездочек...",
"По поводу ""объясненные данные"": если мы возьмём все направления, то мы сможем ""объяснить"" (т.е. описать) все изначальные данные. Если же мы выбросим часть векторов (остальные спроектируем), то мы не сможем объяснить все данные (поскольку потеряли часть информации). 
По поводу ""сумма дисперсий"": будем считать, что чем меньше дисперсия с.в., тем больше она похожа на константу. Таким образом, выкидывая часть компонент, нас интересует какую суммарную долю дисперсий мы потеряли. Как заметил <@U040HKJE7> в собственном базисе ковариационная матрица будет диагональной, следовательно, работает (почти) правило про сумму дисперсий.",
"Можно сказать что мы просто измеряем разницу между истинной матрицей ковариации и нашей апроксимации. Там тогда как раз эта сумма и есть она. Это один способ. Другой как у Бишопа. А остальные попытки из матрицы ковариации получить какие-то скаляры, это не тру. На пальцах то все очевидно конечно, но все таки это data science.",
У него вроде как раз python api,
"Дак все же: почему большая дисперсия - много информации? Если сделать всем объектам один признак = 1 и одному объекту 10^10, дисперсия будет офигенная, а информации мало",
"&gt; максимальная дисперсия достигается на бернулевской сл.в. 
Максимальная в каком классе?",
"А знает кто-нибудь какие-нибудь дельные статьи или подходы для решения типичных задач фотографа с помощью анализа данных?

Я вот по мимо прочего занимаюсь фотографией. В который раз привез пачку фотографий из поездки.
1. Там есть серии почти одинаковых фотографий, которые хочется схлопнуть в одну.
2. Взять серию и отранжировать от самых хороших до самых плохих (В самом простом случае речь идет про четкость фотографии, в более сложных, например, качество портрета: тут глаза закрыты, тут рука лицо закрывает и т.д.)
3. Сразу выделить 10 ""шедевров"" из всех моих фотографий.

По идее задачи, кроме последней не сверх сложные, и какие-нибудь сетки натаскать на это дело уже могли, но с ходу что-то не гуглится. Может кто подскажет где искать?",
"для 1 можешь смело юзать PCA и KNN, какой без сеток
для 2го - чтобы какой то ML использовать для этого - надо самому датасет хороших и плохих нагенерить. Иначе просто лепишь эвристики и не волнуешься - скорее всего это сработает быстрее и лучше",
"<@U04422XJL>: По поводу линейного времени. Я просматриваю фотографии за несколько проходов, и мне бы такая штука жизнь облегчила. Если фотографий одного и того же 20 штук, где в начале ты пристреливался и подбирал настройки можно  хвост серии даже не смотреть, а сразу удалять. Выбирать из первых 2-3 штук. Тоже самое с портретами. 
Чувство прекрасного, да само собой. Тут бы хотя бы с качеством разобраться и по нему все ранжировать.",
<@U0AGVBP6U>: А какой другой не помнишь?,
"вот только кто этим всем заниматься будет, уж очень много сил это отнимает",
Я отвечал на вопрос почему в Долине движуха поутихла,
"А кроме этого, надо разобраться как мотивировать людей учиться",
Как бы я это видел,
"Т.е. отслеживать что ты посмотрел, что следующее, где развилки",
"тем более, что на работе руководство крайне подозрительно относится к тем, кто в свободное время чему-то учится",
"а тех, кто с радостью учится у высокомотивированного тебя ты и так регулярно видишь и лично всех знаешь",
"Интересно, как местные гуру тервера решат эту задачу :simple_smile:
В урне лежат 4 шара, из которых по крайней мере один белый. Из урны наугад берут два шара, и они оба белые. Какова вероятность того, что следующий вытянутый шар также будет белым?",
"И если шары возвращаются, надо уточнить как возвращаются",
"Я может чего не понимаю, но ведь важно как эти шары в урну клали и из какого множества набирали?",
"Я посчитал ""в лоб"" без рассуждений, все равно 1/2. В силу того, что надоставали, возможные следующие варианты шаров в шляпе (без учета порядка, какие шары вообще лежат):

{ббчч}
{бббч}
{бббб}

Ясно что они равновероятны. Тогда 1/3*0 + 1/3*1+1/3*1/2 = 1/2",
"если были возвращения то

{бччч} -&gt; 1/4
{ббчч} -&gt; 1/2
{бббч} - &gt; 3/4
{бббб} -&gt; 1

И получаем 5/8. Просто если с возвращениями то зачем инфа что достали уже 2 белых вообще не понятно",
Да. Но важно как они формируются,
Ты их оба запишешь как ббчч без учета порядка,
"""Тогда корзина чччч менее вероятна, чем ббчч"" -- это тогда опять надо читать как ""я буду доставить из карзины с учетом порядка и записывать исходы""",
"P(W3| W2, W1) = P(W3, W2, W1) / P(W2, W1) где Wn — событие ""в n-м испытании выпал белый шар""",
"P(W3, W2, W1) = sum_L P(W3, W2, W1|L) P(L) где L — конкретная ""ракладка"" шаров",
"Народ, а где-нибудь написано как работает findface? Он делает какой-то эмбеддинг? Типа автоэнкодеров?",
"Током кажется никого не бьет, ни за какие вопросы или сообщения запосченные в чатик. И читать-отвечать каждый сам принимает решение. О чем вообще наезд я не понял.",
"вопрос: где кто рисует картинки структуры сеток для статей/отчетов/дипломов/диссеров? Может слышал кто-нибудь про какие-нибудь сложившиеся стандарты, а то от статьи к статье кто только как не рисует. Пока что лучше всего у меня получалось, как ни странно, в PowerPoint",
как говорят преподы: задачу с возвращением предлагаю решить в качестве упражнения :simple_smile:,
"Ну, задача с возвращениями была решена ещё вчера, единственный оставшийся интересный момент — в какой интерпретации задачи ответом будет 7/12",
"потому что мы суммируем только по тем, где могли 2 белых вытянуть",
"&gt; p(2б|prior) 1/2*6/15     1/2*4/15      1*2/15
Почему здесь только 3 числа?",
не рассматриваю где 0 вероятность вытянуть 2 белых,
"Извиняюсь, видимо сегодня мне сканы никто не перешлет, но как только, то я сразу :disappointed:",
"Подскажите, пожалуйста, как действовать в след. ситуации: у меня есть случайный процесс, в котором я могу оценить конечномерные (параметрические) распределения в любой точке, но с разной точностью (точность контролировать не могу). Мне нужно делать какой-то прогноз (хотя бы матожидание). Какие подходы тут можно использовать?",
Смотрите какую прелесть жена подарила на ДР: <https://www.dropbox.com/s/bhiq4gv3gz4mzk6/2016-05-07%2013.52.54.jpg?dl=0>,
"Это в какой постановке, с возвратом или без?",
"Сейчас нужно задать вопрос, почему симуляция <@U04ELQZAU> неверна!",
"Я не посмотрел, как у него. Наверное, надо сначала выбирать, каким по порядку будет ""гарантированный"" белый, потом заполнять остальные, потом выбрасывать из выборки те, в которых первые два получилось черными. Тогда количество возможных комбинаций будет 4 + 4 + 2 + 2 = 12 (каждое слагаемое здесь соответствует выбранному месту ""гарантированного"" белого). Из них нас интересуют 2 + 2 + 2 + 1 = 7",
я как раз публичную использовал,
"В <#C043ZEF6K> мне не ответили, попробую повторить свой вопрос здесь.

а кто как обычно действует с категориями, которые не присутствуют в train set?
мне на ум приходит вариант объединить редкие категории в одну, заодно и засунуть туда отсутствующие.

ещё можно в некотором проценте случайных записей в train set поменять категорию на отсутствующую.

можно при предсказывании примера с такой категорией - прогнать классификатор со всеми вариантами категории и усреднить

может есть какие-то best practice?",
"<@U0G29N5U4>: вот симуляция <@U04ELQZAU> для случая, когда возврата нет",
"в новом решении там один из белых шаров стал отличим от других, как я понял, так что симуляция не подходит эта",
"в общем, проблема в том, как генерируется последовательность. на примере всего двух шаров поясню. если дано условие, что хотя бы один шар белый, то возможно только два варианта - оба белые или один белый, другой черный. если генерировать так, как <@U04ELQZAU> (сначала как если ограничения нет, потом выкинуть сэмплы, где оба черные), то соотношение между этими вариантами будет один к двум. а если генерировать так: выбираем первый наугад, если он черный, то второй точно белый, иначе снова наугад - тогда соотношение будет один к одному. соответственно, в задаче правильный ответ получается во втором случае. и вроде как он меньше всего допущений содержит",
"Вот, как я и говорил с самого начала, нужно определить априорную оценку",
"То есть, какие ситуации являются равновероятными",
"Твоя сложная - нужно сначала выбрать случайное место, где белый шар, а остальные сгенировать случайно",
"(непонятно, почему такая процедура - это мало допущений)",
"Ну, как она работает, если шаров больше чем два?",
"и я бы еще взглянул на это так: представим сначала, что порядок не важен. тогда априорная вероятность нам встретить два белых или черный и белый действительно должна быть 50/50 - если мы не знаем, кто и как их подбирал, то можем только равные вероятности ставить обоим вариантам",
Это как раз зависит от нашей модели генерации корзины,
Ну тупо как пример - если просто в корзине два шара,
"да, кстати, парадокс хороший - зависит от того, как задать вопрос",
"тогда я считаю как один к двум, конечно",
"Ну, еще раз, все как в парадоксе",
"не, до новой информации 1:2, после - 1:1, потому что я же не знаю, что и как там кто-то мешал и предполагаю наибольшую энтропию. если два черных может быть, она наибольшая в первом случае, а если не может - во втором",
"Я туда заглядываю и говорю тебе - о, есть как минимум один белый!",
"Спрашивается, какое соотношение между вероятностями бб и чб у тебя в голове?",
"Но вот напиши словами, какие корзины с 4-мя шарами равновероятны, если известно, что один белый?",
"в общем осмысленных варианта два - либо условием ""один белый"" лежит в генеральной совокупности, либо нет. Соответственно, это как решить две задачи. Ну и задачи то простые - правило Байеса да и все",
"да не, это я сморозил, это как раз таки неравные",
Можно генерировать так как это делает <@U0G29N5U4> ,
"И именно его нужно подразумевать, когда говоришь ""случайная корзина с белым шаром""",
"не знаю, я в исходной задачи не вижу какая генеральная совокупность. Ну все в точности как у Гарднера. Он имел в виду вариант 2), а люди решили как вариант 1)",
"В авторском решении таки вероятности подразумеваются как у меня, а не <@U04ELQZAU>... Но уточнение про равновероятность в постановке дано: не для всех шаров цвет равновероятен (что невозможно ни в каком случае, как мы видели), а для всех, кроме одного белого. Кажется естественным, меньше всего допущений",
"Допущений одинаково: можно генерировать корзинки по 4, пропуская те, где белых нет, а можно генерировать корзинки по 3, докладывая белый",
"Объясню, почему допущений меньше: если генерировать по три, это соответствует условию задачи и дает при этом максимальную энтропию. Если генерировать по четыре, это не соответствует условию задачи, а когда выбрасываешь ""лишние"", энтропия уменьшается, так что тут существенное допущение возникает, что именно это оптимальный вариант генерации",
"&gt; максимальную энтропию
Энтропию какого распределния? Распределение над ""раскладками"", очевидно, неравномерное, отсюда и энтропия не максимальна",
"&gt; Энтропия распределения цветов остальных шаров, кроме белого
Почему рассматривается энтропия именного этого распределения? Ну и оговорка ""кроме белого"" — это какой-то ad hoc",
"Там все равновероятно с учетом перестановок оставшихся трех шаров, т.е. 8 вариантов, а не 15. Ну, если хочешь еще ""гарантированный"" белый переставлять, будет 32, но распределение то же самое. Я не понимаю, почему ты изначально данное условие про белый шар не хочешь заносить в prior, честно. Это как раз все, что известно до начала эксперимента с вытаскиванием же",
"<@U049HDR2Z>: да, это чеки. У меня такая проблема, я фотки чеков до этой стадии обрабатываю (ищу края, исправляю перспективу, перевожу в ч/б). Как по твоему лучше будет, применить augmentation до обработки или после?",
"&gt; Я не понимаю, почему ты изначально данное условие про белый шар не хочешь заносить в prior, честно
Потому что я уже сделал, исключив вариант чччч",
"<@U0K4F4DGW>: так эта, какую конкретно задачу решаешь? На входе фотография, на выходе вот такое бинарное изображение?",
И для какого этапа хочется агментацию?,
"То есть, какой вход, какой выход ",
"Хм,
- color balance - у меня в итоге ч/б картинки (adaptive threshold), чем смена баланса поможет?
- а зачем замешивать текст? все же чеки одного класса будут (я буду кропить верхнюю часть) иметь почти одинаковую структуру",
"<@U0AD1L5NC> , а где можно почитать про соотношение количества изображений, необходимое и достаточное в тренировочной выборке к размеру/архитектуре сверточной сети?",
"<@U0AD1L5NC>: а почему натренировать SVM будет лучше, чем новый слой с cross-enthropy loss?",
"А два-три слоя даст больше оверфит, чем SVM? Вообще говоря, когда гибридная структура лучше единой?",
"SVM используют когда заведомо понятно, что данных для fine tune недостаточно",
"Как ты, кстати, побил шефа с точками на сканах? End-to-end?",
"Ребята, есть гладкая функция от пяти переменных. Как визуализировать изменение целевого значения от взаимного изменения параметров?",
"А зачем они разработали другое, а не взяли твое?",
0 когда инпут правильного класса больше остальных на дельту,
"всем привет. слушайте, а кого на петербургский data science meetup выступать позвать? мы с Пашей Калайдиным обсуждаем, но пока без конкретики. ",
"Сетка по 2-м (subplot-ы,) и heat-map по слайсам как в матлабе, типа вот так <http://www.mathworks.com/help/matlab/ref/graphics_s9.gif>",
"Ну, мы с ним в Сапсане когда ехали, про какие-то интересные идиоматические конструкции и либы он рассказал. Местами аналогично и в питоне делать можно, если знать куда копать, и выглядит достаточно здраво, как подходы, паттерны, если хотите. Для кругозора, в общем",
какой метрикой вы бы рекомендовали считать схожесть гистограмм в полярных координатах?,
где высота столбика - модуль,
в другой формулировке: по какой метрике сравнивать розы ветров (с присутствующим шумом и по аргументу и по модулю)?,
"если уточнять, то это набор time-series разной длительности, где угол - это аргумент",
"в смысле *x*, где наблюдение есть *y=f(x)*",
"причем, разумеется, есть моменты, где x переходит с 359 на 0. я посчитал, что представление в виде розы - довольно удобное решение, поскольку иначе неясно как учитывать при классификации/регрессии граничные условия (360==0)",
"только, как мне кажется, он достаточно дорогой для большого количества классов",
"Продолжая рассуждение: ""в глубину"" противопоставляется понятию ""в ширину"", то есть обратная связь в сетях происходит рекурсивно , как и при поиске в глубину в графе, в отличие от поиска в ширину, где вместо стека используется очередь. Поэтому правильно переводить deep learning как ""обучение в глубину"". :troll:",
"Завтра, если кто то хочет зайти на data science завтрак, будем всем рады.",
"В гуглгруппе Ветрова как-то делали quiz на тему того, кто какой вариант считает лучше",
Высказывались как лучше переводить на русский?,
"хуже, когда речь наполовину состоит из англицизмов",
"Есть тут те, кто поедут в Одессу 4 июня?",
"<@U0AD1L5NC>: да, есть куда стремиться)",
"У кого-то есть хорошее представление о том, как они это могли делать до Tensorflow?",
"<@U0K4F4DGW>: в AlexNet не так, там напрямую указывалось, какие слои разделяются  на две части и тренируются отдельно, а где они начинают сходиться",
"нет, в смысле выход слоя параллельно в одной ветке сворачивается, а в другой просто передаётся неизменно и потом конкатинируются как в resnet",
"<@U107256SG>: Если б я покупал, а кто ж на это денег выделит(",
"если у тебя попрет на оффлайн базе и это даст бизнесу профит, то почему не заплатить, вроде цены там более чем адекватные",
"Ребят, привет!
Хотел бы попросить вашего совета по одной задачке, которую я хочу попробовать решить у нас в Criteo - классифицировать товары по категориям.
Основные наши клиенты - интернет-магазины.
По каждому клиенту мы видим все ивенты на его сайте - какие страницы просматривались (главная, листинги, продуктовые и т.д.), собираем какие товары посещались и отдельно имеем обновляемые фиды от всех клиентов.
В итоге, для каждого айдишника товара в нашей базе мы видим, сколько человек его посмотрело, положило в корзину, купило.
Отдельно в базах лежат данные из фида, где для каждого айди можно найти название товара, его цену на сайте, маржу продавца, описание товара и категорию товара (как правило просто это категории товаров верхнего уровня с сайта клиента) и картинку (изображение, высота, ширина). Одному id однозначно соответствует одно значение из этих полей.
Задача  - сделать модель, которая будет предиктить категорию товара исходя из фич товара указанных в фиде.
Список категорий, к которым надо классифицировать товары, я хочу задать сам. Это будут категории верхнего уровня - Одежда, Электроника и т.д. Всего штук 10-15. Соответственно, для тестовых данных я ""вручную"" проставлю эти категории.
Как вы понимаете, категории, которые мы получаем от клиентов в фидах очень разные - могут посылать кириллицу, латиницу, цифры, бренды, конкретные названия категорий или прям иерархию (напр. ЖЕНЩИНЫ &gt; Одежда &gt; Брюки &gt; Прямые). Ограничиваемся только фантазией клиента.
С другими текстовыми полями - название товара, описание товара, - ситуация тоже неоднозначная.

Был бы очень рад услывать советы о том как такую задачку лучше решать, какие методики использовать и любую прочую помощь.
Сам я никогда до этого не занимался ML и только учусь.
Спасибо.",
<@U09V0F0HX>: выглядит как классическая задача на много классовую классификацию ,
Текстовые фичи в word2vec и какой нибудь бустинг - вот и решение ,
"<@U0JTMFHDE>: спасибо. собственно, основной вопрос в текстовых фичах,  конечно. А какие альтернативы word2vec есть? Так на будущее",
"в некоторых работах показано, что работает ”лучше”, вроде как жрет гораздо больше памяти",
это зашибенная штука! а можно тупой вопрос задать — почему на julia?,
можно кстати договориться с <@U04BFDYPV> и дать это студентам ФКН как задание на лето. чтобы в R/Python можно было метод дергать,
"если это преобразование представить как непрерывную функцию от времени, то понятно, что можно очень много получить дополнительных картинок",
"А как они его ищут, такое преобразование?",
как раз тогда и есть смысл связываться,
"Когда у вас нет нормального тестового множества и уверенности в том, что модель можно как-то улучшить в плане интерпретабельности/точности, как на основе чего вы принимаете решение о том, стоит ли забить или продолжать polishing the turd?",
"Ну или если совсем конкретно, то стоит ли ждать магии от этих гауссовых процессов, когда все основные процедуры по feature engineering&amp;selection уже были проделаны и линейная регрессия/дерево/РФ показали на них  не особо впечатляющий результат. ",
"Но в рабочем процессе нельзя же плясать с одной задачей бесконечно, особенно когда есть и другие, требующие внимания.",
"Вот я и пытаюсь понять, как нащупать ту грань между “Короче, забейте” и “Может полиномов/интерэкшнов/слоёв добавить”?",
"В общем, так и буду играться, как дурак с соплёй.",
такое может быть если есть какой ни будь не стандартный аналайзер например добавляет синонимы для значения A,
"в общем, вроде бы поборол...
body={
                    ""query"": {
                        ""filtered"": {
                            ""query"": {
                                ""bool"": {
                                    ""must"": {
                                        ""match"": {
                                            ""field3"": 'some_string'
                                        }
                                    },
                                    ""must"": {
                                        ""match"": {
                                            ""field4"": 'some_string'
                                        }
                                    }
                                }
                            },
                        ""filter"": {
                            ""bool"": {
                                ""must"": {
                                    ""term"": {
                                        ""field1"": A
                                    }
                                },
                                ""must_not"": {
                                    ""term"": {
                                        ""field2"": B
                                    }
                                }
                            }
                        }
                        }
                    }
                }
вроде как это работает как надо... может кому пригодится",
"<@U040HKJE7>: спасибо! Вопрос как раз и назрел потому, что модель вполне согласовывалась со здравым смыслом (на грани Капитанства Очевидности) и у меня возникли сомнения, что какая-то другая модель расставит фичи и их веса так, что реальность предстанет перед нами в новом, необычном ракурсе с кучей инсайтов и нетривиальных находок.",
"Если это дает улучшение на кросс валидации, то почему бы и не добавить",
"о, речь типичного халявщика, которые сводят на нет комфорт тех, кто заранее почесался и зарегался",
"а это никак не пофиксить, кроме как разворачивая их назад",
"так что за мероприятие-то было, где пришло в 2 раза больше?",
"кто и что не читает, чувак? если я что-то не читаю, то это не повод говорить про людей. может я плохо читаю. 
если ты про кого-то еще, то сорян. я запутался.",
"может заодно скажешь уж на какое мероприятие пришло в 2 раза больше, чем зарегалось?",
"сорри, наболело. то мероприятие, где переполнение было - это семинары по строительству для инженеров.",
"на продвинутых митапах я видел рабочий waitlist, то есть в зал пускают только при наличии регистрации. Народ реально снимает её в последний момент, когда не может придти, и на освобождающиеся места попадают люди из waitlist и проходят.",
"<@U0AD1L5NC>: 
1. ""Давайте сформулируем задачу так, чтобы более глубокие уровни ""поправляли"" более нижние, то есть всегда могли увести веса в 0 и просто попустить сигнал"". Долго пытался расшифровать. ""Поправляли"" - не лучшее слово здесь, можно оставить только вторую часть предложения.
2. Разве там главная идея была не в том, что в очень глубоких сетях остро стоит проблема Vanishing Gradients, а Residual архитектура позволяет ее кое-как решить?
3.  ""Есть идея вместо двух convolutions делать например один и меньшей размерности"". У identity понятно сохраняется размерность. А как обстоит дело у трех conv layers? Не очевидно разная ли у них размерность и как они результаты двух этих потоков стыкуют в блоке ""+"". К примеру, я полагаю, там должны быть хотя бы один Max Pooling 1x1 слой;
4. По Inception неясно что ты хотел сказать: объяснений немного и они разнородные. Было бы здорово увидеть объяснение почему они используют в одном блоке кучу ""дурацких"" пайплайнов сверток. У Karpathy в лекциях более-менее неплохое объяснение этого было, правда.

Про ResNet неплохой тред, про Inception - лучше бы добавить контента.",
3 - там 3 conv слоя без какого либо пулинга на картинке,
"4 - да, я понял. В треде, в основном, акценты на имплементацию и догадки, почему они в этом направлении всё пилят",
fine-tuning - это когда мы дообучаем готовую сеть на своем датасете?,
"transfer learning -- когда мы используем модель, обученную для одной задачи в другой
Обычно это означает перенос фичей
Условно -- берешь VGG-16, достаешь из нее фичи с последнего слоя -- и используешь их в другой задаче.",
"Спрошу даже точнее: кто с чего начинает копать? Я пока Markov chains и HMMs строю, а какие альтернативы? Типа, что на каггле всех рвет? <@U04422XJL>, <@U054DU76Y>?",
"мне вот этот конкурс понравился, где люди на сырых данных выигрывали у xgb с сотнями  фичей, которыми пытались захватить эту последовательность",
"А тот, где все-таки поменьше?",
"<@U0FF52P7D>: да, конечно, имеют. Можно сделать обычную агрегацию, например, количество увиденных букв. Правда, большинство таких вариантов не сохраняет path dependence. Можно представить их как графы (или пути в одном графе) и кластеризовать. Поверх чего простые алгоритмы работают. Интересны еще подходы",
"Коллеги, всем привет! Хотел спросить, работал ли кто-нибудь с датасетами по пациентам и их историям болезни / профилям (electronic health records) и может ли кто помочь достать достаточно большой датасет для тренировки collaborative filtering модели? Спасибо!",
"по описанию очень интересно, но у меня kaggle мастера нет увы, мне не скачать данные. не подскажете, где можно зеркало найти? или может у вас они есть сохраненные?",
"нет, там перед тем как скачать данные нужно документ до какого-то числа подписать",
"по одному сайту
там есть нюансы, например, на одной странице пять полей для фильтрации, на другой - десять. и каждая галочка на фильтре меняет урл, хотя урл сохраняется только при переходе
плюс, повторяю, сколько в реале вот таких вот элементов - неизвестно.

я не очень понимаю, как сюда hmm прикрутить :disappointed:",
"Добавить еще состояние ""другая страница"", как гиперпараметр",
Кто посмел меня тревожить и в мою дверь стучать?,
А по-английски это как называется? tonality?,
"Добрый вечер!

У меня есть небольшой вопрос касательно оценки фита распределений по данным, надеюсь здесь помогут.

Данные: Есть 8 выборок по 500 000 значений признака в каждой (короче говоря, вектор)

Гипотеза: Я предполагаю, что они распределены одинаково (или хотя бы описываются одним и тем же распределением)

Что было сделано: Я прогнал эти данные через скрипт, который фитил с десяток распределений и считал для них NLogL(обратный логарифм правдоподобия), BIC(Баесовский критерий информации), AIC (информационный критерий Акаике). В итоге, половина выборок хорошо описывается распределением Вейбулла, но некоторые нет. Насколько я понимаю, то для толковой интерпретации надо смотреть разницу по критерию между моделями. 

Вопрос: Как правильно интепретировать разницу по таким критериям (например, по BIC) по такой выборке? 

Мои мысли: В интернетах написано, что сильным доказательством против модели является разницу уже в десять и больше - в моих результатах есть разница в тысячи и я подозреваю что нужно делать поправку на объем данных, но я не очень понял как. 

Заранее благодарен и прикладываю к посту скрин таблицы разниц между распределениями по BIC. (Там где ноль- было наиболее подходящее распределение, от него и считалась разница)",
"в гипотезе важно, какое именно распределение, или просто убедиться, что они одинаковые?",
"у AIC/BIC трудности на больших выборках, так как суммарное правдоподобие серьезно перевешивает штраф на число коэффициентов. можно провести эксперимент с двумя гауссианами и увеличивать число семплом с 100 до 1М",
"Сейчас, могу скинуть график - визуально они хорошо сидят. Меня скорее даже волнует вопрос - нужно ли использовать какой - то критерий для доказательства такого факта? Если я правильно разобрался, то распределения Релея и Райса можно считать частными случаями Вейбулла + первую выборку можно убрать она кривая и остается только два случая.",
"Добрых суток)
У меня есть логи пользователей сайта (вход, выход, серфинг по меню). Нужно спрогнозировать время до определенного действия от первого входа. Никто не встречал в интернете подобных примеров? Правильно ли я понимаю, что мне нужно создать df, где строки - это пользователи, а столбцы это их действия?",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1463072786001459>

Мы в свое время много играли с GPR и получалось, что на выборках до примерно 1000 точек GPR как правило выигрывал у всех (линейные модели, сетки, сплайны, ...). Правда с деревьями не сравнивались, т.к. нам была важна гладкость модельки.",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1463086977001490>

С kNN никогда не сравнивал, но вообще GPR хорош тем, что ухватывает гладкие тренды неплохо, а вот kNN это как раз не очень умеет.",
<@U107256SG>: вот это как раз марковские цепи неплохо делают. Лучше в глубину на пару шагов назад смотреть,
"Уже некоторое время МЦ занимаюсь (см. мои и <@U04423D74> вопросы по подобным задачам выше; к сожалению, кроме них никто ничего особо не рекомендует, ни мне, ни <@U04423D74>). Сейчас расшарю довольно прикольную статью. Она, правда, про ритейл, но такая, можно сказать, классическая. Очень понятно, и заодно рассказано как измерение по времени можно внести в модель (с чем у меня самого были сложности). Но для практического применения надо извращаться посильнее обычно (брать бОльший порядок, фитить не наивно). Про первое есть много статей про marketing attribution - это, кстати, ближе к твоей задаче. Но многие из них посложнее написаны, так что рекомендую эту все-таки изучить и попробовать прям такую простую модель для начала сделать",
"Да, обязательно. Надо priors вводить. А если хочется условные вероятности от прошлых событий, то либо более высокий порядок, либо контекст/память, либо их комбинацию. В общем, такой красоты и простоты, как в этой статье, может не получиться. И единственная идея, как делать прогнозы на бесконечность, у меня пока - через Монте-Карло симуляцию. Но вот будем с <@U04AR6WF0> на днях обсуждать, что тут сделать с высокими размерностями",
"в какую сторону копать и по каким словам искать про уточнение классификации, когда есть размеченная выборка, но известно, что некий процент разметки неправильный, и надо найти и поправить эти строки, пользуясь остальными фичами из данных?",
"Подскажите,  плиз, как написать функцию в tf.  Я делаю как в обычном пайтоне, но мне выдает ошибку ```ValueError: Fetch argument &lt;tf.Tensor 'Softmax_7:0' shape=(128, 10) dtype=float32&gt; of &lt;tf.Tensor 'Softmax_7:0' shape=(128, 10) dtype=float32&gt; cannot be interpreted as a Tensor. (Tensor Tensor(""Softmax_7:0"", shape=(128, 10), dtype=float32) is not an element of this graph.)```",
"для LR еще один крестьянский метод знаю, для текстовых данных более-менее - натренировать классификатор на мешке слов, зарегуляризованный меньше, чем обычно, посмотреть фичи (слова/биграммы) с топ коэффициентами по модулю, найти мусор, найти из каких он примеров",
"есть ли какие-то baseline методы для look alike сегментирования, когда небольшая часть строк размечена только одним классом (типа эти кликнули на этот баннер), а про остальные разметка неизвестна (показы без кликов не хранятся)  и надо найти еще строки с таким классом (кто кликнет)?",
"А я все пытался вспомнить, как оно называлось)",
"Я, если честно, в адресной попробовал поискать, никого не нашел, кто конкретно бы АСТ занимался. Походу тогда только задалбывать по адресу <mailto:info@sberbank-ast.ru|info@sberbank-ast.ru>",
"""Эмулировать API"" -- так вот как это теперь называется.",
"Чтобы пустили, как я понял?",
"народ, а кто как в продакшене решает вопрос с многоклассовой классификацией (когда классов много - тысячи)? модели немаленькие получаются...",
И как в таком случае будет проходить процедура прохождения внутрь? :smiley:,
"если я правильно понимаю, это не огромный экстрополис, а один из залов внутри рабочего корпуса яндекса
то есть, на входе стоят девочки со списками, и пропускают через вертушку/воротца
как я понял, сейчас людей много, и есть вероятность, что в зал просто не поместятся уже люди",
"Лекции John Schulman (работает в OpenAI) про Reinforcement learning (Derivative-free optimization и Policy gradient), уровень - для начинающих
<https://www.youtube.com/channel/UCBOEQxX6zdihFB3VxxJdgHg/videos?view=0&amp;shelf_id=0&amp;sort=dd>
Они сделали платформу, где можно тестировать алгоритмы на различных физических задачах, симуляциях, играх атари и тд
<https://gym.openai.com/>
это он тут на MLSS рассказывал",
"если да, то как оно?",
"Также приглашаем всех кто хочет вживую пообщяться о анализе данных, IT и прочих темах с людьми из сообщества, приходите завтра на Data Science Завтрак (детали в календаре)",
Кто завтра будет - воткните плюсик,
а чо куда тут идти то на дл митап?,
круто а где это хехе?,
NLP-практики ау!)) кто-нибудь укладывал несколько bi-directional RNN слоев в стеки? Не понимаю как перейти от уровня слов к уровню предложений.,
"читал как книжку комиксов:
<https://matthewdharris.com/2016/03/29/boxplot-or-not-to-boxplot-woe-ful-example/>",
не работает в каком плане?,
"у нас кинозал 80, а перевая переговорка максимум 110. Атриум как на фест (300 мест) нам на митап не выделяют",
"кстати, маленький ништячек. наши друзья из газеты Бумага проводят 29го мая в Москве очередной Science Slam
для тех кто хочет купить билет с местом чуть дешевле, вбейте в промокод *opendatascience* :slightly_smiling_face:
<https://scienceslam.timepad.ru/event/326647/>",
"Те, кто придумал conditional random fields, пробовали",
"Ага, кто больше мемчиков накопипастит.",
"— Распознавание изображений, образов связано с работой так называемых нейронных сетей — некоторых компьютерных алгоритмов, которые, насколько я понимаю, ""умеют"" сами себя обучать. Можете рассказать, как устроена их работа?

— К сожалению, нет. До сих пор математического объяснения работе нейронных сетей не существует. 

— Но все же есть какой-то принцип, который лежит в основе этих систем?

— Есть некоторый подход, основанный на работе сетей с большим количество уровней, которые некоторым таинственным образом умеют тренироваться. Мы вводим туда определенный объем информации, они каким-то образом его обрабатывают, выделяют в нем важные вещи и выдают некий новый продукт. Сам этот процесс обучения на самом деле достаточно мистический, потому что непонятно, как это происходит. Там, внутри, конечно, работают некоторые математические алгоритмы, в том числе оптимизационные и тому подобные, но как в целом устроен процесс, мы не понимаем.

Подробнее: <http://kommersant.ru/doc/2982428>",
"и вот сегодня обсуждали, что вроде идея фитить вероятности переходов между состояниями мц как классификацию с учетом какого-то набора фич в ""памяти"" в дополнение к состоянию лежит на поверхности и не противоречит markov assumption. но не нашли примеров в литературе. никто не знает?",
"Тут внезапно оказалось, что кинозал освободился, поэтому как предлагал <@U040HKJE7> будем проводить митап в мастер-слейв режиме: первая переговорка транслируется в кинозал. Регистрация снова временно открыта.",
"Как будто мы в tensorflow прям маленький мозг эмулируем и не знаем даже, что мы там делаем ",
"Хотел с gensim, это вроде как специализированная библиотека, поэтому должна лучше работать, по идее... + там удобная предварительная обработка текста",
достаточно принести в жертву кого нибудь у входа в здание,
Да ты прям ждёшь как скормить кого-нибудь охране,
Вопрос к практикам NLP)) кто-нибудь укладывал несколько bi-directional RNN слоев в стеки? Не понимаю как перейти от уровня слов к уровню предложений.,
"подскажите, как бы попроще симулировать систему частиц в 2d - частицы связаны между собой пружинами, хочется найти конечное состояние системы - координаты каждой частицы. то есть задача определить как их раскидает по плоскости. может есть что-то готовое? что бы задать начальные параметры и получить на выходе результат? (графическое отображение не нужно). я подзреваю похожим способом должны графы рисовать.",
"угу, хотелось бы избежать ручного моделирования. мне даже не принципиальна физическая точность. хочу примерно понять как точки раскиданы, если я знаю часть расстояний между ними",
"спасибо, я посмотрю статью. как-то сходу не понимаю как это может соотноситься",
"<@U04URBM8V>: спасибо, действительно похоже. даже закон Гука упоминают в вики. а не знаешь какие из тулзовин позволяют задавать длинну рёбер и потом забирать координаты?",
"Кто-нибудь знает, существуют ли в открытом доступе таблички с парами (марка по русски, марка по английски)?
Типа (айфон, iphone), (самсунг, samsung)? 

Пытался искать в гугле, но даже не знаю, как сформулировать запрос",
"Специально на бренды не проверял. Лицензия там у них у каждого relation'a своя, но как я понял, они там все открытые.",
"Материалы не знаю, а вот для конкретной задачи имеет смысл поиграться (помимо очевидных размеров сети) с тем как резать на батчи и как часто сбрасывать внутренее состояние (если stateful ячейки используешь).",
"т.е. это не как какой-то параметр выглядит, который тюнить надо, а просто как особенность, которую надо аккуратно реализовать",
"Да, пожалуй надо просто аккуратно реализовать. Из того что тюнить в таком случае -- размер окна остается (т.е. то, как часто в последовательности разливается ошибка)",
"может кому интересно будет как работает grammarly, сейчас про это рассказывают <https://www.periscope.tv/vitaliradchenko/1zqJVkndBnZxB>",
"<@U0M39M6LS>: сорян за глупый вопрос, я просто под самый конец включил, там вроде говорят про проверку орфографии, и про ГПУ, а как там это связано?",
"Stanford nlp может даты извлекать, только там правила скорее всего ручками надо будет прописать, или нагуглить, может кто уже сделал. Я для немецкого несколько простых правил писал, вроде ок работало",
а кто нибудь собирается на этот трешнячек <https://events.techdays.ru/machine-learning/2016-05/> ?,
а то я чот думаю сходить что ли,
"вчера <@U04BFDYPV> (вроде он) рассказал как в линейную регрессию встроить регуляризатор в ажуре, вылазит комбобокс а там regularization: low, medium, high",
<@U04BFDYPV>: это какой модели параметры такие?,
как залогиниться в ажур - это тема для отдельного митапа,
"а от меня ускользает, зачем его смотреть? по ссылке как будто способ понять, что градиент правильно реализован, но сейчас же почти везде его всякие theano/tensorflow сами выводят",
"т.е. если можно сразу сиквенс - ок. Но, как в реинфорсменте, такого нет. На каждый шаг будь добр запросить среду и получить новое состояние в зависимости от экшна.",
"update может обновлять параметры, на каждом шаге. а тебе как бы нужно граф весь составить, и по кускам его дергать.. не знаю как лучше объяснить.",
"abcde - это вход, x? o1, o2 - это типа y? а как скрытое состояние считается? оно зависит от входа и скрытого состояния, или выход тоже туда закольцован?",
"как o1..oN считать, чтоб на каждом шаге их среде скармливать по одному, а обратно получать состояние?",
"как все это делать, не считая все заново? т.е. как можно это сделать лучше, чем просто все заново скармливая каждый раз?",
а зачем тогда градиент по ходу дела считать?,
"тогда берем модель, предсказываем ей следующие шаги, полчаем ответы, в конце имеем всю цепочку и лосс, обновляем параметры как обычно",
"ты, наверное, объяснял уже, почему это не так)",
"у тебя какая функция внутри сидит? если это обычная rnn/lstm, то след. состояние зависит от текущего входа x_i, предыдущего скрытого состояния и всяких матриц весов, так?",
"чисто технически, как мне получить это 'o' на каждом шаге?",
у тебя - ничего) у тебя как будто по формуле от входа h не зависит,
"а какая разница, какой лосс?",
где o_vec - вектор выходов на каждом шаге,
"ага. а как лос считается все-таки? вот модель сказала, что на первом шаге был [0.5, 0.5], а на втором - [0.1, 0.9], а потом эпизод закончился - какой лосс?",
"<@U0FEJNBGQ>: пока конкретно не скажу, статьи завтра буду читать по теме. Очень предполагаю, что там как Роман говорит - все по намешано и garbage in - garbage out. 
Вообще в науке редко строят модели от десятков переменных сразу, обычно фиксируют все переменные (например, процентное содержание чего-то), кроме одной-двух и  их уже меняют. А так же в научных статьях встречаются такие вещи, как графики по трем точкам, которые еще потом сплайнами сглаживают :) и эксперименты для этих точек, которые повторяются в реальности один раз, а на бумаге трижды :)

В общем, кто хочет вместе вторую задачку делать на Azure - давайте объединятся, можно завтра вечером встретится предварительно.

Ставьте лайки, подписывайтесь на канал.",
смотрю как DL в бионформатике используется. нашел статью неплохую: <https://arxiv.org/ftp/arxiv/papers/1603/1603.06430.pdf>,
томита это лишь инструмент: это как pycharm: имея пайчарм код в нем не появится но его будет писать удобнее,
коллеги идиотский вопрос: в k-fold cv вы как фолды выбираете на текстовых данных? просто кусок вырезаете или случайно надергиваете предложений/токенов по всему датасету?,
на майских как раз ничего не было. это 23-24 апреля было,
"Гайз, затегайте кого-то кто делал что-то для FMCG, если знаете. Интересуют кейсы кто что делал, кроме совсем очевидных)",
как примеры - может каннибализационный анализ,
оо)) это кароч такая штука когда ты пытаешься предсказать падение спроса на старый продукт при выпуске нового. Например выпускаешь ты новый айфон и предсказываешь как упадет спрос на айфоны предыдущих версий,
как он в фмсг будет звучать как задача?,
"<@U040M0W0S>: на хабре о хакатоне MS была недельку-две назад дохлая статья. а вчера на хабре засветилась статья R+Azure, где также про хакатон говорилось <https://habrahabr.ru/post/301176/>",
"сорри, просто хорошо дополняет список слов) Ну вообще есть такая задача как планирование промо-акций, там екселем не обходится и потребность у ритейла есть. хз что такое FMCG и чем это от ритейла отличается",
"<@U07V1URT9>: ну не совсем уж с непривычным инструментом, многое там знакомо будет всем r/python-разработчикам (в статье выше, как раз про это)",
Ну так MBA как раз про кока-колу.,
компания кока-кола - это как раз FMCG,
потому что непонятно совсем какие там данные,
"<https://opendatascience.slack.com/archives/theory_and_practice/p1463736923001745>
а какие есть совсем очевидные кейсы? ну и каннибализацию там нужно оценивать не для гребаной кучи ашановских товаров, а для двух вариантов этикеток — кажется это такая больше гуманитарно-excel задача",
"В книге по фотошопу маргулис советует производить цветокоррекцию как минимум в 10 каналах сразу -  R,G,B, Lab и CMYK. Поэтому, наверняка, смысл в этом есть, потому что там разная информация содержится.",
"<@U04BFDYPV> <@U041LH06L> у людей в костюмах с запонками FMCG (fast moving consumerable goods) это просто один из типов товаров для ритейла. Отличается тем, что больше внимание уделяется supply chain management (картоха сгнила и все, пиздец). 

Чаще всего ритейлеры фокусируются и продают только fmcg (магазы типа пятерочки\азбуки вкуса, палатка с  арбузами), так что можно выделить таких ритейлеров в отдельное направление. Cоответственно, все кейсы из ритейла здесь применимы: 
-анализ корзин
-всевозможные сегментации клиентов (как по корзинам, так и времени\намерению)
-next best offer (да и карты лояльности и прочие штуки)
-общая ценовая оптимизация (правка цен с упором на маржу и похожести\взаимозаменимости товаров)
-общая оптимизация раскладки в магазинах (включая пути людей, но это оверкилл и не работает)
-аналитика бренда на основе инфы в сети (только для совсем крупных)
-supply chain оптимизация (предсказывать когда картоха кончится, чтобы экономить на доставке и не терять предложение)

ЗЫ <@U10JSP4NT> чем больше русских слов и общая читаемость, тем выше вероятность что ктото запарится и ответит :slightly_smiling_face:",
"я тут задумался, что сейчас лучше для распознавания лиц: каскад Хаара или CNN. Тут видно, что сетка работает быстрее чем каскад <https://www.youtube.com/watch?v=Ad6GxIR8EpU>

Но как обстоят дела с процессорами. Допустим, я хочу, чтобы это заработало на малинке с `1.2GHz 64-bit quad-core ARM Cortex-A53 CPU` и 1gb ОЗУ",
А где конкретно генхак сейчас? ,
"Осталось понять, где корм не для быдла, видимо",
"Disclaimer: я сам быдло третьего порядка. Т.е. считаю быдлом тех, кто считает быдлом быдло.",
"коллеги а не помните в алекснете какие нибудь средние вычитатются? на сколько я помню просто с каждого канала 127 и порядок RGB, а не как в vgg?",
"Средние вычитаются. Порядок не помню, как натренировано.",
тоже блин порядок не русский какой то,
чот давно не выпивали -),
"<@U10JSP4NT>: я работал в марсе на позиции statistician. В основном занимался производственной статистикой: планы контроля придумывал, контрольные карты внедрял, процессы  и продукты оптимизировал  с помощью планирования эксперимнта(так как там еще больших данных нет, а эксперименты как правило все дорогие) и просто консультировал коллег, когда им нужна была помощь в анализе данных. Но это скорее больше связано не конкретно с fmcg, а со статистикой на производстве.",
"наивный вопрос, уверен <@U04422XJL> знает
есть данные (кому интересно - destinations из expedia), коих 62к*150. tsne на более чем 10k точек считается дико долго и с гигантским потреблением памяти, поэтому я посчитал tsne по кускам: на первых 10k, вторых, третьих и т.д. естественно данные перетасовал, а также додумался их не выбеливать перед tsne 

вопрос: как можно склеить 6 разных tsne в одно большое? пересечения по семплам не делал",
"просто можно же ближайших соседей надергать, многие из которых будут почти точными копиями, а потом можно явный поворот с растяжением одного tsne в другое сделать. 

но я похода закончу эти игры в tsne и запилю свои сатанографы :slightly_smiling_face:
<@U0FEJNBGQ>: эти 150 фич это их семантический майнер, который в обзорах искал специальные  свойства места (типа у пляжа, в историческом центре и т.д.) и это оценка (лог)вероятностей этих разных свойств. тоесть сперва парсили и вытаскивали семантику, потом оценивали вероятность каждого высокоуровневого свойства
если посмотреть на распределения строк (!!!) этого файла, то видно что это однородное плато (скорее всего делали нормализацию на число точек в дестинейшене), из которого вылезает как правило несколько пиков и еще с десяток пиков поменьше. так что я тут буду nlp-подобные махинации скорее крутить. замечу, что tsne посчитался красиво",
"&gt; можно явный поворот с растяжением одного tsne в другое сделать.
А почему ты считаешь, что 2 разные укладки можно совместить афинным преобразованием? Насколько я знаю -- нельзя",
"вообще да, скорее всего полноценное tsne не выйдет так как видно кучу улетающих вдаль точек, кому не нашлось соседей в одной подвыборке, но могли найтись в другой",
"Знает ли кто нибудь, существует реализация t-sne в виде out-of-core алгоритма?",
"коллеги, а в лазане кто нибудь запускал инсептрон_в3 из этого файлика <https://github.com/Lasagne/Recipes/blob/master/modelzoo/inception_v3.py>? чот выполняется долго и очевидно не выполнится, ну может конечно когда нибудь закончится, но не сегодня; судя по процу, комп начинает что то люто вычислять, но на гпу как будто бы ничего не происходит (теано конфиг и все такое норм вгг и алекснет летают)",
"стало быстрее, но все равно что то как то долго совсем",
"как нужно взвесить классы, чтобы оптимизировать средний IoU?",
"если давать маленьким классам большой лосс, то у них выходит много FP, и поэтому несоизмеримо больший U в IoU. Я до этого оптимизировал именно accuracy, а теперь хочу сравниться с  бенчмарком, где метрика - средний IoU.",
"Коллеги, кто в Питере, посмотрите мой анонс в <#C0GQGF2V8> ; вообще, было бы интересно познакомиться. :slightly_smiling_face:",
"Обратите внимание на доклад во вторник чувака из гугла про ~нейронный xgboost~ глубокий бустинг. Как бустить сверточные сетки в докладе не расскажут, но будет презентован алгоритм, который может бустить кое-что посложнее чем деревья высоты 5, и при этом не переобучаться. Трансляция и запись прилагаются.",
"<@U0SBLSTJ4>: как минимум part time, статья вроде аффилирована с google research (<http://research.google.com/pubs/pub42856.html>)",
"Научите, пожалуйста, как зарегистрироваться на Moscow Data Science Meetup с уже закрытой регистрацией? :slightly_smiling_face: Что-то я проспал.",
Кто еще завтра на Deep Boosting идет?,
На завтрашнюю встречу еще можно как то попасть если не зарегался ?,
"<@U0XF4GAM8>: Они не любят когда приходят без записи, но это же не северная корея (пока) скорее всего пропустят",
"Всем привет, стандартный вопрос, кто завтра заглянет на Data Science Завтрак?",
где же будет дип бустинг ,
похоже не увидим кто такой бармалей,
"Надо что бы больше людей ставили реальные фотки, тогда хоть будем знать кто вопросы задает :putin:",
"<@U1AUMFHMH>: можно чуть подробнее рассказать про мероприятие? На кого оно рассчитано? Какое там расписание будет хотя бы примерное?  Решиться потратить выходные почти полностью - не шутка дело, а мероприятие вдруг стоящее.",
"План собрать людей кто занимается Deep Learning в стране (ну и публикуется), и представителей компаний, которым это будет интересно.  Это будет два дня часов по 6 каждый день плюс обсуждение. ",
А записи будут? А то я как раз на этот weekend уезжаю,
"Публикуются у нас пока не так много, как хотелось",
"а когда соберутся те, кто занимается Deep Learning и публикуются и представители компаний, то что они два дня буду будут делать?",
Это не вопрос был. В чём причина и как лечить я примерно представляю. Просто впервые такую жесть вижу.,
"А, я наверно немного неточно написал. Есть обучающая выборка, где есть пол и возраст. И есть тестовая - без них",
"Гайз'н Гелз! В курсе ВШЭ рассказывали, что категориальные признаки иногда модно кодировать ""Счетчиками"": например в задаче бинарной классификации вместо каждого значения признака берется вероятность первого класса при условии этого значения. Еще там кратко упоминали, что вещь хорошая, но может приводить к переобучению. Вопрос! Где почитать про это подробнее, и как это кодирование правильно назыв. по англ?",
"параллельно вопрос, а как правильно нормировать эти фичи, если задача multiclass classification",
"Я кажется нашел заметку от майкрософта, где они называют это ""learning with counts"": <https://blogs.technet.microsoft.com/machinelearning/2015/02/17/big-learning-made-easy-with-counts/>",
"Там так же написано про переобучение и как с ним бороться. Правда на кагле обычно бьют данные на фолды и считают Mean target out-of-fold (например, как тут: <https://www.kaggle.com/rsakata/bnp-paribas-cardif-claims-management/xgboost-with-combination-of-factors>)",
"А как обычный счётчик может переобучиться? :thinking_face:
Только если какая-то категория встречается ровно N раз и никакая другая столько раз не встречается, и все наблюдения в этой категории относятся к одному классу?",
"Как может переобучится я сам пока не разобрался, но вот здесь в последнем пункте вроде бы  рассказывается об этом и как в связи с этим строить CV: <https://dato.com/learn/userguide/feature-engineering/count_featurizer.html>",
<@U0DA4J82H> что то получаю какой то файл <http://barreca.ps|barreca.ps>,
"<@U0ZL0PY6S>: в комментариях к скрипту автор скрипта пишет, как и почему может произойти переобучение",
<@U0DA4J82H>: К какому именно скрипту?.,
"народ, а как дела в Екатеринбурге с ДС/МЛ, может помитапим?",
"А то я вроде как свои best practices более-менее выработал, но всё равно каждый раз при добавлении фичей/моделей/параметров всё превращается в бардак и я начинаю путаться.",
В ближайшие пару дней как раз хочу посмотреть как быстро оно работает на Титане и на 730GT,
"<@U07V1URT9>: расскажи, как посмотришь, интересно",
"у нас тут щас выступал <https://new.vk.com/oulenspiegel> он автор программы SmartThink для шахмат, третья в России прога для шахматного ИИ

в общем он говорит говно ваш Deep RL и DQN в частности, типа его программа круче и порвет всех

чо может позвать его в слак и ~~затралить~~ (кароче я не знаю как зачеркнуть), в смысле устроить батл",
"А можете посоветовать с  какой стороны подойти к решению такой задачи:
Происходит диалог клиента и оператора, клиент задает вопросы, оператор отвечает. Хочется оператору после каждого сообщения клиента предлагать несколько вариантов ответов, по одному из которых он может кликнуть и сразу отправить. То есть нет обязательного требования чтобы система полностью сама вела диалог с пользователем (но сгенереные ответы будут удовлетворять ему -- круто). Слышал, что одним из ключевых слов в таких вещах является intention, прочитал пейпер attention with intention (<https://arxiv.org/abs/1510.08565>), слышал про Dynamic Memory Networks и подобное. Как это все вообще работает на русском языке, с чего можно начать?

Если рассматривать не дип лернинг, то в голову пришло такие мысли (данных достаточно -- 350к диалогов): 
1. По прилетевшему сообщению пользователя искать ближайшее в трейн датасете и смотреть на ответ, в качестве ембеддинга сообщения можно для бейзлайна взять тфидф какойнибудь (но тогда не будет учитываться контекст диалога).
2. Можно сделать аккуратную кластеризацию сообщений/диалогов и каждому кластеру приписать ответ.

Буду благодарен за советы=) Самому кажется, что дип лернинг должен круто сработать, но до сих пор не видел каких-то адекватных результатов его использования на русском языке",
"1.А какая разметка поможет? POS, NER?
2. Как можно профиль для сообщения получать по векторам слов в сообщении? Усреднять?
------
И что можно попробовать, если не быстро и время есть? Хочется генерить хорошие ответы",
"1. Тематика вопроса, грубо говоря какие пункты инструкции оператор вынужден был применять. Возможно некотрые NE -- ключевые слова, по которым оператор делает выводы.
Что-нибудь вроде `У меня стоит &lt;osname&gt;Ubuntu 14.04&lt;/osname&gt;`. Сам шаблон в тему учиться генерить сеткой, а пропуски заполнять из БД. 
Недавно была статья на этот счет, поищу и скину.
2. Да, просто усреднять, или взвешенно исходя из дополнительных соображений.
3. Попробовать `seq2seq` модельки для работы с самим текстом. Но вот как делать чтобы ответы были _полезными_ -- самому интересно)",
"Добрый день. Возникло желание написать систему, помогающую в выборе статей из архива (<http://arxiv.org|arxiv.org>). Для начала хочется относить статьи к соотвествующим меткам, даже если эти метки не были упомянуты в keywords: например black holes, disk accretion etc. Предположим, что набор меток есть, но нет времени вручную размечать статьи для классифицирующей модели. Как лучше всего разметить статьи в таком случае? Я уже попробовал сделать несложную реализацию через fuzzy string matching меток на текстах, а также пробовал использовать TF-IDF и topic extraction через NMF. Меня не покидает ощущение, что эту задачу возможно решить лучше и более точно. Какие подходы к этой задаче можете рекомендовать?",
"как пример того, что уже есть в этом духе",
"мне актуальна как раз другая область: астрофизика. Заодно хочется не пропускать полезные статьи из области физики высоких энергий, и из других смежных областей",
"Да, спасибо за наводку. Сейчас посмотрел исходники: там SVM + TF-IDF, и это похоже работает для каждого пользователя, в случае уже достаточного числа отмеченных пользователем статей. В такой постановке все достаточно понятно, но я сейчас задался подзадачей в немного другой постановке: как разметить тексты в соотвествии с готовым набором меток? Если у читателя еще нет текстов, которые он отметил, хочется сделать разметку статей по его интересам, чтобы началась формироваться эта база статей, на которой можно будет тренировать SVM. Кстати, что приятно, в исходниках есть работа с рисунками, но к сожалению там нет работы с ссылками на другие статьи, а это может быть важно.",
Может быть кто-нибудь знает где найти датасет по моделям автомобилей и их параметрам? Типа mpg dataset в R только посвежее (годов этак 2000х) и побольше.,
"Хей, напоминаю про субботнюю тренировку по машинному обучению. Keynote speakers: <@U054DU76Y> (BNP Paribas), <@U183AU5TL> (Telestra), <@U14L5TKNJ> (Avito Cars). 

В этот раз тренировка будет транслироваться не перископом в салфетнице, а качественно — как другие мероприятия Яндекса. Более того, можно будет задавать вопросы докладчикам, для этого будет сделан специальный канал в слаке — он будет транслироваться на экран в зале.

Тем кто собирается прийти ногами — обязательно зарегистрируйтесь, желательно до пятницы 16:00: <https://events.yandex.ru/surveys/3315/>
Приходить к 11:50 в офис яндекса, БЦ Морозов, четвертый подъезд — рядом со старбаксом, мероприятие будет в переговорке “Синий кит” на 5м этаже.

Кто не в Москве, или просто не удержал себя в руках после Moscow Data Science Meetup — присоединяйтесь к трансляции:
<https://youtu.be/1HrkBzLBJQg>",
"<@U1B2L9G58>: а можешь подробнее описать, какую версию ты использовал и для какой задачи?",
"<@U0DA4J82H>: а зачем датафреймы в спарке, если там sql есть?",
кстати с dask я так и не понял как его правильно сконвертить в pandas,
"Да, пока не понял, как работать со спарком in-memory. Выглядит так, что альтернативы pandas, numpy, sklearn просто нет",
не знаю как им делать предобработку на 10+ гигабайт,
"Я вот иногда монгу использую. Просто сделал docket pull и все, готово. Наигрался и удалил образ, как будто и не было его. Вот так же хочу, чтобы система не засорялась",
"df.cache? И поверх него можно SQL гонять? Попробую! In-memory - когда любые запросы обрабатываются строго в оперативке, как в pandas и numpy, например",
"там еще варианты есть как закэшировать, с сериализацией/компрессией или нет, но это уже вглубь",
"коллеги, кто не сможет прийти на митап, приходите на датафест в <http://www.amigomigel.ru/>, там зарезервированный стол ждет нас на кучу людей с 21-30",
"в 21-00 закончится митап, и в 21-20 где то двинем туда",
"<@U1B2L9G58>: дурацкий вопрос, сходу не увидел в доке. а cache надо делать до того, как сделал read.json или после?",
"<@U0G29N5U4> когда ты делаешь read.json, ничего не происходит",
"когда ты вызовешь первый action, тогда все случится",
в сниппете вверху кэширование случится когда с будет вызов toPandas,
"кстати, withColumnRenamed же не поддерживает атрибуты вложенных json? как я в sql c точкой юзаю",
"вообще с именами колонок как то криво, я все время парюсь",
"я  тоже больше к SQL тяготею. у меня спарк 1.6.1 там синтаксис как в хайве, надо обновиться до 2.0, там вроде ANSI compatible уже",
"Как ни зайдёшь — очередное сырое поделие и плач о том, что оно не работает.",
а как такая штука дифференцируется?,
"гм, забыл. Это для бекпропа нужно?
Тогда пойдем от самой проблемы. Сетка должна предсказывать события, причем несколько событий может быть одновременно. Это была просто идея закодировать события
Тогда как лучше сделать? Можно повесить на выход softmax и брать события как совершенные начиная с какой-то вероятности?",
"А подскажите, пожалуйста, по поводу дифференцирования: из какого класса должны быть функции?",
"как я понял, функция должна быть непрерывной и иметь конечную слабую производную почти всюду",
<@U07KUB3NG>: мы с <@U04URBM8V> видим как ты пишешь сообщение :slightly_smiling_face:,
"я думаю, принимаются вопросы еще от тех, кто смотрит трансляцию не в мейле :slightly_smiling_face:",
А с какого ресепшена отправляются толпа в мигель? ,
"<@U0YTHMN8K>: а какое распределение на выходе у encoder'а? Если там обычное нормальное, то как оно может быть похоже на смесь?",
<@U0YTHMN8K>: интересно! а как ты параметризуешь GMM и как выписываешь KL-divergence для GMM? Я что-то пока не видел чтоб хоть кто-то что-то подобное делал,
"<@U04DXFZ2G>: В оригинальной статье авторы дают две формы для variational lower bound: <https://arxiv.org/abs/1312.6114>. Одна с аналитическим расчётом KL-divergence, другая предполагает сэмплирование. Так как аналитическое решение KL-divergence между гауссианой и GMM я не нашёл и не смог рассчитать самостоятельно, то решил попробовать вариант с сэмплированием. Тем не менее пришлось-таки формулы на бумаге пописать немного. Раз пять перепроверял, но всё равно нужного результата не получаю.",
"А расскажите, какие ключевые пейперы надо про VAE в deep learning прочитать?",
"<@U0J1U64FK> извините, не знал про <#C0804BS5Q>, ну и не так уж наверное страшно когда много каналов",
"В скором времени начну заниматься научной работой в области диплернинга (предположительно буду изобретать что-то в области чатботов на нейросетях), пока по совету руководителя накачиваюсь всякими статейками и играюсь с тензорфлоу, есть некоторый опыт в классическом лернинге. Взял себе с потолка следующую задачку: есть словарь английских слов, весьма толстый, почти 7мб, надо научиться предсказывать принадлежность последовательности символов к этому словарю (определить, является ли слово валидным словом английского языка). Тесткейсы брал вот такого вида, с недавнего челленджа от Hola: <https://hola.org/challenges/word_classifier/testcase/1730486668> , датасет сгенерировал смерджив словарь и 3500 тесткейсов, выкинул дубликаты.  Пробую классифицировать следующим образом - представляю слово в виде последовательности (максимальная длина - 45), делаю паддинг нулями, эмбеддинг, LSTM-слой на 128 ячеек, Dense собирающий выходы в 2 переменные (по классам).  Использую early stopping и learning rate annealing (рублю lr в 10 раз если не улучшился loss на валидационном наборе).  Классы в тренировочном и тестовом наборе сбалансировал. Получаю такие  метрики: <http://pastebin.com/BqPr7624> Как видно из confusion matrix классификатор слишком толерантен и все подряд называет валидным словом. Какие я могу шаги предпринять, чтобы повысить precision не испортив recall? Какие слова стоит погуглить?",
"У тебя же там logistic loss, как всегда?",
"<@U0AD1L5NC> на конфузии для train не смотрел, но лосс там низкий (с одной стороны это вроде должно намекать на оверфиттинг, но я вроде как в каком-то смысле его и хочу добиться, ведь в тесткейсах нет хороших слов не входящих в данный словарь).",
"<@U1BAKQH2M>: я не понял, почему ты хочешь оверфиттинга",
"<@U04ELQZAU>: не понимаю, как тот факт, что KL рассчитывается между q(z|x) и p(z), должен влиять на распределение энкодера. Какой смысл размазывать q(z|x) по двум гауссианам, если изначально хочется, чтоб часть данных соответствовала одной из гауссиан, а часть - другой?",
"но это как раз про то, что Сапунов рассказывал пару месяцев назад на семинаре в яндексе про RNN для мниста, где делался попиксельный обход картинки",
"<@U0YTHMN8K>: ну так q(z|x) и есть энкодер же! Я, кстати, не говорил, что распределение ""размажется"" – это зависит от того, какую KL-дивергенцию минимизировать – прямую или обратную (но я не помню точно, что в каком случае случае происходит)",
"В той статье, что я в самом начале кидал про adversarial autoencoders, авторы как раз неплохо демонстрируют мои мысли на их примере VAE.",
"Что значит наблюдения не согласуются с prior'ом? Ведь пространство z и распределение на на нём - это не нечто реально существующее. Мы сами его выбираем и потом пытаемся ""загнать"" данные в эту модель. Но я, возможно, понял мысль. Возможно, данные трудно трансформировать из x в z. Но так как нейросетки - это универсальный аппроксиматор/трансформатор, теоретических проблем тут быть не должно.",
"<@U075SR8JX>: <http://www.thefunctionalart.com/2016/05/visualizing-chess.html> 
Сразу вспоминается визуализация в PoI когда машина стратегии считала ))",
"мне без разницы, как канал будет называться, главное, чтобы там было много людей, и чтобы он был активным",
"как только станет понятно, что действительно, про spark так много говорят, что это мешает говорить о других темах, тогда да, можно выводить в отдельный канал",
"это и для новичков, я думаю, полезно, что все в одном месте, ты не знаешь, что тебя может заинтересовать.
Я сужу также по другим сообществам, где все сразу дробились так, что все мертвым становилось",
"Зачем забивать, когда можно удалить лишнюю фичу.",
А как регуляризация связана с мультиколлинеарностью?,
"<@U0XR20SA1>:  а как определять коэффициент дропаута, если я вообще не знаю, какой у меня оптимальный размер сети",
"Ну вот с точки зрения теории информации KL(p || q) = среднее количество бит, теряемых при замене p на q. Но я не понимаю, как из этого делать какие-то выводы",
"Но MSE симметричен по обоим аргументам, поэтому нет вопроса, как его использовать",
"&gt;&gt;Но MSE симметричен по обоим аргументам, поэтому нет вопроса, как его использовать

ничего не мешает сделать из КЛ тру расстояние и оптимизировать его",
"вроде как d(p, q) = KL(p || q) + KL(q || p)",
"Это, как я понимаю, если мы в качестве априорной оценки берем производную от эмпирической CDF. У меня другая оценка...",
"А кто может подсказать какие-нибудь интересные данные по поводу churn prediction, не обязательно в телекоме? В сети кроме того искусственного, что лежит на <http://bigml.com|bigml.com> толком ничего и нет. Хотелось бы неанонимизированный датасет, чтобы можно было какой-то анализ провести. Пусть данных не очень много, нужен с образовательной целью. ",
"Можно твикать learning rate и то, как он убывает, можно делать batch norm, твикать регуляризацию и другие методы оптимизации (nesterov momentum, RMSProp, все эти дела)",
А к проценту дропаута нужно относиться как к гиперпараметру,
"<@U1BAKQH2M>: ""+-500 секунд на эпоху""
Когда я стал ждать больше суток на эпоху, тогда стал думать об ускорении :slightly_smiling_face: Уточню, у тебя 1.500.000 последовательностей? И какая максимальная длина?",
"<@U0AD1L5NC>:  ""А к проценту дропаута нужно относиться как к гиперпараметру""
Есть какие-то рекомендации кудае его менять от 50%, уменьшать или увеличивать?",
"а смысл, если в каждом конкретном случае churn будет зависеть от специфических именно для этой области фич?
рисеч на этом не построить, а просто модельно поиграться - так и самому можно обозвать как угодно",
А как этот EASGD работает?,
"Ну изначально хотелось сделать какие-то  заключения о том, какие пользовательские страты больше подвержены к churn, посмотреть на стат значимость отличий признаков в группах ушедших и не ушедших пользователей — все это в образовательных целях, «поиграться» с реальными данными. А если у тебя все столбцы VAR1, VAR2, …  захватывающих выводов отсюда особо не сделаешь",
"DOWNPOUR, с которым они сравнивают, вроде как раз по статье из Гугла",
*Всем привет. Кто какие методы выделения из неразмеченных текстов ключевых слов/фраз (тегов) знает?*,
"Я пользуюсь в основном LDA, но может кто еще с чем сталкивался.",
Тема для отличной статьи на <http://ods.ai|ods.ai>. У кого есть желание взяться и написать? Наш теплый коллектив поможет.,
"стратегий много, всё зависит от того, какие данные подаются на вход",
"<@U0M39M6LS>: я когда-то искал, там очень много всего было, искать это надо по запросам с high cardinality (иначе какой смысл что-то выдумывать), чтобы прямо модули были не знаю, там обычно простые техники
ещё всяких статистических штук много <http://patsy.readthedocs.io/en/latest/categorical-coding.html>",
Про это же <@U054DU76Y> рассказывал на тренировке в субботу. Про то как правильно такой счетчик сделать и не переобучиться на нем. Можно посмотреть в записи трансляцию.,
А где можно найти записи с этих ваших тренировок? ,
<@U064DRUF4>: задача как раз таки - уйти от регекспов,
"ну и можно без них совсем, но если что-то можно описать ими, то почему бы этого не сделать",
"<https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb> - вот тут пример для NER, где места/людей/… выделяют, с брендом/моделью/весом точно так же можно, только фичи другие",
"Ребят, не знаете куда задание тестовое для хакатона Сбера отправлять?",
"Как всегда, буду рад вопросам и предложениям где описать подробнее",
"коллеги, странный запрос.
с чего вы начинаете стат.анализ данных? какой препроцессинг делаете, что смотрите?

я больше про данные опросников/тестов и тому подобное, правда, но и прочие данные тоже интересно",
"Кто-нибудь знает, где можно найти график результатов конкурса ImageNet по годам?",
"хороший вопрос.
как минимум они не совпадают с моими ожиданиями :smiley:
могу позже скинуть - показать",
"это же как раз первый раз, когда нейронки победили всё остальное, IIUC",
последний же вроде как раз 15,
"Всем привет. Есть вопрос к лингвистам. Как следует корректно называть слова, на которые разделен текст? То есть, я так понимаю, что слово это часть языка, в тексте мы встречаем конкретные реализации слов.",
"Как это еще машинисты со своим PCA, tSNE не набежали...",
"Запускаем LSTM, смотрим какая accuracy!",
"Ага, прям как завещал Норвиг (и докладчик из Касперского на митапе в пятницу): Keep It Simple, Stupid.",
"<@U13E1AWCX> я не очень понял, что именно ты хочешь, но возможно тебе поможет гугление таких слов как лемма, лексема, словоформа и словоупотребление",
"Кто то на GPU Computing Meetup #6 идет ?
<http://www.meetup.com/GPU-computing-Moscow/events/231190254/>",
"привет всем, есть вопрос на посоветовать куда копнуть:

Стоит задача сопоставления двух датасетов. Грубо говоря есть два списка транзакций со стороны разных систем, покупатели и продавцы. Каждая транзакция содержит id-шники сторон, идентификатор сделки, дату и сумму. ID контрагентов сквозные,  ID-сделки могут быть кривоватые (не совпадать для одной сделки в разных системах), даты должны совпадать и суммы тоже (но в теории тоже могут быть ошибки к примеру в сумме на какие-то копейки). Требуется провести сопоставление данных из этих датасетов и указать на совпадения и расхождения (не нашли совпадения или нашли но с оговорками). Сделать это нужно максимально эффективно с точки зрения скорости сопоставления.

Конечно можно решать в лоб, разделить датасеты на чанки, раздать воркерам и поехали. Но сдается мне что кто-то подобное из трейдинга уже делал",
" Зависит от данных, но если речь о финансах и трейдинге, то задача не из простых. Когда я в банке работал у нас несколько команд этим занималось, и работы было край непаханый. В основном это были написанные аналитиками от руки правила, и так примерно в лоб и решали",
"Там как раз со скоростью всё очень хорошо, сопоставление ищется за константное время. Но нужно построить хороший хеш.",
"попробуйте каким-то образом быстро искать несколько похожих записей к входной записи, после чего построить какие-то метрики близости записей (уже более сложные) и обучить классификатор на определение идентичности записей по этим метрикам
также можно строить чанки искусственно по некоторым факторам (например, разница между id одного типа)
да, какие объемы количества транзакций?",
<@U04422XJL>: а кто за 5vision участвовал?,
"можно попробовать взять популярные сплиты у random forest и добавить их как фичи в лин. модель 
но я о таком только слышал, не факт что это поможет",
<@U0AF2AZCM>: к какому из вариантов это относится?,
"rulefit использует набор ""правил"", где каждое правило фактически пришёл ли пример в конкретный лист дерева.",
"а кто говорит про проблемы? я говорил, что на практике реализация rulefit которую я использовал не всегда давала улучшение в качестве модели",
"ну да, просто из описания лично мне казалось что должно как минимум всегда не хуже работать. но нет.",
"<@U04URBM8V>: а как часто они проходят? просто у меня завтра экзамен в это же время, поэтому я не попаду :disappointed:",
"<@U04URBM8V> вы там когда собираетесь? Я как раз буду мимо проходить утром, до 11 часов время есть, могу позадалбывать людей вопросами",
"“А еще наша мечта – создать настоящее российское Big Data сообщество. “

в общем наше их чот не устроило",
"как второго орга зовут, не помню",
"<@U06J1LG1M>:  зачем ты туда поедешь, лишний полтос, или в маркетинг решил податься?",
"<@U0AD1L5NC>: ну вообще говоря это как-то мало, я хотел начать разговор на тему дальнейшего расширения. Например хотелось бы понять, как вообще подбирать архитектуру (какое влияние оказывает размер слоя, нужен ли мне второй слой?) Нашел статью где обсуждается что-то подобное <https://arxiv.org/pdf/1506.02078.pdf> Но из нее сложно сделать какие-то заключения. Есть какие-нибудь rule of thumb на эту тему? Исходя из разнообразияч всяких архитектур встречающихся в статьях рискну предположить, что нет",
"Всем привет.
В качестве курсовой пытаюсь разобраться в Machine Learning на примере обнаружения плохого и хорошего траффика при DDoS.
Опыта в ml мало, времени до сдачи еще меньше, но есть опыт в python и какая никакая математическая база.
За пару дней вроде в чем-то разобрался и что-то получил, но хотелось бы улучшить результаты.
Был бы рад получить пару советов о том, как это сделать, т.к. в условиях нехватки времени дельный совет бы не помешал.

Ниже излагаю кратко суть задачи и текущие результаты.

За основу для начала взял CSIC 2010 dataset <http://www.isi.csic.es/dataset/> где есть norm, anom и norm_test запросы
Хочу на нем получить более менее приличные результаты, а потом использовать обученную модель на каких-либо прикладных задачах (есть пара логов небольшого ддоса сервиса, +есть настроенные машины и сеть для имитации ддоса по локальной сети)
Как я понимаю мне нужен Anomaly Detection и Unsupervised Learning.
На основе публикаций из интернета и своих предположений выбрал 14 фич:
features = [""len_of_request"", ""digits"", ""letters"", ""spaces"", ""other_symbols"",
            ""method"", ""content-length"", ""content-type"", ""host_len"", ""payload_len"",
            ""requests_per_session"", ""user_agent_len"", ""protocol"", ""accept_language""]
Там где слово len используется длина, в остальных случаях заменяю на численные значения 0,1,2 и т.д. Исключение: requests per session - просто подсчет количества запросов с той же сессией.
По идее из них надо выбрать самые значимые, как я понял для этого можно использовать либо тупо перебор наборами по K штук,
либо метрики типа chi2 и SelectKBest Либо RFE / RFECV - пока не до конца научился запускать их на нужном алгоритме, по сути это готовая обертка для перебора с кросс-валидацией как я понял.
С помощью DecisionTree делал RFECV, получал графики, оптимальное количество и ранжирование фич, но как я понимаю для RFECV надо использовать алгоритм максимально близкий к используемому, разные алгоритмы по-разному выбирают.
Пока использовал на 14 фичах - подумаю еще по поводу выбора оптимальных. Если убрать часть по идее результаты должны немного улучшиться.
Для начала попробовал Supervised, из перебранных результатов пока KNN вроде неплохо себя ведет по сравнению с остальными, но дает только 0.65 примерно:
[ 0.65764698  0.66719592  0.65769171  0.65272715  0.63841492] (на кросс валидации)
Хоть supervised и можно посмотреть для оценки насколько хороши features, все-таки надо делать anomaly detection
Взял SVM из примера с сайта scikit
clf = svm.OneClassSVM(nu=0.1, kernel=""rbf"", gamma=0.1)
Получилось тоже довольно много ошибок, особенно для anom - больше половины.
error train: 24711/104001 ; errors novel regular: 26841/104001 ; errors novel abnormal: 66079/119586
(0.23760348458187902, 0.25808405688406844, 0.5525646814844547)
Как я понимаю параметр gamma и nu надо еще перебирать, желательно рандомно в каком-нибудь интервале(гляну документацию) и выбрать лучший результат.
Из других моделей для anomaly detection пока нашел только IsolationForest - но его не было в сборке anaconda.
Скачал scikit из репозитория и собрал из исходников - там он есть, чуть позже попробую.

Хотелось бы получить советы по поводу пунктов выше - как побыстрее улучшить результаты, как лучше выбрать фичи из трех озвученных способов и какие алгоритмы должны лучше подойти к этой задаче.

************
Доп ссылки:

Если нужен пример публикации по набору CSIC 2010 dataset: <http://gicap.ubu.es/publications/2015/PDF/2015_c05_Neural_Analisis_of.pdf>
Список фич и результаты на странице 6.

На stackoverflow для anomaly detection предлагают еще h2o, думаю его еще попробовать если успею, пока только в scikit хоть как-то разобрался :
<http://datascience.stackexchange.com/questions/6547/open-source-anomaly-detection-in-python>",
"<@U0JCGHU9H>: хм, на проде в онлайне приходит объект, для него считаются фичи, и что дальше? как здесь обойтись без 2gb модели?",
"<@U0JCGHU9H>: хм, интересная идея, правда, много вопросов сразу возникает.. например, ошибки первой модели передаются второй, а к ним прибавятся ошибки второй модели, или, например, не очень понятно, почему вторая модель должна быть меньше первой...",
"<@U0JJ69UB1>: как выложите, скиньте ссылочку обязательно, интересно почитать...",
"Задача - заполнить фрейм с известной структурой, чтобы выполнить необходимое действие в рамках ограниченной предметной области (домена).
На входе фраза на заранее известную тематику с выражением просьбы сделать то-то с такими-то атрибутами и условиями, а потом сделать с полученным еще что-нибудь.
Основная проблема - устранение неоднозначностей - решить куда в целевом фрейма подставить таки, например слово “сумма” (трактовать как атрибут “сумма” сущности или как просьбу просуммировать результаты) и т.п.
Пространство решений неоднозначности - все места фрейма, куда можно подставить слово. 

Фраза прежде всего обрабатывается и слова приводятся в нормальную форму и заменяются на семантические токены по словарю синонимов. Но это не суть важно.
Есть пара придуманных, но еще не провалидированных гипотез, что оценивать качество полученной интерпретации (графа с подставленными значениями-словами) можно просто посчитав некие характеристики самого графа: локальную связность(насколько узлы хорошо кучкуются), соответствие порядка слова в графе (вводится атрибут) порядку слов в предложении. Может придумаю какие-то еще алгоритмы.
Соответственно качество оценки помогло бы приподнять наличие синтаксических связей между словами, т.к. и структура фрейма и синтаксические связи вроде бы отражают смысл и можно ожидать, что они будут похожи",
"<@U0XR20SA1>: Я вот услышал про Bidirectional LSTM (один слой смотрит на последовательность в прямом порядке, другой в обратном), набросал эту штуку на keras и пробую ее. Вроде как сходиться начало быстрее, но учиться ей осталось примерно часов 11 еще, так что наверное рано выводы делать",
"<@U1BAKQH2M>:  Я не смог получить лучшего качества с Bidirectional LSTM, возможно есть задачи, где лучше работает. Больше слоев дает больше прироста в пересчета на слой",
"<@U0XR20SA1>: у меня последовательности сравнительно короткие - максимум в 45, в среднем еще меньше. BLSTM уже дало результат чувствительно лучше однослойной, а оно все еще в процессе обучения. Конечно попробую и просто дополнительный слой, как этот вариант доучу",
xgb как минимум может использовать несколько потоков,
"впрочем, как мне казалось, даже в 1 поток xgb обучался быстрее, чем бустинг в sklearn.",
"Хм, а какой смысл в нетренирующемся реккурентном слое?...",
"Теоретически, можно сделать другую какую-то сеть, которой дать на вход новую и прошлую картинку и ключевые точки дать как один из каналов",
"Возможно, можно рассматривать как детекшен на видео, хм...",
Свежие FractalNet же как раз про это,
"А шо такое бигдата? Шо то до сих пор не пойму. Говорят, это когда данные в ОЗУ не лезут, так у меня на моем 486SX много шо не лезет, и все бихдата?",
"коллеги, знаете ли вы какие-н метрики для задачи иерархической классификации? как такие задачи обычно решают и оценивают: максимум лог-лосса на каждом уровне иерархии и взвешенная их комбинация или есть более подходящие и строгие методы?",
"<@U04423D74>: не, классификации :slightly_smiling_face: это когда у тебя классов не только много, но и они както осмысленно сгруппированы

<@U049HDR2Z>: можно брать комбинацию ошибок на разных уровнях иерархии. например жестко наказывать за то, что ты не угадал что на фотке животное, но меньше наказывать за ошибки на внутренних уровнях (например крыса vs чихуахуа)",
"<@U040HKJE7>: да, понял, спасибо! :slightly_smiling_face:
тогда немного другой случай:
для примера, пусть у меня 1к классов, каждый промаркирован 2мя лейблами (A и Б). каждый лейбл - категориальный, размер множества возможных значений каждого из лейблов, пусть, равен 100. есть ли готовые подходы к тому, как выбрать каскад ('сначала А, потом Б внутри каждого класса А' или наоборот) кроме полного перебора иерархий?",
"Коллеги, посоветуйте готовое решение для распознавания продуктов и брендов на фотографиях. Задача сформулирована так: 

&gt; То есть если пользователь сфотографирует iphone, система выдала, что это iphone grey.
&gt; Если сфотографировали велосипед: то мы стремимся к тому, чтобы распознало как ‘велосипед Canondale красный’.
&gt; Чтобы еще точнее понять задачу:
&gt; представь, что ты выкладываешь товар на продажу, поюзанный ноутбук, или велосипед,
&gt; Мы хотим чтобы наша программа подготовила описание товара, чтобы это было выставление в один клик.
&gt; Есть ли АПИ или библиотека, которая распознает максимально точно нашу специфику и не дольше чем за 3 сек?",
"прикол именно в том, что я видел статьи, где рассказывают про этот метод, но имплементаций не нашел",
"Сейчас найду статью, где объясняется как сделан переход 2D-&gt;3D",
"У них есть вариант, когда по одной фотке восстанавливается 3д-модель",
"О, это вроде как раз то, что нужно, большое спасибо!",
"Друзья, а может ли кто-то поделиться опытом получения PhD (по тематике, конечно же) ? В частности <@U040HKJE7>, не расскажешь, как твои дела с TUM в итоге?",
"<@U0H7VBQQ1>: ""Мне понравилось про ошибки в градиентах. Очень жизненно""

Прочитал статью, но честно говоря не прочувствовал сильных преимуществ. Сами же почти во всех примерах показывают, что если делать правильно, то шум в градиенты дает незначительную прибавку. Хотя, конечно, техника простая, так что почему бы и не попробовать, по реализации, как dropout вроде как.",
"<@U040M0W0S>, <@U09BY2N3X>, <@U0B4374S1> как успехи на хакатоне?",
"<@U065VP6F7> в авито 3.85 топ-1, топ-5 там как бы не второй знак после запятой, просто есть нетривиально устранимая ошибка между некоторыми парами классов",
"Коллеги, <@U0QPYUM5M> нашёл в себе силу и мужество написать для нашего сайта статью, посвящённую архитектурам нейронных сетей.
Предлагается помочь ему в этом благом начинании. Для этого создали googledoc: <https://docs.google.com/document/d/1Isd2tGm1OGkS0VNZDsemRAFKLOMrJdUKBQfwkTcJmI0/edit?usp=sharing> с  первом, сырым видением того как это должно быть. 
Очень рассчитываем на ваши комментарии, дополнения, замечания и опыт",
Больше выглядит как глава в книге,
"да, мне тоже показалось крутовато, когда я всё написал. Но можно же разбить на серию?",
"Как пример - я сколько-то подробно описывал Inception, это уже три поста",
"Кстати, а где они есть реализованые? Только торч и каффе?",
"Я все еще external PhD student там, так как никуда не тороплюсь и изредка пишу статьи более менее по своей теме (со своим проффесором в соавторстве). он немного в шоке что он теперь оказывается соавтор <@U04423D74> <@U041LH06L> <@U04AR6WF0> <@U04422XJL> <@U041SH27M> <@U043814R6> <@U04FHEY0M> <@U041P485A> и <@U0989QUVC>, но никак не присоединялся к нашему научному пати-вэну",
"Если правильно помню, то сначала ты там был очным студентом на phd - как вообще TUM в этом плане, годное место? Кажется что там инженерная школа достаточно сильная, но вот в плане ML я что-то не часто натыкаюсь на его упоминания.",
"как место весьма годное, плюс по проектам - тесная кооперация с ETH Zurich и универом EPFL",
"такс, а в каком канале принято про hadoop спрашивать? тут?",
"Есть sparse матрица bag of words c 1/0 значениями. Умножаем случайные столбцы на -1, получаем sparse матрицу, у которой среднее среди ненулевых элементов = 0, а не 0.5, что вроде как должно немного помогать SGD. Почему так не делают? Попробовал, эффект небольшой, но вроде есть какой-то. Или это глупости все, и на этот ~0 mean пофиг, если мы intercept подбираем? Или наоборот, баян это все?",
А почему нулевое среднее должно помогать SGD?,
"хотя я сходу не пойму, зачем среднее вычитать",
"спасибо! вопрос, значит, в том, чтоб понять, зачем иногда советуют 0 mean данные делать - это только чтоб intercept не подбирать в самой модели, или еще зачем-то",
"а насчет центрирования - как раз и думал об обходном пути, чтоб в dense все не превращать",
"когда делают feature hashing, похожий метод используют - половину колонок в минус уводят",
"Коллеги, подскажите, какими бесплатными инструментами принято пользоваться для парсинга текстов. Т.е. из более или менее natural language выделять адреса, имена, контакты. Есть огромный список тулзов: <http://www.kdnuggets.com/software/text.html>
Может кто сориентирует?",
"пересмотрел сейчас лекцию у Ng и Hinton про центрирование, на примере лин. регрессии понятно, почему помогает сходиться, но в более сложных моделях мне все не так очевидно",
"Я просто увидел, как <@U064DRUF4> начал набирать текст и решил угадать.",
А расскажите как дела с deep learning в NLP на русском,
А лемма как вытаскивается из слова?,
"Еще вопрос - я кажется когда-то видел статью, где пытались найти отдельные embeddings для синонимов слова",
<@U064DRUF4>: а может вспомнишь где было это сравнение?,
это когда между “работает плохо” и “не работает” нет разницы,
"Сейчас на Диалоге как раз говорили про то, что власть в руках компаний которые имеют силы и деньги на разметку, и тем самым не двигают науку сами - ибо скрывают размеченные корпуса",
"можно брать как словарь, сверять и если совпали слова и есть окно - ты скорее всего можешь говорить об определенной синтаксической связи",
"кстати интересно что в статье где делали парсер на seq2seq сети, помогало обучение на выходе менее качественного парсера, т.к. он по-другому ошибался",
просто достало как julia ведет себя когда большой корпус или память кончается. но в итоге пользовался все равно ей),
"<@U0P95857C>: и да - по всем вопросам по <http://cosyco.ru|cosyco.ru> можно обратиться к <@U0PF7KGQG> 
она как раз и сделала всю работу с научником",
"трансляция из сколтеха. вдруг кто не знает
<https://www.youtube.com/watch?v=uoRwjxaDgt0>",
"пинганите, плиз, когда начнется вновь",
Ну и доклад. Прям как на мероприятиях про блокчейны :levenchuk: ,
А какая у него лицензия? ,
"<@U064DRUF4>: да, про то что такое полнота, и на чем проверяли меня тоже это интересует, сейчас спрошу - как раз они на постерной сессии. Тэгсет они новый сделали, да, сводили то что выдает compreno к нему. Планируют дальше на нем обучить TnT и разметить весь ГИКРЯ",
"<@U0UJ10A9J>: про лицензию авторы не знают, но говорят что она будет допускать коммерческое использование.  не выкладывают в открытый доступ пока что чтобы узнать кто скачивает, а так все будет :slightly_smiling_face:",
"А почему для регрессии используют метод наименьших квадратов, а не модулей? :baby: ",
"0. постановка задачи: есть x, y, eps ~N(0, s), и нужны коэффициенты a, такие что Ax = y + eps
1.а. нас интересует обычная евклидова норма разноцсти
1.б. интуиция: мы хотим штрафовать за большие отклонения и нам плевать на мелнькие
1.в. бонус - это можно классно состыковать с методом максимального правдоподобия, учитывая нормальность невязок
2. нам надо минимизировать квадрат разности. когда это можно - когда градиент равен нулю. клевая удобная формула для аналитического решения",
Ну как следствие еще олс на любом языке можно запилить довольно быстро ,
"Спасибо, <@U0JHK9001>. Но гугл просто без души -- не троллит меня как бы я не пытался задать эдакий вопрос. ",
"обменяю классы оценок на доказательство, почему симметричность не нужна :slightly_smiling_face:",
"или обратное доказательство, почему несимметричные ошибки все портят",
"<@U040M0W0S>: доказано, что МНК дает best, соответственно МНМ - не best
вообще говоря они могут совпасть для какого-то набора данных, но вероятность такого события = 0
если ошибки распределены по Лапласу, то оценка МНМ по сути совпадает с оценкой методом макс.правдоподобия (ММП)
точно также как для нормальных ошибок оценка МНК совпадает с оценкой ММП",
"<@U040M0W0S> что то докладчик от IBM был не совсем по конференции, но видимо кто смог тот и пришел",
"<@U040M0W0S> c другой стороны, про watson трудно докладывать что-то конкретное, так как это больше бренд чем продукт",
"Какая сложно прикология) если распределение ошибок симметричное, то эти две оценки совпадут) ровно по той же причине почему медиана и м.о. совпадает) я бы не сказал что это прямо такое редкое явление что ""вероятность этого 0""",
"Ммм, есть какой-нибудь конструктивный способ? Я знаю, как свести минимизацию L1 к ЛП, но не наоборот",
"после получаса прослушивания непрерывного неразборчивого квакания становится понятно, почему все вокруг так плохо думают про Сколково",
"<@U040M0W0S> не подскажешь, на каком слайде было про 60 миллиардов? что то листаю туда-сюда и не вижу",
"А не знаете, есть ли бенчмарки про вытягивание causal связей из данных, но не из ""статичных"" пар effect-cause (типа как тут <https://webdav.tuebingen.mpg.de/cause-effect/>) , а так чтоб агент мог интерактивничать со средой, содержащей effect-cause связи. Ставить эксперименты без учителя.",
"плюсую вопрос, потому что часто вижу как после cv рисуют график с доверительной трубкой... А для меня это выглядит как ""верить ли доверительному интервалу, посмотренному по 3-5 точкам?"" Наверное я чего-то не понимаю /:",
"хотя на kaggle наблюдал картину, когда делаешь изменение, по CV становится чуть лучше, std у CV такое, что можно было это изменение и не делать, но на public и private оно помогает",
Есть какие-то best practice как смешивать результаты сегментации?,
"Это наверное для реальной жизни, когда прирост в 0.003 не так важен",
"да, они будут пересекаться, как это может испортить дело - хороший вопрос. 
В каком-то из недавних соревнований <@U054DU76Y> так делал (BNP Paribas?), надо отчет поискать, может я ошибся в пересказе",
"то, где всех раскидало по -1000 из топа, и был script, сильно заточенный под public lb",
"привет!
задача предсказания тегов по вопросам. есть два столбца с токенизированым текстом - заголовок и тело вопроса, и несколько численных фич, посчитанных на них. вопрос: единственный путь создать нормальный трейн из этого - сделать  спарс матрицу по всем словам? разумно ли к ней приклеивать пару не спарс столбцов с посчитанными фичами? или может есть какая то штука, которая прямо со словами это все может схавать? 
и еще: есть мысль, что слова из заголовка гораздо лучше коррелируют с полученными тегами, чем все, что в теле вопроса. можно ли как-то этим словами придать вес побольше в модели? что-то не соображу, как это реализовать",
"Сегодня в сколтехе поднимался это вопрос. Похоже затыка пока в том, что для звука нет предобученных сетей масштабов как на ImageNet",
"Допустим сл.в. нормально распределена  у 0 с достаточно не большой дисперсией, чтобы помышлять о том что далее. У меня есть сложная функция от нее. Я хочу жахнуть ее в ряд тейлора и взять сколько-нибудь слагаемых. Как оценить огрубление? Я знаю как оценить м.о. огрубления, не понятно, достаточно ли этого. И вообще так делают? Покидайтесь в меня чем-нибудь про.",
"Реквестирую сеть, которая сама пишет поп-музыку (можно будет сделать pop-music-as-a-service, а потом набрать звукорежиссеров и композиторов, и продавать их услуги за доп деньги. PopNet), и эпическую музыку / музыку для кинофильмов (идея та же, EpicCNN какой-нибудь). Можно, кстати, ровно то же сделать, что и с рисованием - когда пишешь набор нот случайных, а он тебе их аранжирует автоматически под выбранный стиль. Или проговаривешь голосом лирику (генерация лирики будет отдельной сеткой), загружаешь в сеть, а она тебе - и аранжировку, и твой ""вокал"", подогнанный под мелодию, и соло сама зафигачит. Потом делаем 10 аккаунтов на бандкампе в разных жанрах, и начинаем продавать оттуда музыку, ""сгенерированную искусственным интеллектом"" (очень модно, все должны прям заслушиваться).",
"<@U0JJ69UB1>: а можешь рассказать в общих чертах?
Пока виже, что две сетки и 3 регресии. И сетки вообще не глубокие, по сути mlp. Но сходу не понял как определялся реворд для них.",
"<@U07V1URT9>: на самом деле у нас был стандартный  n-step q-learning, где n было достаточно большим 20/50. Сеть имеет столько выходов сколько действий, то есть 4. Выходы из сети имеют смысл будующей суммарной награды, если  действия будут выбираться как  argmax по выходам сети.   На каждом шаге создается чекпойнт, далее для каждого действия делается шаг, соответствующий этому действию и дальше делается 20/50 действий, которые выдает сеть. Считается награда полученная за эти шаги, к ней прибавляется   максимальное значение  выхода из сети  на последнем состоянии. Так получается таргет для 1 действия и 1 состояния. Соответственно такая процедура повторяется для разных действия и разных состояния.",
"У меня вопрос: я в книге <https://www.manning.com/books/taming-text>  увидел такую вещь - латентно семантический анализ  охраняется патентом, поэтому для опенсорса не интересен, т.е. я не могу его сам реализовать и юзать в коммерческом по?  кто в курсе этого вопроса?",
"А кто-то process mining пробовал? Какой софт, что там за алгоритмы внутри? Какие-то интересные результаты были?",
"только опять же, как туда впихнуть CNN?",
"Туда же, куда в speech recognition?",
"ага, + активное использование чекпойнтов, так как с ними данных можно было набрать в 4 раза больше, + для регрессий помогало брать для обучения те примеры, когда происходило изменение 35 фичи и  случайно с вероятностью 0.7 остальные примеры, + лучше работало именно policy iteration, когда на одной политике проходится уровень, набирается выборка для обучения, а потом политика дообучается(с replay memory и асинхронным q-learning было хуже)",
"Ну то есть взяли политику, прошли уровень, сделали апдейт, повторили? Это более-менее понятно. (Да)
А много потребовалось итераций до сходимости? (50)
И почему чекпоинты давали в 4 раза больше данных? (Не самое лучшее действие берем, а все 4 возможных варианта хода)

* это я таки прочитал pdf-ку",
"А где там возник curriculum learning? Это тот момент, что некоторые стейты рандомно пропускаются?",
"<@U04422XJL>: Для регрессий до сходимости надо примерно 10 итераций, но делали 20, а затем веса усредняли, давало лучше результат
для сетей сходилось примерно к 20 итерациям, но на всякий случай с запасом делали 50
про чекпойнты все верно
Что касается curriculum, то да, рандомно пропускали стейты из которых действие не приводило к изменению 35 фичи, так как там похоже reword был как-то завязан на значение этого стейта, и если он не менялся, то таргеты для разных действий получались одинаковые, идея была чтобы action gap с помощью этого увеличивать. Хотя уже в конце самом сложилось впечатление, что для сетей можно было этого и не делать. А регрессии расходились без этого стабильно.",
"Gephi поудобнее и покрасивее, да, но вот у меня Cytoscape на каких-то больших графах, типа вершин под полмиллиона, гораздо лучше работал. Он там может еще сам разделять на компоненты связности, и красиво их рядом друг с другом представлять, тоже удобно может быть, когда как Gephi так не может",
"ты не знаешь, из какого он города?",
Марков расказывай куда модели спрятал )),
"Как по мне, при 20к узлов глазами можно посмотреть только в том случае, если сеть представляет собой что-то очень и очень осмысленное (типа цитирований и соавторства статей). На хрень типа френдов и лайков в соцсетях при таком количестве узлов смотреть особого смысла нет, т.к. это в любом случае будет гигантский «волосяной шар».",
"Слушайте, а есть уже, наверное, такой алгоритм, когда берется Markov Chain, а вероятности перехода фитятся как условные относительно не только последнего стейта, но и некоторого его расширения (типа, памяти)? Понятно, что можно наделать много-много стейтов, но тогда будет слишком много параметров (модель же полносвязная), да и некоторые атрибуты в памяти - действительные числа, т.е. пространство с бесконечным количеством состояний тогда получится. Это в чем-то похоже на HMM, но никаких латентных переменных нет",
<@U0G29N5U4>: Звучит как MCMC fitting,
"но есть такие особенности, из-за которых это не совсем графическая модель markov chain:
1. тут бигдата (для начала 10 gb логов и десятки миллионов записей)
2. я не знаю, есть ли понятие restricted markov chain или нет, но тут смысл в том, что граф состояний далеко не полносвязный на практике

соответственно, я решил не делать mcmc, а делать многоклассовую классификацию (например, логрегрессию) для каждого состояния, ограничивая набор возможных состояний

т.е. беру эмпирический граф, по нему определяю, какие переходы в принципе возможны (в т.ч. отсекая аутлайеры), а потом уже делаю фиттинг в зависимости  от расширенного стейта",
"Ребят, подскажите, пожалуйста, как в tensorflow загнать картинки с переменным кол-вом label'ов?",
"Как в прошлый раз будет он-лайн трансляция и запись. Кто собирается прийти, зарегистрируйтесь: <https://events.yandex.ru/surveys/3390/>",
Супер! Я как раз такое искал недавно,
"хотя я помню на конфе когда этот товарищ выступил, были разные дискуссии куда вставлять",
"спс, ты подтвердил мои догадки что нужно перед параметризированным слоем, я чот как то не так сформулировал - до после нелинейности если там нет пулинга :smiley:",
"Я тогда спрашивал про ситуацию, когда есть старая и новая картинка",
<@U04422XJL>: Где ты это прочитал? Я людям с этим контр-примером по мозгам пару месяцев катаюсь.,
"Пацаны, а вот я узнал, что к вероятности есть оказывается несколько подходов. Значит, баесовский и другой -- frequentist. А последний он когда применяется? :om_symbol: ",
"кто занимается predictive maintenance, как вы модели учите и валидируете, если в реальной жизни поломок очень мало (и цена их может быть очень велика)?",
"как я слышал от одного умного дядьки, какие бы хорошие модели не были, всегда есть дядя Вася, который точнее любой модели предскажет, когда и что сломается (речь шла про нефтянку)",
"<@U041P485A> как раз актуально, было бы интересно посмотреть (если ещё не постили - не удалось найти в чате)

<https://opendatascience.slack.com/archives/deep_learning/p1462606122000970>

<https://opendatascience.slack.com/archives/deep_learning/p1462617206000973>",
"&gt; На заседании семинара ""Структурные модели и глубинное обучение"", которое состоится 16 июня (четверг), в 16.30, ИППИ РАН (<http://iitp.ru/ru/contacts.htm>), 6 этаж, 615 аудитория; охранник на проходной будет предупрежден
&gt; будет представлен доклад о байесовских методах машинного обучения.
&gt; 
&gt; Докладчик: Сергей Бартунов (ВШЭ) <@U13E1AWCX> 
&gt;
&gt; Тема доклада: Вариационный вывод в непараметрических и несопряженных байесовских моделях.
&gt;
&gt; Аннотация: Вероятностные модели со скрытыми переменными широко применяются в различных областях машинного обучения и анализа данных: тематическом моделировании, кластерном анализе, глубоких генеративных моделях и т.д. Введение скрытых переменных позволяет существенно увеличить выразительную силу модели, однако требует построения процедуры вывода, то есть, моделирования апостериорного распределения на скрытые переменные при известных значениях наблюдаемых переменных и параметров. Методы вариационного вывода рассматривают задачу вывода как задачу приближения истинного апостериорного распределения другим распределением из заранее определенного семейства, что для каждой конкретной модели, вообще говоря, является отдельной математической задачей. В данном докладе будут рассмотрены алгоритмы вариационного вывода для важных байесовских моделей: модели смеси на основе последовательного метрически-обусловленного процесса китайского ресторана (англ. sequential distance-dependent chinese restaurant process) и непараметрическом байесовском расширении модели Skip-gram на основе процесса Дирихле. Также будет рассмотрена программная библиотека для низкоуровневого вероятностного программирования, позволяющая конструировать сложные вероятностные модели и реализовывать для них вывод с использованием нейросетевых вариационных приближений, которые недавно стали прорывом в связи с возможностью выполнять вывод в традиционного сложном классе несопряженных моделей.",
"Еще вопрос вдогонку - что делать в случае, когда samples_per_epoch не кратно batch_size. Т.е. последний batch получается меньше batch_size. Просто отбрасывать крайний batch или брать обрубок?",
"Как минимум, можно вспомнить устаревший, но очень любопытный теоретико-игровой подход",
"<@U13E1AWCX>: а не подскажешь, по каким материалам лучше осваивать программу этого курса: <http://bayesgroup.ru/courses/bayesian-methods-in-machine-learning/> ?
У Бишопа многое есть, но может есть что-то еще?",
"Котаны,такой немного :vonni:  вопрос: а кто-нить для нейросеток измерял, как лучше делать мультикласс/мультилейбл (для примера, на 10 классов) - one vs all бинарная классификация с 10 нейросетками или одна нейросетка с 10 выходами? Метрика пусть будет multiclass log loss. Или типа это будет одно и то же?",
"Я такой нерешительный и не могу определиться, мне интересны оба случая и вообще с какой стороны начинать рассматривать такой вопрос  в случая возникновения одной из этих 2х задач :slightly_smiling_face:",
какие нибудь типичные rule of thumb,
"ага, и по ним выбирать какие классы должны остаться в предикте (от 1 до всех)",
"ща попробую раскурить как свести ужа с ежом, оставаясь только в функциональщине",
<@U0ZHHV83C>: а какая задача и какой в нее заложен смысл?,
еще раз: берешь какой нибудь differential evolution и засовываешь IOU напрямую в оптимизатор,
"как вариант можно это через перемножение определить, и минимум и максимум",
"а дальше уже твои дизайнерские решения, как определять что ты считаешь пересечениями",
"продолжая тему keras. пробую обучать довольно большую сеть из нескольких embedding слоёв, которые группируются и слдом идут три больших полносвязных слоя с dropout. на выходе softmax на сотню выходов и categorical_crossentropy в качестве loss. буквально через несколько десятков итераций (не эпох) loss становится nan. Пробовал RMSprop\Adagrad оптимизаторы, немного крутил lr и epsilon - не помогло. Есть идеи как бы это побороть?",
"cейчас попробую сообразить как clip сделать в keras, спасибо",
"Можно глянуть, как компьютер «думает» над следующим ходом в шахматы: <http://www.bewitched.com/chess/>",
"но в плане визуализации прикольно, правда не всегда из черточек очевидно какой ход она сделает",
В принципе как регуляризация makes sense,
"у меня dropout ухудшает результат сейчас, как раз другие варианты могут пригодиться",
"Тут совсем недавно обсуждали, что придумали как dropout в RNN применить",
"Друзья, кто-нибудь участвует в соревновании Di-Tech <http://research.xiaojukeji.com/competition/> ? Или знаете того, кто да? Хочется датасет. Если что, поделитесь, пжл.",
"теоретический вопрос. допустим у нас есть две фичи - `a` и `b` И я знаю, что у них разная важность. Я создаю две новые фичи `a*10` и `b*20`. Для каких алгоритмов предпочтительней удалить исходные, а для каких оставить?",
"А зачем умножать, если knn может веса на вход для признаков получать?",
"везде где есть нормализация, оставлять нет никакого смысла, только вред",
"кажется если ты не знаешь как получить признаки которые бы влияли на веса a и b или алгоритм расставил не большую разницу в весах, но ты по жизненному опыту уверен что a весомей чем b, то можно и самому умножить на веса которые родил в голове, как говорится в старой пословице один интеллект хорошо, а два лучше",
"<@U181T6SJV>: изначально об этом и речь. вопрос в том, что с исходными делать? нужна ли исходная информация и если да, то в каких случаях? Как вариант, можно добавить `a/(b+1)`, например, ну или что-то подобное",
"Столкнулся с задачей: есть n элементов, есть матрица nxn, каждый элемент которой расстояние между соответствующими элементами. нужно среди элементов выделить 1000 кластеров. Думаю, что задача распространенная, подскажите куда смотреть.",
"<@U108YLBU1>: а посмотреть на графике AUC, какому значению TP соответствует какое значение FP, и самому посчитаеть precicion/recall вручную для этих значений?",
"сап чат. 
1. внесем ясности в происходящее
-деревьям пофиг не только на домножения, но и на любые монотонные преобразования (можно не логагрифмировать).  до тех пор пока порядок точек в фиче не меняется, толку от преобразования нет (ибо ищется сплит по порядковому значению)
-линейным моделям пофиг на аффинные преобразования, но не пофиг на все остальное *UPD* но не пофиг в случае сильной регуляризации 
-не пофиг на домножения моделям, учитывующим расстояния (особенно евклидовые) внутри ""строк"" фич. тоесть домножения будут подстраивать (надламывать) KNN, kmeans, GMM и прочие (а проекторам на это пофиг)

2. когда удалять?
-нет смысла держать много версий аффинных преобразований одной и той же фичи. каждая такая фича снижает ранг матрицы (и всякие методы, трепетные к этому будут вылетать с ошибками)
-в линейных моделях можно держать одновременно исходную фичу и преобразования от нее (log, sqrt, ^2, ...). и модели сами разберутся в их значимости
-как уже написал выше, в деревьях нет смысла монотонные преобразования делать в первую очередь. плюс как верно написали
&gt; <@U136KEUHY> ну и вообще деревья лучше тебя знают что из фичей тянет",
"сам был в нелегком удивлении, когда видел, что перед регуляризацией таки выбирают фичи другими способами",
"Насчет профита - это как профит от того, чтоб задавать разный prior для разных фич; от задачи зависит. На практике обычно крутить веса фич, домножая их - это хак какой-то.",
"<@U1A5T444C>: у тебя, видимо, численные проблемы. Как их отладить в theano, я не знаю",
"Надо посмотреть, где может возникать бесконечность, которая потом неправильно взаимодействует с другими числами",
какие функции активации и функция потерь?,
"<http://www.mit.edu/~mnick/deep-learning-workshop-2016/schedule.html> 
Там есть доклад про эквивалентность  FFNN и RNN, хорошо когда интуиция подтверждается ))",
"Сейчас смотрю повтор трансляции. Дмитрий рассказывает, как первый чувак заоверфитился на лидербор и когда открыли полную выборку, то он скатился на 200 позиций. Ничего не напоминает?",
"<@U07V1URT9> напоминает santander, много кто бумкнулся, и было больно но поучительно",
"Мы с <@U0J834KT5> применяли традиционные методы компьютерного зрения, чтобы найти контуры левого желудочка и площади сечений , а потом посчитать объемы. Затем использовали это как признаки для random forest.",
"А потом банально переобучились, когда попытались сделать стекинг.",
"Как стоит проверять гипотезу о равенстве распределений двух многомерных выборок?
В предположении о нормальности распределения, можно, видимо, проверить равенство среднего и дисперсии. Нормальность распределений можно тоже принимать по критерию.
А что делать в случае не нормального распределения?",
"Я где-то читал, что для проверки гипотезы о равенстве распределений двух многомерных выборок достаточно успешно применялся хи-квадрат тест и в той же статье было сказано, что проверка подобной гипотезы — открытая проблема. Как вариант можно посмотреть на тест Колмогорова-Смирнова (<http://v-scheiner.brunel.ac.uk/bitstream/2438/1166/1/acat2007.pdf>) и, если все таки речь про нормальность, то на  MANOVA",
"а импутить разные данные -- как для ансамбля? или не рекомендуется? 
ну а потом разные модели",
"кстати, вот в статье говорится о таком способе представления данных:

&gt; If you have missing values in a binary feature, there’s an alternative representation:
&gt;    -1 for negatives
&gt;  0 for missing values
&gt;   1 for positives

Как это называется по правильному?",
"<@U0DA4J82H>: обычно это все же нюансы интерпретации опроса/исследования
потому что тот же ответ ""затрудняюсь ответить"" можно интерпретировать и как отвказ от взаимодействия, а не реальные затруднения. поэтому при анализе данных такое надо учитывать, когда исследование может иметь такие нюансы",
"Вопрос каналу, Кто нибудь пытался писать свои слои для theano, не на python уровне, а пониже на C++?",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1465826291000050>

Нашел там такое:
random forests with XGBoost (yes, it’s possible with some undocumented options).

Кто знает что это за зверь такой?",
"где бы внятно почитать, когда имеет смысл делать log(feature) перед логистической регрессией, а когда нет?",
"Если кто не знает, то желательно под событием Data Sceince Завтрак ставить плюсы если собираетесь заглянуть. Мы всем рады, даже если на полчаса заглянете.",
"Друзья, а накидайте каких-нибудь ключевых слов, как решать классификацию с вложенными классами (иерархическая?)",
"важна ли ирерархичность? если не очень, то можно самые низкоуровневые классы предсказывать, а какой порядок классов?",
"А не встречалось ли кому таких работ, где бы агент должен был выдавать программу действий? Т.е. есть состояние среды S_current, и нужно перевести ее в состояние S_target за, допустим, наименьшее количество действий {a}. Но нужно не совершать эти действия, а просто ""выписать на бумажку"" ВСЮ последовательность действий, которая по мнению обученного агента  приведет из S_current в S_target.",
"встретилось в одном очень древнем диссере по обучающимся манипуляторам
 ""The basic idea is to cluster the state space into regions based on some similarity measure. Each region of states is then treated as an abstract state and reinforcement learning algorithms are applied to this abstract state space.
Another alternative (taken by Nilsson) is to construct a region graph showing the connectivity of regions by actions. With such a graph, solution paths can be found by graph search.""

То есть идея, как я понимаю, такая: нейросеть вытягивает какие-то выскоуровненвые признаки, и кластеризует состояния системы по мере схожести относительно этих признаков. Дальше между этими кластерами прокидываются направленные пути, которые показывают что из  кластера А  можно попасть в кластер В через такое-то действие. И тогда составление плана сводится к поиску кратчайшего пути на этом вот графе кластеров. Звучит клево имхо. Но что-то эта идея там дальше не разививается, а интерсно было бы.",
"Еще вспоминается статья из DeepMind, где они генерировали код по описанию карт из Hearthstone",
"<@U1D6QAJPJ>: а зачем такая постановка задачи? Последовательность действий не выписывают обычно из-за того, что непонятно, в каком состоянии будет среда после первого шага, и это может повлиять на дальнейшие шаги.",
"чтоб что-то планировать нужна модель среды, т.е. надо как-то уметь не только выбирать правильный action на основе текущего состояния, но и предсказывать, в какое состояние среда перейдет после этого action-а",
"Если полностью (или почти полностью) детерминирована, то почему нет",
"<@U064DRUF4>: я хотела придумать себе какую-нибудь интересную задачу, чтоб решая ее вникнуть в deep learning. Выбор пал на задачу обучающегося 2д манипулятора, который имеет n сочленений. Ему показывают картинку того, как он должен разместить предметы на плоскости. Т.е. сначала они разбросаны, например - и это текущее состояние среды. 

Подход через планирование мне понравился вот как раз тем, что вы сказали - это позволяет проверить ( в детерминированной среде) насколько агент смоделировал среду.",
"В Гугле это активно исследуется, как я понимаю",
"можно без планирования просто смотреть, как работает, запускать агента на модели среды) все эти TD(lambda) алгоритмы внутри ""планируют"" на несколько шагов вперед несмотря на то, что предсказывают только 1 action, т.к. Q-функция, которую тренируем - это оценка не reward, а return (суммы всех будущих reward)",
"я еще нашел Joinery, но оно, как saddle, уже год не обновлялось",
"хочется чего-то простого, чтобы легко можно было колонку вытащить, как коллекцию, а дальше уже можно джавовские стримы использовать, там и груп бай и все что угодно",
"только абстракций правильных нет, как в дата фреймах",
"Да, но еще join и group by. Group by есть в джаве, но он Map возвращает вместо таблички. Вообщем не так удобно как хотелось бы, много лишних манипуляций по сравнению с пандами ",
"напиши потом, кто приживется из этого зверинца",
"А почему cpu ориентирована, а не gpu? Экономия заряда?",
"<@U0ZJ45AD8>: всё что через sgd - всё без проблем должно быть возможно заставить оптимизировать напрямую. Про готовые реализации - сложнее. sklearn точно не поддерживает, даже как кастомную функцию. Линейную модель можно, наверное, обернуть через scipy.",
"Будет как AIC, BIC, CIC, DIC, EIC, ...",
"Ребят, подскажите следующий вопрос, может кто сталкивался.
Имеется база слов. Для каждого слова идет запрос в giphy через api и вытаскиваются гифки для данного слова. Ясно, что система очень часто ошибается и выдает по слову не то, что нужно, по этому приходится модерировать выдачу вручную. Вопрос, есть ли какие то технологии, с помощью которых можно будет проанализировать, что находится на гифке и понять, соотносится ли это со словом? Если слово — существительное, то более менее понятно как делать, а вот если прилагательное или глагол — не очень.",
"В принципе да, но выглядит как серьезная задача",
"<@U1H595XKJ>
Расскажу для начала как можно распознавать век по картинке. Допустим в нас локализировани лица. Сдесь хорошо работают пирамидальние ЛБП фичи, так же дипфичи, но я мало с ними игрался, больше игрался мой коллега.

Для ЛБП фич как и дип фич хорошо работает Ординал Регрессион. Если у ви знаете для каджого лица точний век, то можна просто тренировать регрессию минимизируя емпиричиский риск.  Такое можно учить с помощю СВМов.

Если нужно добавить пол, то ето можно сделать с помощю лейтент варіейблс. Тоесть, у вас в класификаторе будуть отдельно компоненти для МЖ, і чтоб определить пол і возраст, нужно брать максимум по полу и по веку. Но тепер в аннотаци вам нужно иметь и пол и век, если пол будет отсуствовать, то можно сформулировать обучение так же, но задача будеть невипуклой.

Еслы без пола, то можна посмотреть тут как ето можно учыть.
<ftp://cmp.felk.cvut.cz/pub/cvl/articles/antoniuk/Antoniuk-TR-2016-07.pdf>",
"<@U1F94QVMF>: спасибо. Как первый шаг, я решил сначала различать пол. Но я не стал локализовывать лица - входные данные для обучения это аватарки из соц сетей.",
"Да, только я не знаю где взять размеченные данные для них. Мой план такой - беру 1к картинок, руками отсеиваю зверей и мусор. обучаю сеть на распознание 2х классов",
"не знаю как сеть будеть работать, но знаю что еслы локализовывать - работает неплохо",
"<@U1F94QVMF>: как быть, если я локализую лица, они будут все разного размера (кто-то ближе, кто-то дальше) для классификатора мне надо их все проводить к единому размеру?",
В <#C04N3UMSL|nlp> насоветуют как конкретно искать,
"вопрос ребром: кто в практике юзал metric learning?
<@U041P485A>: ?

у меня коллеги в TUM юзали его для предобработки (NCA и LMNN), я когдато пытался его юзать для того же, но реализации были отстой полный",
"для тех кто не в теме, вот основная статья 
<http://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf>",
"Но если трактовать это как <@U04ELQZAU>  то да, занимался :)",
"<@U1H6URAMA>: а можешь рассказать, для каких целей используют self organizing maps и для чего они хороши ? ",
"Делаю регрессию, есть прям очень-очень много фич (порядка 900), но подозреваю, что между многими из них есть какие-то очевидные зависимости. Что в таких случаях обычно делают, какие кейворды гуглить? Насколько легально вообще пробовать снижать размерность каким-нибудь pca, tsne, перед тем, как пихать данные в регрессор?",
Как раз PCA и делать.,
"А по тому, сколько собственных чисел этой матрицы близко к нулю можно понять, сколько таких лишних фичей (но не какие именно). Собственно, если потом обнулить только эти, то и получится PCA",
А feature selection почему не предлагаете? (мне самому интересно),
"Там, как я понимаю, нюансов в разы больше, чем в исходном вопросе.",
"А почему во всяких xgboostах не используют активно lr annealing, как в нейросетках?",
"используют это, как я понимаю, только для того, чтобы ускорить обучение в начале, других плюсов нет",
"на kaggle ещё видел как вручную тоже меняют, просто каждую итерацию бустинга делают своим циклом",
"а какой смысл ставить больше одного, если xgboost итак все ядра задействует",
"если мне не изменяет память, кроссвалидация по возможности учит параллельно эстиматор на каждом фолде, поэтому как-то привык выставлять там n_jobs=-1, так как сайкитовские эстиматоры в основном тормозные и не сильно параллельные",
"кто нибудь знает, можно как-то в bokeh.charts.Chart управлять надписями на осях? все перерыл, пырюсь в код уже полчаса, не найду",
"Пасаны! Слезно прошу посвятить меня недалекого до высот науки. Раньше, когда читал про CNN, я понимал под convolution следующее: берем пиксель применяем к нему и пикселям вокруг него фильтр, который оформляем в виде блока сети и обученаем этот фильтр. Тогда 3x3 conv - это сжатие 9 пикселей в один, ну если грубо. 
А вот теперь я ниче не понимаю, когда читаю inception. Что такое 1x1 conv?",
непонятно зачем упоминать про страйд отвечая на поставленный вопрос ) 1x1 conv это на самом деле 1x1xN  conv. Для пикселей RGB это по сути смена цветового пространства,
"А сегодня в ИППИ в 16-30 или в 18-30, как в календаре написано?",
"<https://arxiv.org/pdf/1407.4023.pdf> подскажите, пожалуйста, на стр. 4 справа сверху написано, что он создают такие фичи, как LUV, модуль градиента и 6-bin histogram for RGB. Как мне кажется, последнее должно быть 6х6х6 тензор, но у них это 20х20х6. Как они его считали?",
"Всем привет! Почему иногда вместо сингулярного разложения Х используется разложение XX' то есть Х помноженного на транспонированную матрицу? В чем преимущество, в чём разница? Было бы здорово прочитать про это где-нибудь. Заранее спасибо!",
"Народ, тут ведь есть люди, которые используют H2O? Вопрос на миллион. Как это дело вызвать из джавы? Т.е. из джава кода, не из спарка, не из R и не из питона",
"не, GUI не такой удобный как кажется по видосикам",
"пока я представляю это будет как скидки, для разных групп пользователей",
"Привет, всем… линков на книги по грамотному price optimization никто не подкинет? хочется найти что то продвинутое и фундаментальное

суть в том чтобы продавать товары по максимально возможной цене для каждого пользователя, но при этом чтобы он покупал :slightly_smiling_face:

пока я представляю это будет персональные как скидки, для разных групп пользователей",
"но с другой стороны может быть win-win сценарий люди у которых много денег платят и не парятся, а те кто хотят но дорого ждут скидок",
"как бы да, но если это дискриминация на основе персональных данных...",
тот кто ограничен во времени вынужден покупать тот же товар в том же месте дороже,
почему тогда товар изначально дешевле не продавать?,
"да кого это беспокоит, внутри компании :slightly_smiling_face:",
"одно время было много шума, как раз с билетами, в сми
можно погуглить, по-моему все так же",
"А почему это вдруг не законно? Главное там везде понаписать что это не оферта, окончательная цена при добавлении в корзину или как-то так ",
"<@U0ZJ45AD8>: тут еще подумалось, что если это будет как система скидок, то по умолчанию товар должен быть по максимально конской цене, такой что у большинства пользователей волосы встанут дыбом.",
"Не, экселька в архиве, где указано - цена по запросу",
"для этих Request Quote возникает новая задача: с точки зрения покупателя нужно угадать как компания строит кластеризацию, и каким кластером наиболее выгодно прикинуться",
"Ребят, дайте совет. Есть сильно несбалансированный датасет из нескольких классов. Одного класса на порядок больше, чем остальных, распределение примерно такое
1    26376
2     7076
0     2023
3      321
4       27
5        7
6        3
9        1
8        1
7        1

соответственно, требуется адекватно предсказывать принадлежность объекта к классу (они упорядочены). Нашел либу, которая делает сэмплирование, но не уверен, какой именно метод из нее использовать.",
"если классы упорядочены, то почему бы не делать регрессию вместо классификации?",
"<@U17S9F6KV>: если честно, выглядит как Poisson counts и можно соотвествующую регрессию натянуть",
"Относился к нему просто как к поиску, но вот узнал, что можно держать свою библиотеку. Но он не позволяет с легкостью понять, от какой статьи пошло дальнейшее развитие, например (не строит граф), да и редактировать внутри него нельзя, насколько я понимаю.",
"<@U065VP6F7>: Спасибо, Сереж. Как раз Mendeley я и юзал. Но вроде в плане организации статей все, что там есть это возможность разложить по папкам. С точки зрения хранения заметок он хороший, это да.",
"Хочу синхронизовать календарь митингов со своим, не получается. А кто знает как это делается? Подскажите плз че да как?",
"а есть любители caffe? там в батчнорме у них в блобсах 3 learnable параметра
len(layers_caffe[name].blobs) -&gt; 3
а что это за параметры? ну логично предположить что гамма и бета (выученный скейл и шифт), а вот третий как то не пойму",
"если бы было 4, то я подумал бы что последние 2 это какая нибудь глобальная статистика по датасету (средние и варианс)",
"статьи тут, как я понимаю: <http://arxiv.org/abs/1508.06576> <http://arxiv.org/abs/1603.03417>",
"но качество очень гавно, как видишь",
"Где Яндекс.Реп, когда он так нужен)",
"<@U135CSYJY>: В Web of Science был функционал для рисования графа цитирования. Но он платный был. Не знаю как сейчас, давно его не использовал",
"если бы <@U041P485A> не подсказал бы куда лезть, так вообще наверное забил бы на резнет -)",
"Не подскажет ли коммьюнити, где можно найти коллекцию датасетов по машинному обучению? Что-то вроде uci machine learning repository . Желательно для задач классификации, объем данных не очень большой (меньше чем на kaggle) , желательно присутствие какого-нибудь бенчмарка для сравнения или результатов других исследователей
Нужно для проверки разных гипотез при тестировании одного подхода классификации при числе классов &gt;2",
"<@U0ZHCGY5T>: увидел в <#C04DP7BUY> твоё сообщение и поэтому не могу не спросить (тут, в <#C0KGMMCAX>, так как кажется здесь это уместнее) про твоё поступление в  TUM, поведаешь былину прохладную? :slightly_smiling_face:",
"<@U1D4RRA7K>: легко!

Это было просто. GRE не надо, тоефл надо средненький (я поступал на Computational science, но он везде средний -- около 90 баллов).
У меня уже был неплохой английский, когда в ноябре прошлого года я начал готовиться к тоефлу. Занимался этим делом неполные 4 недели, если нужно -- расскажу лайфхаки.
Сдал на 100 ( завалил [21/30] письмо, подвела краткость).
Параллельно готовил мотивационное письмо, рекомендации(тоже могу рассказать детали)
В TUM надо всё отправлять в печатном виде,поэтому стоит подготовить все документы заранее.
Вообще, туда несложно поступить, как по мне.  
Может быть ещё собеседование, но у меня не было, насколько я понимаю из-за хорошего диплома.
Вдохновила вот эта статья:
<https://m.habrahabr.ru/post/169791/>

Кроме TUM, я подал в UvA , меня туда тоже взяли, но пока без стипендии.
Ещё я готовился и сдавал GRE два раза, если кому-то нужно рассказать про этот ад, могу поделиться.

Для TUM, судя по quora checklist выглядит так:

1. Диплом с отличными оценками;
2. Нормальный TOEFL;
3. Хорошее мотивационное письмо.

Если что-то упустил -- скажи.",
"Подскажите, пожалуйста. Какой метрикой лучше всего мерять расстояние между векторами большой длины? Если есть два вектора, полученные как bag of words из текстов",
" Jaccard еще. Все что дает 0, когда один из компонентов 0",
"Я от местных студентов тоже слышал,  что поступить в немецкую магистратуру несложно, так как многие немцы ограничиваются только бакалавром. Поэтому конкурса никакого нет",
"А вот еще вопрос. Могу ли я рассчитывать хоть на какое то место в маге технического вуза, если образование у меня (со всякими там заслугами, стипендиями и красными дипломами) экономическое?",
"коллеги,  может, кто-нибудь видел документацию по тому, как сравнивать statistical inference, полученный в парадигме frequentist analysis, и в парадигме bayesian analysis?
например, как показать, что проверка гипотез с помощью bayesian anova на марковских цепях и иерархических моделях корректнее/надежнее, чем на классической anova?

пока абстрактный вопрос, чуть позже спрошу уже конкретнее %(",
"И, кажется, в западных универах есть  т.н. office hours, когда можно придти к преподу (= его аспирантам) и подоставать их своей глупостью",
"<@U1D4RRA7K>: 
Так, небольшой список лайфхаков по TOEFL:

Как любой тест, в первую очередь проверяют то, как хорошо ты изучил правила их игры, а не то, как ты знаешь язык. 

1. Reading.  Чтобы сдать его хорошо, нужно просто побольше почитать (каждый день) по-английски, в том числе что-то вроде научпопа. Там нет никаких сложных слов, никаких оттенков и полутонов значений, это обычный уровень B2 с накруткой околоуниверситетской лексики. Вполне реально сдать на очень высокий балл, если просто быть внимательным.
2. Listening. Чтобы сдать его хорошо, нужно знать, что будут спрашивать. Будут спрашивать университетские диалоги про тяжелую жизнь студентов, в т.ч. отсутствие общежитий на лето и т.д. Будет одна лекция про предметную область, могут рассказать вкратце и на пальцах про квантовую механику, про хитин пауков или про оперу. Нужно просто понять и потом ответить наиболее подходящим  вариантом. Лексика вполне простая, но если с языком проблемы, лучше поучить релевантные теме слова.
3. Speaking. В основном тут бывают проблемы. Почему: а) мало времени; б) если понял не всё -- приходится угадывать и тут уже не вытащить контекст из вопроса, так как он открытый. Опять же, темы все около университета. Чтобы хорошо сдать эту часть,  нужен тайминг, т.е. берешь и записываешь на диктофон свой ответ, стараясь запихнуть в 45 секунд ключевые моменты и ответить именно на этот вопрос. Если есть разговорный английский, то проблем быть не должно. Важно говорить без длинных пауз и использовать всё отведенное время. Можно стараться говорить со скоростью 0,75х от своей обычной, мне помогло. Можно говорить с любым акцентом и не стесняться.
4.  Writing. Надо писать много и можно использовать шаблоны. Никто не требует глубины мыслей философа и откровений, можно врать напропалую, можно писать банальности. Главное -- много и без ошибок. Ну и должна быть структура: введение с тезисом в конце, аргументы, вывод. Можно написать очень хорошо. Моя жена  начала нормально учить английский лишь за 5 месяцев до сдачи экзамена и сдала TOEFL Writing лучше меня.


Из материалов могу посоветовать оригинальные тесты ETS, там есть даже симулятор экзамена с модельными ответами. Еще слышал, что можно за $45 заказать реальный TOEFL, сдать его онлайн и посмотреть где пробелы. Из книжек я немного пользовался Barron's. Некоторые рекомендую <http://magoosh.com|magoosh.com>,  вполне подходит для того, чтобы натаскаться.
Если какие-то вопросы предметные или что-то непонятно, то я доступен в личке.",
"Пора потренироваться. На Московскую тренировку собираемся в эту субботу, 25 июня, в 12:00 в офисе Яндекса (БЦ Морозов).
Форма регистрации на мероприятие: <https://events.yandex.ru/surveys/3423/>

Сначала разберем парочку кеглов:
1. Андрей Кирьясов из Челябинска расскажет про то, как его команда заняла 6е место на Home Depot Search Relevance
2. Дмитрий Алтухов и Станислав Семёнов (<@U054DU76Y>) — про Expedia Hotel Recommendation

А потом Дима Дремов (<@U053R9RS6>) расскажет про израильский Kenshoo Data Hackathon и задачу предсказания конверсионной воронки рекламной кампании по терабайтам логов. Кстати говоря, Дима — основатель тренировок и ненадолго приехал в Москву. Не упустите шанс с ним пообщаться, в частности — про новый формат.

Ссылка на он-лайн трансляцию: <https://youtu.be/LJH6tnN0WDE>
Записи докладов будут выложены на канале тренировок: <https://www.youtube.com/channel/UCeq6ZIlvC9SVsfhfKnSvM9w>

Кроме того, обратите внимание на Киевскую тренировку в этот четверг! <@U0M39M6LS>, напиши подробности",
а есть какая нибудь книжка в которой алгоритмы визуализации описываются или мб курс какой?,
"Теперь про GRE.

Обычно его сдают бакалавры для поступления на Masters или PhD в Штатах. Касательно других стран не могу сказать, в Европе требований GRE не встречал. Может быть актуальным в Британии, наверное.

GRE состоит из:
1) Две математические (quantitative) секции по 35 минут каждая и по 20 вопросов каждая;
2) Две verbal секции по 30 минут каждая и по 20 вопросов каждая;
3) Секция-секрет-сюрприз (либо математика, либо verbal, не оценивается, но вы не знаете какая из секций экспериментальная *дьявольский смех*) 30 или 35 минут;
4) Два сочинения (essay) ( о них подробнее дальше).

Оцениваются первые две части по шкале от 130 до 170, сочинения оцениваются по среднему из двух оценок в диапазоне 0-6.

Мои баллы:
Первый раз: математика 157/170, verbal 147/170, сочинение 2.5 / 6.0
Второй раз: математика 159/170, verbal 157/170, сочинение 3.0/6.0

По порядку:
Я записался на сдачу GRE на 4 декабря в сентябре прошлого года. Открыл пару модельных тестов, посмеялся над математикой, закрыл. Это было главной ошибкой. В общем, готовиться я начал только в начале ноября и времени было катастрофически мало, так как готовился параллельно с TOEFL. 

1. Математика там простая. Но : 

a) нужно решить 20 задач за 35 минут;
b) не все задачи равнозначны и бывают с хитростями. 
Придется вспомнить геометрию, комбинаторику, основы статистики, проценты, доказательство по индукции, простые дроби. Звучит смешно, но стресс и отсутствие времени творят страшное, забывается абсолютно всё. Особенно, если вы слегка ипохондрик и постоянно перепроверяете то, что решили. Это лечится прорешиванием в скоростном режиме и воссозданием условий теста.",
"2. Verbal. Ходят легенды, что для этой секции китайцы (ну или кто там синоним старательных людей) учат две-три тысячи слов уровня impertinent, chagrin, decry и inure, и всё равно часто не сдают достойно. В общем, оказалось, что английский язык очень богатый и имеет крайне много полутонов значений. Вопросы базируются либо на тексте, который нужно прочитать и понять, либо на контексте, который нужно уловить. Скорее всего, в начале работы над этой секцией будет полный разрыв мозга, однако непостижимым образом он заживет и со временем вы начнете чувствовать текст и его оттенок, он будет ключевым при ответе. Забавно, но в отличие от TOEFL, в случае GRE вы сражаетесь с американцами в знании их собственного языка (подбодрю вас тем, что американцы тоже учат слова по карточкам для сдачи GRE).
3.  Секцию-секрет можно понять по странным заданиям (а можно и не понять, я вот не понял).
4. Сочинения--отдельная песня. Одно представляет собой анализ проблемы текста и подбор к нему контраргументов. Как правило, в тексте имеется тезис, который нужно подставить под сомнение. Конечно, ваша критика должна быть четко изложена и логична. Не должно быть мутных моментов с пустой болтовней (как можно в TOEFL, например). Очень полезно почитать реальные ответы людей на реальные тексты, вся база тем доступна на сайте GRE. У меня, например, был текст про обоснование необходимости постройки новых ТЭЦ, дескать, негоже строить новые мощности, раз цены на фотоэлементы падают и люди по опросам стараются экономить на электричестве. Логические прорехи обычно очевидны, но их надо хорошо обосновать и предложить какие-то свои исправления (ну это в идеале). 
Второе сочинение -- развернуть/оспорить тезис типа: каждый лидер нации отражает национальный характер. Ну или: во всех странах необходимо ввести единый образовательный стандарт . Нужно писать просто отлично. Для примера прочтите модельные сочинения на тематических форумах по GRE, те, что получили 6/6. Возможно, некоторые пробьют на слёзы. 
Если серьёзно, то нужно набивать руку писать последовательно, вспоминать какие-то моменты истории человечества. Если у вас хорошо с эрудицией -- не стесняйтесь её продемонстрировать.

Скажу сразу, будьте готовы к обыску на входе на экзамен и к плотной загрузке. В первый раз я сдавал в Нью-Йорке, там вообще было всё строго. Второй раз (через 2 месяца) сдавал в Москве, там всё попроще было.

Теперь о том, как справиться с этой трехглавой гидрой. Всё, что перечислено ниже доступно на торрентах и в вопросах Quora,  если вы меня спросите в личке, то могу скинуть некоторые материалы и сайты.
1) Математика лечится хардкорным учебником 5 lb. Book of GRE Practice Problems - Manhattan Prep  + Magoosh тесты. В учебнике 800 страниц, которые все желательно прорешать. Впрочем, если вы спокойный гений или просто быстро и хорошо решаете, то математическая часть покажется вам легкой.
2) Verbal лечится чтением, например, Guardian (всей тематики) + Magoosh тесты + мнемонические запоминания слов + Докинза, например, можно почитать в оригинале. Про запоминание слов: есть много приложений (в том числе от magoosh), в которых удобно учить слова. Потом они волшебным образом будут всплывать у вас в памяти и выручать в нужный момент. По мнемонике много приемов, их легко найти, я предпочитаю абсурдный подход к запоминанию, но кому как.
3) Essay. Нужно читать других авторов, их много в различных книгах по подготовке к GRE, которые доступны в интернетах. Нужно писать и следить за своей мыслью. Нужно быть небанальным, нужно выражаться ясно.

В общем и целом, могу сказать, что во многом всё зависит от личного настроя, бэкграунда и времени. На это стоит выделить время, если вы настроились серьёзно. Наиболее похож на реальный GRE их собственный PowerPrep, модельный тест, который можно найти на сайте. 
Magoosh советую, у них хороший контент с обучающими/объясняющими примерами.
<@U0G29N5U4>:",
<@U0DA4J82H> а как считать Jaccard для небинарных массивов?,
"<@U0DA4J82H> <@U040HKJE7> <https://en.wikipedia.org/wiki/Jaccard_index> -- это как раз то, что я хотел узнать! Спасибо большое!",
"<@U040HKJE7>: к сожалению, я не сразу понял, что ты имел ввиду и поэтому тупил... Я понял, что можно построить более обобщенный вариант для расстояний между измеримыми функциями, как ты предложил. А если нужно будет померять расстояние между множествами -- взять в качестве функций индикатор этих множеств. Тогда min == пересечению, max == объединению и из определения интеграла Лебега: интеграл от индикатора == мера того, что под индикатором. Собственно, получится обычный IOU.
Ещё раз извиняюсь...:pensive:",
при этом там как раз один месяц magoosh триал бесплатный,
"тексты TOEFL очень интересно читать. Читаешь читаешь такой, думаешь что это за невиданные звери? А это какие-нибудь улитки просто оказываются. Я пока готовился очень много про животных вообще узнал.  Не знаю как с gre general (наверное также), но  gre subject in mathematics надо обязательно писать несколько раз в год -- задания повторяются. А это плюс очки. Если познать китайский интернет, то можно найти как множество китайцев принесли себя в жертву, сходив запомнить вопрос и выложили его на форум.",
"коллеги такой вопрос. какая из state of the art архитектур имеет меньше всего параметров (нужна будет предобученная сеть для transfer learning, качество не сильно важно, критична скорость forward pass)",
"если взлетит - напиши в чат, я чот не верю в нее -)",
"Всем привет! Может поможете советом?
У меня есть довольно большой корпус текста переписки с тех. поддержкой.
Я хотел сгруппировать вопросы по тематикам.
Для этого хотел лихо-задорно построить граф, где узлами будут уникальные слова, а ребрами связи между словами.
Связью считаю наличие двух слов в одной беседе клиента с тех. поддержкой.
После удаления стоп-слов и стемминга. У меня получилось где-то 130 тыс узлов и порядка 14 млн ребер.
И тут я понял, что ничего не выйдет. Даже небольшие (50 тыс.) сэмплы из этих ребер отрисовываются с большим трудом.
Скажите, я нормальный или вообще не в ту степь полез?..
Может можно как-то еще слова сгруппировать, а потом уже граф строить?
Или графы в NLP строить пропащая затея?",
"Ребята, с помощью каких методов, в первую очередь, вы бы решали задачу прогнозирования продаж для сети магазинов с учетом различных геоданных, включая данные о передвижении потребителей (пересадочные траспортные узлы и т.п.)? 
Я понимаю, что задача классическая и вопрос, вероятно, тупой. Но для меня анализ продаж пока нов. 
Меня пытаются убедить, что нужна простая панельная регрессия с кучей категориальных фич. Мне этот способ кажется не самым лучшим.
Свои идеи есть, но очень хочется услышать опытных людей.",
"Я в этом не спец, но если хотят чего-то интерпретируемого, то это плюс к регрессии, а если хотят геоданные, то приходит на ум Spatial Econometrics, нет? Главное усилие тут в том, как разные геоданные, условно говоря, ""в матрицу положить"".",
"Spatial Econometrics тема обширная. Вопрос как раз в конкретизации методов. Интерпретируемость нынче можно вытащить из большинства ML методов, на мой взгляд.",
"единственное, что если в перевозках есть нечто внесенное руками, например, были распродажи билетов, или есть какие-то регулярные распродажи, или известно о будущих распродажах, то можно это знание как фичи занести",
"вы таки будете смеяться, но у знакомого продажи подросли, когда он видимую с дороги вывеску сделал",
"Мы с <@U0AL6V5TN> об этом как раз после мейлрушного митапа говорили, когда в 5 утра надо было едальню в центре найти.",
"<@U14D53D2Q>: а можешь рассказать в личку, как ты его используешь? это для корректировки приоритетов, а то сейчас подавляющее большинство пользователей — либо студенты Воронцова, либо не cпешат давать фидбек.",
"А завтра кто нить по TS спец будет? Есть пара вопросов, был бы рад помощи ",
"<@U0Q3USC0Z>: По-моему это не причина делать костыль поперек чужого бизнеса. Как бы Вы назвали врача, который лечит пациентов тем, что они сами у него попросят?",
"ну смотря на какую рекламу попадёшь, можно попасть на серый магазин, который дженерики без рецепта из индии продаст :pill:",
А кто щас трансфер стайл мучает ?,
"чат, смотрите, а как мне эт победить можно? я пытаюсь сетку дообучить, но походу у неё формат новый",
"коллеги, я пушну свой недавний запрос. вдруг кто что подскажет :wink:
<https://opendatascience.slack.com/archives/theory_and_practice/p1466424872000299>",
"кто-нибудь знает, как в `tensorflow` применить функцию к тензору большей размерности, которая принимает тензор меньшей размерности по заданным координатам?",
"<@U04DXFZ2G>: только если ты не дообучаешь эти лееры, где умножаешь-сдвигаешь",
"<https://www.tensorflow.org/versions/r0.7/api_docs/python/script_ops.html#py_func> есть такая штука, но она применяет покоординтано. Пока думаю, как можно её подкрутить для моей задачи",
"если воспринять батчнорм как трюк для ускорения сходимости, потом впендюрить то, что он выучил в свертки и забыть что он был вообще",
Как будет работать на практике - вопрос :slightly_smiling_face:,
"<@U0G29N5U4>: ты мне? вопрос не в данных, а в основаниях сравнения. в байесовой статистике - параметры и гиперпараметры, плюс bayes factor. в фреквентистской - p-value, effect size и CI. вопрос в том, как их сопоставить, как понять, какой метод анализа дает более приближенные к истине выводы.",
"так я говорю: какой предиктит лучше, тот и верный! практика - критерий истины",
"а если серьезно, то 99% статей на мою тему использую анову на сильно скошенных данных. и когда говоришь ""ребята, так нельзя"", все кивают - ""мы понимаем, но дай прозрачный, понятный и сравнимый метод, что просто так критиковать"". 
поэтому и нужны примеры, объясняющие, что, как, почему и чем лучше/хуже. а не просто ""это опасная смесь"". сообщество ригидно и требует понятных примеров и инструкций.",
"плюс дать работающий алгоритм, показать на примере, как надо обрабатывать данные.
а то ""используйте байесову статистику"" - очень круто, но для рядового рисечера непонятно, как использовать на практике.",
"про косяки ну много где есть. надо на примере показывать, что допущения (гомоскедастичность, например) не выдержаны, а математикой метода требуются. под каждым же методом классической статистики есть своя математика (по большей части, частные случаи байесовских моделей)",
"вот смотри. например, та же нормальность распределения. или гомоскедастичность. как на данных показать, что нарушение этого требования  влияет на содержание статистического вывода? кроме апелляции к конструкции метода?",
"забутстрапь и покажи распределение среднего. после чего покажи, какое среднее дает формула - где оно в этом распределении",
"во, нашел, как раз для твоих коллег рекомендуют:
If you want to walk from frequentist stats into Bayes though, especially with multilevel modelling, I recommend Gelman and Hill

вот pdf: <http://bacbuc.hd.free.fr/WebDAV/data/Bouquins/Gelman%20-%20Bayesian%20Data%20Analysis.pdf>",
"да. блин, еще был какой-то туториал, где он учил mcmc, но никак не вспомню... там прям совсем все просто",
"да никто не спорит, что байесовщина - труЪ
хм. а представить анову как частный случай какой-то байесовской модели - это любопытная идея.",
"а ты о каком бутстрапе говоришь? о параметрическом, процентильном, просто случайном?
степеней свободы достаточно, чтобы налажать",
"друзья, я был уверен, что этот вопрос уже поднимался, но запрос по слову ""spot"" ничего не принес. 

как удобнее всего сетки обучать на спотовых инстансах? просто создать один раз AMI-образ и накатывать с него каждый раз на машину, подключая к ней EBS? или есть какие-то еще неочевидности?",
"Ребят, подскажите, пожалуйста, где можно найти информацию о политических отношениях между странами? Какой-нибудь дэйтасет, в котором попарные отношения описаны простыми категориями, например, война, мир, сотрудничество",
"<@U04423D74>: а на примерах из синтетических датасетов? типа, как на днях про p-value hacking. генерируешь одним каким-нибудь ненормальным распределением сет, берешь половину, считаешь анову, показываешь на второй половине, что она наврала, а байес точнее",
"вот я про это и спрашиваю - как показать, что байес точнее? если у них совершенно разные основания для стат.вывода?",
"Помогите, пожалуйста, понять в какую сторону ""курить"".
К примеру, у меня есть огромный список имен и фамилий и мне нужно найти одинаковые. То есть кто-то написал Ivan Ivanov, кто-то Ivanov Ivan, кто-то Ivan Ivanoff, а кто-то вообще опечатался и написал Ivan Ivaniov. Как это сделать за O(n)?",
<@U0QTS1LRF> а время в признаках есть? И как ты делишь на фолды?,
"судя по инфе в инете есть разные подходы к решению этой задачи, как чистая математическая оптимизация с ограничениями так и ML методы",
"у меня кэширование есть, при след. запуске не надо старое пересчитывать, т.е. можно новую модель вставить и обучаться будет только она
когда модели часами считаются - очень удобно",
коллеги а есть у кого нибудь ILSVRC2012 или аккаунт погонять на <http://www.image-net.org/> ?,
"Где-то есть правила, когда сколько фолдов брать?",
"Вот я и пытаюсь понять, почему много фолдов плохо",
"по теории leave one out - единственный unbiased estimator, как я вчера в ночи перечитал",
"на стекинге, возможно. если хочется именно разные модельки, как регуляризацию такую...",
"тут, похоже, как с бустингом, комбинация слабых предсказаний может сработать лучше, чем с хорошими",
как можно смотерть tensorboard на удаленном сервере?,
"я просто не знаю, как это делать :slightly_smiling_face:",
"вообще во всех случаях, когда надо доступ к какому-то порту получить, я юзаю <https://ngrok.com/>",
<@U0M39M6LS>: как то нарезан второй доклад,
"подскажите, пожалуйста, есть ли смысл в следующем подходе: нейронка на выходе выдаёт несколько label'ов, и loss считается, как min_i[ loss(y, y_pred_i) ] ?",
"Так, а что за `loss` и какой `y`?",
"Кстати, как ты инициализируешь веса у последнего слоя? Если они вдруг одинаковы, то это плохо и нужен symmetry breaking",
"дак по ссылке есть все, это как dropout в сетках",
"Вопрос в тему XGB. Был у кого-нибудь опыт запиливания кастомной objective function. В частности интересует такие цели, где надо расщипить выборку наилучшими образом. Ну там information gain или KL divergence?",
"<@U0DA4J82H>: Ну да. Просто у меня критерий хитрый. У меня бинарная классификация. Где надо предсказать условную вероятность. Тоесть есть еще одна фича, которую нельзя показывать модели, она является условием. И мне надо предсказать лейбл при условии, что эта фича чему-то равна",
Сейчас на кэггле как раз выложен сет с текстами объяв,
<@U0DA4J82H>: а в каком текущем авито?,
"<@U1CF4A6BT> а можно почитать где нибудь работы кафедры про выращивание и оптимизиацю сетей через ГА? По-моему, там должны быть серьезные проблемы с ростом объема сетей, интересно, что с ними делают в таком случае",
"Не знаю почему для вас математика казалась сложной, но я 3 года назад сдавал Quantitative часть GRE и получил 163/170, а мой друг 165/170, пользуясь только этим сайтом <http://www.majortests.com/gre/quantitative_skills.php>",
"Ребят, а кто-нибудь нашел решение для получения геолоцированых постов из Instagram после вступления в силу нового API в июне? У меня пока что получилось получать список популярных локейшнов и их фото, что сильно ограничивает выборку, хотелось бы как и прежде получать фото для заданных lat lon.",
"Ребят, может знаете, где можно достать достоверные данные о количестве нападений на врачей?",
"Коллеги, кто-нибудь знает, можно ли заставить gensim.Doc2Vec не учить word embeddings, а давать ему уже заранее готовые? Или там вообще фишка как раз в том, что и word vectors, и doc vectors учатся вместе?",
"Коллеги, подскажите со следующей задачей, пожалуйста. У меня есть множество последовательностей векторов с числами (последовательности размером в 5 - 20 векторов), причем последовательности могут быть разной длины (вектора все одинаковой длины). *Порядок векторов в последовательностях важен*, я не могу сделать bag-of-words. Я хочу сделать что-то, что позволит мне такие последовательности векторов отобразить в векторное пространство, когда похожие будут лежать недалеко друг от друга. Можно ли это сделать, и если да, то чем? Это типа если у меня word embeddings, и я хочу сделать нечто вроде doc2vec на готовых векторах, но чтобы сами вектора “слов” не изменялись.",
"Да можно просто с помощью RNN. Как раз то, что нужно делает: hidden state получается из прошлого hidden state и нового слова.",
"<@U1GK3944A>: а где всё-таки нашли инфу про нападения на врачей? нашла вот  результаты журналистского хакатона, поняла, откуда ноги у вопроса растут)",
"<@U0H7VBQQ1>: про рекурсив тоже ничего не слышал, но выглядит мощно, спасибо. А как с помощью обычных RNN учить word embeddings? Я не очень понимаю механизм, с классификацией вроде все ясно, а как сеть обучается вот так отображать сиквенсы разной длины в векторное пространство, мне не очень ясно. Можно почитать где-нибудь про это / какие-нибудь ключевые слова для поиска? ""RNN learn word embeddings"" выдает использование RNN со входом в виде word embeddings, а не обучение",
"embeddings можно и не учить, а взять gloves/word2vec. 
Тогда это просто таблица, по номеру слова выдающая вектор, как пользоваться -- надо смотреть на используемый фреймворк.

Я бы предложил курс Сохера cs224d :slightly_smiling_face:
Там есть несколько лекций по всему этому, со слайдами и ссылками на нужные работы.
Ну или по этой статье",
"Класс, интересно! Если белков про которые известны кластеры ""много"", то можно попробовать сделать rnn и либо предсказывать на выходе кластер, и тогда брать embedding белка как активации в предпоследнем слое, либо в явном виде учить метрику что белки из одного кластера близки, а из разных далеки.",
"Спасибо :slightly_smiling_face: Не так много как хотелось бы, несколько тысяч, с кластерами по 100-200-400, к сожалению, и вообще тут есть глобальная засада - тот факт, что пара белков имеют одинаковую функцию, формально не означает, что они не могут выполнять другую функцию; при этом по кластерам в данных они разбиты четко, без какого-либо fuzzy матчинга, т.е. метрика не проходит, хотя очень хотелось бы.",
<https://vk.com/topic-44016343_34179927> - кто что думает?,
"<@U0M7UMFDF>: в deep learning есть результаты, когда вектора умножаются на некую константу по очереди",
"В смысле, первый вектор в предложении берется как есть, второй умножается на alpha, третий на alpha^2 итд",
"и еще детские вопросы) может их лучше в другом канеле задать -- вы скажите где тогда) 

как я понял, в итоге сеть преобразует пространство, чтобы классы стали линейно разделимы. а если классов больше, чем два, то все тоже самое? только будет N гиперплоскостей, отделяющих каждый класс от всех остальных?",
"а какой датасет картинок бы заюзать для создания так сказать нулевого класса? т.е. класса, который мы и не пытаемся распознать?",
<@U040M0W0S>: а какая бизнес задача-то стоит?,
"Есть методы one class learning aka outlier detection когда у нас только положительные примеры есть, а мы хотим отделять их от отрицательных",
"как бы обучить сетку, чтобы она сказала, что это не то, что она должна распознавать)",
"всем привет. ищу работы по решению задачи оттока клиентов. у кого есть ссылки на годные материалы, подскажите, пожалуйста.",
Кто на Prisma сегодня идет?,
ну можно спросить как он так быстро работает,
Ну или как DL приложение для массовости деплоить,
"Да, реально интересно на каком фреймворке у них продакшен",
"<@U04URBM8V>: если пойдешь спроси про фреймворк, пожалуйста, и где хостятся",
А ты сам почему не пойдешь?,
"На связанную тему, завтра кто на завтрак зайдет? )) Будет два спец-гостя завтра.",
"лол нет. чаевые пошли не по адресу. вот когда дойдут куда предполагалось, ""все норм, все устаканенно"".",
"ну какая там у Призмы. вряд ли она сильно удивительно архитектуры, так? :slightly_smiling_face:",
"зачем они свой run_cv пишут, если это уже есть в sklearn?",
"<@U0FEJNBGQ>: у них есть есть ранг, но он как раз о веса зависит: больше вес - больше ранг",
"газ, а где проще всего порисовать такие деформации пространства?",
<@U14GG4E69>: почему не на R?,
"а какая разница, на чем валидировал сначала? это на гиперпараметры влияло как-то? Доверять нельзя, раз train просочился в val2. Насколько все плохо - от данных зависит, и от сетки",
"не, выстрелило то потому что прикольная штука. людям нравится. но так как это работает быстро то это стало возможным проектом без кошмарных счетов за сервера",
"тяжело масштабироваться, когда нужны сервера с видеокартами. дорого. плюс даже с ними базовые варианты трасфера стиля работают порядка минуты",
"между минутой и долей секунды -  большая разница, когда это время ожидания для человека на телефоне",
"но я тут принципиально не согласен. есть куча ситуаций, где оправдано взять что-то работающее и пофиг на скорость. но не когда любой запрос пользователя требует дорогущее железо и минут времени. (если это масмаркет с кучей запросов)",
"Основной риск у них - отсутствие исследований хоть каких-нибудь рынка , даже данных не собирают о пользователях никаких. Как бы не было слишком поздно об этом думать сейчас",
"А кто-нибудь сталкивался с проблемой, когда lua не даёт занять 4ую GPUшку ?

То есть на первых 3х код запускается, на 4ой — нет, вылетает с 
```THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-3218/cutorch/lib/THC/THCGeneral.c line=176 error=2 : out of memory
/home/user/torch/install/bin/luajit: /home/user/torch/install/share/lua/5.1/trepl/init.lua:384: cuda runtime error (2) : out of memory at /tmp/luarocks_cutorch-scm-1-3218/cutorch/lib/THC/THCGeneral.c:176```",
"Коллеги, а кто-нибудь помнит как называлось и где есть данные соревнования, где по историям болезней (посещениям врачей), нужно было предсказывать количество дней на больничном в течении года?",
"Еще вопрос, который сегодня забыл задать на ods завтраке: Как связан lr и batch size?",
как вообще оптимальный размер батча считать?,
"можно было найти того кто зарегался, но не пошел -- от его имени пройти типа). вроде могло сработать. по слухам",
А зачем делать отбор признаков перед использованием сети? ,
"<@U1J56K7RA>: если ты сможешь по ""коду"" (т.е. по представлению, полученному из автоэнкодера) понять, какие фичи там самые важные, то можно просто максимизировать соответствующий нейрон по входу, т.е. найти вход, максимально активирующий этот нейрон – это поможет понять, какие фичи важнее, а какие нет (и, возможно, что в ""коде"" вообще выучилось)
Правда, это будет хорошо работать, только если фичи ""кода"" более-менее независимы между собой",
Для определения важности можно еще Layer-wise Relevance Propagation сделать как вот тут: <http://arxiv.org/pdf/1606.07298>,
"Чат, если грузить 4 видюхи из 4х, то обычно когда 3 загружены под завязку, вылетает:

```THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-3218/cutorch/lib/THC/THCGeneral.c line=176 error=2 : out of memory
/home/user/torch/install/bin/luajit: /home/user/torch/install/share/lua/5.1/trepl/init.lua:384: cuda runtime error (2) : out of memory at /tmp/luarocks_cutorch-scm-1-3218/cutorch/lib/THC/THCGeneral.c:176
stack traceback:
        [C]: in function 'error'
        /home/user/torch/install/share/lua/5.1/trepl/init.lua:384: in function 'require'
        im2im_test.lua:2: in main chunk
        [C]: in function 'dofile'
        ...user/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x00405ea0
```",
"а кто какую реализацию tsne использует?
есть базовая: <https://github.com/lvdmaaten/bhtsne>
есть, похоже, основаная на базовой, но с нормальным биндингом к питону <https://github.com/danielfrg/tsne> и почему-то другой реализацийе деревьев.
кто-то из сравнивал по скорости/качеству?",
"Ребят, кто может с tensorflow помочь?",
"Народ, кто сталкивался с ""floating point exception"" при обучении сети? После 1ой эпохи вылетает ошибка. В чем может быть причина?",
"Ну, керас ничего кроме ""floating point exception"" не выводит..Есть соображения как получить более полный вывод?",
"Думаю, надо выяснять на каком леере случается проблема",
"хотя, все делаю как у авторов",
"Это как рассказывать про перемножение матриц с помощью `np.dot` — тривиально и должно быть известно каждому, кто занимается вычислительной математикой",
<@U04ELQZAU>: а ты где такой вариант встречал?,
"<@U040M0W0S>: такой — это с разложением по тейлору? Нигде не встречал, просто как пример придумал :simple_smile:",
Почему .7/.3 ^ 2 ?,
а у меня почему-то пропали GPU споты из амазона. это почему такое может быть?,
"Граждане, а в какие вузы Питера принято поступать у школьников, которые метят во что-нибудь ит ориентированное. И что еще более важно, на какие факультеты?",
"И, как следствие, по сложности поступления. На первые 2 выпускнику, заранее об этом не побеспокоившемуся, не попасть, а вот на мат-мех вполне реально",
"коллеги, подскажите. вот есть данные размеченные на 3 класса типа bad, avg, good. как лучше перейти к модели которая просто ранжирует все в пределах [0,1] ? один вариант - переразметить данные (оценку из [0,1] тогда придется придумывать самому, например сделать 0; 0.5 и 1)...",
"Сугубо личное наблюдение, выпускники политеха как-то не встречаются мне нигде, а вот итмошники наоборот, да и итмо сейчас набирает обороты, у црт там своя кафедра, по сути. А почему по-вашему, на мат-мех поступить проще, чем в другие 2 вуза? Победы на олимпиадах принимают и там и там, но сам экзамен по математике, как мне кажется, на мат-мехе точно не проще",
"Его восхваляют за пионерскую работу в области поведенческой экономики (behavioral economics), также рекомендую пару книжек от Dan Ariely в этой области. Про когнитивные искажения и Биасы знали и до этого, но в популярную оболочку не многим удавалось завернуть в то время (щас у нас есть HPMOR например). И мне ближе к концу этой страницы не нравиться что он описывает Bayes rule как будто это закон природы а не математическая концепция.",
"у него интересно не про bayes rule, конечно. там очень хорошо рассказано как раз про когнитивные искажения. с хорошими примерами и очень интересно",
<https://stanford-qa.com/>  если кто не увидел на reddit’е,
"Он, в частности, пишет как рейтинговые агентства перед кризисом не догадались в своих формулах заменить вероятность на условную и из-за этого все их прогнозы облажались.",
"Кстати, да. По-моему на бейсболе я как раз и застрял.",
"&gt;&gt;а у меня почему-то пропали GPU споты из амазона. это почему такое может быть?
может у тебя спот инстансы? -)",
"А кто-нибудь в курсе, сети с архитектурой а-ля Inception предсказывают медленнее, чем с архитектурой как в AlexNet (сначала свертки, потом полносвязные)?",
"тук, тук. 

подскажите где подтянуть знания по nosql (в частности mongo db) и mapreduce.",
На сайте монги вроде есть курс. А с sql как дела обстоят? ,
"у меня 60 Гб логов. надо придумать как распараллелить и уменьшить затратность обработки.
ибо не хватает ресурсов и медленно.)",
Кто-нибудь занимался построением графов дорог из данных gps авто? Как быстрее лучше и быстрее сделать?,
"<@U11ATF3GR>: а ещё батчем предсказывать как правило быстрее, чем по одному изображению ",
"Ну CNMeM, как мне кажется, это просто свистелка сбоку: полезна, но и без неё нормально жить можно",
"если надо обработать все 60 gb, индексы не помогут. они помогают, только если надо мелкие единицы процентов где-то выбирать по одному запросу. и монга тоже точно не поможет. самое быстрое, когда надо большой объем обработать - тупо последовательное чтение с диска",
Возможно глупый вопрос. Зачем для нахождения коэффициентов в линейной регрессии использую градиентный спуск если есть аналитическое решение с помощью МНК?,
"<@U1J7A5067>: когда матрицы становятся неадекватного размера как в ширину, так и в длину, становится эффективнее использовать спуск. плюс спуск можно by design обвешать любой осмысленной регуляризацией (для которой аналитических решений не существует)",
"Привет всем. кому не жалко - объясните человеку, как работает призма, навскидку? вопрос именно к dl-составляющей сервиса. Интересно же)",
"коллеги, есть совершенно примитивная задачка. ""найти по набору признаков похожий на заданный интервал другой временной интервал в недалеком прошлом"". в качестве признаков могут быть выходные, какие-то другие кастомные события и прочее. как бы вы это делали?

у меня на данный момент совершенно дубовое решение - беру каждый день года, от него откладываю нужный мне период, считаю стату по целевым признакам. а потом беру тот интервал, который ближе по времени к заданному. но это долго и неэффективно, на мой взгляд. может, еще как-то можно сделать?",
"уточни ""похожий"", а то как-то разнообразно получается. можно сделать словарь вида ""рабочий"", ""выходной"", ""новый год"" и сравнивать как строки каким-нибудь edit distance, можно продажи по дням евклидовым расстоянием",
"Всем привет. Может, у вас завалялся датасет с картинками машин Авито или вы знаете того, у кого завалялся? Поделитесь ссылкой, пожалуйста.
Спасибо.",
"<@U06J1LG1M>: Я уже наверное раз 5 людей встречал которые хотят разобраться в логике ResNet'а, но выходит не очень, давай семинар устроим? С меня площадка и трансляция/запись, могу с слайдами помочь тоже, разберем статью, алгоритм, и ты раскажешь как конвертнул в лазаньку. Как тебе ?",
"Вспомнился анекдот про то, как два нолика увидели в пустыне восьмёрку. ",
"<@U06J1LG1M>: Ну так нужно объяснить почему он работает, теорию про вектора представлений",
"вся москва сейчас пилит вторую призму. и хочу транслировать деловое предложение для тех, кто тоже хочет в этом разобраться. в общем, надо на aws развернуть сервис с api: картинка + стиль -&gt; превьюшка (256 px, например) и картинка + стиль -&gt; hi-res (от 1200px)

первое api должно отрабатывать примерно за 10 с, второе - дольше. можно по оригинальной статье гатиса - лишь бы быстрее запустить в прод. но такое время отклика надо выдержать. вроде, <@U06J1LG1M> утверждает, что это реалистично

мой товарищ предлагает бюджет за первый прототип (сама нейронка, веб-сервер, средства администрирования спотового инстанса) - от 20 тыс. руб.

p.s. если что, конкурировать с призмой не планируется, это для сервиса печати принтов на футболках и проч. поверхностях",
"чот маловато по моему предлагает, но я мимокрокодил",
"а кто во сколько это оценивает? спрашиваю, чтобы дать обратную связь",
"<@U13E1AWCX>: мне казалось, что mc высокого порядка - это все-таки про условные вероятности по стейтам на несколько шагов назад. но тут какие есть отличия:

1) вообще говоря, в профиль входят не только прошлые стейты, но и другие атрибуты объекта, условные вероятности переходов хависят и от них;

2) горизонт, на который надо смотреть назад, часто весьма далек; кажется, что практичнее генерировать флаги ""когда-то был в таком-то стейте"", а не откручивать назад всю цепочку;

3) и как сказал, граф далеко не полносвязный, а вот стейтов относительно много (десятки); хочется минимизировать количество параметров",
"есть много, конечно, же статей о том, как отмасштабировать mcmc и вар. вывод в таких условиях",
"если хочется как у призмы, то да придется хорошенько покурить",
"вообще в той модели есть какие то аномалии, я прост в тф не понимаю еще что в кишках, и кароче там же лосс сеть есть, и она вся не трейнабл, но почему то фреймворк пытается аллоцировать память под градиенты этой сети",
и хз как это отключить,
"Объявляю эту неделю — неделей ML-тренировки.

Очередная тренировка по машинному обучению пройдет в ближайшую субботу, 9 июля:
- Василий Лексин (<@U1GG0N16H>) из команды Avito расскажет про RecSys Challenge 2016 (ребята заняли 7е место)
- Михаил Першин расскажет про Kaggle Facebook Check-in Prediction (контест завершится через два дня, болеем за Михаила!)
- возможно, будет рассказ про Kaggle Draper Satellite Image Chronology
Мероприятие как всегда пройдет в московском офисе Яндекса, начало в 12:00.
Регистрируйтесь и приходите:
<https://events.yandex.ru/surveys/3492/>",
<@U06J1LG1M>: плюсую за доклад про резнет. Я вот до сих пор не пойму как у них в юните сумма делается если один из входов после свертки уже другой размер имеет,
"это как бы дает возможность сети не использовать слои слева, если они не нужны",
"&gt; Или вопрос в том, что делать если изменился размер входа?
Как мне кажется, всё сводится к тому, правда ли используется zero-padding, так что свёртка считается в каждом пикселе входа, поэтому и выход будет того же размера",
"Подскажите, какие бывают источники данных, желательно текстовых (фотки точно не нужны, ну разве что может описания к ним), с геопривязкой? Кроме твитов и FQ, они уже есть. Хорошо бы что-то с привязкой к конкретному пользователю.",
"Нет, только лично общался. Да и стал бы кто выкладывать ноу-хау в этой области.",
Хотя вот <@U0AL6V5TN> когда работал в похожей конторе стал :smiley:,
"<@U06J1LG1M>: Ты видишь как люди запутались, давай семинар сделаем )",
FB уже давно не выдает через апи такое. Так как речь идет о работе с как бы открытыми данными к смс естественно тоже доступа нет.,
"<@U04URBM8V>: если соберётесь делать семинар по резнетам, можете попробовать <@U0CQN3R0W> позвать. Насколько я знаю, он работает с ними и может что-нибудь рассказать про то, почему они так хорошо работают",
"Если кто провозгласит новые кеглы, рассказывайте про них в <#C043ZEF6K|kaggle_crackers> пожалуйста",
"К сожалению, это все не актуально, так как данные нужны по Лондону",
<@U070Y25AS> А почему такой вопрос?,
Это примерно как увидеть их в выдаче Гугла.,
"<@U1CE1A01J>: про скоринг на данных соцсетей и телефонов есть разные кейсы, могу рассказать при случае, но материалов как то не помню",
"Для тех кто еще не был на завтраках:
1) Если хотите рассказать или только послушать приходите
2) Поставьте плюсик под событием что бы я примерно знал сколько столов понадобиться.
3) Завтра мы возможно по экспериментируем с ведением на русском и английском встречи",
<@U04URBM8V>: а для новичков и нубов - как опознать вас можно будет? :slightly_smiling_face:,
<@U0AL6V5TN>: а зачем тебе аккаунты? чужие переписки читать?,
Вот такие беседы как раз нормализует подобные практики,
"Вопрос, в каком фреймворке для DL вы смотрели на имплементацию Batch Norm и она внятно написана? желательно python/C++",
"<@U14GG4E69>: Это форк какой то, или в master ?",
"А почему не добавили, как думаешь ?",
"это как - “а что бы ты сделал, если бы у тебя был миллиард долларов?”
то что он попадет честным или нет путем - не оговаривается",
а есть у нас тут кто нибудь гуру тф?,
<@U040M0W0S>: Кто ж славы не хочет?,
"в лазане нормас да, я недавно резнет когда портировал с каффе, потупил слегка от того что сразу не осознал как в каффе сделан БН",
"Хм, а как на следующей слой отдавать признаки?",
"Ээээ, точно так же, как и всегда",
Куда не ходить не помню ,
"гайз, а как можно вопросы к текстам генерировать? или по каким ключам искать статьи?",
но так как есть для обучения машин -- я бы рад посмотреть и на них :slightly_smiling_face:,
"у меня гуглится много про ответы на вопросы, но как из ответа сделать вопрос). хм. это куда проще, как я могу представить.",
Я видел два подхода в DL как по тексту генерировать вопросы,
А кто делает клон призмы ещё?,
"вообще, банки выносят довольно странные решения. Реальный пример: заемщик с чистой историй, но выглядящий как гопник из 90-ых с сомнительными документами получает кредит и конечно же его не платит, а программисту из крупной ИТ компании с подтвержденным по 2-НДФЛ доходом в over200к отказывают, потому что в студенчески годы он взял кредит в 50 тысяч, потом у него возникли трудности и у него появились просрочки.",
"Кроме скоринговой модели, в банках есть хардчеки разной степени интеллектуальности. Программист просто попал под один из них. Если выдачи из истории происходили не в один момент, то ""гопник"" мог попасть пока риск у портфеля был нормальный, а программист когда уже не очень и скоринг уже ужесточили (включили хардчек на кредитную историю, например), чтобы держать средний риск на портфель за период допустимым.",
"ну и случайный элемент есть, насколько помню, в этих моделях, то есть клиенту с определенным рейтингом дают добро с вероятностью X целенаправленно. Не помню только, зачем именно так, многорукого бандита может тренируют",
"а есть фотки, где Саша не говорит?",
<@U107256SG>: а на каких позициях?) ,
"<@U107256SG>: имел ввиду ситуации, когда выводы аналитиков не имеют важности при принятии каких либо стратегических решений, и на деле аналитик что-то делает, а толку от его работы по сути и нет",
"<@U04CH4QBD> кстати, ты когда уже студенток приведешь? ",
Я уже 4 года как не преподаю на регулярной основе.,
"<@U04URBM8V> и всем кто пришел, спасибо за отличный завтрак!",
"Я тут сейчас пытаюсь делать topic modelling на твиттере, нужно в перспективе кластертзовать пользователей по интересам. Нарыл статью, обсуждающую именно этот подход (конкатенируем тексты пользователя, lda по ним).  Единственная проблема - у меня данные по лондону за последние 2 недели, а там полный бардак, референдум и футбол. Как бы этот временный шум отфильтровать попробовать, если в прошлое я посмотреть не могу? А то сейчас у меня все подряд твитят про политику и футбол, а основные увлечения маскируется под этим",
"В том, чтобы высыпаться и просыпаться когда хочешь, вне зависимости от режима к которому привык. 
Закрываешь окно, так чтобы не зависить от естественного цвета. Ставишь волшебные лампочки от филипса за много денег. Лампочки меняют цветовую температуру и яркость перед сном, чтобы подготовить организм. И плавно повышают интенсивность перед пробуждением, чтобы ты проснулся как с естественным рассветом",
"А если вставать по будильнику, то ощущение будет как разубили посреди ночи",
"Когда рассказал, то дико захотелось попробовать",
"Ну там есть свои сложности, сначала нужно научиться тогда по хорошему классифицировать твит на футбол/брексит/не знаю, и делать это достаточно качественно, чтоб всю дату не уничтожить. Меня как раз интересуют подходы к фильтрации. Попробовать тот же топик моделлинг и вручную убрать соответствующие темы, например?",
"Не подскажете, какой ""канонический"" способ сравнения двух слов в w2v модели? Взять евклидову норму разности векторов? Косинусное расстояние?",
"только зачем расстояние, если можно cosine similarity",
"А если сравнивать предложения, то как лучше делать - усреднять вектора слов или средняя попарная похожесть слов?",
"ну среднее это самое простое, на самом деле можешь посмотреть как в homedepot делали
ты же для конкурса это делаешь",
"<@U11ATF3GR>: зачем вам word2vec, у вас и без него неплохо получается :slightly_smiling_face:",
"&gt; А если сравнивать предложения, то как лучше делать - усреднять вектора слов или средняя попарная похожесть слов?
А это не одно и то же, если косинусная мера близости?",
"аналог продать легче, но кто будет покупать сырой аналог, когда есть опережающий всех оригинал?",
"<@U040M0W0S> в реальность коммерческого успеха? нет, не верил) и до сих пор как-то не очень верится. потому что не понимаю бизнес-модель, не понимаю, кто будет платить за такой сервис. и поэтому еще менее верится в коммерческую успешность клонов. аналогично как и с MSQRD, кстати, про который пошумели-пошумели и быстро забыли.",
ну вот разве что продаться крупняку какому,
"<@U0AD1L5NC>: а какие модели, с твоей точки зрения, могут быть жизнеспособны?",
"Ребят, как *правильно* добавлять validation на tensorboard, чтоб не было конфликтов summary?",
"спросить, как же вы там без Cliff Click что-то делаете",
А почему не созвать R meetup?,
"Тот факт что это он к нам обращается, исходя из того что люди говорят про H2O, можно его твиты почитать, <https://twitter.com/raymondpeck3/> , много интересных технических деталей я не заметил, хотя он пишет о себе как ""Hacker at amazing #opensource #BigData #machinelearning startup @h2oai.""",
"А вообще байесианство, как текст подсказывает, это в том числе про умение отличать посылку от следствия, так что всё правило пацан делает :thumbsup:",
"ну как бы говорить - не мешки ворочать. Встав на дорогу евангелизма, писать будешь только в powerpoint, программировать будет некогда.",
"преобразование с целью уменьшить размер тех или иных данных. ""сжатие""? как я понимаю, к сеткам ее применяют после обучения.",
это когда float в int превращают?,
"а где можно было бы прочесть на высоком уровне абстракции, что изменилось в математике лет за 50?",
"Но это всё теоретическая математика, в математике прикладной все эти области используются как инструменты для решения конкретных задач. В непрерывной оптимизации, например, без (суб)дифференцируемости всё очень плохо",
о. выложили видео со стажировки в deephacklab. засим вопрос: а кто туда пошел?,
":but_why: я сам вряд ли досмотрю. Миша говорит, что разговорный интеллект решили делать. и описывает поведение животного на схеме, как из учебника кибернетика -- обратная связь, программы поведения.",
"Подскажите, пожалуйста, какой инструмент лучше и быстрее всего учить для разработки одностраничных веб-интерфейсов (backend - python). Из предполагаемого JS / TrueScript, JS -&gt; NodeJS / AngularJS и т.д. Стоит ли вообще лезть в JS или есть более стройный и/или эффективный язык?",
"Я Егору ответил -- не тебе. Ной дальше. Но лучше приватно, как ты умеешь. ",
"Нет, не должно. А какой браузер / javascript включен / пробовал страницу обновить?",
"у меня как раз есть намерение кое-что разработать, но знаний о JS (как и опыта) явно недостаточно",
"Народ, как лучше использовать имя пользователя в качестве фичи при обучении? В ngrams и CountVectorizer сверху? LabelEncoder? Или использовать вероятность?",
"<@U04ELQZAU>: на math.stack... все работает. Исправил, оказалось chromium сам блочит скрипты, так как считает их небезопасными",
только как ключ для схлопывания история профиля,
"Стат фичи активности пользователя? Таких уже вагон, само собой. Проблема в том, что люди не указывают пол, ибо поля с полом нет, по идее его можно вытащить с имени, но в ручную лень, а готовых ништяков не нашел. Та же беда с  возрастом, т.к. необязательное поле. При ручном анализе обнаружилось, что всякие молодые Анны пишут не ""Анна"", а ""Анечка"". Вот и подумал, что как то использовать имя в качестве фичи, поможет сгладить эти две проблемы",
"взять с готовым корпусом и посмотреть как там имена расположены вообще, может они в двух кластерах находятся",
"Чтобы определить пол можно посмотреть с какими фамилиями имя чаще встречается . Если с фамилиями на ов, ев, ин значит мужской , если на ова, ева, ина — женский . Дамп <https://vk.com/catalog.php> есть в <https://github.com/alexanderkuk/crawl-vk-catalog>",
"хотя не помню, где я это читал",
"&gt; думаю фейк
Ну да, наверное как следующая по популярности комбинация",
"я вот когда обеспокоился о приватности в сети, было уже поздно, и фамилию они меняли только по предоставлению фотки паспорта :disappointed:",
"У Яндекса как-то слишком хорошо получилось найти все мои профили в соцсетях. И теперь все, кто от меня получают письма, могут кликнуть и посмотреть мой вк",
Не знаю как они это сделали. Имейлы разные,
"Почему может inception v3 переучиваться?
Тренерую фром скретч на датасете авито по определению категории объявлений. 194 класса, 655к сэмплов в трейне.
Использую mxnet. Batch 96. Аугментации: crop, mirror, aspect ratio, rotate, shear. С lr 0.01 доходит до 0.38 акьюраси на валидации и дальше она падает вне зависимости от lr. 
При этом Inception BN доходит до 0.51 акьюраси с теми же аугментациями, но с батчем 192. Googlenet BN доходит до 0.45 с батчем 200.",
"коллеги, какой может быть вариант решения для такой задачи: у меня строка есть гигов 20, мне нужно организовать поиск подстрок",
думаю пока как индекс прикрутить,
"положим у меня строка не прерывная, мне даже тогда разбить не как",
а подстроки какой длины могут быть?,
а какой длины могут быть подстроки?,
"пока не понятно какой они могут быть длины, но по идее могут приближаться даже к 1Гб",
вообще какой подход можно в этом случае попробовать?,
"а сам поиск - попробовать индексировать первые n символов, и затем пройтись там, где эти n символов нашлись",
"но я в биоинформатике не силен и не знаю, как серьезные мужики это делают",
"можно индексировать несколько кусков подстроки размером n и потом смотреть, где наиболее вероятно это подстрока целиком может быть",
"надеялся, может кто подскажет, как делают взрослые дядьки)))",
"только вот, понимаешь ли, тут придумалось, а на нипсе уже 12 лет тому назад это было...

для кластеризации walks по графу, который представляет собой марковскую цепь, можно представить эту цепь как смесь цепей, введя латентный параметр

а вот и статья: <http://papers.nips.cc/paper/2519-simplicial-mixtures-of-markov-chains-distributed-modelling-of-dynamic-user-profiles.pdf>

<@U04ELQZAU>: и кстати, таки да, они тут вариационный вывод тоже заюзали",
<@U07V1URT9>: from scratch какая архитектура?,
А почему другой batch size?,
"Я меняю lr, когда точность по валидации перестаёт меняться. ",
Я не знаю какие у тебя еще есть,
"питерцы, а не устроить ли нам посиделки завтра?
<@U0G29N5U4> <@U04F2H8FM> <@U0471Q5AU> <@U04A7SNGU> <@U0BLCTAJK>, кого еще забыл ?",
"~То никто не заморачивался, как было размечено, так и подавали. Данные грязные~",
"Спасибо. Еще один вопрос. Можно ли как-то на пальцах объяснить, какой принцип работы у алгоритма отделения стиля и формы объектов на изображении? Чтобы представлять мой уровень понимания, я пока представляю выходы слоев сети , как набор картинок, которые представляют из себя свертки через множество фильтров. На первых слоях мы имеем примитивные градиентные фильтры, и подымаясь к верхним слоям фильтры обретают форму конкретных объектов. Но как из этого всего выделить стиль, мне не очень понятно.",
"Еще хочу спросить по основам самой сверточной сети. Получается один нейрон - это один сверточный фильтр? И если так, то если мы проходим по изображению серией фильтров-нейронов то на выходе у нас получаются features maps для каждого нейрона в отдельности или они как то суммируются в одной карте?",
А каналы соответствующие каждому цвету обучаются совместно или как три отдельные сети?,
"Слушайте, а что такое нейрон? что-то туплю. вот есть формулы, по которым считать свертки и тд, берем то-то отсюда, умножаем вот эту матрицу на вот этот вектор. Какие их части - это нейроны?",
"Изучение нейронных сетей с ""учителем"" идет куда быстрее :slightly_smiling_face: Спасибо большое за ответы на простые вопросы.",
"Так, теперь с последней полносвязной частью сети остается вопрос, в итоге у нас есть какое-то количество карт и входы первого полносвязного слоя к каким ""пикслелям"" карт подключаются? Если рассмотреть скажем первый нейрон.",
"А понятно почему avg, а не max?",
"То есть, компании которые делают API для определения объектов на картинке, как правило сами тренируют свои сети? Интересно с таким огромным числом нейронов и связей, сколько времени занимает обучение на 1000 классов",
"Всем привет, а кто-нибудь из тех кто едет на тренировку сегодня хочет перед тренировкой пообщаться/позавтракать?",
"<@U053R9RS6>: я подъеду к 11, куда пойдем?",
<@U04BFDYPV>: Я собрался как раз обсуждать новую задачку в Братьях Караваевых,
"Уменьшил немного batch size - стало меняться.
Какого размера вообще должен быть этот параметр относительно количества объектов?",
"а на что смотреть, если я хочу реализовать какой-нибудь алгоритм, который описан в статье и выложить это на гитхаб?
как понять, что я ничего не нарушаю? учитывая тот факт, что на статью будет ссылка",
"Я тут сижу, жду когда кто зайдет",
"Ну гоняться они смогут только за теми, кто на территории сша. Вроде как патенты только там действуют",
"<@U0JHK9001>: правильнее всего написать авторам и спросить, какие есть ограничения. Правда, они редко отвечают... :confused:",
"а если сидеть в зоне EU на ec2, то вроде как и законы не нарушаются?",
"&gt; Ну гоняться они смогут только за теми, кто на территории сша
Ассанж и Сноуден не просто так прячутся там, где прячутся. У штатов есть соглашения о выдаче с большинством цивилизованных стран. Правда, вряд ли это применимо к таким преступлениям",
Всем привет! Разбираюсь с работой сети VGG19 пропустил через нее картинку получил на выходе 1000 значений. Где взять соответствие номера выхода к классам объектов?,
"Зашел сейчас на Market Fair, нет ни <@U04URBM8V> ,  ни девушки, ни магнитофона. Вообще, тягостное впечатление, как будто это выставка поделок пионеров из 80-ых годов прошлого плюс демонстрация работы 3D принтеров.",
"помнится на датазавтрак робот приходил с датчиками и ML, как он, интересно...",
"Похоже на то, ближе к 13-14 людей побольше будет и все готовы. Это не американский Maker Faire конечно, но это начало,  организаторы хотят поднять это сообщество в Москве, потому что все довольно разобщены, я им рассказывал как сообщество аналитиков поднялось через чат и на ивентах. Хотел бы что то подобное для DIY/Maker/Hardware Hacker’ов увидеть",
"но в контексте автоматического фича инжиниринга, dl же все равно какой там -- были б закономерности/синтаксис.",
"И еще, когда я не мог понять CNN, мне очень помогла вот эта лекция <http://cs231n.github.io/convolutional-networks/>",
"Ребята, у кого есть cudnn для линукс, финальная 5-я? 5.005 тоесть . Можете мне кинуть в личку",
"Серьезно, или у вас ссылка может у кого есть? Иногда на университетских серваках архивы индексируются.",
"Только начинаю разбираться с предсказанием врем. рядов. Вопрос по задаче:
Надо предсказать продажи клиентам. Клиентов ~100к. Частота рядов месячная. Ряды разреженные: в среднем по 4 ненулевые точки за период с 1997 года по 2014 (много клиентов, имеющих покупки всего в одном месяце). Предсказываем продажи на 2015 год.
Первое что приходит в голову: предсказать нулевую продажу клиентам, не имеющих покупок, скажем, в течение последнего года. Тут вопрос: какое лучше правило выбрать: отсутствие продаж последний год/полгода, что-то более сложное?
Далее, для оставшихся рядов что имеет смысл пробовать? Какое-то скользящее среднее?",
"Надо для каждого клиента предсказать продажи на каждый месяц 2015-го.
Данные непредставленные я посчитал бесполезными: какой именно продукт покупал клиент (их немного вроде), адрес клиента.
Цель - как можно точнее предсказать.",
Для точности предсказания какая метрика используется?,
"если это учебная задача, то может быть там и спрятали какую-нибудь хитрую сезонность, типа какие-то клиенты покупают первый понедельник какого-то месяца, но искать это непонятно как",
Есть у кого-то реализация или хорошее описание как правильно делать OOF?,
"а где почитать, какие классы функций можно приближать нейронными сетями?",
"Да, я хочу больше технического творчества, объединения сообщества, новых ивентов, интересных проектов, спрос на новое железо и алгоритмы, как для науки, искусства, хоббиизма так и для прототипирования коммерческих вещей.",
"А кому то платят как на основной работе за это. В компаниях которые продвигают товары, и государственное финансирование при универах и центра дополнительного образования.",
"<@U14GG4E69>: Какой уровень ты хотел бы увидеть? Гуманойдные роботы? Свои VR / AR хедсеты? 
Это маленькая мини ярмарка летом, она не исчерпала “инновационные” проекты в Москве/России",
"Так вот, у тебя есть мысли, почему на выставке нет качественных поделок на ардуино?",
"Помогите придумать способ визуализации. Есть несколько классов точек с координатами (lon, lat) в пределах одного города. Хочется как-то на одной картинке изобразить плотность распределений, чтобы их можно было удобным образом сравнить. Просто нарисовать bivariate kde - не очень красиво получается, так как плотные области накладываются друг на друга и картинка становится унылой совершенно.",
"Они сильно пересекаются друг с другом, но, скажем так, максимумы плотности у большинства явно не накладываются. Мысль просто вырезать максимумы какой-нибудь изолинией была, но тогда теряется информация о поведении за ее границами, а она потенциально может быть интересна.  Вот пример другого распределения (тут фокусы как раз почти совпадают, но поведение на удалении от максимума разное, вот их например интересно может быть сравнить) <http://imgur.com/a/NeMkp>",
"В чем может быть косяк - играюсь с архитектурой сверточной сети, но какие слои ни добавляю/удаляю - точность не меняется никак",
"Тут бэкграунд примерно такой: сначала пользователей соцсетей кластеризую по их интересам/предпочтениям, потом смотрю, как локализованы их посты с геотегами. Потом еще во времени нарисую, может там динамика перемещения какая-нибудь есть. Есть проблема - не очень понятно, на сколько кластеров kmeans этих пользователей бить. В качестве одного из инструментов для оценки качества кластеризации хотел использовать такие картинки - если географическая локализация у кластеров пользователей почти идентична, то, наверное, их можно сливать в один.",
какая изначальная цель - проверить где кучкуются люди из соцсетей по интересам?,
"значит надо сразу 3д карту рисовать, так как кластера тоже обитают во времени (и кластеризовать 3д точки)",
как сделать чтобы она училась? :slightly_smiling_face:,
"расскажу про честный статистический подход

есть легальные способы, но они технически сложные.тебе нужно будет считать честный KLD между например смесями Гауссиан если делаешь это с EM (с грамотным процессингом данных в том числе чтобы ковариации не схлопнулись в 2д), причем в предположении что число компонент смесей может не совпадать. либо придумать эвристику из ракушек и говна :slightly_smiling_face:

я вопрос задал к тому, что судя по постановке задачи, тебе надо работать с 3д плотностями и искать кластера в них (плюс продумать модель ""фонового кластера"" ввиду типа данных). если повезет, конкретные типы юзеров всплывут в подавляющем большинстве в отдельных кластерах (в предположении примерно равных пропорций юзеров, иначе еще и на это придется делать поправки). если нет, надо будет по каждому типу юзеров натянуть наилучшую адекватную модель плотности и посмотреть, где они пересекаются, а где выделяются  (но это тот еще квест)

еще можно схитрить, обучить какойнибудь rbf-svm разделять классы и выделить области данных, где точность максимальна",
"Кажись я понял…. Когда я y переводил в категориальный массив, я его по-разному закодировал в train и test :facepalm:",
"<@U04ELQZAU>: а чо, pretrained models какой на этих autoencoders есть?",
"<@U04ELQZAU> есть такой афоризм, что писать код проще, чем читать. А как ты думаешь, к текстам о баесовском подходе это не относится? :motokozak: :levenchuk: ",
а почему степени свободы в распределениях зовутся так? что за свобода-то?),
"<@U0QTS1LRF> считал недавно лстмки на 960, долго. Большой размер батча не взять, так как памяти мало. Но она и стоит весьма недорого нынче",
"<@U06MTEXQQ>: я буду говорить про байесовкий подход, потому что разделение частотный-байесовский подход исключительно в интерпретации, математика одна и та же.
Всё начинается с вероятностной модели твоих данных p(x, y). Тут тебе нужно из каких-то соображений понять, как данные могли бы быть сгенерированы. Например, линейная регрессия предполагает следующий вероятностный процесс: сначала мы откуда-то берём `x ~ p(x)`, а потом y генерируется как линейное преобразование x с каким-то шумом `N(W x, Sigma)` (В зависимости от размерности W, y может быть как вектором, так и вектором размерности 1, т.е. скаляром)",
"Ну это как раз то что в книжках пишут, вопрос как раз в том, как правильно генеративную модель сформулировать",
"Если ты вводишь ещё одну переменную W, то, на самом деле, надо рассматривать модель p(x, y, W), но ты можешь на W положить априорное распределение, сконцентрированной в одной (неизвестной) точке, и трактовать W как параметр, а не случайную величину",
"ага мне вот как раз интересно, где этому обучиться?)",
а какие генеративные модели (чаще всего) встречаются помимо линейного преобразования с шумом?,
"<@U040M0W0S>: любое параметрическое распределение, где параметр приходит из какой-то комбинации других переменных",
:slightly_smiling_face: ну да. вроде как тоже большой пласт генеративных моделей.,
"&gt; любое параметрическое распределение, где параметр приходит из какой-то комбинации других переменных
<@U04ELQZAU>: а приведи, пожалуйста, пару примеров :koala: походу я их как-то не так гуглю.

<http://reference.wolfram.com/language/guide/ParametricStatisticalDistributions.html>
вот, например, список параметрических распределений, но у кого из них параметр приходит из комбинации переменных?)",
"спасибо! а если вернуться к вопросу anokhinn, то глядя на приведенные гистограммы какую вероятностную модель можно выбрать?",
"кстати. для чайников - посоветуйте что-нибудь базовое по вероятностному подходу? потому что я достаточно активно играюсь/смотрю на распределения
но понимания этого всего как парадигмы нет",
"Спасибо!
А где можно найти код, который показывает какие картинки максимально активируют определенные нейроны? (другой способ понять что представляют собой веса)",
"а чем плохи bic или результаты кросс-валидации как model selection? есть что-то другое разве? я со статистическими моделями работаю ровно так же, как с ml - тупым перебором. смотришь, на что похоже распределение evidence чисто визуально, как правило, это классическое распределение, но либо overdispersed, либо mixture. соответственно, придумываешь несколько вариантов, как предикторы могут это генерировать, составляешь модели, сравниваешь по упомянутым критериям",
"как мой любимый пример, overdispersed - negative binomial (gamma-poisson) распределение. когда целевой показатель очень похож на poisson, но дисперсия не равна лямбде. значит, нужен prior на лямбду, который выразится в том, что лямбда сама является стохастической функцией от каких-то предикторов (не обязательно gamma-распределение, можно аппроксимировать и нормальным)",
"<@U0G29N5U4>: если датасет большой и репрезентативный, то все будет примерно одинаково хорошо работать. вопросы начинаюстся, когда реально есть риск переобучения, либо необходимость жесткие гарантии предоставить",
"<@U0G29N5U4> то есть ml это когда можно брать простые модели потому что много репрезентативных данных, но так как их много, то без компьютера их не посчитаешь? я примерно правильно понимаю твое определение?)",
"<@U0KQ5M6KX>: 
&gt; Если не знаешь теорему, то говоришь, что это ML и ""тут так не принято»
я своей завлаб так и ответил однажды, когда она стала мне какие-то вопросы про распределения задавать.",
"ага, как раз деревья, леса и бустинг чем и круты, что решают проблемы со странными распределениями ошибок",
"мне интересно, какие эвристики имеет в виду Дмитрий :slightly_smiling_face:",
"Да какая тебе разница, ты же всё равно в этом ничего не понимаешь?",
"<@U040M0W0S>: ну, для начала, куча алгоритмов - это само по себе сплошная эвристика. pca, kernel trick и svm, факторизационные машины опять же. почему именно они должны сработать в обучении - какая-то математика это объясняет. но кажется, что иногда эти объяснения получены постфактум",
"посны, а кто нибудь юзал в лазане TransposedConv2DLayer?",
"или вот <@U054DU76Y> не так давно рассказывал, как он использует в качестве фич средние значения по группам, но при этом их несколько стягивает к глобальному центру. для этого есть целая теорема, что именно такой estimator минимизирует дисперсию, но он, говорит, вроде сам придумал",
"у меня сцука эта размерность не ту выдает, на 1 меньше, как будто при последнем страйде не делает свертку, а если увеличить пад (в нотации TransposedConv2DLayer это crop), то выдает на 1 больше",
"<@U1NMKU9DY>: есть результаты что k-means это результат сильно притупленной Гауссовской смеси, если бы ковариации были одинаковыми Id. есть даже вариант притупить вариант EM для смесей

но это скорее пример того, как хак (алгоритм, придуманный в 50-ых) впоследствии нашел себя как частный случай теории и моделей, развивавшихся сильно позже",
"Если тему моделей чуть шире взять, на примере финансовых моделей, то рекомендую посмотреть лекции Кирилла Ильинского: <https://www.lektorium.tv/speaker/3058> (первые 3 например). В них автор пытается ответить на вопросы, что вообще такое модель, как ее получить, как оценивать результат. Но будет понятно только тем, кто в теме опционной математики.",
"В этот четверг в 18.00 в Мэйле все-таки состоится лекция Рэймонда Пека из Н2О. Так как все это спонтанно, регу сделать не успели, будет только завтра. 

Аннотация:
&gt;H2O is the leading open source big data machine learning platform. This talk will cover the distributed parallel in-memory data storage and compute architecture. We will then move on to examples of using H2O from R, Python, Java and Scala. Depending on interest we can also cover Deep Learning in H2O, integration of H2O into smart applications, and/or the optional integration with Spark.",
"Кстати, насчет gpu, вот здесь: <http://www.phoronix.com/scan.php?page=article&amp;item=nvidia-gtx-1070&amp;num=4> автор тестирует на caffe alexnet, результат примерно как у титана, уже сейчас можно купить 1070 чуть больше  30 тысяч, потребляет 150 ватт. Этот же автор через неделю обещает протестировать 1060, которая будет продаваться за $250, NVIDIA обещает уровень gtx 980.",
"кстати про гомоскедастичность. тут больше нет упорков кроме меня, кто угарает по явному моделированию дисперсий (как отдельных функций) в рамках восстановлений полных распределений? вроде много нового народа появилось :slightly_smiling_face:

heteroscedasticity is so 80s, все дела",
"фактически готовый код есть, как хакнутым xgboost-ом условную дисперсию восстанавливать (но не до конца честно так как истиные сплиты по дисперсии хз как сделать)",
очевидный пример со всеми вытекающими - когда  тебя интересует количественная оценка рисков,
"Да, когда матожидание известно и нужно оценить риски",
"хм. раз уж зашла речь о параметрических игрищах
ведь у нас далеко не всегда целевая переменная распределена нормально, например, там какое-нибудь скошенное трехпараметрическое распределение - как можно оценить влияние отдельных фичей на параметры распределения?

ну и в целом, есть ли практика качественной интерпретации параметров, или это блажь и важнее качество фита?",
"<@U0FL6RNHM>: в СПбАУ очень сильный народ был, когда он открылся. Туда со своих программ ушли люди из СПбГУ и из ИТМО, в т.ч. парень, который в том году ACM выиграл с командой СПбГУшной",
"<@U040HKJE7>: а чего это гетерогедостичность это ""so 80""?  Как диагностика моделей мне кажется точно актуальность не потеряла. Да и методы лечения какие-то другие есть? 

Вот кстати к разговору ""ML vs Statistics"" . Мне кажется можно моделировать среднее хоть бустингом сетей, но потом остаточки посмотреть как статистика учит это святое дело",
"возможность смотреть на остаточки никто не отменял :slightly_smiling_face: просто из инструментария есть: 
-гипотезы с которыми нужно смиряться
-взвешенные поправки на линейную регрессию
-гауссовские процессы (ок, для малых данных даже интересно местами)
-хардкорные модели (старые на процессах, полноценные байесвоские модели, попытки как чуваки из ETH моделировать все целиком)",
"как бонус, последний вариант вообще ведет к моделям с conditional uncertainty на более глубоком уровне чем гауссовские процессы. в любых задачах",
"по-крайней мере как перетащить эту uncertainty на классификаторы есть хорошее понимание (не говоря о регрессии, которая эталонная задача)",
"Почему мартингалы, разве слабой стационарности не хватит? Это вроде слабее. ",
"Ну если заниматься, то конечно да) я скорее имел ввиду про ""_and_practice"" Прост часто вижу как люди делают чот крутое и сложное, а на остатки не смотрят. Мне это кажется странным. :flushed: ",
"раз про практику заговорили - как мерять, хорошо ли промоделирована дисперсия?",
Предлагаю ответить на более простой вопрос даже: как вообще измерять дисперсию?,
"<@U040HKJE7>: а как ты меряешь вообще что такое дисперсия, чтобы обучаться? или ты только для финансовых данных?",
оцениваю как и среднее внутри сплитов (можно считать произвольные статистики внутри),
"Можно) Я чего спросил то (возможно мой вопрос звучит как-то по дурацки). Просто для финансов это целая отдельная задача оценить дисперсию. Потому что если ты оценил ее  true, то сразу понятно как заработать. Ну а такая метрика как ты написал, работает не очень. Поэтому я испросил)",
а ты про какую именно? временной ряд или оценки в сплитах (пробовали такое?),
вы как с отдельным рядом с ней работаете?,
"Когда работал, то да, как отдельный ряд. В том числе, из всяких деревативов выковыривались IV.  По сплитам мне кажется понятно, почему плохо: у тебя могло среднее внутри сплита переключиться, а дисперсия (которая на самом деле существует) не измениться. (Не во временном ряде это тоже возможно). А в оценке она сильно изменится. Выходит, в начале надо хорошо научиться прогнозировать среднее. И задача обрастает задачами)",
"На хабре развернулось целое обсуждение, до чего же физтех жаден, требуя 250к за год этой магистратуры, как при очном обучении
<https://habrahabr.ru/company/mipt/blog/305296/>",
"там вроде бесплатно будет первый год, так как они вряд ли наберут больше 10 человек",
"<@U040HKJE7>: есть же распределения, где дисперсия входит в градиент правдоподобия (на самом деле, до фига таких в экспоненциальном семействе). Соответственно, читал, как народ EM-алгоритмом это все фитит, но сложновато выглядит, конечно. Либо MCMC, если позволяют ресурсы. Или я неправильно тебя понял?",
Я почему то думал там 6к. 20-ка как-то многовато,
"Онлайн-магистратура должна бы дать возможность учиться людям, у кого нет возможности очно это делать в Москве. Для регионов 250к - это дохуа",
"<@U1FLG6YR1> действительно, проиндексировали. Я поступал когда 160 было. Все равно это прям заметно ниже чем в, например, вышке, или даже бауманке.",
"<@U1CGKK865> Если честно, не знаю, как сейчас. Я сдавал две сессии без троек и пересдач в прошлом году, может что-то и поменяли.",
"Перенесу вопрос с <#C04DP7BUY> : Есть у присутствующих отзывы о <http://www.mtuci.ru/structure/faculty/otf1/> - кафедра , которая  выпускает направление (01.03.04) Прикладная математика. По перечисляемым дисциплинам кафедра кажется очень актуальной. Присутствующие знают , насколько это дисциплины хорошо преподаются ? Какой у кафедры статус ?",
"Если есть хотя бы 230 баллов с трех предметов - можно поступать в пристойный вуз, ну на платное конечно, но, вообще говоря, это не так и страшно (найти 20к в месяц на обучение в год могут многие). Другой момент - есть предположение, что особой разницы где учиться - нет. Да, хороший вуз с хорошими преподами по всяким базовым математикам прям ощутимо помогает, но если нет особого желания что-то учить, то это будет только в тягость, лучше уж наоборот выбрать максимально халявное заведение, чтоб не тратить время на противные вещи.",
"Ну в шад чот сложно поступить, я посматриваю регулярно на задачки экзамена, это как раз выучить самому, без внятного базиса из института сложно достаточно, как мне кажется. А потом еще и на собеседовании деревья на вайтборде разворачивать...",
"<@U1QLTT2L9> ну бывает, я, например, завалил информатику, так как мне перед егэ было веселее пофрилансить чутка на апворке на британцев, чем тестовые задачки решать, но 55 - это совсем мало.  Если хочется - всегда можно попробовать платку мфти, может и возьмут. У нас вроде учился парень с баллами в районе 210, но он как раз вообще не знал, что он в вузе делает, ничего не добился, естественно",
Так почему без олимпиад неразумно в вуз идти?,
"Потому как получить 100 баллов по предмету по олимпиаде проще, чем 80+ по егэ набрать",
"Ну, как водится, поступать можно:
- по ЕГЭ
- по олимпиадам + ЕГЭ (обычно по 1 предмету, непрофильному)

Так вот. При условии существования второго варианта, который кажется лично мне более простым (по крайней мере для меня; хотя бы потому что я считать не умею и ЕГЭ никогда на full 100 не писал), первый кажется мне менее рациональным.",
"&gt; Потому как получить 100 баллов по предмету по олимпиаде проще, чем 80+ по егэ набрать
&gt; Для этого правда нужно весь год задрачивать олимпиадные задачки, на что меня не хватило
Это какие-то взаимоисключающие парагрфы. Я в своё время набрал 90+ по математике и информатике, не прилагая особых усилий, а ты тут целый год задротить semi-ACM предлагаешь",
"<@U04ELQZAU>: ну как, я пришел вообще без какой-либо подготовки на олимпиадку по математике, не добрал одного балла до дипломчика победного. Как мне кажется, чтоб реально надежно там что-то выигрывать, нужно готовиться реально. Просто готовиться к олимпиадкам более надежно, чем к егэ, т.к. олимпиадки можно. условно говоря, пересдавать неоднократно",
"А ну это просто лайфхак получается тогда, как средство поступления. Я думал речь о том, что если ты не такой умный и не решаешь олимпиады, то все, в вуз можно даже не пытаться идти",
"<@U1BAKQH2M>: ""Потому как получить 100 баллов по предмету по олимпиаде проще, чем 80+ по егэ набрать"" &amp; ""только егэ по математике чертовски простое"" =&gt; олимпиада по математика проще чем чертовски просто",
"Всем привет. А есть тут кто из шад""а? Кто учится/учился?",
"Или везде, куда ни глянь, сплошная nvidia?",
"А тебе для каких целей? Так-то есть AMD ATI, но они к обеду на рынке научных вычислений опоздали",
"Мне в основном из праздного любопытства. Интересно, как устроен рынок железа для deep learning",
"Увидел, как курс акций nvidia вырос вчетверо за последний год, и задумался",
"<@U1CSA6ADS>: большинство фреймворков завязаны на cuda от nvidia, т.к. она очень много инвестировала в научные вычисления ( присутствие тут <@U14GG4E69> как сотрудника nvidia естественное следствие этого ). Есть вялые подвижки среди разработчиков фреймворков в сторону  OpenCL – открытого стандарта вычислений на видеокарте (и не только), но, как я понимаю, держится оно только на энтузиазме",
"Ну как бы основа dl, это BLAS, то есть NVIDIA всего лишь предложила способ как это можно сделать при помощи CUDA, распараллеливание расчетов. То же самое мне кажется можно сделать и на картах AMG и не Xeon phi, и на FPGA. ",
"<@U04CH4QBD>: Если точек не ахренеть как много, то folium может подойти.
<https://folium.readthedocs.io/en/latest/>
<http://nbviewer.jupyter.org/github/python-visualization/folium/tree/master/examples/>",
не знаю где ещё спросить :disappointed:,
"Хочется сделать кластеризацию с кастомной метрикой, а вот distance matrix считать не хочу (150к векторов, больно жирная матрица выходит). Какие у меня есть опции? Писать свой kmeans с подходящим под метрику определением центра вместо mean как-то грустно",
"мне кажется тут про кастомную метрику основной вопрос

как прокластеризовать по кастомной метрике, не считая 150^2/2 матрицу",
"мне самому интересно, как это можно хакнуть",
"к слову про dbscan. шняга, пытался прокластеризовать результат tsne. думал ""о, это точно задача для dbscan"". хрен там был. зато быстрая имплементация hclust отлично справилась с задачей. прям не ожидал от такой штуки разумной работы как раз на 100к точек",
<@U1BAKQH2M>: метрик много. вдруг ты захочешь тут DTW запихнуть - одна пара будет считаться по времени как вся кластеризация в евклидовом пространстве,
Только до меня чот не доходит как хотя бы попробовать написать метрический тензор для этого,
"посмотри как это делается во всяких LMNN, или в статье с closed form solution для geometric mean metric learning с последнего ICML что я скидывал",
"это как раз норм, даже многовато для metric learning'а (отсюда и идея с маленькими сабсетами на 1к точек например)",
"Идея попробовать обучить нейросеть мне прям очень нравится, как раз надо за лето обязательно что-то из этой области изучить. Нет пока нормального понимания того, сколько ресурсов надо для таких экспериментов (только лстмки в разных вариациях гонял пока). Есть вообще смысл пытаться начинать над этим думать и что-то читать, если доступ пока есть только к одной gtx960?",
"что-то для ""Хочется сделать кластеризацию с кастомной метрикой"" metric learning реально как перебор выглядит",
хмм а почему с неевклидовой нельзя?,
"есть варианты как можно переформулировать расстояния, сводящиеся к евклидовой постановке (см <http://stats.stackexchange.com/questions/81481/why-does-k-means-clustering-algorithm-use-only-euclidean-distance-metric>)",
"<@U04CH4QBD>: да. В доках написанно, как обрезать карту по стране",
"попробовать запилить метрик лернинг - выглядит как очень интересное упражнение, вечером позадаю тупых вопросов на эту тему, как с какой-нибудь статьей ознакомлюсь.",
"как я ее полностью считать буду, куда мне ее пихать?",
"Stochastic Gradient Descent в задаче регрессии, где может быть только не отрицательные значения, выдает много отрицательных,  приходиться для отрицательных  ставить равным 0 - есть еще какие-нибудь подходы?",
"<@U040HKJE7>: читаю статью про gmml. До меня правильно доходит постановка задачи? Уже есть какой-то сорт кластеризации (который как раз позволяет составить матрицы S и D) и хотят найти такое вложение(? можно это так назвать), чтоб в новом пространстве примерно такое же разделение точек получалось при измерении расстояний стандартной метрикой, так?",
в ней достаточно инфы на куда больше чем 1к пар того и другого типа,
"не уверен (прочитал, сходу не понял как переложить)",
"я же и так знаю, как посчитать точное расстояние между точками, мне не критичен weak supervision, я могу и strong замутить",
"как только проекция построена, сразу смотришь на первые 2 оси на всех данных. если все хорошо, то ты увидишьразумную картинку что с тренировочными что со всеми остальными данными (распределения должны быть похожи)",
"Так, а оценивать работоспособность подхода как будем? Взять какой-нибудь датасет боль-менее внятных размеров, посчитать на нем матрицу расстояний с желаемой метрикой. Сделать вложение данных через проекцию, посчитать матрицу евклидовых расстояний, сравнить?",
"эм. я правильно понимаю, что легальность происхождения этих данных - как минимум сомнительна?",
<@U1BAKQH2M>: а для какого корпуса топики выделяешь? ,
"чат, посоветуйте, пожалуйста, куда гуглить.

есть временной ряд, и подозрение, что часть ряда примерно описывается одним трендом, а часть другим, направленным в другую сторону. хочется определить точку смены тренда.

я сходу придумал только кусочную линейную функцию, но кажется, что для такой задачи есть какие-то более зрелые решения?",
почему фотка от сюда а тебя не видно?,
"Хмм, у нас много пушей. Но честно говоря не вижу что тут оптимизировать? Большой поток разумно объединяем в один пуш. Отображением рулим на клиенте. Интересно было бы узнать что тут можно оптимизировать, какие метрики.",
"Как пример: ясно, что пуши для новостей с утра открываются гораздо лучше",
Может есть какие статьи/модели известные.,
"Если редкий поток, то засылаем как есть.",
"Вспомнил про историю математика из России, который работал на Wall Street, писал код для очень крутой компании, взял немного своего кода на флешку, когда решил уволиться, его задержали и посадили на 6 лет",
Кто поднимал CUDA + cuDNN + torch на маке?,
"<@U1FLG6YR1>: <@U1G303UTW> хотел про пуши и бандита спросить — правильно я понял, что предлагается запустить бандита примерно в таком режиме — один и тот же пуш присылаем одному и тому же пользователю утром, днем и вечером и смотрим, когда он будет на него нажимать? Я просто как раз думал, что можно попробовать предиктить под каждого пользователя, но тут ведь можно с того же самого начать, нет?",
"Бандит, как я понимаю, подразумевает однородность пользовательской базы",
"Ну да, но его идея вроде в том, что ты предоставляешь пользователю и А, и Б, и другие свои вариации и смотришь, какую активнее используют",
"Чуть-чуть читнул, вроде понял — идея в том, чтобы использовать однородность пользовательской базы и сначала раскидать их по вариациям пушей равномерно — например, только утром/днем/вечером. А после того как это сделано, трэчить CTR для каждого времени отправки и переводить все больше пользователей в выигрышную вариацию. Оно?",
"Вообще, за счёт однородности, мне кажется, можно заранее не фиксировать ""группу"" пользователя, а случайно выбирать, когда ему пуш отправить, в соответствии с текущим распределением",
"меня тут консультант сейчас битый час уверял по телефону, что грант получить как два пальца",
"ну если за грант, то почему бы и нет",
Зачем мы обучаем модель первого уровня на всем тренировочном сете? ,
"Были времена, когда Owen Zhang просто менял `random_seed`, делал ансамбль из 10 xgboost'ов и выигрывал соревнования, но это было пару лет назад :slightly_smiling_face:",
"А как это дело стакать? Сначала складывать 10 xgb, а потом стакать? ",
У меня вопрос какие параметры тренируются в ходе собственно distillation,
"ну это можно делать, когда предсказания сильно нестабильные, иначе какой смысл?",
"<@U0ZHHV83C>: я не пробовал, но не вижу никаких технических преград для этого. Веста - просто матрица, куда её потом передавать (gpu/cpu) не имеет разницы",
Какая разница получились они с gpu или с cpu?,
"Как и читать ответы, я так понимаю...
&gt; <https://www.tensorflow.org/versions/r0.9/how_tos/variables/index.htmlc>
&gt; там про это ничего не сказано, вроде бы)",
"Еще вопрос. 
При стекинге у нас получается, что новая фича составлена из предсказаний нескольких моделей. 
То есть там есть как грубые предсказания случайного леса, так и более точные, например, предсказания нейросети. 
Есть ли смысл делать так, чтобы каждая фича была сделана из предсказаний родственных методов? Допустим, одна вся сделанная нейросетями, другая вся сделанная деревьями и т.д.",
"когда дело заходит на уровень 2 и выше - есть смысл попробовать _все_
и так, и все подряд всем подряд, и навыворот (леса предсказывают сети, сети только леса)

но скорее всего лучше и проще всего будет работать полный фарш всего со всем",
"со стороны это похоже на подростковые эксперименты с тем, кто смешает более дикий ерш из всего бухла, которое удалось достать",
"<@U0FEJNBGQ>: привет! хочу попробовать применить LSTM к задаче классификации временных рядов различной длительности (от 3 до 10 отсчетов на ряд). можешь, пожалуйста, дать ориентиры? например, какие методы предобработки чаще всего используются (e.g. resampling перед LSTM), экспоненциальная фильтрация, т.д.",
<@U049HDR2Z>: а что на входе? почему б данные не скормить в модель как есть?,
какой длины ряды в среднем?,
3000 это не так. здесь уместно рассматривать каждую последовательность как сэмпл,
"как раз хотел написать, что лучше сгенерить сколькото фич описывающих каждую из мини-последовательностей и классифицировать обычными моделями
в зависимости от обстоятельств, можно еще фичи каждой сессии-испытуемого добавлять",
"еще можно заморочиться нормализацией, как в координатах, так и в скоростях после фильтрации, сделать фиксированное начало-конец, простой resampling (линейный) до одинаковой длины, а там использовать евклидово расстояние между последовательностями как ядро",
"по-хорошему, не такие уж простые. т.е. если подумать как юзер может их выполнять, вариантов много. вариативность во времени, углы, сила надавливания (если это имеет значение)",
"так и аугментация относительно легко делается :slightly_smiling_face:, и выборку можно быстро дополнить. у меня вопрос был, скорее, в самой модели LSTM: насколько она подходит для данной задачи, и какой препроцессинг желательно делать явно.",
"<https://habrahabr.ru/company/goto/blog/305526/>
Кто то из goto в чате есть?",
"Чушь какая-то. Да, если взять 20, то шансы больше, чем если брать одну при прочих равных. А 21 ещё круче. Но как это приближает к решению, если выбрать все равно только одну надо? При чем тут Монти Холл вообще не понял",
"как остановить процесс, когда случайно в стандартный вывод бинарник или что-то огромное выкинул?",
"<@U0ZHHV83C>: я пока досконально не разбирался, но я не увидил никакой специфики относительно типа переменных или используемого вычислительного бекэнда при сохранении. У TF свой бинарный формат (вроде как на базе protobufs) хранения переменных, который, как мне кажется, не привязан к конкретным вычислительным библиотекам/функциям. Иначе было бы странно. 
см. <https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/kernels/save_op.cc>
<https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/lib/io/table_builder.cc>
<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/tensor_slice_writer.cc>",
"<@U0RV376MC>: Нет проблем никаких, я просто спрашиваю, кто что знает про эту движуху.",
"В ближайшую субботу будет праздничная тренировка. На прошлой неделе завершилось соревнование Avito по выявлению дублирующихся объявлений, топ лидерборда заполнен отечественными дата саентистами. А Станислав Семёнов, наш постоянный участник, стал абсолютным чемпионом мира на Kaggle!

На тренировке:
1. Разберем Kaggle Avito Duplicate Ads Detection
2. Андрей Зимовнов вместе со мной расскажет про опыт участия в соревновании от OTP-банка для конференции ECML/PKDD 2016
3. Презентуем учебную задачу для тех кто только начинает участвовать в соревнованиях. Кроме того, обсудим, что сейчас можно порешать из больших соревнований.

Форма регистрации на мероприятие:
<https://events.yandex.ru/surveys/3562/>
Встречаемся в субботу, 23 июля, в 12:00 в офисе Яндекса (БЦ Мамонтов), в ШАДе.",
"Появилась задача предсказывать уход клиента по его поведению в консоли пользователя и при работе через API (предоставляем услуги полупубличного облака). Может кто-то уже занимался таким и может подсказать, где поискать кейсы или статьи полезные? Или же может поделиться опытом.",
"А кто чем LDA считает? scikit-learn, gensim, vw, bigartm, еще что-то, и почему?",
"lda в R, потому что знаю R лучше питона. и немного пробовал gensim, так как по нему первому материалы увидел.",
а как оно по скорости выходит?,
"но у меня как-то медленно все считается в scikit-learn, 150K документов по ~300 слов каждый, 50 топиков - часа полтора на 16-core машине; если CPU time считать, то больше суток :) вот и думаю, не налажал ли где",
"там языки произвольные, я не знаю, как в этом случае стемминг делать",
"я могу сейчас соврать очень сильно, но вроде как разные формы lda воспринимает как разные слова",
"нормального ничего не встречал, в gensim wrapper есть, но я не знаю как там все устроено
c весами там вроде скрипт на perl у них репозитории лежал",
если в scikit-learn - cython реализация того же алгоритма (ну как я понял),
"что вы с этим lda потом делаете, друзья? прям на топики глазами смотрите или как фичи для дальнейшей работы?",
"у меня и глазами, и как фичи",
"но vw при этом использовал 1 ядро, а не все, как scikit-learn",
а какая размерность была на выходе?,
<@U064DRUF4>: а ты это исследование публично повесишь где нибудь? На github или еще где?,
"Друзья, может кто поможет понять : я тут пытаюсь запустить паблик скрипт из StateFarm, он основан на pretrained vgg16 модели",
"в упор не вижу, почему у меня 16 layers перед загрузкой весов?",
"да, наверное мой вопрос в том, как видоизменить граф, чтобы он был компактнее но все еще был совместим с vgg16",
"можно статью написать об этом, если найдешь как :slightly_smiling_face:",
Это как раз для любителей) Для про -- свой ящик с парой Titan'ов под столом),
"коллеги, очень наивный вопрос. как делать ml, когда всего два класса, и их соотношение в количестве записей - 1:30, да и наблюдений не миллионы строк?
можно сбалансировать численность классов в обучающем семпле (например, добавить дубликаты записей малочисленного класса), но мне эта идея интуитивно не очень нравится.",
"1) Взвесить лосс 
2) Смириться (в смысле, использовать как есть)",
"если данных мало, можешь как раз mboost позапускать со всякими gamboost и glmboost. он прожорливый по памяти, но эвристиками старается оптимизировать AUC напрямую, а не просто как eval-function в xgboost",
попробуй и если стало лучше - почему нет,
"пацаны, а какие еще архитектуры для CV встречаются?",
"сейчас не могу из-за времени влезть в глубокий анализ адекватности. но на практике, когда мы решали churn задачи как классификацию, соотношение классов 1 к 100 как раз оптимизируя по AUC давало более чем разумные результаты",
"я правда хз как такое трактовать, но картинка получится залипательной",
"узнал кто победил в евро-16, день прожит не зря :grinning:",
"а кто подскажет как в питоне проще всего прикрутить autoencoder, просто для снижения размерности (Sparse ?). Через Theano или есть другие либы (от Andrew Ng?)",
а почему в ШАДе `Основы статистики в машинном обучении` только в 4 семестре?,
а что это меняет? почему бы ему не быть как можно раньше?,
а как спец.курс по выбору хорошо,
"Коллеги!
Посоветуйте факультет для абитуриента в Москве. Где у нас дают хорошую базу для big data кроме МГУ, Вышки и варианта ""матчасть+ШАД""? Чадо выразило желание заниматься big data, способности к математике-информатике есть, уклон пока скорее технологический - т.е. нам интереснее построить хадуп-кластер и наполнить его данными, чем строить стат.модели на этих данных. Хотя понятно, еще сто раз передумает)) Спасибо!",
"Коллеги, а как можно (и нужно) решать задачу бинарной классификации двух строк.
На входе у меня есть корпус текстов из 1,969,791 пар, где каждая фраза 15-20-30 слов и соотвественно класс — 1 или 0.
Идеи которые пробовал:
 - Stemmer для приведения к нормальным формам (да, это английский язык)
 - Собираем вектор из каждой пары как BagOfWord (ngram/n-skip-gram в диапазонах: 1, 2, 3, 1..2, 1..3, 2..3) и дальше либо вычитаем один из другого, либо просто собираем в один большего размера.
 - IDF и ChiSq для выбора важных пространств пробовал.

В итоге сейчас auROC в районе 0.75, что несколько расстраивает.",
"попробуй обучить w2v
и добавь cos векторов как фичу (и сумму векторов)",
"<@U04CH4QBD>: там тексты как-то относительно связанные между собой. Связаны они ""общими словами"" и IDF не дает какой либо пользы.",
"Вопрос был не на что идти, а куда идти.",
"С момента когда учился я, все изменилось",
Почему не мгу и вышка?,
"МИФИ 19 июля:

ПМИ: БВИ 1(+0), ЦП 2(+0), ОП 1(+0), ОК 439(+17), Оригинал 47(+2), (258..308)*, как минимум один низкобалльник"" добавил документы — нижняя граница сместилась вниз на один балл;

ПМФ: БВИ 1(+0), ОК 336(+11), Оригинал 19(+0); (233..298)*;

Физика: ОК 324(+19), Оригинал 23(+0), (216..289)*.

Идет вялый поток документов, не БВИшников, которых поглотил пылесос ВШЭ, не новых высокобалльников не наблюдается. Единичный порог оригиналов еще не преодолён ни на одном направлении.

* диапазон баллов указан по оригиналам.",
"но там уже как видно тоже бюджет не получится. лучше в вышку тогда, если платно",
<@U1R8X3V54>: Какой из факультетов относится к ИТ ?,
"<@U0ZSV3Y83> а что мешает подать документы везде и смотреть? Когда я поступал, подал доки в 5 вузов, по 2 направления в каждом, и спокойно ждал-смотрел на рейтинги, хоть и еще до экзаменов точно знал, куда пойду.",
Так ему оригинал надо отдавать же куда то,
"w2v. 
косинус - да, а вот сумма или разница векторов как фича может не взлететь, хотя попробовать стоит",
"+ синонимы как предложил Алексей стоит попробовать (как в tf-idf, так и w2v)",
и то в каких классах словосочетания - как фича,
"Спасибо всем кто пришел на завтрак, если нужно на что то из обсужденного ссылки выложить, спрашивайте, попробем найти.",
"<@U0J31JTU4>: В голову новый формат приходит, скринкастинг jupyter ноутбука где все считается, а люди в чат кидают советы ))",
"Подскажите, как вытащить где-то 1 млн. координат по адресу?",
"<@U0PEXP9PH>: у меня это два класса пользователей, притом предмет интереса - именно малый класс. а фичи - поведение пользователей. 
я правильно тебя понял, что ты предлагаешь попробовать определять вот этот малочисленный класс как аутлаеры, а не просто классификатором?",
"на этот митап регистрация с подтверждением от организатора, а в какие сроки будет это подтверждение раздаваться?",
<https://opendatascience.slack.com/archives/theory_and_practice/p1469013478000189> вот как раз это nominatim и делает,
"В спбгу вообще много разных айтишных и околоайтишных программ, возможно, найдется такая, куда с 230 получится поступить",
"ну дальше можно cosine посчитать, в LSA засунуть и взять самые первые компоненты как фичи и много что еще",
а как качество топик моделинга измеряете?,
"Я, может быть, чего-то очевидного не вижу, но как в генсиме получить просто матрицу docs*topics? Можно конечно вызывать get_document_topics для каждого дока, но это как-то тупо",
"<@U09JEC7V0>: Пытаемся, ты заходи как нибудь )",
"вообще не важно о чем говорят на ""завтраках"" -- идея в том, что это общение людей, которые так или иначе занимаются дейтасаенсом :slightly_smiling_face:. а там уже как разговор повернется. ну в чатике у нас так же.",
"а куда именно пытался, в Армению, Азербайджан или Рязань?",
"А они постоянно меняются, как и автокомплит в Убере (черт бы их тоже побрал). Один раз куда-то в район шереметьевской улицы, два раза мимо аэропорта куда-то в сторону Твери (к служебному въезду? Один раз не уследил, другой раз вовремя свернул).",
"Мораль сей басни: с этими роботами люди совсем совсем перестали думать головой, лень даже в карту самим посмотреть и проверить лишний раз. Тем более когда человек может опаздывать на самолет, например.",
А это какой batch size?,
"А как backward делается тогда, в resnet же BN?",
"<@U04URBM8V> а чего так строго? если есть место лучше, то почему нет?",
А какие цены в той кафехе что вы сейчас? Дороже чем в Живаго чтоль?,
Кстати практически как в статье,
" Народ, кто едет на KDD:  мы организовываем русский междусобойчик на KDD. У кого есть желание вечерком, после конференции, собраться в SF и совместно попить пиво отпишитесь в личку , я вас в канал приглашу.    ",
"А кстати, амазоновское K40 где примерно в этой полоске?",
"<https://opendatascience.slack.com/archives/edu_academy/p1469009851000090>
Закончил бакалавра в Бауманке на ИУ, на каких только лабах мы чего не крутили. Причем очень часто  нужно было крутить то, чего нет или разваливается прямо на глазах.  Все все делают по принципу ""никому ничего не надо"". Все обучение почти всегда сводится к многостраничному отчету с тонной воды и отсутствием смысла. все работы измеряются количеством страниц и размером шрифта. Отношение к студентам отвратительное : есть ты никто и звать никак. При этом постоянный контроль посещения даже абсолютно бесполезных лекций, которых очень много.   Когда я поступал у меня было по ЕГЭ 296, мог пойти куда угодно. МГТУ был явно не лучший выбор. ВШЭ после МГТУ мне показалось раем на земле. Я бы лучше взял кредит и учился бы на отл, учитывая скидки скоро бы учился бесплатно, но в отличном месте.  
Disclaimer: Естественно это все ИМХО, но это имхо после 4 лет прошедших почти абсолютно напрасно. Учился вроде норм, красный диплом дали, но я прям очень не советую. От ребят, кто остался  в магистратуре не слышал, чтобы что-то поменялось. 
Вот есть статья, которая очень точно отражает дух происходящего там : <http://rusbase.com/longread/art-mgtu/>  
Не так давно там почти перестали отчислять. Уж не знаю почему, то ли чтобы федеральных денег было побольше, то ли еще ради чего. Но это просто катастрофа для образования. Видимо цель стала деньги, а не образование.  конечно везде есть хорошие преподаватели. Но весь вуз они не вытянут к сожалению. рад буду ошибиться",
"От факультета к факультету по разному. В среднем специально не убивают, где-то жестко 2 пересдоса, где-то сколько успеешь. Но к определенному сроку кто не успел закрыться все на выход",
"Из инженерных не плохой МИСиС. У них много денег, много специалистов. Есть лабы все дела. Правда это все касается ""сталей и сплавов"".  Примат у них есть, но как там дела я не знаю",
"Ну 3 часа на самолете) Зато в академгородке очень приятно жить. Из моих знакомых, кто хотел -- все успешно уехали на пхд в хорошие места, с разных факультетов.",
"<@U04ELQZAU>: нуу, в 18 лет это как сомнительно выглядит",
"Ну не город мечта. Архитектура действительно унылая. Заводы умерли, это не индустриальный город. В целом досуг есть как провести. Тем более рядом алтай и шерегеш.",
"<https://opendatascience.slack.com/archives/edu_academy/p1469034814000106> а так везде, где много студентов. Вот для примера про божественный физтех (кстати, очень близко к правде, за исключением нескольких художественных преувеличений)",
"тексты веселые, но я бы Сергею Заварину не очень доверял. Раздолбай, которого все раскусили и троллировали, у меня такое впечатление осталось. А он при этом думал что знает как боженька)",
"Народ, если я правильно понимаю теорию рекомендательных движков на SVD/NMF, то там можно поддержать учет дисконтирования по давности, например, просто уменьшая оценки в зависимости от того, как давно выставлены, да?

В нашем случае оценки бинарны ""покупал/не покупал"" и кажется, что для предсказания будущего можно таким образом сделать что-то вроде добавления priors, чтобы последние покупки имели больший вес. Или так не получается и надо подобные факторы как-то отдельно учитывать?",
"<@U0G29N5U4>: взвешивать норм варик. Либо сайд фичи использовать, где тайминг и будет учитан. Обычно там линейная композицая весов, сайд фичей и латентных факторов",
Как-нибудь попробую поставить куду на винду. Сразу после того как поставлю хгбуст под винду.,
"Вот вопрос в том, когда примешивать к оценкам другие факторы - до разложения, в процессе (как я понял, так работает SVD++) или после",
"лично мне нравится история когда формулируется модель вроде p(r_{ij}=1)~Ber(sigma(\mu+\beta_i+\beta_j+u_i^T v_j + ANY_FEATURES)) и потом оптимизируется с помощью макс. правдоподобия. u, v - это как раз разложение и есть, а ANY_FEATUTRES - это веса твоих произвольных фич. Кстати, тут и прайорам место найдется при желании",
"<@U0AD1L5NC>: в лазане вроде как обычный батчнорм, это в каффе не учит параметры, а только статистику собирает и нормализирует, там нужно скейл слой вставлять после бн",
у меня проблема как раз с установкой этой штуки,
"я думаю тут мне врядли кто поможет, попробую пожаловаться",
"как раз в разгар правильной дискуссии. 
1) У кого нить есть личный позитивный опыт дружбы Tensorlow или Theano c CUDA под винду? какие подводные камни? (да я знаю, что правильные люди используют линукс, но условия проекта - винда)
2) На сайте nvidia cuDNN под 7 и 10. Что делать людям с восьмеркой?
3) Если установка через конду - вся последовательность действий та же что и под линуксом? 
Прошу прощения за идиотские вопросы, но я последний раз виндой пользовался года полтора назад.",
"проблема была только с CUDA toolkit, остальное все как по маслу",
"<@U0AD1L5NC>: поясни, что там происходит и зачем так делать",
"Это случается везде, где есть crossentropy",
А зачем ему делать clip?,
"а кто нибудь сталкивался при использовании cuDNN с ошибкой AssertionError: precision must be in ['float16', 'float32', 'float64']?",
какая у NVIDIA vps (videocards per second)? :troll:,
"мы вчера начали в <#C040HKJF1> обсуждать тюнинг xgb 
так вот, в продолжение беседы - как можно подбирать оптимальные параметры когда данных дофига и больше и модельки строятся 3 дня?",
"практика показывает, что честный random search на сабсэмпле редко когда транслируется на нормальный скор на большой кросс-валидации",
"вот например, если взять подвыборку в 5-10%, то мне кажется, что некоторые параметры могут там вести себя совсем не так, как на полном трейне. Например, subsample",
"ну сид фиксировать это всегда хорошая идея, а вот насчет colsample тоже не знаю, его-то как раз, наверное, стоит менять",
а почему фиксировать сид - хорошая идея?,
"Чтобы из за изменений сида не летать на лидерборде в диапазоне от 200 до 2000, как было на Сантандере :trollface:",
"пацаны, где б скачать ILSVRC2012_img_val.tar?",
"А кол-во итераций и lr как выбираете? У меня в голове пара 1000 и 0.01 и от нее пляшу. Хз насколько это тупо, но, кажется, тупо",
"прибиваешь lr какой нравится, потом на `<http://xgb.cv|xgb.cv>` смотришь сколько деревьев нужно чтобы разойтись на валидации",
"ребятки, я подозреваю, что вопрос этот неоднократно обсуждался. 

вот есть у меня данные за m лет по месяцам для N-организаций. некоторые организации прекратили свое существование. 
хочу предсказать, кто следующий. 
похоже на задачу churn prediction. но нагугливаются описание как в качестве предикторов используются какие-то агрегированные параметры. 
а я хочу на основе таких многомерных time-series
подскажите, пожалуйста, что б погуглить на такой случай.",
"<@U0AF2AZCM>: ну не совсем оверфит, ты же не смотришь на тестовый фолд, когда делаешь `<http://xgb.cv|xgb.cv>`",
то есть просто подбираешь параметр как обычно в кроссвалидаии,
А почему там не маска? Чтобы можно было параллелить и не ловить дедлок?,
"correctly identified each of two colors 80% of time - а он с одинаковой вероятностью ошибался, когда показывали зеленый, и когда синий? Или это в сумме, а  например, чаще ошибался, когда зеленый был вместо синего, и реже, когда синий вместо зеленого?",
"<@U070Y25AS>: похоже на задачку из книжки Канемана. В твоих обозначениях искомую вероятность можно записать как
P(Blue|Correct) = P(Correct|Blue)*P(Blue)/(P(Correct|Blue)*P(Blue) + P(Correct|Green)*P(Green))",
"Ребят, может, кто знает, есть ли какие-то книги/что-нибудь по tensorflow? Буду благодарен!",
"вы так говорите, как будто это плохо :yen:",
"также часто бывает (как, например, в книгах выше), где 90% книги о машинном обучении и 10% о том, как это сделать в tensroflow.",
"Всем привет, такой вопрос:
Есть приложение-игра для детей, где они решают математ. примеры разной сложности и получают какие-то игровые плюшки.

Нужно по решенным примерам определять, какой пример показать следующим. Т.е. если не решил несколько сложных примеров - давать попроще, и наоборот.  Или давать больше примеров на тему, в которой невысокий уровень правильных ответов.
Применимо ли ML к такой задаче? И если да, то в какую сторону смотреть?",
"вот я как раз по следам обсуждения вспомнил, что статью такую находил..",
вроде как для java тоже пилили,
"гайз, где можно встретить сетки без пулинга? чтоб одни конволюции c ReLU и регуляризациями?",
"вообще не нада к пулингу так относиться, как к чему то другому, пулинг это те же свертки, только не обучаемые",
как это пулинг -- это свертки? :thinking_face:,
"кстати да, как максимум закодировать сверткой?",
одна функция получается входной volume а вторая какая хочешь,
я не понимаю как чисто математически обычной сверткой как в сетках сделать максимум,
а в Network in Network состекали 1x1 сверту и свертку где рецепторная область равна выходному объему из предыдущей 1х1 и назвали это все cascade cross channel parametric pooling (CCCP pooling) - китайцы чо ),
и 1x1 conv и FC сначала результатами работы не отличались? до того как ими стали размерность менять,
"на выходе чисилки разные ессесно, это как трюк в VGG, замена 5х5 на последовательные два слоя по 3x3, получается что рецепторная область одинаковая, f: R^25 -&gt; R, но параметров меньше 25 против 18",
когда мы говорим 3х3 мы всегда имеем в виду 3х3хD,
когда мы говорим 3x3 мы имеем ввиду размеры ядра свертки,
"Коллеги, всем привет!
Кто-нибудь знает как можно заполучить НКРЯ (весь корпус, желательно с разметкой)? Слышал, что для исследовательских целей его вроде бы могут предоставить. Если это действительно так, к кому/куда следует обратиться?",
"Получить для исследований != использовать где угодно
ОпенКорпора - про второе",
"Кому-то дают, кому-то нет.
Но почему бы не попробовать - ведь ваши цели вместе с создателями могут пересекаться и они могут быть заинтересованы в вашем исследовании.",
а чейнером кто то пользуется вообще?,
ну когда -то Делфи был одной из самых удобных сред для программирования,
"ну глянуть то можно, я думал, мож кто что порекомендует :slightly_smiling_face:",
"именно такое приходит в голову, когда видишь код на js",
А с какой точностью сеть классифицирует? И с какой размерности пространство на предпоследнем слое?,
"У меня знакомый пишет книгу по спарку для маннинга уже года два. как я понял маннинг на авторах экономит и подкатывает к людям попроще, без высоких запросов, и довольно случайным образом",
"но я все равно покупаю иногда, как правило со скидкой 50%",
"High Performance Spark есть,  зачем я тогда купил? хаха",
"когда ее допишут, пришлют же бумажную версию?",
"ну я на работе книжки храню, а когда место работы меняю, то обычно там их и оставляю",
"обычно есть что-то типа библиотеки, куда вся эта макулатура от уволившихся сотрудников складывается",
"Да. Это же почти то же самое, что комп. А от электронной бумаги не устают.
Большие читалки есть, просто пдф нормально читать они, как правило, не умеют. А чтобы и заметки позволить - это вообще нет.",
Когда вижу такое хочется убивать.,
"у меня был коллега, который аж стол завалил статьями, но его уволили за бесполезность из компании, где можно было протирать штаны годами",
"эх, где доллар по 33",
дежавю в pdf разве не как картинки экспортируются?,
"<https://opendatascience.slack.com/archives/edu_books/p1469480385000146>
как картинки,обычно размер увеличивается в несколько раз. Но даже если книга будет 50-100 мегабайт это нестращно",
может кто-то знает все-таки хорошую типографию? там если в 10м собираться то реально получается меньше 1000 с человека. А книгу печатную часто хочется. Просто я в не знакомое место опасаюсь получить 10 томов брака и куда их,
"надо значит хайв майндом решить, где печатать",
"сап чат. по мотивам обсуждения здесь и теме в <#C16CCPQCS> , у нас была идея распечатать в Москве книжку Бенжио <http://www.deeplearningbook.org/>
кто хочет присоединиться?",
по-моему когда я осенью смотрел в мягкой обложке и без 0 главы получалось ~800,
<@U0FEJNBGQ> Дешево это когда тебя лично приглашают и еще проживание готовы оплатить,
"<@U1UNFRQ1K>: а можешь пожалуйста рассказать, как там в аспирантуре ФКН? что происходит, как построена работа, какие возможности есть и какие твои впечатления?  ",
"<@U0H7VBQQ1>: 
&gt; Можно ли получить более полезное представление, если сеткой генерить вектор параметров для случайного распределения, подмешивать это случайное распределение, и над тем что получилось делать линейный слой и софтмакс?
Наука у нас экспериментальная, надо проверять :simple_smile: 
&gt; Такой аля VAE, только вместо реконструкции классификация. (если я правильно VAE понимаю, конечно)
Можно прикрутить Conditional VAE, но вообще эти VAE в основном как генератвные модели используются.",
"вопрос, а почему в keras.preprocessing.image.ImageDataGenerator.flow нету target_size а в flow_from_directory есть? как тогда керасовским препроцессингом за ресайзить одну картинку? в самом ImageDataGenerator ресайзинга вроде не видать",
<@U041LH06L> это там где были все-все книжки по R? ,
"Почему не надо? 
<https://opendatascience.slack.com/archives/welcome/p1469555535000109>

Да хоть та же призма -- у них своя сетка. Или в каком смысле сетка? ",
В курсаче как раз можно,
"Привет.
Можете подсказать по CNN?

<http://cs231n.github.io/convolutional-networks/>

Parameter Sharing. Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. Using the real-world example above, we see that there are 55*55*96 = 290,400 neurons in the first Conv Layer, and each has 11*11*3 = 363 weights and 1 bias. Together, this adds up to 290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number is very high.

Мне почему-то казалось что к каждой точке применяются одни и те же 96 свёрток
Почему же тогда ""and each has 11*11*3 = 363 weights and 1 bias» если они одинаковы для всех 55*55 точек?",
"<@U06TZHSSJ>: я туда пошел к Кузнецову С.О. после его курса по теории решеток в МФТИ. Читал дико вяло, на отвали, но этот тот редкий случай, когда победило чувство математической красоты. В-общем, окунулся я в Анализ Формальных Понятий - немного эзотерическая ветвь прикладной теории решеток, вникать интересно, пока не начинаешь ощущать, что это вещь в себе. Однако сейчас не жалею - нахожу применения в машинном обучении.",
"Как только поступил, ништяков было много - кадровый резерв, стипендии, спец стипендии наполучал. Сейчас вроде с этим уже потуже. Но есть вариант академ аспирантуры - там стипендия 35к и стажировка на 4-6 месяцев оплачивается. Если писать хорошие статьи - будут поддерживать одну поездку в год на крутую конфу уровня A-A*. Я так покатался неплохо в Ниццу, Прагу, Буэнос-Айрес.",
"&gt;  Анализ Формальных Понятий - немного эзотерическая ветвь прикладной теории решеток, вникать интересно, пока не начинаешь ощущать, что это вещь в себе. Однако сейчас не жалею - нахожу применения в машинном обучении.
А где это в МЛ можно применять?",
"в РАНовских аспирантурах тоже? (или как сейчас называется то, что от РАН осталось)",
"<@U1UNFRQ1K>: спасибо большое за ответ. А как дело с поступлением обстоит? Сложно было попасть? Я так понимаю, что нужно заранее знать к кому идти в аспирантуру и уже определиться с темой работы?  Я так понимаю академическая аспирантура и просто аспирантура в ВШЭ это разные вещи? Я правильно понимаю, что аспирантура и работа это несовместимые вещи? И если не работать то жить на 30т ? А что за стажировки бывают на 4-6 месяцев? ",
"<@U06TZHSSJ>: c поступлением проблем не было. По аглицкому у меня TOEFL был, философию почитать - одно удовольствие, а сейчас и вообще вроде упразднили ее как вступительный. Специальность лояльно принимают. Там дохрена всего конечно, но глубокого понимания не требуют, так.. обзорно. Да, надо заранее определиться, к кому идешь, и подкатывать к нему, я 2 недели от Кузнецова ответ ждал, хоть я и свой, физтеховский, и даже согласился не работать помимо аспирантуры. Да, академ аспирантура - это другое. Но вообще нынче что-то стоящее в науке сложно сделать, поэтому аспирантура действительно выглядит как full-time, по крайней мере в Вышке. Про арбузолитейные тут вроде речь не идет. Я в итоге только в Вышке, но со всеми резервами/грантами, стипендиями/преподаванием около 90 получал. Сейчас хуже, но и как раз заканчиваю и работу ищу.",
"да, спасибо, проскочил как-то
а откуда тогда набирается в сети 60 млн параметров
в полносвязных слоях 16 000 000 + 4 000 000
параметров у свёрток не больше миллиона
где искать еще 40 000 000 ?",
а где про это написано? найти не могу,
"про вычисления понятно, но параметров то в свёртках не так много
я не пойму где найти 60 млн?",
"простите я со своим оффтоп вопросом: как сделать word2vec так, чтобы must, can и should были сильно разными?",
"<@U07JH63AA>: а почему can, must, should должны быть сильно различными?",
"может можно на RFC попробовать обучить? там часто эти слова встречаются, и чётко определено в каких случаях какое использовать",
"и снова школьный вопрос! :baby: 
я когда вижу интеграл в формуле -- представляю себе сумму (отрезков под кривой даже). а вы? :filosoraptor: есть какие-то более полезные представления? 
до меня просто идея интеграла долго доходила. какой-то PTSD",
"я всегда представляю функцию как плотность, а интеграл как массу",
"интересно. а можешь ссылкой кинуть, а то я не представляю как это)",
"если попробуешь чего-нибудь, то отпишу как вышло, пожалуйста :slightly_smiling_face:",
"ну сложно охватить такое в каком-то интуитивно понятном примере, прост площадь под графиком, как мне кажется, сложно представлять в голове, когда интеграл не одномерный",
а как комплексные интегралы представлять?,
"постановка задачи совсем непонятная) must/should/can и так разделены ведь уже, они пишутся по разному, зачем их еще как-то разделять",
"справедливо! 
но мне кажется, вопрос в том, как бы дотюнить word2vec. почему бы и нет? :slightly_smiling_face:",
"Скажите пожалуйста, а как можно получить (пусть даже платно) доступ к социальному графу инстаграмма? Я поискал в гугле и у них в доках -- об этом ни слова, в datasift и gnip тоже ничего нет",
"Снижают порог входа для участников. ПО мне нормально. Если бы оба доклада были узкоспециальными для тех, кто уже давно занимается обучением с подкреплением, я бы не пошёл.",
<@U0KPD6R89>: а когда это было? нынче политика получения доступа к API нетривиальна. Это для каждого пользователя API-review проходить?,
"хочу распределение соотношения собственных чисел эллипса-среза муравейника! 
как жаль что для него надо поднять DL",
"Праздный вопрос к яндексоидам: может, кто пробовал МатриксНет на Kaggle? Как он в сравнении с Xgboost?",
<@U040HKJE7>: как насчет распознавания сторон света по фотографиям муравейников?,
<@U1UNFRQ1K>: его все-равно нельзя будет потом заслать как решение,
"<@U04BFDYPV>, <@U0KPB45TK> и <@U0KPCJWAC> вроде как пробовали на данных из дотки для inClass Kaggle.",
Прост дорожки между развилками и тупиками. Интересно кстати посмотреть какой модели случайного графа соответствует ,
"Как я позже узнал, я реализовал только часть фичей. В самом матрикснете куча еще всего. Я бы советовал юзать xgboost и не сильно париться)",
"народ, опять я с вопросом, теперь на тему CRF (conditional random fields), есть ли какие-то прямо маст рид материалы на эту тему? какие фичи лучше заходят, как лучше тюнить и так далее, то, что читал - в принципе ничего полезного для себя не нашел, все примерно о том, что, возьмем префиксы, постфиксы слова и его соседей и так далее, ну то есть достаточно очевидные вещи, а хотелось бы что-то более advanced",
"ага, юзаю как раз твой враппер к CRFsuite",
"Обычно под crf имеют в виду, по сути, кучу логистических регрессий с доп. фичами на основе соседних пар выходных тегов, которые все дружно оптимизируются, чтоб дать макс. правдоподобие на всей цепочке. Фичи, которые для каждого токена извлекаются, с префиксами-постфиксами этими всеми - подбирать точно так же, как для лог. регрессии imho. Может иметь смысл посмотреть детально, что происходит с цепочками выходных тегов, и может как-то поменять под это дело токенизацию, или придумать другие теги, которые помогают больше всего поймать (ну всякие BIO, BILOU схемы и тд.) Если использовать CRFsuite, то там никакого рокет саенса нет :)",
"Такой вопрос. Знает кто-нибудь пакет, в котором удобно делать нейросеть с совершенно нерегулярной структурой, когда  после дропаута кучи синапсов нет, а оставшиеся у разных нейронов разные и могут грубо нарушать слойную структуру. например вести с первого слоя прямиком на пятый.",
"Лазана английскими буковками как будет, а то что-то плохо гуглится...",
и желательно чтобы он был с машиной а то как эти 20 томов забирать о_О,
"Наверное, в правильный канал. 
Из вакансии Wrike привлекло внимание:

Before starting development, our product team will ask you to estimate the impact a feature will have on business: how many people we expect to use it, how it will change user behavior and customers' satisfaction, and eventually Lifetime Value

Собственно, про подход. Как это сделать, куда смотреть? Больше в сторону эмперических/статистических методов. Кто-нибудь встречался/делал такое?
Спасибо.",
"Типа, не было кнопки Х, добавили кнопку Х. Она делает что-то доселе невиданное и новое. Как это повлияет на продуктовые метрики.",
"Скажем, если бы кнопка Х конвертила что-то в pdf, можно было примерно понять как она будет использоваться по косвенным признакам, типа,— сколько вообще конвертят, сколько вообще pdf из общего числа документов и т.д.",
"описательную статистику по пользованию делать несложно, но даже перекраска имеющейся кнопки может произвести какие угодно изменения на метрики",
"&gt; измерить воздействие, до изменений, когда, по сути, нет данных
_А, слушай, зачем так обидно сказал?! (с)_

Во-первых, конечно же, данные есть, просто вы на них не обращаете внимание.
Во-вторых, не надо считать непонятные ""продуктовые метрики"", считайте деньги: появилось больше новых клиентов, а старые стали платить больше - значит, хорошая фича.
Осталось только понять, какая именно из 74 выпущенных за последнее время фич хорошая. :slightly_smiling_face:

В идеале, вам нужно рассчитать графовую модель, в которой учтены все переходы с их вероятностями.
И после ввода новых функций вы пересчитываете этот граф и смотрите на разницу.
Ваша цель - увеличить вероятности положительных терминальных состояний (""клиент оформил заказ"" или ""пользователь кликнул на банер"") и снизить вероятности отрицательных (""клиент удалил эккаунт"").
Если по ним эффекта нет, то все прочие метрики (например, ""клиенты на 30%  больше кликают на зеленую кнопку"") бессмысленны.
Для начала можете строить марковские модели. Но, вообще говоря, вероятность перехода может существенно зависеть от пути, что сильно усложняет модель.

Но самое важное - такая модель покажет вам, что бесполезно вводить новые функции, которые в графе размещаются в 6-8 шагах от желаемых терминальных состояний, потому что какими бы крутыми эти функции ни были, но на реальный финансовый результат они не повлияют.",
"Коллеги, есть у кого-нибудь ссылка на статью, которая описывает, как делается style transfer (prisma) только инференсом?",
"<@U0KQ5M6KX>: а где можно посмотреть пример или более подробное описание вот такого графового подхода? книги/статьи/публичные отчеты/кейсы, etc?
выглядит любопытно, хочется чуть больше понимать.",
"Хороший пример на эту тему — всяческие call hunter'ы (это когда окошки долбают с просьбой сообщить телефон для перезвона). С одной стороны туда попадает множество клиентов, что вроде бы хорошо. С другой - общее количество денег для многих контор оказывается таким же или чуть больше, а система при этом не бесплатная. Хороший пример, что нужно смотреть по деньгам.
Второй пример — мы в один момент повысили цены вдвое. Клиентов стало вдвое меньше, объём денег остался тот же. Но при этом нагрузка на саппорт снизилась раза в 3-4, самыми проблемные клиенты отвалились.",
"А вот это как раз относится к разряду эффектов, которые можно пытаться предсказывать если у вас в прайсе есть позиции с разной ценой.
Строим модель ожидаемой прибыли от всех фич клиента Стараемся сделать её хорошо. Дальше для каждого клиента эмпирически считаем произодную ожидаемой прибыли в зависимости от ценника на услуги, которые он потребяет в вашем прайсе, меняем цену на немного и смотрим как изменится для данного клиента предсказание. Дальше предполагаем, что если изменить весь ценник в целом то аудитория среагирует примерно так же, и смотрим как изменится прибыль, как изменится нагрузка на саппорт, потому что которые из клиентов обращались в сапоорт мы знаем, и как поменяются другие параметры системы.

Конечно довольно умозрительно, но попробовать можно.",
"в зависимости от того, какие паттерны существены",
"для начала можно дистанцию считать в k-мерном пространстве, где k - количество отсчетов",
"Ну там по физическому смыслу интересны выбросы как раз. Это данные по лондону периода со всякими брекзитами, муслимскими праздниками, матчами по боксу и т.д. А выбоины да, везде одинаковые, сервер-собиралка падал",
"я бы всё-таки разобрался с корреляцией, почему она не работает",
"Это я знаю, просто вывод какой если да и если нет не понял",
"<@U0U2ENJ4U>: ну в каких-то случаях наверное... Хотя кажется что ряды могут быть совсем не похожими, но коинтегрированы. А про ""одновременно колеблются вокруг общего среднего"" это тоже можно придумать примеры когда будут два ряда колбаситься с разным периодом вокруг противоположных трендов,
но будут коинтегрированы ",
"<@U1BAKQH2M>: вспомнил, как выяснить что не так с корреляцией. Надо scatterplot нарисовать и посмотреть, может какой-то вид outliers ее ломает, или в каких-то значениях нелинейность странная",
"<@U0FEJNBGQ>  не дошло, скаттерплот по каким координатам?",
"а что обычно используют, когда хотят сегментировать (выделить клетки, ядра и тп)/обработать картинки клеток из микроскопа? есть стандартные тулзы в области? (я нашёл cellprofile, но не понимаю имеет ли смысл разбираться в нём, писать что-то своё, или искать что-то ещё)",
"как минимум хочется выделять клетки/ядра, мембрану",
"Видимо `gemmlowp` не так вылизан, как Eigen",
"<@U04423D74>: Про graphical models можно смело начать отсюда <https://en.wikipedia.org/wiki/Graphical_model>
Чуть более углубленно можно прочитать у Бишопа (Pattern Recognition and Machine Learning) в 9-й главе, а у Мэрфи (Machine Learning: a probabilistic perspective) в 17-20 главах.
Ну а дальше как обычно - гуглить актуальные статьи по авторам и ключевым словам.",
"<@U0KQ5M6KX>: спасибо, посмотрю
а конкретные практические кейсы есть, чтобы посмотреть, как это применяется?",
"Всем привет. Есть датасет, в котором шума больше, чем нужной информации. Однако кое-кто говорит, что отфильтровал шум с помощью t-SNE. Как это делается, что почитать?",
"Если шума больше, чем нужной информации, то не очень понятно как t sne поможет",
"Это конечно если выбросов не очень много, если много, то лучше их почистить перед тем, как pca делать",
"Звучит прекрасно, а как выбросить?",
tsne звучит как может быть,
"Ну так вот мне интересно, как с помощью t-sne выкинуть что-то",
<https://opendatascience.slack.com/archives/theory_and_practice/p1469735969000621> примерно так же как и с pca,
<@U0DA4J82H>: singular spectrum analysis. Когда мы вкладываем временной ряд в конечномерное пространство,
"о, это просто. раскладываешь в траекторную матрицу с окном в точек 50, смотришь на сингулярные числа и там где сильный спад - режешь",
"<@U0DA4J82H>: делаешь матрицу ковариций лаговую, а потом pca как обычно",
"<@U040HKJE7> А нет примера реализации, чтобы я посмотрел, как это вообще происходит все?",
"Ужас, как все это сложно",
"в оригинале делают svd, так как его заоптимизировать можно",
"лол, но лично я у мамы машин лернер, так что когда возникает такая задача, я беру траекторную матрицу и сую ее в gbm/xgboost",
"ща, вот тут чот не понял, а как это в xgb пихать вообще?",
"Как обычно, только фичи - это значения ряда за N точек до целевого",
"из ряда получаешь матрицу, в которой фичи - последние w_щирина_окна наблюдения. и суешь их
прогноз на длительное время правда делать сложно, так как в момент времени t у тебя доступны последние w наблюдений для предсказания t+1. 
для прогноза на t+2 нужно использовать наблюдение в t+1, с чем могут быть проблемы (но можно подставить прошлое значение). так что на длинный интервал из коробки gbm-ом сразу не сделать",
"я както делал дикий папусский танк, где gbm предсказывал те же самые компоненты временного ряда с M точек вперед, а потом из нескольких прогнозов реконструировал динамику на M вперед",
"<@U1PDQ6VAS>: ага, но я сейчас в общем про задачи временных рядов рассказывал. это было больше полезно тем кто как и <@U1BAKQH2M> продажи будет предсказывать :slightly_smiling_face:",
"<https://www.gov.uk/government/collections/road-accidents-and-safety-statistics#publications-2016> вот тут что-то есть. С другой стороны, не знаю какой кофе пьют британские дальнобои) Хочу проверить утверждение о теоброминовом воздействии из растворимого кофе. Наши-то пьют растворимый почти все.",
"<@U0AF2AZCM>:  как я сказал, это зависит. смотря что за микроскоп, что за клетки, какие пробы, 2D/3D. например, у нас для сегментации ядер Farsight toolkit используется, а для ядрышек он не подошел и в итоге заюзали Trainable Weka Segmentation плагин для Fiji. начать можно с того, что проще, может у тебя общие методы сработают, thresholding типа otsu, active contours, watershed",
"<@U0AF2AZCM>: смотря какие клетки - имелось в виду какой они формы и насколько они близко друг к другу расположены, из ткани или из культуры
раз 2д, для быстрого начала можно попробовать scikit image, а когда не получится, начинать смотреть на более сложные вещи <http://scikit-image.org/docs/dev/user_guide/tutorial_segmentation.html>",
"из пробы/культуры. они довольно плотно расположены, но не так, как бывает на некоторых картинках
общий cv я использовал как делать segmentation в общем случае представляю, просто доменная область такая, что должно быть много готовых вещей (так и есть). хочется волшебных библиотек, как всегда. что бы в один вызов функции)",
"много ""готовых"" вещей которые написаны как куски кода на матлабе, плагины к imageJ, которые работают для определенных сочетаний тип микроскопа+типа пробы+тип клеток и тд)",
<@U18447W4E>  На каких GPU обрабатывается?,
"Паш, и что ты там будешь обсуждать и с кем ? )",
"Только не понятно, как они с gcc 5.4 собрали окружение, точнее понятно -- руки прямые, но это не тривиальная штука как оказалось.",
"как я понял база получена по хештегу selfie, так что первая мысль такая",
"<@U04BFDYPV>: спасибо, посмотрим как это можно использовать",
"Привет. А что почитать про сегментацию изображений / определение объектов на картинках? Какие сейчас подходы модные, кто что использует? Суперпиксели + CRF над ними - это прошлый век или нет?",
"пацаны, а почему артефакты видео похожи на результат сверточного слоя?))",
почему не нужно?) для создания клона сетка будет единственным препятствием ж.,
А зачем сетки в оси? Лучше вынести в облако и по апи дергать,
"нет, не шутка! а в смысле по апи дергать? как можно дернуть апи, чтобы вычисления на устройстве случились?",
"к этому и идет ведь. как только нормально считаться будет фастфорвард, то скоро сетки будут на телефоне. или они типа уже там, как я понял из ремарки Саймона.",
так почему же тогда сеткам не входить в ось? что-то вы меня запутали,
"Должен ли быть он встроен в OS - вопрос сложный, как всегда",
А CRF  уже никто для этих целей не использует? Где оно вообще используется сейчас? ,
просто почему бы не поставить две с разными версиями Python,
"Как я понял, анаконда уже очень давно поддерживает 3.4 только в виде виртуальных енвайрментов.",
"лучше без анаконды вообще, она только мешает, особенно когда пайчарм предлагает установить зависимости, которых нету в конде",
"Помню когда я на винде ставил, у меня была дичь с компиляцией, я его убить хотел))",
"Один раз у меня тоже встал, а потом фигня какая-то началась. Но когда запустил pip из папки py34 всё встало куда нужно.",
"вот кстати как поставить pycuda на anaconda под виндой <https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_PyCUDA_On_Anaconda_For_Windows?lang=ru>
только ещё нужно добавить в PATH С:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5\bin",
а matplotlib уже когда точно знаю,
"Интересное наблюдение.  В Туториале теано есть статья про менеджмент памяти в Питоне: <http://deeplearning.net/software/theano/tutorial/python-memory-management.html#python-memory-management>
И всё, что написано в этой статье как минимум в 3.4 питоне уже пофиксили.",
Там где ты мясо и овощи покупаешь,
"Опять-таки, смотря на каких задачах",
"<@U04422XJL>: там где что-то покупают и продают :) в данном случае услуги/решения по DL. Подозреваю, что у нас его пока особо нет, но было бы интересно послушать про стек технологий на подобных проектах и почему выбрали именно ""то, а не иное""",
"Все очень кастомно и зависит от задач, как по мне. Деплой решения -- это одно, ресерч -- это совсем другое.",
"тут зависит от цели знакомства, т.к. есть путаница в терминологии, часто вижу, что event logs почему-то называют time series (да, у событий есть timestamp, но это не совсем цифровая величина, меняющаяся во времени, как термометр на стене), еще есть нехилое пересечение с signal processing",
"<@U0FEJNBGQ>: у меня цель хорошо разбираться, так как это будет полезно для экономических приложений",
"Всем ня. Можете в двух словах объяснить, как работает hdbscan? Что значит массив, который он рожает? -1 присваивается объектам, для которых не удалось найти кластера?",
"Просто я вначале преобразовал матрицу с помощью tSNE, а потом то, что получилось, прогнал через hdbscan, и получилось очень хорошо, но я не понимаю, почему",
"<@U1CGKK865>: вот еще репозиторий, где её пересобирают часто, может тут будет посвежее <https://github.com/HFTrader/DeepLearningBook>",
"<@U0QTS1LRF>: тф имхо сырой, недавно пробовал модель из стайл трансфера делать на тф и на теане, на тф какое то хитрое управление памятью, то что в теане в гпу занимает 700мб в тф занимает 8гб",
Насколько полезно конспектировать прочтенное? Или у кого какой подход к достижению понимания статьи/учебника?,
"в смысле - хранить реплику базы данных себе дороже получается, когда это очевидно нормальные структурированные данные?",
"<@U0QTS1LRF>, это не совсем про ML,  но как введение сойдет",
"Гугл для меня не всегда работает как ""extended memory"" - слишком много информации. вики лучше, если написал сам, то все вспоминается моментом",
<@U1HHX1QS3>: у нас вроде чота переливают потихоньку. ,
"для мака, да. А, у него не мак(( я как на мак перешел, то без него не представляю свою жизнь))",
"Господа, у меня есть глупый вопрос
Примерно в 00:30 третьего числа у меня самолёт, и второго вечером в яндексе будет семинар, куда я очень хочу сходить и уже зарегистрировался. Так вот, можно ли и где лучше всего, если можно, оставить в офисе на время семинара сумку, которая у меня будет с собой? Поездка длительная и поэтому сумка немаленькая, ну и не огромная",
"&gt;&gt;&gt; Очевидно, что матрица расстояний МЕЖДУ городами характеризует не сами города (узлы графа), а нечто (ребра) между ними.
Так вот суть спектрального разложения матрицы состоит в том, что параметры между узлами переводятся в характеристики самих узлов. То есть по тому, как узлы (города) связаны с другими, мы можем охарактеризовать сами города, - рассчитать вектора состояний узлов.",
"<@U0RV376MC>: Семинар Deep Learning Moscow (<https://events.yandex.ru/events/yagosti/02-aug-2016/>) будет в Мулен руже, там много места, сумку будет где положить",
и попутный вопрос: почему это зовется *спектральным разложением*. почему спектр-то? есть какая-то аналогия с оптическим спектром или вообще спектроскопией (Фурье?)? :filosoraptor:,
"Народ, кто здесь DQN на Atari запускал? Какой препроцессинг для изображения лучше использовать? Как в оригинальной статье?",
<@U0AS548A1>: а нужно 20? это минимальный тираж или цена становится приемлимой. какая там вообще зависимость цены от тиража?,
"привет всем. пытаюсь обучить syntaxnet на корпусе русского языка, взятого из gold dependencies. ошибка: tensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value softmax_bias/init . в интернетах говорят, что где-то надо добавить строку tf.initialize_all_variables(), только вот где конкретно? demo.sh работает исправно, а вот при попытке обучить на другом корпусе такая вот фигня.",
"<@U040M0W0S>: )) <@U0ZHHV83C>: просто внутри syntaxnet хренова туча файлов, куда вписывать-то?) с tf.session он нашёл штук 20 файлов.",
"С сессиями этими всегда мучился)
Если у тебя один файл, то создавай interactive_session и потом tf.initialize_all_variables() строго после того, как объявишь все tf.variable",
"в том-то и дело, что файл там далеко не один) и в каком конкретно прописывать строчку, не могу понять. есть session.py, который падает с ошибкой, но не уверена, что именно там это надо прописывать.",
"судя по логам переменные должны инициализироваться до того, как он обращается к этому файлу",
ну а все переменные создаются в какой момент? До обращения?,
"Ты в структуре файлов разобралась? Постарайся найти место в коде, где все переменные уже создались, но ошибка ещё не случилась, и туда вставь эти 2 строчки с созданием сессии и инициализации переменных
У меня так прокатило",
"и создавай именно интерактивную сессию, если я ничего не путаю, то она менее чувствительна к месту, куда её пихать",
"не знаю как вы, но в переводе я бы ее читать не стал",
"Кто нибудь из чата, живет в Берлине? Напишите мне в личку. )",
"вообще когда читаешь трудные книги создавать себе дополнительную сложность тем, что книга еще на иностранном языке неправильно",
Как буду за компом могу скинуть pdf'ку ,
вот представим прочтешь ты ее в русифицированном виде. как вопросы задавать на stackoverflow\github issues? как статьи по ней писать?,
интересно было на протяжении курса как по разному CS-преподы и Math-преподы преподают ML,
тогда зачем тебе русифицированная версия :trollface:,
"<@U0U2ENJ4U>: Но представлял, как он делает это не прилюдно что ли? :smirk:",
<@U0U2ENJ4U> расскажешь кто и где?,
"А emotiv - типа ментальные команды уже сделали. Правда, кто тестил - говорят пока сыро очень",
"Я бы даже иначе выразился, как свидетель. Вон я на фоточке. ",
"Только непонятно, как это меняет дело? Люди будут хуже над ними думать,  опасаясь, что их код пойдёт в очередной бар/гард? ",
"Как стать математиком и ученым? Иметь способности и попасть в хорошую школу, потом вуз. Родился в селе Неурожайка - 8 классов, ПТУ, комбайнер. Нет способностей, но хорошая школа - в лучшем случае средненький специалист.  В худшем будешь мучить себя и других.",
"Вообще не люблю когда фейспалмами злоупотребляют. Считаешь, что я неправ - обоснуй.",
"&gt; Ректор университета просмотрел смету, которую ему принес декан физического факультета, и, вздохнув, сказал:
&gt;- Почему это физики всегда требуют такое дорогое оборудование? Вот, например, математики просят лишь деньги на бумагу, карандаши и ластики. Подумав, добавил, а философы, те ещё лучше, им даже ластики не нужны.",
а есть какие нибудь датасеты вконтакта?,
<@U04ELQZAU>: у этих чуваков кстати офигенная серия постов как запилить DL архитектуры с нуля и заоптимизировать код (с gpu),
А <@U040HKJE7> как в воду глядел! Кто же это от Мейла-то придёт. ,
"Задачки окей в плане того, что есть люди, которым это надо. 
Но за пару дней хакатон АСИ -- много кто уйдёт дальше презентация. С тривиальным ещё выводом типа ""было бы круто, если бы мы сделали такое приложение"". 
В любом случае выглядит лучше, чем без. ",
"ну, Дьяконов огорчается когда в top-10 не попадает :slightly_smiling_face:",
"задача абсолютно реальная, но только в упрощенном варианте. когда мы делали исследование по депрессивным юзерам, думали также фичи аватарок вытащить. типа депрессивные не будут улыбаться + будут фотки чернобелые :slava:",
"было бы круто по фотке определять под какими веществами человек. но опять же - датасета нет, а самим собрать дорого и сложно",
как будто скандалы это что-то плохое для машинного обучения,
"как досмотрите до конца - повторите эксперимент с 
<https://www.youtube.com/watch?v=IBH4g_ua5es>",
"&gt; плиз потратьте 5 минут на это видео
выкинул 5 минут своей жизни; так и не понял зачем",
"да, prior-ы в явном виде
а вот сетка знает кто тут нытик а кто пидр",
"у нее еще просто курс есть на edx вроде, где она удачно его продает в начале. мол никогда не понимала математику и вообще ее избегала, а потом после того как ушла из армии...",
"&gt;а потом после того как ушла из армии..
началась у нее совсем другая жизнь",
"Может кто знает, каким образом можно сравнить два spatial distribution? Вот есть один набор точек, другой набор точек на одной и той же карте, как мне оценить схожесть 2д-паттерна? Гуглю, но нахожу в основном только всякие автокоррреляции(Moran's I), но это вроде не о том",
"не дошло, как второй восстановить по пса первого?",
"но есть проблема, все эти метрики, как я понимаю, подразумевают наличие какого-то исходного предположения (ну нормальность распределения, например)",
"<@U1FLG6YR1>: а для каких целей? Для небольших экспериментов, вероятно, можно. У неё 1260 CUDA cores и 6 Gb VRAM, а у GPU инстанса на Amazon 1536 и 4 соответственно, что достаточно близко. Я бы перед покупкой взял spot instance на денек и проверил будет ли его достаточно для моих целей",
"Окей, мой выбор сделан :smile:
Кстати, а кто их где обычно покупает?",
"В своё время для своих задач придумал датасет, который позволяет на взгляд определить на сколько хорошие обобщения освоила сеть. Мне нужно было, чтобы сравнивать разные алгоритмы и режимы обучения.
Как народ считает, нужно такое, или лучше на реальных задачах сразу учиться, а ориентироваться по кроссвалидации?",
"это как вообще ""на взгляд""? :thinking_face:",
"Ну в каком-то смысле можно так считать. Вход - две координаты. Удобно тем, что входные данные заведомо не содержат информации. Выход - три числа - ожидаемый цвет по каналам. Учишь сеть, потом смотришь чему она научилась На взгляд сразу видно какие у неё обобщения.",
"Такой вопрос. Есть допустим тестовый датасет, два входа, три выхода, который позволяет на глазок, глядя на картинку, определить какие обобщения сумела освоить сетка. Я использовал для сравнения разных алгоритмов обучения. Но на практических данных бесполезный. Только для изучения и сравнения алгоритмов.

Вопрос, такой датасет интересен народу, или лучше не парить мозг и тренероваться на разных реальных данных, следя за результатами кроссвалидации?",
<@U040M0W0S> что это и почему этого не было в календаре?),
"ну или просто вдруг кто тут знает, есть шанс за казенный счет получить MBA за границей?",
"да где угодно, но в Британии было бы тоже весело, да)",
<@U075SR8JX>: а зачем тебе MBA?,
"лол, на директора резюме как правило не подают :levenchuk:",
в Европы и Англии вроде как всё относительно размазано по началу весны,
"mba-шники идут толпами в консалтинг, где зарплаты не дата сатанистские на начальных позициях. а рост до хотябы менеджера и многих занимает 4+ лет",
<@U075SR8JX>: а готов?) у нас тут назревает всякое куда мы готовых фигачить исследования собираем по-штучно :slightly_smiling_face:,
"<@U065VP6F7> как раз проект по музлу делает, вот прям спишитесь раз готов :slightly_smiling_face:",
Там как раз любители мба сидят,
"На первой фотке этот парень выглядит так, как будто у него один зуб золотой и он сейчас кому-то одной левой организует знакомство со стоматологом",
"Отказались говорить, кто в Мейле за это отвечает)) в итоге выглядело так, что я на них наехал своим вопросом.:epta: ",
"По азам подскажите, пожалуйста. Вот три связанных вопроса. 
1. Есть фичи F1-Fn. На cv дают какое-то значение Vn. Если при замене Fn на F(n+1) есть прирост, то есть новая фича полезнее, то за счёт чего результат может снизится на F1-Fn-F(n+1)?
2. Если F(n+1)..F(n+k) - это фичи, основанные на Fn, то есть [F(n+1), ..., F(n+k)] = X(F(n)), то почему результат на F1-Fn-F(n+1)-F(n+k) может быть ниже Vn?
3. На фичах F1-Fn прогнали xgboost с заданными параметрами P1-Pm и получили Vn. Теперь у нас фичи F1-Fn-F(n+1)-F(2*n). Какие параметры надо поменять в xgboost, чтобы он наверняка получил не меньше Vn?",
"Есть статья про bayes by backpropagation. <http://arxiv.org/pdf/1505.05424>
Я видел попытки сделать имплементацию частных случаев типа dense layer, но меня не устраивал такой подход, я хотел очень общего инструмента, типа отнаследовался и погнали. Вот я как раз отнаследовался под метаклассом и почти без пыли отправил модель на обучение.  Предполагаю, что это будет так же просто для conv/lstm и прочих вариантов, где переписывать под bbp не у каждого хватит терпения.",
"Так вопрос как раз в нахождении подходящих фич. То есть нашли какой-то вариант, а теперь что-то добавляем, заменяем одно на другое, чтобы понять ценность. 
Опять же, <@U1LNBRZ29> , каким образом определить релевантность фичи?",
"Табло бесит, когда начинаешь вычисляемые столбцы использовать. Во-первых, все жутко тормозит, во-вторых надо постоянно следить за тем, как происходит аггрегация (по строкам, столбцам или ячейкам).",
"Всем привет. Подскажите, пожалуйста, как правильнее решить задачу сокращения оттока пользователей? 
Построить не глубокий лес на различных признаках, посмотреть их важность? 
Или же просто строить кучу графиков в различных разрезах и делать вывод на их основе? ",
"Сильно сомневаюсь, что тут в чатике есть тот миллионер, который знает как эту задачу правильно решить в общем случае.",
"<@U1V7HFNDP>: в чатике есть минимум две дюжины тех, кто ее решал в разных ипостасях",
"<@U040HKJE7>: И что, хотя бы один из них кто может утверждать что решил её правильно? Вообще решил, совсем! Причём именно так, как правильно это делать?",
"<@U1X3ULLR2>: самих уходящих уже удалось определить? 
это задача с 3 составляющими:
1. научиться предсказывать уход 
2. понять какие фичи на это влияют в надежде выстроить связь с причинностью (например надобность в тех поддержке, фрод/спам, изменение в поведении и т.п.)
3. попробовать удержать уходящих (когда все предыдущее сделано)

по первым двум все просто, по третьей - надо грамотно продумать взаимодействие и А\Б тестирование
в каждом случае оно очень сильно зависит от возможностей того, что вообще уходящими юзерами можно делать и как с ними работать",
"<@U1V7HFNDP>: решил правильно = решил задачу так что бизнесу была польза. можно пофантазировать на тему правильности, но ключевой фактор тут всего один

сами решения с точки зрения DS могут различаться, но в самой задаче ничего сложного нет и подходы как правило одни и теже. берешь и решаешь",
<@U1X3ULLR2>: а что именно по нему? как значимость фич строить в lm и xgb?),
"дважды сталкивался с ситуациями, когда аналитики делали предиктор, а бизнес потом не мог его применить никак",
"<@U1G303UTW>: самая сложная, ага. все что происходит с момента предсказания - попытки угадать как можно повлиять на человека, а таких данных в исходной задаче обычно нет. 

бывают задачи из серии ""есть отдел который работает над возвратом клиентов - надо просеять тех, на кого не надо тратить время"". но и данные в такой задаче по их фидбеку уже есть",
"<@U1X3ULLR2>: Как правильно решить задачу я, как и многие другие, не знаю. И третий пункт реально самый сложный, но он, как правило, относится к области ответственности маркетологов, а не ds.

Я бы второй пункт попробовал решать так. Во-первых некоторые модели, такие как sgb умеют в явном виде дать список важности фич. Из них руками следует отбросить те, которые явно имеют интерпретируемое объяснение, которое никто менять не собирается. Если никто с годовых контрактов на полуторагодовые переходить не будет, то фичу ""контракт был подписан 11 месяцев назад"" можно смело отбрасывать и решать заддачу ""при прочих равных"". Например рассматривать отдельно клиентов, которые по этой фиче явно разделяются на две группы и отдельно решать задачу для тех, кто отказался продлять, и тех, кто сбежал при действующем контракте.

Даже если модель о важности фич отчиталась она вам не говорит как именно эта фича влияет на уход, и одинаково ли она влияет на разных пользователей. Если бы влияла одинаково, то ds мог бы и не понадобиться. Поэтому я хочу попробовать сам такой вариант:

Имея готовую модель ухода предполагаю, что она хорошая и не переобученная. Для каждого ушедшего пользователя построить вектор ""как разные фичи повлияли на уход"". То есть дал по каждой фиче небольшой прирост каждой фичи по отдельности, и посмотрел как изменилось предсказание ухода для данного пользователя, по сути производная по фиче. Нормирую вектор, типа на сколько нужно поменять каждую фичу чтобы предсказание ухода поменялось на 1%. Это позволит сравнить важность несравнимых фич. Дальше на полученном сете сделать кластеризацию, потому что мало ли, вдруг у вас две прямо противоположные причины ухода, для одних слишком дорого, для других слишком дёшево - усредните получите кашу. дальше для каждого кластера посмотрю какие фичи превалируют, какие именно изменения выталкивают пользователя по мнению модели.",
"Подчеркну - я не знаю как правильно решить эту задачу, и тоже считаю, что сложнее всего имея предиктор что-то суметь изменить. Но когда я на этой неделе буду задвигать своему начальнику тему про то как сократить отток я буду предлагать что-то такое.",
"Идея, хорошая, почти так и начинал делать, особенно с производными, бизнесу это нравится. 
Теперь у меня возник ещё один вопрос. 
Проблема в ответе на вопрос: если я изменю эту фичу на 10, то модель показывает, что отток сократится на 14%, при этом точность модели 70%. Как отсюда построить 95% доверительный интервал на изменение оттока? ",
"&gt;То есть дал по каждой фиче небольшой прирост каждой фичи по отдельности, и посмотрел как изменилось предсказание ухода для данного пользователя, по сути производная по фиче. 

имеет смысл модифицировать только те фичи, на которые бизнес может повлиять",
Или хотя бы как интерпретировать эти 14%,
"<@U1V7HFNDP>: берем линейную регрессию например, часто работает не сильно хуже xgboost на реальных данных. 
на сколько поменять какую фичу чтобы получить изменение в 1% - тривиально, благо все вдоль линий. но можно ли вообще менять эти значения?

возьмем абстрактный пример экстремальной задачи ухода пользователей - ухода под воду на Титанике. чтобы выжить на титанике большинству нужно было бы сменить пол и помолодеть. мало того, что это было невозможно ввиду уже сложившихся закономерностей в данных. но проблема в другом - поменяв параметры не факт что такие точки были в исходной выборке (и при адекватном числе могли бы повлиять на модель). 

в общем случае, это шаманизм. в частном, когда есть жесткая выписанная модель поведения юзеров - можно попробовать (без каких либо гарантий). ну или можно жестко по монте-карло миллиарды семплов гонять",
"Ты не сможешь строго интерпретировать эти результаты на больших интервалах, причём в принципе. Есть в экономике такая теорема, что система дифференциальных уравнений (которая есть экономика) хорошо линеаризуется вдалеке от особых точек. Но если ты поменяешь любой из параметров на много, то наверняка проскочишь какую-нибудь особую точку. Предсказать можно только мгновенные производные.

Простой пример: главный фактор смертности в индии - дизентерия. Сокращение дизентирии на 1% увеличивает средний срок жизни для индии больше чем на год. Это не значит что полная победа над дизентирией позволит людям жить до ста лет. Просто когда эта причина перестанет быть главной главной станет другая причина.",
"Но опять же, на сколько поменять фичу, чтобы получить изменения в 1%. При этом модель же работает с точностью 70%, как это учитывать? ",
"<@U040HKJE7>: часто работает не сильно хуже xgboost на реальных данных - в тех случаях, когда вся изменчивость в данных объясняется одной основной закономерностью. Фокус в том, что если ты будешь пробовать нелинейные модели только если результат линейных совсем плохой ты никогда не узнаешь, что ошибка нелинейных могла быть в 30 раз меньше.",
"вернемся из теории в практику. знать причины ухода людей - полезно, но как правило с уходящими юзерами можно сделать очень ограниченный набор вещей: в б2б как верно сказал <@U0DA4J82H> - позвонить, в б2с - как правило писать письма счастья и раздавать ништяки в приложениях и играх. и редко чтото больше",
"Планировать какой функционал развивать в игре на следующей итерации. Чудовищно важно и, как правило, делается геймдизайнерами ""в слепую"".",
"<@U1V7HFNDP>: вот сюрприз :slightly_smiling_face: 
мое сообщение стоит толковать в другую сторону - во многих реальных приложениях, ты хоть 10 слоев xgboost'а состакай, но линейная регрессия будет работать не хуже. но чертовски быстрее, устойчивее и как бонус более трактуемо",
"<@U1V7HFNDP>: а как планировать будущие фичи продукта, основываясь на оттоке?",
"<@U1G303UTW>: Например, вас интересует вопрос какие фичи развивать в игре, PvP или PvE. При этом основной отток у вас случается когда заканчивается очередной турнир и вы по статистике видите, что клиент только из-за турнира и продолжал играть.",
"<@U040HKJE7>: Так я же не спорю, я говорю только то, что такая модель хорошо работат только там, где основной фактор один.
А в играх, например, есть игроки предпочитающие PvP и PvE. И причины их ухода не просто разные, они прямо противоположные.  А линейная регрессия их усреднит.",
"x4 - это вроде как 3 ГБ/с в одну сторону, за глаза должно хватить",
Ну... Норм материнка под Multi-GPU систему (х99 какая-нибудь) обойдется в 30+к рублей. Что как бы ненамного дешевле карты,
"Когда вижу все эти яркие слоты и ажурные системы охлаждения, то понимаю, что плачу за ненужные понты. Но альтернатив, похоже, нет.",
"Когда все эти карты отбросишь, то выбор остается очень небольшой",
"Чат, SentiWordNet имеет базу русских слов? Если нет, то где можно взять для русских?",
"А есть ли платы, куда можно поставить и i7 на sandy bridge, и GTX 1070? ",
"Корпуса классные, домой бы себе взял, но есть два фактора: 
- Корпус, который я нашел, стоит 5к
- Я поставлю его в лабе, где будет жарко и шумно и видеть его не буду",
"Удлинитель тоже интересная идея. Но выглядит как лютый колхоз и хотелось бы, чтобы корпус выдерживал транспортировку хотя бы в соседний кабинет.",
"просто так совпало, что там 4 неделю вчера добавили и как раз эта тема фигурирует",
"Вкину на вентилятор еще одну свежую мысль про отток. Смотрю что уважаемые синьоры еще не успели ее помусолить. Если кратко: лет 10 назад мудрые иностранные дяди придумали моделировать условную вероятность отклика клиента, при условии воздействия на него. Назвали это uplift modelling или net modelling. И теперь все кто в теме тычут всем с умным видом, что так и надо решать задачу удержания (-оттока). Ну в т.ч. удержания.. Вот и я тычу. И сам сделал на работе так. На отложенном контроле выглядит правдоподобно. Скоро планируем проверить в бою.",
"еще до того, как он DL занялся",
"А какие требования к материнке (ну кроме сокета) и к корпусу (кроме того, что видюха должна по размеру влезть)? Тоже задумываюсь о том, чтобы коробок себе собрать",
Какой там выбор камней под z170 есть?,
"а надо бы для их семинаров отдельный канал, как для мл_тренировок, сделать",
"И у меня пока нет понимания, как на эти хотелки влияют поколения железа",
"<@U04422XJL>: 
Вот из этой таблички
<https://en.wikipedia.org/wiki/Intel_Core#Core_i7>
следует, что из core-iN подходят процы нескольких архитектур, и все на 2011-3 сокет, например 5960X -- 40 PCIe lanes, PCI 3.0, 64GB RAM отлично, только дорого, от 75к + материнка выйдет ~30к
На 2011-3 сокет есть еще пачка Xeon, с более широким спектром вариантов. Скажем 8ядерный E5-2620 v4, 40 PCIe lanes и памяти можно больше воткнуть -- 33к, но у него частота меньше.

Кроме обычных х99 материнок под этот сокет есть чипсет С612, но там глаза разбегаются, варианты с n-PCIe и 64GB есть, а вот чтобы больше памяти и 2 карты надо искать.
В целом, такой сетап выходит довольно дорогим.

Я сейчас присматриваюсь к варианту со старым чипсетом x79-C602 на 2011 сокете. К нему есть бушные процы E5-2670 на 8 ядер и вроде бы PCIe3.0 для пары карт. Вот интересно, собирал ли себе кто такое недавно?",
"А если я хочу проц на 4 или 6 ядер, до 32 рам и 1 видеокарту, какие мне платы надо рассматривать?",
"или LSTM сменила оболочку :eyes: и он будет как терминатор, перенимать у людей облик",
"ну он вроде на настолько технарских мероприятиях, как сегодня, замечен не бывал",
да на такие непонтяно вообще кто ходит,
"Тут недавно было околофилософское обсуждение о смысле собственных чисел и векторов матрицы графа (adjacency matrix). Ну я тогда подумал, что раз есть собственные числа и вектора - значит линейный оператор, значит первое, что надо понимать - откуда куда действует. Дошли сейчас руки погуглить на эту тему - чот не понял, не могу книжку прочитать.  Пишут здесь <http://math.stackexchange.com/questions/270058/intuitive-interpretation-of-the-adjacency-matrix-as-a-linear-operator> И здесь <https://books.google.ru/books?id=yR8tq_YznMwC&amp;pg=PA625#v=onepage&amp;q&amp;f=false> (второй параграф на странице 626). Если кто разбирается в графах, можете чуть подробнее обьяснить, что тут происходит?",
"коллеги, вот и я, наконец-то, планирую окунуться в мир dl, подскажите, пожалуйста, какой инструмент для этого сейчас стоит выбрать (4python), вопрос сейчас особо актуален, так как не так давно вышел tf от гугла, но, как я вижу, многие пишут, что он еще сырой",
"а где пишут, что tf сырой?",
"для начала keras с  разными бэкендами хватит, когда уже сильно глубоко надо залезать будет, то сам уже сможешь для себя выбрать",
"<@U0JHK9001>: ясно, thx, да, сейчас надо начать, потом то будет ясно куда двигаться и что для этого нужно",
"ещё mxnet как вариант, тут вроде тоже хвалят",
"<@U1CF4A6BT> Почитал википедию про это и не понял что в ней такого что не делается уже и так при сборе статистики. Жаль, потому что новое умное слово мне бы могло сегодня пригодиться. Вероятно раз ты сделал так на работе, то понимаешь как оно устроено и было бы очень полезно объяснить на простых примерах. К сегодняшнему разговору уже не успею понять в чём идея, но жизнь же сегодня не кончается.",
"<@U1V7HFNDP>: идея такая: забить на тех кого не отговорить и занимать только теми, кто передумает если с ним правильно работать. Причем ты прямо в модель закладываешь инфу о способе воздействия на него, так чтобы она выделяла только тех, на кого подействует именно это целевое воздействие (угрозы, шантаж, плюшки, облизывание и пр.).",
"Вообще в информационной воне традиционно выделяют три группы и три цели. Три группы, те кто будут с тобой что бы ты не делал, те, кто могут изменить своё мнение, и те кто будут против тебя что бы ты не делал. И соответственно три цели: Первым ты должен продемонстрировать примеры подтверждающие что их выбор правильный, и которые они могут использовать как аргументы в твою пользу в дисскуссиях которые они будут по своей инициативе вести. Вторых ты должен убеждать в своей правоте аргументами, которые они способны оценить (в этом качественное отличие от информации для первой группы) и наконец третья цель - блокировать возможность высказывать своё мнение и убеждать для третей группы, так называемые затыкающие аргументы.

Думаю, что в маркетинге в котором поток информации идёт не в одни ворота, а ещё и общение между клиентами идет, например с игроками игры, которые активно обсуждают в форуме, должен быть какой-то подобный же принцип.",
"мне кажется вчера все испортила девочка которая говорила что все вопросы потом, мероприятие превращается из семинара в митап; а еще некоторые люди начинают допрашивать докладчика как будто он автор статьи",
"Мопед, как вы понимаете, не мой и связываться надо с автором поста",
"Я, конечно, мимокрокодил и не умею в статистику, но сложилось впечатление, что в первыых 3/4 второго доклада разобрались те, кто уже и так уверенно понимал, о чем речь идет. Такие вот rigorous выводы чего-либо в митапоподобном формате вообще не воспринимаются, одно дело, когда на лекции преподаватель теорему доказывает и мелом по доске пишет, а другое - когда слайды с лютыми стенами мелкого текста мелькают. Сложилось ощущение, что можно было смело пропустить весь вывод и сразу сказать, что вот, есть такие то псевдочисла, значат они вот это вот, привести пример со станцией, идти дальше, было бы как-то бодрее",
"а вообще надо сразу с демок атари начинать, а потом уже объяснять - как это работает",
"<@U0RV376MC>: Проц старый вот и не поддерживает, поставишь новую карту на старый процессор, считай что карта будет не x16, а x8 по факту, так как pci-e 3 версии от 2 отличается удвоением скоростей. Но как тут уже писали это не так критично, если не две карты будут. Но это странно: иметь ~ $1000-1300 на две новые карты и не иметь денег на проц за $200-300",
"во-вторых, даже владея матаппаратом, нужно время на осознавание того, зачем тут эта формула",
"и почему конкретная омега - одновременно и и в индексе, и в степени",
"Друзья, кто-то может порекомендовать действительно хороший разделитель на предложения и токенизатор для русского языка в открытом доступе? Желательно на Python/Cython или Java.

Пока нашел <https://github.com/tiefling-cat/ru-syntax>, где есть разделитель на предложения на regexp. Есть <https://tech.yandex.ru/mystem/> умеет разбивать на токены внутри  и делает еще не всегда нужного, в общем не совсем подходит.",
"Я извиняюсь за наглость, а можно я скину, какая у меня пекарня, а вы скажете, рассматривать ли вообще вариант её модификации, или проще сразу новую собрать?",
"Раз уж нам запилили канал, то у меня давно назрел вопрос: Почему для DL нужен топовый проц как минимум с 8ядрами/потоками?
Моя практика показывает, что даже i5 6600 хватает, чтобы загрузить 2х Titan X аугментациями типа кроп, поворот, наклон, ресайз, аспект.",
"Часто всплывает random forest от h2O, которые и категориальные фичи умеет холдить из коробки, и тут вот xgboost даже уделывает  <http://datascience.la/benchmarking-random-forest-implementations/> Кто нибудь пользовался, стоит ли он себя?",
"<@U0JHK9001>: а я как раз подумываю, потому что под старый xeon не надо будет новую мать брать, можно будет в тот же сокет под IvyBridge воткнуть",
"ну если куда воткнуть парочку, то круто :slightly_smiling_face:",
"Привет! По поводу вчерашнего семинара (<https://events.yandex.ru/events/yagosti/02-aug-2016/>) в Яндексе по RL.
Изначально план был делать относительно ""продвинутые"" семинары, т.е. не посвящать значительного времени обсуждению что такое Q-learning или что такое MDP. Это мне, да и в целом нашей команде, не очень интересно, потому что мы и так это знаем. В то же время, если кто-то захочет сделать такие семинары - я буду это только горячо приветствовать.
Последние два семинара старались сделать по такой схеме: 1-ый доклад более ""популярный"" и меньше тех. деталей, 2-ой доклад с деталями вывода какого-то подхода. Вопрос, как избежать демонстрации стен мелкого текста из формул, и показывать только ключевые идеи -- интересный и открытый :slightly_smiling_face:
В конце концов, я думаю мы придем к формату именно семинара, а не чисто доклада с презентацией. То есть обсуждение, комментарии (по делу) и вопросы по ходу презентации. По поводу времени и необходимой подготовки -- замечания справедливые. В следующий раз напишем это яснее, ну и время, конечно, более четко спланируем.",
"открыл страницу материнки:
Слоты расширения
1 x PCIE x16 (PCI Express 2.0) 
2 x PCIE x1 (PCI Express 2.0)
1 x PCI
А то, что он второй версии, а не 3, не критично, как я понимаю?",
две тренировки назад как раз Михаил Павлов рассказывал про blackbox challenge и какие подходы для этого взлетели с практической точки зрения,
"как написал <@U14GG4E69> 
&gt; x4 - это вроде как 3 ГБ/с в одну сторону, за глаза должно хватить
а у тебя 2.0 х16, который 8 ГБ/с",
"а есть ли еще какая-то метрика, которая покажет skewness данных? Не знаю, как правильно объяснить. Например, если значения вектора более-менее равномерно ложатся на ось X, то метрика уменьшается, а если они скучиваются в начале (и скажем, для визуализации лучше взять логарифмическую шкалу), то метрика увеличивается",
"А подскажите примеры того, где может пригодиться смешанная длинка? (как целой части, так и дробной)",
"кто-нибудь встречал удачные примеры, как на одном графике одновременно визуализировать и распределение данных (например, гистограмму) и соотношение количества отсутствующих / присутствующих значений?",
"<@U0RV376MC>:  вроде все берут как в оригинальной статье,  так как  на результат сильнее  влияет не  препроцессинг картинки, а новые методы обучения",
"типа зачем делать лишний месяц, если константа по сути будет наш базовый месяц",
Кстати в статье Дьяконова которую я выше кидал как раз про разные кодировки и чего с эти делать вообще,
"если рассматривать gtx 1070 founders edition, то какую фирму-производителя лучше брать? Zotac, ASUS, MSI, Gygabyte?",
"<@U04ELQZAU>: ну все такие одно дело   когда матрица плохо обусловлена и другое строго зависимые добавлять самому, когда можно не добавлять ",
"а где тут отстутствие данных? ""with 90 percent of data""? как-то не очень понятно",
"да это понятно, что можно дорисовать что угодно, интересно посмотреть, кто как делал, что если пропорции сильно разные (например 1000 к 1)",
"Отдельное спасибо <@U14GG4E69> 
Без его замечания я бы не взял подходящий корпус и придумывал бы что-то непонятное с тем, как вкорячить 3ю видюху",
"Пацаны, Братаны, Байесиане и Байесианки, есть такой вопрос, связанный с DPMM - тут кто-нибудь этим занимался? Я сейчас реализую метод МакИчерна (MacEachern). Вопрос о том, как после сэмплирования по Гиббсу и сходимости процесса делать вывод, так чтобы не просто взять последний сэмпл, в котором могут быть с ненулевой вероятностью совершенно левые компоненты из ада, а как-то получить корректное усреднение по *n* последним компонентам, причём так, чтобы это не приводило к увеличению числа компонент в выводе. Может есть какие-нибудь публикации на эту тему? Всё, что я находил сводилось к `repeat until happy`.",
"Приветствую! С мыслями ""сейчас я быстренько разберусь с этим вашим диплёрнингом"" сел погуглить наиболее полные и структурированные обзоры на основной мат.апрарат и алгоритмы/ курсы и т.д. по теме и не нашел ничего, кроме раздробленных статей. Знает ли кто-нибудь волшебный сайт/курс, где бы рассказали достаточно, чтобы базовое представление получить о области? ",
"ну если они слипаются, в общем случае как их вытащить то?",
"<@U0U2ENJ4U>: где то была статья про ручное избавление от высоких версий glibc, но чтобы все случаем ну поломать, лучше брать rhel 7.2",
"я один раз попробовал обновиться с 1 на 2, сервак умер когда уже почти всё заработало :smile:",
вот советуют как вариант под chroot поставить другую версию,
"ладно, взял другой сервер из закромов, поставил 0.10RC, как мне теперь убедиться, что CUDA работает?",
"скорость, как мне кажется, крайне мала, у меня домашний комп в два раза быстрее в виртуалке его гоняет",
"соответственно, какая из версий TF моё старьё понимает?",
"ок, перед тем как я побегу за новой - на сколько  examples/sec в CIFAR10 я могу расчитывать с 1080 ?",
"чота совсем не густо, 640 я на обычном CPU получаю",
"я куда как, где-то потусить, где-то узнать",
"если с икрой, то почему бы и нет",
хотя тут где - то было про интерактивную библиотечку под питон которая на больших обьемах вроде как работает,
"Мы тут на работу купили две 1080, но что-то они не заводятся - драйвера нормально устанавливаются, но после установки куды система дохнет. Используем убунту 16.04. У кого-то похожее было?",
После куды их пришлось переставить,
"Там у куды 8 есть указание, что для 1080 придётся переставлять ",
"Да я писал, у меня такая же проблема была: 16.04 ставлю куду 8 и система дохнет",
какая версия драйверов в итоге завелась?,
<@U0H7VBQQ1>:  а какую версию дров в итоге поставил?,
"Итого, драйвера (с удалением всего nvidia*),
Куду 8 из deb пакета
Драйвера переставить,
nvidia-smi проверить,  nvidia-samples/device query проверить, если не завелось, проверить с судо",
"Господа, у меня довольно интересная проблема. У меня ноутбук с более-менее адекватной GTX 860M, на котором я обычно всякие сетки обучаю. А сегодня, когда новую сетку поставил обучаться, через несколько десятков итераций у меня ноут начинает жутчайше виснуть, вообще ни на что не реагирует, кроме как на выключение кнопкой. Как-то раз он отвис, а процесс в питоне был выключен с кодом 137. Я загуглил, это значит, он был убит. Из-за чего такое может происходить? Может, перегревается?",
"Приглашаю всех желающих принять участие в тестировании прототипа платформы для работы data scientists. 
Четверг, 11 августа, в 13.00, Ленинский пр. 30А, коворкинг ""Рабочая станция"". 
Для желающих будут пицца, осетинские пироги, фрукты и напитки.
Кто собирается прийти, пожалуйста, напишите в личку. Подробности - там же.",
"У меня ещё вопрос по поводу реализации DQN. Получается, если мы 4 последних изображения берём как state, то у nextstate 3 первых изображения будут совпадать с 3 последними у state?",
"Кто где хранит модели МЛ?
К примеру есть сервис на проде, одна модель весит &gt;200мб
Нужно допустим 10 разных версий моделей (АБ тестирование и тп)
Ну и так как это прод - просто подкладывать их на машину руками - это не тру-продакшн",
"когда у меня были модели, используемые в рантайме, я подкладывал их в редис",
"коллеги, а что у нас среди тех кто не в питере в эти дни?",
Каким образом это логистическая регрессия?,
"<@U04CH4QBD>: например, нужно сделать выборку по эвентам определенных пользователей, а потом среди них что-то посчитать (распределение по какому то параметру хотя бы). сейчас все делаю в ipython через обычные, но может есть более высокоуровневое решение",
"А, кстати, кто-нибудь знает, как оно там работает? Примерно как в адабусте?",
<@U0JHK9001> единственный кто там отписался,
А у кого нибудь есть инвайт туда?,
"docker с GPU? В смысле, какие ресурсы подразумеваются, сравнимо с aws.g2 скажем?",
"проблема бесплатных мощностей в том, что всем хорошие конфиги не раздашь, а на слабых очень долго все прототипировать 
выдавать можно, например, во втором туре, когда остаются только 10 лучших человек и уже соревнуются в одинаковых ограничениях по железу
при условии что это важно для самой компании, т.к. подобные ограничения обычно только отпугивают",
"<@U1B2L9G58> когда я пробовала, было не очень стабильно",
"лучше скоростью
у них довольно разные реализации, поэтому результаты разниться будут, но думаю xgboost и по точности лучше, как правило",
"Статью прочитал наискосок ибо она про :nor:, но исходное название отчасти могу понять: логистическая регрессия, вообще говоря, это регрессия вероятностей с logit loss'ом, регрессор можно и нелинейным взять. Конечно, когда мы говорим про логрег, мы всегда имеем в виду обобщенную линейную модель",
"Ну вообще обычно просто эмпирически приближается мат. ожидание \int p(z) f(z) dz = 1/n \sum_{i=1}^n f(z_i), где z_i сгенерированы по Гиббсу, скажем",
"Большая часть постов, этого инфопульс, которые я видел, имела косяки как в содержании так и в оформлении.",
"Да, это хорошо, но когда каждый сэмпл - это одна точка или что-то однозначное. Но тут случай немного сложнее, такой что не ясно, что именно усреднять. Т.е. мы получаем параметры кластеров на кажом сэмпле, и точки из выборки, к которым эти кластеры принадлежат. Для каждой итерации может быть разное число кластеров - это уже первая проблема при усреднении - т.е. что с чем усреднять. Следующая - это как раз эти метки для каждой точки - какая относится к какому кластеру. При одних и тех же параметрах кластеров и одном их количестве может так получиться, что метки распределены по-разному. Это меньшая из проблем, т.к. с ней можно разобраться потом. Есть вещи, которые можно сделать просто из соображений здравого смысла и получится результат вполне приемлемый, но строгого обоснования нет. В общем так я сейчас и обхожу проблему, а хотелось бы делать не по наитию, а в соответствии со строго обоснованным методом.",
Кто подскажет - можно ли использовать датасеты с кегл в научных статьях?,
"Какие должны быть минимальные спецификации ноута, что бы гонять почти все задачи kaggle локально?",
давай так ответим - смотря за какое время :slightly_smiling_face:,
"люди часто покупают топовую комплектуху, а потом не знают - где взять времени на играть во всё",
"а под него, между прочим, был закуплен не один комп со всякими шлемами ВР (еще задолго до того. как появился окулус и это стало модным)",
<@U1UMQM200>: мат. ожидание какой функции ты хочешь посчитать?,
"Есть данные. С помощью DPMM я разбиваю эти данные на смесь гауссиан. На каждом сэмпле я получаю вектора z - метки кластеров, т.е. для каждой точки метка, к какому кластеру она принадлежит, и Theta - параметры кластеров, в данном случае - это матожидания для каждой гауссианы. Нужно получить набор из минимального числа гауссиан, максимально точно описывающих данные.",
Это будет выглядеть как вектор из матожиданий и вектор из весов кластеров.,
"В каком месте возникает желание усреднить? По количеству кластеров, что-ли?",
"А если по тупее, просто жахнуть EM алгоритм перебирая числа гауссиан? На выходе же такое хочется, как я понял",
"помимо тривиального классов эквивалентностей, когда чиселка=класс, есть еще поинтереснее. типа сравнение по модулю?",
"не знаю даже. они какие-то все разные, что ли. и когда описаны рядом -- я теряюсь.",
"вот у тебя 3 примера. треугольники -- это треугольники. что-то простое, что можно нарисовать и подобие их -- ну понятная вещь. через отношение сторон/углов определяется. 
а второй пример. возведение в степень переходит в единицу группы. а как это возводить в степень надо, чтобы в единицу перешло? к тому же разве в группе есть возведение в степень? :thinking_face: там же операция сложения)
а множество N*N -- вроде совсемпросто, но где оно встречается на практике. т.е. кажется чем-то абстрактным. ммм. походу мне без примеров сложно. т.е. если множество N*N, то его члены 1*1, 1*2, 1*3, ..., 1*N, 2*1, ... 2*N, ..., N*1, ..., N*N, а если (p;q) например (2,1), то в этот класс эквивалентности попадут 2*1, 4*2, 6*3. или я ничего не понял, о чем и подозреваю)))",
но как теперь на этих примерах будет выглядит факторизация? :thinking_face:,
то теперь любой сложный можем представить как композицию простых,
"Друзья, подскажите как грамотно (желательно ссылки, литература и т.д.) провести cannibalization analysis для группы товаров в торговой точке",
"чат, посоветуйте, пожалуйста, как поскейлить данные, чтобы изменить min / max, но оставить текущий mean",
"вот отсюда:
<https://habrahabr.ru/post/307078/>

&gt; Взять две картинки одного класса. Разбить пополам и склеить. Подать в обучение. Всё.
&gt; Я не очень понимаю почему это работает (кроме того, что это стабилизирует выборку: 5 миллионов сэмплов это круто, это не 22 тыщи). А ещё у ребят было 10 карточек TitanX. Может это сыграло важную роль.

кто-то так делал вообще на практике?
и почему это вообще дает прирост к точности модели?",
"<@U1YGZHJ59>: пьем кофе, разговариваем про анализ данных и сопряженные темы. Иногда обсуждаем прочие проблемы и как их решить с аналитическим подходом",
"насчет завтрака, да, прикольная тема,  пришел бы, но этот случай когда геолокация имеет значения, не ехать же за 300км ради этого)))",
"чтобы среднее не менялось нужно справа отрезать столько же площади сколько и слева
поскольку в данном случае правый хвост тяжелее, то можно просто сделать таблицу - сколько столбиков слева надо отрезать на заданное число столбиков справа
а потом рубануть где больше нравится

или можно чуть более научно - через интегралы хвостов",
"Может кто скинуть почитать, как разные сети объединять? допустим у меня две сети с разными входными данными (как в статье выше - кусок изображения и полное), я могу просто мержнуть их в последнем  слое и подавать разные данные на входы?",
"А что такое ""объединение""? Какие функции от него ожидаются?",
Мержнуть можно на любом слое -- только доучивать придется как минимум с точки снияния и дальше,
Нужно проверить в итераторе как готовятся кадры с пересечением или нет,
"Не расстраивайтесь, приходите в эту субботу на тренировку-зарешку, как на прошлых выходных! :slightly_smiling_face:",
Это на каком семинаре и про какие числа?,
"<@U1CGKK865>: Это на семинаре про обучение с подкреплением и те числа, которые вычислялись оценочно доля точек, которые посещены на самом деле небыли, но как будто были потому что слеплены из кусков посещённых.",
"Всем привет!

Мы в Chatbots and AI Community тоже организуем митапы и хакатоны.

С недавнего времени к нам стало присоединяться все больше Data Science специалистов, так как часто используются NLP-решения, нейронные сети и т.д.

В этой связи мы недавно анонсировали Chatbots and AI Hackathons Cup 2016 - это 6-месячный чемпионат по хакатонам с большим финалом в декабре.
Вот так неделю назад прошел 2ой тур <http://bit.ly/chatbots-and-ai-2>

Приглашаю всех заинтересованных на 3-ий тур <http://edhack.misis.ru/>

А также приглашаю в наш телеграм-чат по нейронным сетям <https://telegram.me/joinchat/ABI4pz6rz2iVzWUzaVqpmA>, где регулярно выкладываются интересные материалы и кейсы. Уже 200 участников.",
"просто цифра в 1500 когда в паблике фейсбука состоит 500 человек кхм, несколько biased",
"вот странно, что у меня друзья из вк, кто совсем не имеют отношения к дс, всё равно подписаны на те паблики",
"вот да, поэтому я и говорю. реальная цифра сообщества - куда меньшая оценка от онлайн пабликов",
"мне интересно, зачем они подписаны",
"ну как зачем, +3 к крутости",
"Кто знает какой-нибудь пост или статью, где есть подробный обзор алгоритмов оптимизации стохастической целевой с бенчмарками? В особенности меня интересует сравнение adagrad и adam.",
"Похоже, что нельзя. Ну я как посмотрел - почитал пост, там написано, что натренировали на Universal Dependencies Treebank, залез на <http://universaldependencies.org/#ru>, там пишут, что корпус для русского - под CC BY-NC-SA 4.0",
"ничто не мешает подобную функцию добавить с adam, и флажок сделать в вызывающем коде, примерно как в scikit-learn делают",
"это даже не сильно много работы вроде бы, тесты могут быть ""работает примерно так же, как adagrad""",
"это очень круто, что lasagne можно как библиотеку использовать, а не как фреймворк",
это как книги в стол писать? :grinning:,
"Какие там призы в этом чатботовом хакатоне? Не нашел сразу, а осматривать все закоулки сайта лень",
"коллеги, помогите осмыслить смеси распределений.

допустим, у меня есть эмпирическое распределение, сильно скошенное, с длинным правым хвостом. я могу его зафитить разными экзотическими функциями, но мне такой вариант не нравится.
есть мнение, что можно использовать какие-нибудь смеси, солянку распределений, чтобы не через мнк или еще как подгонять параметры экзотического распределения, в просто собрать несколько простых распределений,  и с их помощью зафитить.
то есть, что-то типа 
`y ~ exGauss(mu, sigma, tau)` vs `y ~ norm(mu1, sigma1) + lnorm(mu2, sigma2)`

можно ли так делать? или смеси лучше брать тогда, когда есть мультимодальность?
и если можно, то как именно подбирать компоненты смеси?",
"Перепутал со с.в. :) А зачем подгонять распределение, если есть эмпирическое?",
"нуу, чтобы поиграться с симуляционными экспериментами, например. или чтобы запихнуть в mcmc и еще какой бисовой байесовщины сверху накрутить",
"эм. моя плохо понимать, по-настоящему, и mcmc, и байеса...
я это представлял следующим образом - у меня есть эмпирическое распределение (точнее, два распределения, на две сравниваемые группы), я знаю их параметры
потом я с помощью mcmc и распределения параметров генерю чортову кучу семплов, и считаю, в какой доле семплов, условно, медиана по одной группе больше медианы по другой группе
ну или еще как проверить вероятность различия групп

я знаю, что это как-то очень криво звучит, но не знаю, в какую сторону копать, чтобы сделать это правильно и логично :disappointed:",
"<@U13E1AWCX>:  о том и речь, какие именно другие? их же много %) может, есть какие-то типовые подходы, как это делается?",
А книгу какую конкретную посоветуешь?,
А на картинке нам показывают как к чипам крепятся радиаторы?,
"Правильно ли понимаю, что если у нас есть набор фич: f1, .., fn, на которых XGBoost строит определённый набор деревьев при заданных параметрах, то если вместо fn использовать g(fn), где g() - строго монотонная функция, тогда у нас результат будет тем же, причём деревья будут ""совпадать"" (при тех же параметрах), кроме условий в узлах. 
Верно или где-то ошибся?",
"<@U04ELQZAU>: а где можно пруд почитать? похоже на правду конечно, но я немного отделаться от картинки в голове типа такой",
"почему? Другими словами вы разрешаете делать разные монотонные преобразования над фичами и утверждаете, что порядок сплитов и по каким переменным каждый сплит сохранится?   Хотелось бы почитать доказательство. Если на шаге i выбрана таже фича для сплита, что в исходном дереве, то вопросов нет. Но почему будет выбрана имена она?",
"в формуле значения фич вообще нет, только как условие сплита",
"чёт сложно сходу прикинуть, обычно среднее между двумя точками трейна берут как сплит
чисто из-за округлений чтоли в разные попадёт? :thinking_face:",
"<@U0KQ5M6KX>: да, но если это все дело параллелить, то понадобится уже куда больше Гб, да и вот как раз интересуюсь, вдруг посоветуют что, так то да, для начала и этот вариант покатит",
"Я ненастоящий сварщик, но кажется сетки будут сильно дольше работать. там ведь надо матрицы умножать  это сложна. А для дерева кажется что надо просто держать деревья поиска по каждой фиче и когда приходит текст на классификацию в каждое его вставить это проста",
"<@U064DRUF4>: да, если не взлетит идея с одним классификатором, то там есть куда думать, но для начала хочется разобраться с одной моделькой",
"<@U064DRUF4>: так то там можно рассматривать задачу иерархической классификации и строить уже не одну модель, а совокупность, но, как мне кажется, это куда сложнее и муторней",
"это да, всякие nce / hierarchical softmax пилить что-ли, или structured output использовать, который явно иерархию учитывает. самому интересно, что тут сделать можно, и как это по-нормальному делать",
"хотя конечно если там времени пока грузится страничка, то надо сокращать как только можно",
"Зачем k-means, когда можно топик модель?",
"<@U0AS548A1> это принципиально отличается от рф тем, что позволяет хранить не 2гб деревьев, а 3к векторов. Там проблема будет в том, что эта дичь сработает только если каким-то чудом все классы локализованы в пространстве, чтоб каждый из них в свой кластер собрался. Это как-то маловероятно, как мне кажется, но если есть векторизованные тексты - попробовать 10 минут",
"<@U1BAKQH2M>: да сорян, я чот глюканул и прочитал как k-nearest",
Это когда корпус из разного количества топиков,
"<@U1BAKQH2M>: хм, а как ты предлагаешь оценивать вероятность принадлежности к классу? точнее к кластеру, и как принимать решение о том, что объект не входит ни в один класс",
ну если кминс - то к какому центроиду ближе,
"это, я что-то не очень понимаю, как в такой задаче поможет топик моделлинг, можете пояснить? То есть мне даже кажется, что здесь он бесполезен",
"Можно вопрос, а как в таких задачах используется словарь слов и что делать с новыми словами не из словаря? ",
"разница как бы в том, что классификация - supervised, а кластеризация - нет",
"Тогда предлагаю такой пайплайн: делаем топик моделлинг на текстах описаний продуктов, кластеризуем описания. Сажаем девочку прямо читать глазами семплы из кластеров и соотносить их с категориями. Если соотносится хорошо - ок. После этого любым костыльным методом (логрегрессия, как предложили выше, или что-то более тупое) получаем вероятности принадлежности к кластерам, на их основе ставим категорию/отсутствие.",
"на самом деле надо пойти еще раз посмотреть решение задачи иерархической классификации википедии на каггле, где Дьяконов почти обошел всех, используя практически только knn",
"Наверно этот вопрос лучше задать здесь:
А как в задачах, где очень большой словарь и есть опечатки, используется сам словарь и что делать с новыми словами не из словаря?",
А от второго какой профит?,
то есть идейно это одно и то же) ну как мне кажется,
"так а жалоба то изначально на что была? как я понял, именно на жирность рандомфореста",
"да скорее интересно, как люди решают такие задачи",
"Т.е. можно свести все к бинарной классификации где данные для тренировки будут вида (страница, категория, 1/0)",
"да, эта идея очень крутая на самом деле, это я уже использую в других задачах, <@U0DA4J82H> а какие бы вы фичи лепили для бинарной классификации?",
"О, меня как раз недавно спросили",
"Это про типа как связать представление типа  tf-idf с классификацией, да?",
"<@U0DA4J82H>: а почему так делают? Я часто видел что не вычитают, но не понимаю почему, это же неправильно ",
"<@U0AD1L5NC>: ага, там же все очень sparse, на размерность вектора по большому счету пофиг. От гугла еще вроде статья была недавно, где к такому вектору еще нейросеть добавляют над каким-то ембеддингом - ну примерно то же самое, что <@U0DA4J82H> советует, только end-to-end",
как раз для косинусной меры хороший LSH есть,
"JFYI (не знаю куда написать - давайте тут)
тут у знакомых встал вопрос с разметкой данных (aka mechanical turk)
и вот я им посоветовал crowdflower - он конфетка по сравнению с mechanical turk
так вот им вчера ответили про подписку - 35k$ (это просто такой членский взнос :smiley: )",
"я им года 3 назад пользовался, было как обычно - платишь за разметку - берешь результаты из api",
"я может затупил, но когда пробовал (эти же 3 года назад)), было можно платить только картой из сша",
"Приглашаю всех желающих принять участие в тестировании прототипа платформы для работы data scientists. 
Тестирование будет завтра, 11 августа, в 13.30, 14.00, 16.30, 17.30, 18.30 - Ленинский пр. 30А, коворкинг ""Рабочая станция"". 
Для желающих будут пицца, осетинские пироги, фрукты и напитки.
Кто собирается прийти, пожалуйста, напишите в личку или на почту.",
"Какой правильный способ отладки деревянных моделей типа xgb? Вот у меня есть модель которая хорошо работает в целом, но на некоторых примерах показывает какие-то безумные результаты. Есть ли какой-нибудь простой способ понять значения каких фич привели к этому?",
"<@U0KQ5M6KX> А почему ""смотришь"" в кавычках? Есть какие-то специальные способы ""смотреть""  на фичи? ",
тем самым ты как раз и генерируешь другие данные,
"<@U0DA4J82H>, вот для RF моделей есть treeinterpreter, который по каждому примеру разделяет какой вклад дала какая фича. Но для xgb готового не знаю.",
"<@U0DA4J82H>: можно добавить, но самый сложный вопрос в том, каким образом должна быть его BSD в MIT упомянута...",
"она всё позволяет, могу и код копировать, совместима с MIT, но как обычно совмещают - не знаю",
"я пока вообщем вот что делаю: беру несколько ""правильных"" результатов, один ""безумный"", и поочередно подставляю значение фичи из правильных наблюдений в неправильное наблюдение и смотрю, как меняется вывод модели",
"И еще такой вопрос - какую игру взять из Атари самую простую, на которой сеть быстрее начнет сходиться и результат будет более заметен? Думаю взять boxing",
"Просто хотелось бы как можно быстрее заметить то, что мой код содержит косяки, и не ждать целую неделю :)",
"По поводу классификации текстов на большое число классов - вот что можно посмотреть - <http://lshtc.iit.demokritos.gr/> , на кагле даже был контест года два назад
Также точно неплохого работает(по крайней мере на википедийных классах) следующий алгоритм:
1. Считаете w2v для слов - кластеризуете их любимым алгоритмом
2.Текст представляете в виде статистики в кластерах(относим к ближайшему по метрике алгоритма)( это полный аналог Bag of Visual Words)
3. Получаем новое пространство размерности числа кластеров и в нем делаем что-нибудь - gradient boosting, rf, vw и так далее.
Чем ниже уровень в иерархии классов, тем больше кластеров нужно, так как информация начинает теряться
Кривовато описал
<https://opendatascience.slack.com/archives/theory_and_practice/p1470767220001245>",
"<@U040M0W0S> в простейшем случае, если иерархию классов представить как дерево (то есть каждая вершина является ребенком какой-то другой вершины), то уровень - высота вершины от корня",
"<@U0RV376MC>:  на первом хакатоне deephack у нашей команды вроде бы был expirience reaplay на 300к и результаты были примерно такие же как при 1кк. Что касается игры, то можно еще breakout попробовать, так как в статье приведен график обучения из которого видно, что скорость обучения на первых эпохах  достаточно высокая. Могу еще посоветовать потестить на gym, например classical control. Там все быстро обучается, тока состояние там не картинка, а вектор, поэтому может сеть и memory надо будет переделать",
"<@U0JJ69UB1>: Спасибо! Memory у меня так сделан, что переделывать ничего не надо, только сеть другую вставить. Я потом, когда всё доделаю и удостоверюсь, что всё работает, код выложу на гитхаб, может кому-то интересно будет) Я как раз в gym breakout пробовал, и, по-моему, он у них косячно сделан, так как в совершенно случайный эпизод игра может не начаться и будет один и тот же кадр повторяться. А какой из classic control лучше взять? А то насколько я помню, mountain car при неизменённом state representation и reward не сходится.",
по каким критериям выбирать между tensorflow и theano?,
"""ну словарь все равно никогда не будет максимально полным -- и морфологию все равно надо учитывать""

Уверен, что это не совсем правильное утверждение. За исключением имен собственных возможно составить полный словарь, как сделали это для английского в проекте wiktionary.",
"воронцов как раз пишет, что оно математически удобно :slightly_smiling_face:",
"Это как приор распределение на темы берется - насколько помню. Наиболее вероятно документы будут порождаться из небольшого числа тем(одна, две), как примерно и есть в жизни. Мотивация вроде была в этом. Насколько помогает - спорный вопрос.",
"Ну и, как я помню, Воронцов любит разреживающие регуляризаторы выводить из дирихле (вроде, даже из несобственного дирихле)",
"c'mon, ну кто виндой для серьёзных задач пользуется",
"Когда работаешь над решением, надо часто тренировать в полную силу",
"<@U0AD1L5NC>:  много времени занимает подготовка данных, выстраивание цепочек и т.д.; если ""компилируется"" (пусть и с ограничениями), то не надо будет терять время на машине, где достаточно мощности",
"<@U1LJE9MCK> бери теану - старая и проверенная; я вот столкнулся с тем что одинаковые модели на теано и тф весят 700мб и 8гб соответственно; мой коллега написал тикет в тф (а таких же там уже было несколько), а получил ответ из серии типа мы видим развитие нашей системы и так то и так то спасибо за инфу когда нибудь мы заметим вас",
"&gt;&gt;&gt; 

Книга посвящена современному этапу формирования байесовской парадигмы (модели) научного вывода, возродившей в математической форме главные идеи научного прогресса методологов науки XIX века У.Уэвелла и Ч.С. Пирса. Исследуются причины, по которым байесовская парадигма научного вывода приходит на смену известной гипотетико-дедуктивной модели научного познания, и ее главное достижение — обоснование свойства объективной самокоррекции научного знания. Подробно анализируется байесовский вариант решения проблем абдукции, индукции и научного прогресса.

Книга адресована студентам, аспирантам, молодым ученым, а также всем, кто специализируется в области истории, философии, логики и методологии науки.",
"я бы сказал, что и сейчас они вполне могут быть в некоторых областях
другое дело, что монографии на русском сейчас пишут очень уж странные люди, те кто адекватнее - пишут статьи и на английском",
"Кто читал ""Маятник Фуко"" - чуваки из URSS походу восприняли описание ""Мануция"" как бизнес-план",
"<@U040M0W0S> потому что процесс Дирихле это случайный процесс, то есть, не является распределением, но существует его конструктивное определение, с помощью которого можно как бы записать априорное распределение над другими распределениями",
"а я вообще правильно понимаю, что случайный процесс не является распределением, но в общем случае является типа n-мерным распределением? где n -- это количество сечений. которые может быть и бесконечным?",
Как число сечений может быть конечным? :psyduck: ,
"я представляю, как это можно решить ""вручную"", но это займет какое-то время.",
а почему нет связи A+С?,
"смотри, в общем виде задача стоит так - ""визуализировать, какие еще продукты выбрал пользователь, вместе с продуктом А""
то есть, как связаны вторичные продукты между собой  - не так важно",
"угу. ятоже уже о долях подумал. но как оно будет организовано на диаграмме? потому что в базовом варианте хорды выходят из основания параллельно, а если брать долю - маппинг на основании не очень получится, не?",
так а почему в таком случае ты выбрасываешь связь В+С?,
кстати хороший вопрос. <@U041LH06L> как в графах N-арные связи визуализируют? это же важная инфа что связь именно между тремя узлами - такое как-нибудь исследуют не парными связями? визуализируют так?,
"&gt; но как оно будет организовано на диаграмме? потому что в базовом варианте хорды выходят из основания параллельно, а если брать долю - маппинг на основании не очень получится, не?
да нет, почему. все логично.",
"тут уже вопрос дальше метафорический: если человек купил А, затем Б, затем С, какая вероятность того, что связь А-Б существенней связи Б-С? Началась ли это новая цепочка пользователя или это все еще старая цепь?",
"вот, да, тут вопрос в визуализации. Как с топографическими картами) есть проеции земли, на которых все углы (относительно вертикали карты) одинаковы в реальности. Есть проекции, на которых линии всегда идут по кратчайшему расстоянию на земле, но углы в разных широтах различаются. вот у тебя какая задача? максимально честно показать пользователей или максимально честно показать связи?",
"эээ, мне не нравится постановка вопроса в формале ""или"" :disappointed:
мне надо показать сколько пользователей совместно с продуктом А выбрали продукт Б или С или Д или...N, и так дла каждого продукта.
в идеале, чтобы ширина хорды отражала, какая доля пользователей от каждого продукта сделала этот выбор (начало хорды А+Б - доля пользователей А, которые взяли продукт Б, конец хорды - доля пользователей, выбравших продукт Б, у которых также есть продукт А)",
"кто знаком с text mining’ом, подскажите следующую вещь
есть текст, хочу получить его эмоциональную окраску (не просто positive/negative), то есть сделать сентимент анализ. с чего лучше начать? где найти корпусы и тд.?
прогуглил сам уже, наше только VADER но это не совсем то",
"подскажите пожалуйста следующую вещь.
есть текст, хочу получить его эмоциональную окраску (не просто positive/negative), то есть сделать сентимент анализ. с чего лучше начать? где найти корпусы и тд.?
прогуглил сам уже, наше только VADER но это не совсем то",
"мда. все же либо корректное основание, либо правильная ширина хорды, даже если брать доли. 
ну либо учиться как-то делать пересекающиеся в основании хорды. я посмотрел доки по chords, там где-то есть новая фнкция для устанвоки углов для хорд, но пока не очень понятно, где и как этим пользоваться, а толкового мануала не вижу %(",
"все равно все оперируют десятком номинаций эмоций, остальное - либо лиетратурное, либо как синонимы",
"В реальной жизни десяток цветов вполне норм, но для исследований каких-нибудь маркетинговых - нужно побольше. Иначе как узнать, почему не покупается новенький автомобиль красивого цвета индиго? Возможно, потому что с ними у клиента ассоциируется баклажан.",
"Но придумать, чем рассмотрение такой величины как случайного процесса поможет в сравнении с рассмотрением её как случайной величины, я не могу",
"эмоции сильно зависят от среды, поэтому оптимально разметить на своём тексте, как мне кажется
если разметить на фб и жж сильно разные классификаторы получатся",
ну ты пишешь какой то текст и указываешь свое текущее состояние,
"(вдруг) может кто знает - где взять дамп грейглиста
он меня банит постоянно =( даже если раз в 10 секунд запрос",
"так сказал, как будто бы это что-то плохое!",
"Вообще говоря, в PixelRNN есть оптимизация, что RNN по пикселям считается как convolution pass",
"В таком виде, как хотелось бы",
"<@U040M0W0S> Я как раз готовлю материалы для докладика на тему RNN'ок для пространственных данных, и различия с CNN'ками
Через пару недель, может месяц устрою доклад на семинаре",
"<@U04CH4QBD>: речь исключительно про xgboost
На вход алгоритм получает только числа.
С точки зрения сложности вычисления будет ли разница, если заменить один столбец с 10-ю различными значениями, десятью столбцами, где только по два значения?

",
"<@U1LJE9MCK>: если фича категориальная по своей сути, то если это будет один столбец, то xgboost будет ее обрабатывать как количественную фичу",
я тоже не понял - для него пофиг какой столбец,
"то есть если ты хочешь, чтобы фича оценивалась алгоритмом как категориальная, то без ohe этого не добиться",
"<@U1LJE9MCK> как я понимаю, для хгбуста имеет значение порядок на категориальных фичах (ну это понятно, делаешь сплит по значению и в зависимости от порядка разные значения в ветви попадут). Разным образом закодировав фичи можно разные результаты получать, <@U04BFDYPV> говорил, что есть пруф, что если отсотировать фичи по частоте, будет хорошо разбиваться на любые сабсеты",
"а, это да. Он помнит с какого адреса ты акк регал. Закон о персональных данных, все персональные данные принадлежат государству и все такое.",
когда много категориальных фич ohe будет существенно быстрее,
"Всем привет, кто работал с фейсбук api? 
Вопрос в том, если пользователь авторизовался в приложении, можно ли через какое-то время (когда все пользовательские токены протухли), получить доступ к информации, к которой он предоставил доступ?",
<@U0AD1L5NC>: и как? какого размера сетка и сколько времени forward pass?,
А как думать про эндогенность по байесу?,
"<@U0JJ69UB1>: Выложил код агента, вот <https://github.com/Fritz449/DQN_Agent>
Я, кстати, по вашей презентации делал, поэтому немало контента и текста взято оттуда :slightly_smiling_face: 
Написал довольно подробный readme, код закомментировал как можно подробнее, но если что-то непонятно, задавайте вопросы",
"кстати, интересно, что будет, если вместо циклического буфера в experience replay перезаписывать случайную запись, когда размер до максимума доходит",
"я когда ER реализовывал, делал так по дефолту, но не смотрел, как сильно оно влияет на результат",
"в каком курсе подробнее такие ""интуиции"" рассмотрены?",
"<@U0JJ69UB1>: А кто-нибудь пробовал использовать в DQN не просто одношаговый Q-learning/double Q-learning, а Watkins's Q(λ )?
Можно создавать временный xp-буфер, где хранить переходы, для которых состояние при подсчёте target ещё неизвестно, и обновлять их там, и когда совершается нежадное действие, скидывать эти переходы в обычный буфер, затем обучать",
"не такое не делал, делал только n-step q-learning, но это уже on-policy алгоритм, поэтому xp replay там нельзя использовать. Там был аналог xp реплея, когда мы проходим эпизод на одной политике, набираем переходы в буфер, обучаемся, обнуляем буфер и с новой политикой снова набираем переходы в буфер. при этом переходы там в виде St, Rn, S(t+n),  то есть награду берем на за 1 шаг а за n шагов",
"И что в итоге, какие результаты?",
"<@U0JJ69UB1>: Я тут сел реализовывать bootstrapped DQN. Как лучше сделать - K полностью разных нейросетей или K ответвлений после сверточных слоёв? 
Ещё - стоит ли обращать внимание на то, что маски для обучения можно генерировать кучей всяких способов или оставить только распределение Бернулли? 
И последнее: о чём идёт речь, когда говорится о gradient normalization? Просто деление всех градиентов на K, или нормализация градиента только у свёрточных слоёв?",
Для каких компонентов вообще нужны эти lanes?,
"эти lanes используются для передачи данных между процессором и PCI-E устройствами, когда видяха считает что-то внутри себя, она не нагружает шину PCI-E",
"я в свое время реализовывал, делал сверточную сеть общую, а полносвязные отвелтвления отдельные.  Мне показался этот варинт более логичным, чтобы несколько сверточных слоев не учить, но так хз, что на самом деле лучше будет. Я особо не запаривался по поводу масок брал просто распределение бернулли. Нормализацию градиента тоже не делал, просто learning rate подбирал, для апдейта весов использовал adam и rmsprop, думал что там сами подстроятся. Хотя вроде как по смыслу нормализацию сверточных слоев надо делать, если они общие. Вообще интересно сравнить результаты, если делать  такую нормализацию и не делать.",
"<https://news.ycombinator.com/item?id=12085044>
Вот тут обсуждают статью, где люди приводят какие-то математические рассуждения в пользу того, что LSTM круче марковских цепей",
"это пока td(0), т.к. мы смотрим только на один шаг назад (ну или вперед, смотря какая терминология)",
"но если после этого выбрать ""открыть холодильник"", то Q(s,a) уже будет &gt; 0, т.к. там для a_best будет вариант ""выпить колы"", который, как мы уже видели, приводит к reward=1",
"если после того, как ""открыть холодильник"" уже имеет оценку return &gt; 0, и мы выбрали из ER памяти пример с ""подойти к холодильнику"", то r_t + gamma*Q(s,a) опять будет больше больше 0",
"<@U064DRUF4>: есть существенные моменты, из-за которых используется n-step q-learning.
1) он быстрее сходится в табулярном варианте хранения q-value (это можно объяснить)
2) в случае DQN мы используем не табулярный способ хранения, а аппроксимацию глубокой сетью, и когда мы обучаем сеть на transition с открыванием холодильника, не факт, что сеть после обучения будет давать верное значение на открытии холодильника",
"<@U064DRUF4>: 1step q-learning с experience replay было самое первое, что пробовали, и сеть толком не сходиласть.  n в n-step Q-learning подобрали перебором разных значений от 1 до 100, в итоге получилось, что 20-50 было оптимально. Обучение там быстро проходило, поэтому была возможность просто перебором гиперпараметры подбирать. Что касается объяснения и примера выше, то я бы объяснил так: в примере выше для того чтобы обучить значение q для ""подойти к холодильнику"" нам надо сначала просемплировать переход ""выпить колы"", а потом просемплировать переход ""открыть холодильник"", так что мы минимум тока на 3 шаге обучения сможем как-то обновить значение q для  ""подойти к холодильнику"", если бы мы использовали  3-step q-learning, то мы бы уже на первом шаге смогли обучить значение q для этого состояния/действия. А если состояний очень много и награда очень задержана во времени, то это может имеет еще больший эффект. Плюс еще к примеру выше можно добавить, что все апдейты q должны ддомнажаться на learning rate, что еще медленее должно делать сходимость. В пределе, результат наверно не должен отличаться, просто если брать награду на несколько шагов вперед, то быстрее сойдутся значения q.
Кстати интересно, что в статье count based exploration от дипмайнд(<https://arxiv.org/pdf/1606.01868v1.pdf>), они для расчета td-error использовали сместь td(0) (просто семплингом из xp replay) и td(1) по сути монте карло семплиногом точной награды до конца эпизода. B писали, что это лучше работает раздел 7.1. Как они написали это - Mixing in a 1-step target with the Monte Carlo return is best thought of as a poor man’s eligibility traces",
"<@U0RV376MC>: 1) Скорость сходимости будет, видимо, зависеть от того, как долго обычно reward ждать надо. Если он и так быстро приходит, то не факт, что явный n-step лучше будет. Но не уверен. 2) про это не понял. 3) про проблему тоже не понял :)",
"1) Ну вот n как раз и надо подбирать в зависимости от того, как скоро обычно приходит reward
2) Вот допустим мы пытаемся нейронной сетью аппроксимировать задачу про холодильник и колу. Вот мы выпили колу и обновили переход ""выпить колы"". Но это не обновление непосредственно значения функции в точке перед выпиванием колы, это изменение каких-то весов, притом с маленьким шагом, что не гарантирует того, что нейросеть в точке перед выпиванием будет иметь высокий q-value на действие выпивания. Оно продвинется к нему, возможно, но этого недостаточно.
3) Когда мы используем n-step q-learning, у нас есть состояние на момент t и на момент t+n, и мы хотим обновить q-value в изначальном s, двигая его к взвешенной суммарной награде за n шагов + gamma^n * max_a_Q(S(t+n),a)
Но это корректно делать в том случае, если агент, следуя жадной политике по q-value, которому у нас есть, как раз пришёл за n шагов в это состояние. Но когда мы уже обучили нашу сеть на чём-то, то мы уже не можем гарантировать, что сеть придёт состояние S(t+n) таким путём. Поэтому обучать на таком переходе можно только один раз сразу после того, как он был получен",
гайз а как по стандартной ссылке найти статью?,
"Ну вот например статья про deep unsupervised learning (Chen et al., ArXiv 2016) - гугл молчит как партизан",
<@U06TZHSSJ>: да.  :anguished: как ты ее нашел? Это какая то магия?,
как достать от туда тысяч сто картинок?,
Спасибо! А как там отправлять запрос? Я нашёл только для конкретного пользователя,
"<@U0ZHHV83C>: уточни свою задачу. что за фотки и для чего ты ищешь?

может быть ты хочешь собрать фотки из альбомов юзера, в которых есть отметки людей. самих людей ищешь по какой-нибудь целевой аудитории. например подписчики паблика_X с более N фоток и аккаунты не младше T

а может тебе подойдут фотки из альбомов всяких ивентов, где как правило есть дофига людей. например, в каких-нибудь прыжковых группах 100к фоток в альбомах наберется с нескольких самых активных групп (много лет еженедельных мероприятий, 300+ альбомов с дофигищей фоток)",
"а какой в этом смысл, кстати?",
"чтобы узнать, что на фотке 1-10 человек, тебе надо выкачать с фотки ее отметки. иначе узнать о наличии на фотке людей, а не каких-нибудь мемов или фото котов с пейзажами, сложно. а именно, прогнать фотку через сторонний сервис, или проверить своими глазами

чтобы получить отметки, тебе надо знать у какой фотки ты их хочешь собрать. например, тебе надо выбрать юзера, выбрать всего его публично доступные фотки, и уже для каждой из них получить метки. 
следующий вопрос, откуда брать юзеров - можно случайно дергать id наугад, но это чревато. чтобы не прогадать, ты собираешь с большим запасом юзеров, скажем, из какогонибудь крупного паблика. причем скорее всего ты еще отберешь некачественные записи, у кого нет фоток и кто скорее всего бот.",
"<@U1G303UTW>: ищу закономерности, комбинации разных признаков. 
А как надо тогда делать?)",
"сделать greedy search, как вариант",
"или ты забиваешь хрен, и как я подсказал, берешь паблик в котором только фотки юзеров (например со всяких мероприятий). и тупо качаешь их, в надежде что мусора будет не очень много",
скорее всего из большого шума. так как твои новые фичи могут быть бесполезны. такое у меня предположение,
"&gt; скорее всего из большого шума. так как твои новые фичи могут быть бесполезны
Ну пусть они бесполезны, и что? Когда строится индивидуальное дерево, выбирается фича, наилучшим образом разбивающая датасет. Бесполезные фичи просто проигнорированы должны быть",
"изначально 30, но проблемы начались, когда я добавил еще 10",
"Журавски участвовал как минимум в этой статье: <http://arxiv.org/pdf/1603.09727.pdf>. так что как бы обязан про нейронные сети, что-то говорить в новом курсе.",
"Часто встречаю такой импорт:
from keras import backend as K
а далее 
K.function(…)
K.variable(…)
и т.д
Если в паре слов, то зачем все это нужно? И в какой части документации кераса можно про это почитать?",
"<@U0XF4GAM8>: keras может работать с разными backend'ами, как с tensorflow, так и theano",
но что-то не соображу как такое делать,
"вроде как ты написал, наоборот независимые получаются",
"А с марковскими моделями такой эффект вроде нельзя промоделировать, он как раз совсем не марковский",
"Спасибо!
Есть еще непонятка
Вникаю в код трансфера стиля, который нашел на керас 
<https://github.com/llSourcell/AI_Artist/blob/master/Network.py>
Мне очень непонятны строчки 251-252 . Почему вход для сетки делается формы `shape=(3, 3, img_width, img_height)`  , хотя по идее должна быть 1 картинка а не 3 (в коде сгруппировали 3 картинки в 1 тензор - контент, стиль и результат)
В оригинальном коде VGG-16 <https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3> первый словй выглядит совсем не так",
"В каком контексте? Моделирование количества покупок юзерами? Там, как правило, overdispersed и можно извращаться с Gamma-Poisson. Кажется, в твоем случае это тоже может помочь - как раз, может, сделать prior на лямбду прыгающим, как в статье выше. Но у меня по факту логнормальное достаточно хорошо аппроксимировало количество покупок за период, чтобы извращаться перестало быть интересным, если говорить о практике",
"Вау, как совпало. Пока писал, пришла приглашалка стать ментором (что бы это ни значило) на вновь запускающемся на Курсере PGM! Я, конечно, ментором не буду, но курс всем, кто не проходил, крайне советую. Байес с погружением по самые локти",
"Если я правильно помню, representation - это был первый раздел того курса. По ходу, они его на несколько побили. Прошли те времена, когда в некоторых курсах было часа по три лекций в неделю...",
"Не могу найти ссылок. Но вроде как у Обамы в 2012м было все плохо и все думали что победит Ромни, но саентологи спасли Обаму с помощью медиа",
"<@U0ZHHV83C>: я вот тоже наконец-то полностью закрыл первую часть, теперь думаю в терминах ""а как это взломать""",
"джентльмены, как в elasticsearch pdf запихнуть? я чего-то и плагин поставил и в base64 перекодирую файл, но обратно получить по текстовому запросу ничего не получается. 
что-то не так я понимаю. подскажите, пожалуйста. или задайте ваши вопросы)",
зачем тебе сам pdf там хранить?,
если тебе нужен pdf как файл проще его как ссылку хранить,
это как раз тот плагин,
да почему то он просто стринг,
обычно написано какой аналайзер еще,
или может оно еще в _all добавляется как и другие поля по умолчанию,
"Самое забавное, что Боне тараторит, как ненормальный, и при этом это самый понятный в плане речи курс",
"коллеги, допустим, у меня есть  ~50 событий, например, облако тегов. как можно относительно безболезненно визуализировать разные варианты их совместной встречаемости? так, чтобы можно было в визуализацию заложить частоту встречаемости каждого паттерна/комбинации событий?
комбинации событий - 1-10 в вместе, примерно.
",
"я бы попробовал отобразить как network, размер нода по частоте, при наведении/выделении - отображаются связанные (частота больше трешхолда?), отдельно контуром и текстом подствечиваются наиболее интересные",
"ну как в xgboost: сначала глубину и `min_child_weight`, потом `gamma` и после этого `colsample_bytree` и `subsample`",
"в деревьях почти везде mse на регрессии, вроде как нету смысла другие брать",
"Всем привет. Сориентируйте пожалуйста чайника в какую сторону смотреть, какие слова гуглить.
Есть производство нестандартных штук, надо предсказывать загрузку мощностей и выход готового на пару недель вперед.",
"Гайз, а у вас batchnorm в керасе работает? Я когда создаю его, у меня следующая ошибка:
Exception: You are attempting to share a same `BatchNormalization` layer across different data flows. This is not possible. You should use `mode=2` in `BatchNormalization`, which has a similar behavior but is shareable (see docs for a description of the behavior).
В issues читал, что это баг, его вроде пофиксили, но у меня всё ещё не работает",
а как python vs r определяет логику решения задачи? в почему вдруг именно к питону прибито mm?,
"У меня проблема с ""по умному"" и как задать эту вероятность",
"Вот думалось про второй путь, потому как задача предсказания времени производства отдельного закаща сама по себе ценная",
"есть лог производства, грубо говоря какие были шаги предприняты, это пару раз в день, можно пробовать извлечь из этого статус продукта, но жто не так тривиально",
"если вариаьбильность большая, то регрессия мне все равно выдасть то же среднее, как и для маленькой, но по ошушениям итоговое суммарное количество  продукта будет разным... или нет?",
"Мне кажется, или такие задачи можно решать с помощью таких методов, как в книжке bayesian methods for hackers?",
"а есть какие-то предположения, какие параметры влияют на скорость роста генов?",
Это отдельная совсем задача. Но может кто в теме. Есть тексты 4х буквенного алфавита  и есть результат попытки произвести их определенным процессом. хотелось бы предсказать успешность  нового произвольного текста. Есть какие-то общие подходы?,
"ну где надо, а где и нет",
"Понятно что у вас олигосинтезатор, просто уже вроде как перешли на сборку олигов из триплетов",
"Все-таки хотелось бы вернуться к задаче в общем виде. Есть текст и к нему параметр ""легкости"", как можно из него извлечь фичи для  предсказания, не зная сути текста?",
"можно на ngramы бить и искать мотивы, но как быть с фичами типа ""повтор 10 букв на расстоянии от 5 до 100 друг от друга""?",
"Не вижу так же, чем наличие физическиз свойств мешает анализировать последовательность, как текст. Они конечно есть и они обуславливают легкость, но напрямую  к ним не так  просто подойти.  Впрочем если есть какие-то соображения прямиком из предметной области буду конечно рада",
"алгоритмов для извлечения подобных признаков можно разных попридумывать, вопрос в том, какую целевую функцию оптимизировать",
"Посмотрите - в чем тут может быть проблема?
Делаю GridSearchCV
```
from sklearn import svm
method = svm.SVC()
parameters = {'kernel': [ 'rbf','linear'], 'C':[0.25, 0.5, 1, 2, 4, 8, 16, 32, 64]}
clf = grid_search.GridSearchCV(method, parameters,cv=5,scoring='accuracy')
clf.fit(X_train, y_train)
print ""Best score={} with params= {} for method {}"".format(clf.best_score_,clf.best_params_, method)
```
Когда вывожу `clf.grid_scores_` получаю очень странную картину:
```
[mean: 0.30769, std: 0.00857, params: {'kernel': 'rbf', 'C': 0.25},
 mean: 0.73373, std: 0.07378, params: {'kernel': 'linear', 'C': 0.25},
 mean: 0.30769, std: 0.00857, params: {'kernel': 'rbf', 'C': 0.5},
 mean: 0.73373, std: 0.07378, params: {'kernel': 'linear', 'C': 0.5},
 mean: 0.31361, std: 0.01048, params: {'kernel': 'rbf', 'C': 1},
 mean: 0.73077, std: 0.07823, params: {'kernel': 'linear', 'C': 1},
 mean: 0.31361, std: 0.01048, params: {'kernel': 'rbf', 'C': 2},
 mean: 0.73373, std: 0.08191, params: {'kernel': 'linear', 'C': 2},
 mean: 0.31361, std: 0.01048, params: {'kernel': 'rbf', 'C': 4},
 mean: 0.73373, std: 0.08191, params: {'kernel': 'linear', 'C': 4},
 mean: 0.31361, std: 0.01048, params: {'kernel': 'rbf', 'C': 8},
 mean: 0.73373, std: 0.08191, params: {'kernel': 'linear', 'C': 8},
 mean: 0.31361, std: 0.01048, params: {'kernel': 'rbf', 'C': 16},
 mean: 0.73373, std: 0.08191, params: {'kernel': 'linear', 'C': 16},
 mean: 0.31361, std: 0.01048, params: {'kernel': 'rbf', 'C': 32},
 mean: 0.73373, std: 0.08191, params: {'kernel': 'linear', 'C': 32},
 mean: 0.31361, std: 0.01048, params: {'kernel': 'rbf', 'C': 64},
 mean: 0.73373, std: 0.08191, params: {'kernel': 'linear', 'C': 64}]
```
Почему все колеблется строго между 2 значениями?",
это как вариант извлечения более сложных признаков из цепочек,
"как вариант - обучить свёрточную НС, с помощью которой пытаться детектировать общие проблемные места у сложных последовательностей",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1471431613001941> в длиннопосте про генерацию Карпатый показывал, какие нейроны на что триггерятся, там интересные картинки рисовались",
по вашему вопросу об интерпретации - можно использовать такую интересную штуку как attention,
"вот пример. это для speech recognition, когда алгоритм сам подбирает выравнивание.",
"а все-таки, чат, вопрос к знатокам - есть ли какой-то метод, который может научиться извлекать значимые фичи сам? Как в примере выше - мы знаем, что процентное количество букв GC является предиктором. Какой алгоритм, нейронная сеть или еще что, могла бы сама вывести функцию ""процент букв G и C в последовательности""?",
а как этот класс фич формально представить?,
"кстати, а process mining тут не зайдёт? по идее как раз по логам событий воссоздаём модельку, а дальше проверяем новое на ""compliance""",
"Я сама  разбивала последовательность на н-граммы и смотрела какие ""коррелируют"" с результатом, но если нужна связт между ними... или  о чем речь?",
"работать хз как будет, если честно",
но почему бы и нет))),
<@U0RV376MC>: а зачем в последней строчке модель вызывать как функцию? может `self.model.compile(...)`?,
<@U0QTS1LRF>: А как потом выход как theano-переменную получить?,
"немного покопал по поводу того, как вытаскивать слои в theano переменные:
```
from keras import backend as K
get_last_layer_output = K.function([model.layers[0].input],
                                   [model.layers[-1].get_output(train=False)])
```",
а у кого из событий на сентябрь уже есть программа?,
"```
Спикеры расскажут о применении таких технологий как deep learning, чатботы, умные дома и предприятия, интернет вещей, роботы
```
hype bingo?",
все как в прошлом году %),
"т.к. нахаляву, то зарегаться имеет смысл и пойти прицельно на интересные доклады, когда их объявят",
"<@U041P485A>: интересно, какие из перечисленных контор имеют отношение к перечисленным тобой темам",
"Есть же много книг, которые несколько переводчиков переводили независимо (и.е., допустим, три русскоязычных издания одной зарубежной книги). Как на базе этого сделать экстрактор стиля - это, конечно, вопрос тот еще. Какой-никакой, но корпус. :sobayed: ",
"Коллеги, принципиальный вопрос по tensor flow - как там с гибкостью и кастомизацией моделей? Как известно, основное ядро tf на C++ и cuDNN, на питоне лишь описание графа.  А вот если захочется написать свою передаточную фукнцию / лосс-функцию / изменить порядок гейтов в LSTM на свой безумный вкус и т.д.  Это можно  в принципе реализовать в TF или нужно ждать пока Гугл заимплементит и включит в очередной релиз ?",
"С гибкостью примерно как в Theano, можно знатно все переписать.",
Я torch-ок )) не знаю как в theano...,
"ребята, есть задача - распознать год записей разговор в колл-центре из голоса в текст - как это сделать? посоветуете?",
"посоны, а как в табле сделать простой джойн?",
"почему все создатели якобы ""простых визуальных"" средств изобретают супергеморройные средства для простых задач?",
и почему даже сложный документированный способ через EditRelationship не работает и даже не может сохранить настройки?,
"Я когда думал об этом, заметил что стоимость распознавания людьми не так уж высока",
"ладно, джойн я домучал, при загрузке мышой указал поля - чота произошло. Как теперь сделать Scatterplot?",
"<@U1BKR1ANA>: для днк последовательностей вот в этом направлении можно посмотреть в сторону таких вещей как DeepMotif и Deep GDashboard
<https://arxiv.org/abs/1605.01133>
<https://arxiv.org/abs/1608.03644v2>",
самый простой вопрос - как сделать unzoom?,
"а тот, кто DSW  организует сидит в слеке и специально в выходные ничего не проводит, чтобы не было пересечений?",
"кстати, есть же <#C0MQQT6E6> -- может там кто-то подскажет, кто это не читает",
"а кто посылает людей на DSW? я думал, они сами приходят",
"Всем привет, спасибо тем, кто откликнулся в личку на прошлое предложение по product matching. Есть вопрос по анализу текстов в нашу  платформу он-лайн обучения. Для создания персональной траектории обучения мы используем не длинный «учебник», а набор микрознаний. Мелкая фрагментация позволяет точнее строить персонализированный путь. Сейчас «учебник» разбиваем на кирпичики ручками и прописываем связи между ними. Вопрос - можно ли скормить некоему алгоритму текст «учебника», чтобы он определил в нем базовые элементы и описал связи между ними? Например, выделил глоссарий, основные формулы или события и связи между ними. <@U1XVC0TJS> присоединяйся к обсуждению",
"как вариант можно предложить разбить текст на сегменты по 200-500 слов, каждый сегмент представить в виде BOW, засунуть в LDA. Дальше получится, что один кластер - одна тематика/микроскилл в учебнике. Для кластера можно связывать по присутствию в них наиболее значимых слов из других кластеров",
"насколько я знаю, с выделением формул именно как формул всё плохо",
Как будто с выделением философских понятий дело обстоит проще.,
"у нас сейчас идёт проект, где мы занимается разбором структуры научных статей",
"что показывает разбор, как результат?",
"в первой версии у нас была смесь правил, оперирующих на семантических сетях (синтаксические деревья + некоторый вариант srl) + сегментация по схеме, как описал выше",
"то, что он использует для `more_like_this` запросов подойдет - я думаю там как раз tf-idf",
как получить vk token через терминал?,
more_like_this как раз то что тебе надо,
"Какая-то аналогия с cnn, когда учим модель выделять фичи, а потом на общую модель докручиваем сверху нужную функциональность.",
"<@U22KS1MK5> ответ, конечно, можно, но есть масса нюансов в том, как именно это сделать",
"Но при этом насколько мне известно всякие модели а-ля вариационный автокодировщик очень хреново работают на текстах, совсем не так, как мы ожидаем",
"При этом символы это дискретные величины, нельзя так просто по ним оптимизировать, как это делается с пикселями",
А зачем это делать напрямую через сеть?,
"а почему с word level плохо в языках с богатой морфологией? нельзя, например, отдельно лемму предсказывать, и отдельно форму (падеж и тд), ну и все это вместе оптимизировать? я все попробовать хотел, но никак руки не доходят)",
Какой метод (точнее библиотека) оптимизации параметров модели сейчас лучше всего работает? я слышал про bayesopt и spearmint,
"Привет всем! У меня вопрос по поиску строк. 
Нужно искать строку (наименование продукта без пробелов) в файле, файл может содержать ошибки т.к. он является результатом распознавания изображения. Проблема есть такая: ищем ""печеньеварианта"" и есть строки ""печеньевариантб"" и ""печеньевариаhта"". Т.е. первую нужно отсеять т.к. скорее другой продукт, а вторую принять т.к. скорее просто ошибка в наименовании.
Что посоветуете для такого поиска? Может есть у кого идеи или ссылки (если на питоне, то вообще здорово)  :slightly_smiling_face:",
"<@U0VQJFN81> ну как я понимаю, вы хотите ошибкам в визуально-похожих буквах давать меньшее расстояние (н и h намного ближе, чем а и б), чем ошибкам в непохожих буквах (вы ведь обрабатываете изображение), причем в таком контексте вам нужно анализировать именно замены, ну так напишите сами такую метрику",
"<@U1Z78RL3X>: вроде какой вес у  операции ""замена"" не возьми, строки из примера будут отличаться на этот вес как раз?
<@U0VQJFN81>: если идти от редакционных расстояний, то, наверное, нужно усложнять Левенштейна тем, что 1. стоимость замены конкретных букв друг на друга будет разная (вы же можете оценить вероятность того, что н распознается как h, и, например, н распознается как y) 2. стоимость операций в разных частях слова будет разная.",
<@U0KQ5M6KX> а как ошибки распознавания исправлять? свои правила написать? это здесь не главная разве проблема-то?,
Far0n на форуме писал что это примерно как сид поменять,
"На самом деле, когда на лб битва за тысячные аука - бывает влияет",
А какие годные варианты аугментации есть для текстов? ,
"Вопрос: есть ли библиотека или готовый способ в jupyter notebook построить график для нескольких линий, а потом на легенде выбирать в чекбоксах (или кликами просто) какие показываются а какие нет, что бы интерактивно можно было сравнивать подмножество линий? Вроде plotly умеет нечто подобно, а с помощью matplotlib или seaborn то можно сделать?",
"А мне кажется, или что-то тут не так? Вроде как GAN для генерации картинок приспособили, а тут про текст",
"вообще брать деньги с компаний чтобы они рассказывали другим компаниям и слушателям(из других компаний, лол) какие они крутые - нормальная тема",
"ну понимаю, с вендоров брать, а с докладчиков научного семинара то за что? или они это как публикацию в ВАКовском журнале засчитывают?",
"<https://labs.500px.com/> есть мысли, как они это делают? Например, как картинки индексируются?",
"вопрос в том, как они изображения заколотили в его индекс",
"интересно если фичи из LSH представить как ""слова"" в люсине, сработает?",
"Как можно максимально просто переделать форму входа в сетку? Был (a,b,c) нужно (a,1,b,c)",
"оптимизируемые параметры - разные, они включают как параметры сети типа размера внутреннего слоя, так и параметры обучения типа размера minibatch и learning rate",
"Это из области планирования эксперимента (просто тут эксперимент ""активный""). Делают в выбранной по каким то соображениям (если они есть) точку и вокруг неё считают ""полный план"" позволяющий вычислить градиент. Потом по максимуму (или минимуму) градиенту добавляют точку и переоценивают градиент повторяют пока не придут в оптимум. Никаких гарантий, что это глобальный оптимум естественно нет. Но если вычисления целевой функции очень дорогие, то и выхода особо нет.  Есть целый раздел про активные экспериментальные планы, в основном вокруг response surface optimization и experimental designs for computer experiments  можно в обзоре посмотреть  <https://cran.r-project.org/web/views/ExperimentalDesign.html>",
как раз эту бумажку читаю ),
"что делают с наблюдениями которые не попадают под sequence? Я хочу делать байесовскую ариму и допустим у меня спецификация (2 0 1) что делать с последними двумя наблюдениями, где лага нету? Аналогичный вопрос для порядка интегрирования, у меня хвост тоже пропадает",
и какой традиционный подход в моделировании сезонности?,
"Запостил сюда, думал, вдруг кто-то прочитал не как я)",
"<@U0AD1L5NC>: ребята предлагают не ждать, пока посчитается точный backprop, а тренировать параллельно на том же уровне сеточку, предсказывающую его. Соответственно, мы можем обновить веса слоя, не дожидаясь точного градиент, а потом, когда он придет, обновить апроксиматор. Также они делает задел на то, чтобы при forward pass, тоже не ждать данных с нижних слоев.",
"А какая архитектура сети хоть, которая предсказывает?",
"посоны, а чо делать, если фич гораздо больше, чем примеров? какие методы смогут с таким работать?",
"может первые наблюдения имелись в виду, там лага нет. Для моделирования берутся только те элементы, где все необходимые лаги есть,  train set получается короче, но ""выкинутые"" элементы таки учитываются как лаги для последующих взятых",
А какие модели модели можно дообучать на новых данных и насколько это эффективно?,
"Эффективность зависит то того, насколько новые данные отличаются от старых как мне кажется",
более интересен вопрос - из каких моделей можно удалить совсем старые данные без полного переобучения?,
"может я немного неправильно скажу, тогда поправишь
у меня есть данные, на которых я сильно переобучаюсь
я могу часть их выкинуть, тогда переобучение будет намного меньше и терпимое
но те данные, которые я выкинул, скорее всего несут в себе часть очень нужной инфы
вот я думаю как ее получить :slightly_smiling_face:",
"это если ты обучал на последних T временных интервалах, предсказывал на T+1. когда  пришло время T+1, ты можешь вручную забустить ошибку предыдущей модели на новых данных (на данных с T+1 для предсказания на T+2)",
"&lt;@U0AD1L5NC&gt;: да, градиент по активации помноженный на известные производные активации по весам.  Авторы не уточняют, как они инициализируются и какая архитектура.",
"Пусть Y={0;1} P(Y=1)=p. Если запусти случайный классификатор ""монетка"", какой будет pr auc? ",
А когда говорят про случайный классификатор у которого roc auc 0.5 то какой имеют ввиду?,
Но как это на pr auc переложить и посчитать у меня не выходит ,
А я как <@U07JH63AA> попался ) Ну можно же легко проверить,
Нагенерировать случайных чисел и посмотреть какая будет кривая,
"Кажется, что при рандомном классификаторе precision будет приблизительно равен p, в то время как recall будет пробегать от 0 до 1. Поэтому и площадь должна быть p. ",
"Ты сначала усреднил precision по реколу, а потом ещё раз усреднил по нему же) конечно так получится :) Надо честно понять как precion от recall будет зависеть и проинтегрировать. А для этого надо как-то внятно определить что такое случайный классификатор, а у меня не выходит ",
"Когда recall 1, то наверное precision p в среднем, а если меньше? То я уже не уверен. Может там быстрее падает ",
"Я ничего не усреднял :slightly_smiling_face:  
у тебя случайный классификатор это то же самое, что взять равномерно чиселки из (0,1) и присвоить объектам в виде вероятности принадлежать классу 1. 
Дальше, как считается PR curve: располагаешь все эти чиселки в порядке возрастания и начинаешь двигать по ним бегунок, начиная слева (или справа) и после прохождения каждой чиселки пересчитываешь таблицу TP, FP, FN, TN. Причем precision это всегда TP / (TP + FP) то есть это доля положительного класса из всех тех, которые  оказались справа  от бегунка. Если классификатор рандомный, то классы в этом ряду будут в среднем располагаться равномерно с частотой p (на p единиц будут приходиться 1-p нулей в случайном порядке), поэтому precision при любом заданном recall будет в среднем равняться p. Причем чем ближе к будет подходить бегунок к правому концу (читай – уменьшаться  TP и как следствие уменьшаться recall), тем больше будет будет разброс precision относительно своего матожидания p.",
"друзья, а экономическую статистику по Москве/Федерации вы где бы смотрели? росстат?",
"Если прогнозировать лог, что в большинстве случаев и правильно, на самом деле, то получается такая линейная модель: log(y) ~ sum(w_s * x_s) + w_t * x_t + sum(w_f * x_f) + e, где индексы s - про сезонность (x_s - категориальные переменные, например, по месяцам, чему угодно, можно мешать периоды, добавлять праздники, если данных много), t - тренд (x_t - какой-то скаляр), индексы f - про фичи модели, e - традиционная нормально распределённая ошибка. Можно фитить как обычную регрессию (только регуляризовать лучше сильнее, l1), можно и как байесовскую, с доверительными интервалами. Все преимущества простой линейной модели прилагаются",
"<@U0G29N5U4> в общем, если хочешь статью, нужно тогда сделать упор на полезный кейс и на то, как мы побили ариму и прочие стандартные подходы",
NASA открыла кучу данных. Каталог наборов данных и инструкции как пользоваться <https://data.nasa.gov/developer>,
"2. Иногда в декабре бывает четыре уикенда, а иногда -пять, и это даёт значительную разницу. Наверное, можно сделать это отдельной фичей, но тут допущение независимости сезонности от неё, как это получается в ARIMA, может быть слишком сильным. Возможно, они хитрее взаимодействуют, нелинейно.",
"Какой самый простой способ визуализировать данные из spark. Использую spark просто из idea на 1 машине. Локально. Хочется ничего особенно не ставить, просто добавить библиотеку и выводить графики. Даже тупо в виде генерации html + js + canvas. Есть такое? Или нужно не лениться и ставить zeppelin?",
"Привет! Такой вопрос. Сделали модель, прогнозирующую значение определенной величины. Модель предполагается использовать на разных датасетах, где порядок и распределение целевой переменной может отличаться. 
Вопрос -какую функцию ошибки нужно использовать, чтобы можно было в полевых условиях сравнивать хорошо ли спрогнозировала модель данные конкретный датасет или нет
 Ну т.е есть допустим два датасета, абсолютно отдинаковых, но во 2м целевые значения увеличены в 10 раз, MSE будет разным, но само качество прогноза нет, и просто сравнивать mse нельзя.
 Есть идея делить MSE на среднее целевой переменной, но есть сомнения.",
<@U2194SMBM>  а как она по английски называется - что гуглить?,
"еще как вариант, вычислять относительную ошибку по сравнению с более примитивным предсказанием, например просто тем же средним",
почему не подходит классический R^2?,
"Известно ли заранее, какой дотаяет подается и что внутри него y измеряется в одних и тех же единицах? Если да, то в чем сложность приводить к одной базе?",
Можно тогда (если все как в вашем пример) прост сделать регрессию одного таргета на другой,
"<@U0AS548A1>  единицы измерения одни.  Т.е грубо говоря есть лог посещений разных веб-сайтов разными пользователями. И на одном сайте люди допустим  заказывают в 1% случаев, а на другом сайте 5%.   Датасеты совсем разные но данные одинаковые. Мы сделали модель работающую на имеющихся у нас датасетах, но как убедиться что она хорошо обучилась на  датасетах заказчика",
"представь, что есть 2 типа клиентов: те кто покупают дорогие вещи, и те кто покупают дешевые. если ты можешь просто предсказать, клиент купил дорогую или дешевую вещь, у тебя высокий R2. если ты берешь подвыборку на один тип клиентов, R2 заметно падает, так как дисперсия целевой переменной стала меньше",
"Если фичи одинаковые и таргет тоже, то почему нет?",
"<@U1QN13664>: Привет! Можешь примерно рассказать какие задачи в adtech решаются с помощью мл? Думаю к рекламщикам на собеседование сходить, интересно что они там обычно делают",
"<@U0DA4J82H> смотря какие рекламщики, вообще -- рекомендации и А/Б тесты везде актуальны",
"<@U040HKJE7>: потестировал по твоему совету highsharts - оно падает вместе с браузером уже на несчастном миллионе точек, а нужно как минимум на порядок больше (а лучше на два)",
Склейка профилей - это когда пользователи с разных девайсов выходят? ,
"<@U0U2ENJ4U> а расскажи, как ты на этом быстрые тесты гоняешь?",
"типа, получают координаты где чувак был и на основе этого кластеризуют юзеров на ""любителей бухать"" и ""любителей шопинга"" и тп",
"А напомните, на какой архитектуре сейчас достигается SOTA в object localization? Раньше вроде это R-CNN был, а нынче?",
"ну вот я и ищу библиотеку, где сэкономили на чем-то другом :slightly_smiling_face:",
"И вообще,  как такие конторы конкурируют с гуглами и фейсбуками? У тех же данных намного больше ",
"Данные много у кого есть. У Яндекса все клики со всех ресурсов, где есть Я.Метрика. У Google - где Google.Analytics.",
"А какие вообще подходы к сегментации используются в случае таких бирж?

Кто то приходит (аналитик) и говорит:

 ""Теперь ищем юзеров, которые любят носки, вот вам выборка из таких людей о которых мы знаем что они 100% их любят, предскажите остальную пользовательскую базу""

Или какие-то более хитрые варианты?",
да даже если не крупный. Много кто же позволяет по мобилке рекламу давать,
но они прям чисто супервайзд на основании какого то вручную выбранного сегмента (а ля я вот решил что это полезный сегмент) ?,
"<@U041SH27M> не в нашем случае ты (рекламодатель) ставишь кусок жаваскрипта на сайте, так что мы знаем кто купил носки",
"кстати интересно, если делаешь какой-нибудь классификатор на несбалансированных классах, на данных dmp, кто-нибудь пробовал как семплинг меньшего класса попросить отгрузить у dmp то что они lookalike найдут у себя?",
"<@U1QN13664>: а данные доступны только на тех сайтах, где у вас сборка идет? Те нет какой-то биржы данных где можно как нить ими обменяться? Или из-за того, что они деанонимизированы - то это невозможно?",
"ну глобальной биржи данных нет, все как бы p2p матчат данные",
"а чужие cookie id не ловите им, как второй ключ чтобы опознать человека в случае чего",
<https://opendatascience.slack.com/archives/theory_and_practice/p1471890349002383> а какие фичи обычно используются для этого? ,
у меня просто чот не достаточно разнообразные получались,
"<@U1HHX1QS3> 
Многие браузеры(например safari под iOS) по умолчанию блокируют 3rd party cookies, усложняя тем самым жизнь всем, кто хочет отслеживать ваши перемещения по сайтам.
Тут описаны методики получения истории даже при отключенных 3rd party куках. Начиная от фингерпринтинга браузеров заканчивая использованием кэша браузера, E-Tag и т.д.",
"а посоветуйте какую-нибудь книгу по Bayesian, скорее очень высокоуровневую, о том как этим жить
прохожу курс на Coursera (<https://www.coursera.org/learn/bayesian-statistics>), задания решаю, а цельной картинки в голове не складывается",
"<@U0AD1L5NC> <@U07159GKF> по поводу статьи где градиенты предсказывают, сейчас почитал статью, там в Appendix указана архитектура, написано, что градиенты предсказывает линейная модель по активациям + опционально one-hot encoding для лейблов, для которых активация рассчитана",
"Ну в статье написано, что работает. Самый верный способ, конечно, проверить самому. Вот ещё что интересно, когда они тренируют свои модели для предсказания градиентов, то как таргет берут не точный градиент, а приближенный посчитанный с помощью моделей на впереди идущих  слоях сети и распространенный назад с помощью chain rule ",
<@U0FEJNBGQ>: вот это вообще выглядит как надо - поищу про неё подробнее,
"кто-нибудь знает, что такое part_size() и как он его проверяет? я что-то так и не нашла нигде в их коде, как определяется этот метод",
"Кстати, какую точность для cross-device linking  сейчас можно достичь? Насколько меньше 100%?",
"Сильно зависит от топологии сети. В банковской сфере, где у всех одинаковая техника и одинаковые браузеры, и всё это за NATом - в районе 0%.",
и dpi на самом деле очень мало у кого есть,
я в блоге DCA на хабре прочитал про 97% но там непонятно как они это делают,
"насколько я знаю, у акадо как раз нету dpi",
"Можно корень извлечь. Зачем такое надо -- не знаю, всякое бывает",
И стало сложно. А какие там альтернативные места предлагались? ,
но лично у меня больше отношение к этому как в лекциях у Воронцова,
ну тип как хорошие эвристики для оптимизации,
"Вопрос про классическую статистику.
Есть несколько многомерных временных рядов одинаковой природы (допустим, наблюдения за ёлочками: обхват ствола, высота и число иголок, измеренные на протяжении жизни ёлочки от проклёвывания до засыхания), то бишь y^i_t \in R^3. Есть знание (надежда), что показатели ёлочки удовлетворяют авторегрессионной модели первого порядка, то бишь y^i_t = A * y^i_{t-1} + b.
Как оценить A, если у нас есть только один такой многомерный ряд, понятно: VAR (<https://en.wikipedia.org/wiki/Vector_autoregression>) и вперёд. Как найти A, если у нас только два момента времени — тоже понятно: ставим всех в жирный столбик и строим multivariate multinomial regression данных в t2 на данные в t1.
А что делать, когда есть всё вместе? Наверняка кто-то что-то публиковал ещё в восьмидесятых, не видел ли это кто-нибудь из чатика?",
"Есть смутная фантазия о возможности прикрутить EM-алгоритм сюда, но непонятно, куда прикручивать.",
"из приколов. Недавно читал ""Ложная слепота"" Питера Уотса, узнал про такой способ визуализации данных как лица Чернова (Chernoff faces), начал гуглить и узнал, что даже есть пакет для R который позволяет это делать <http://flowingdata.com/2010/08/31/how-to-visualize-data-with-cartoonish-faces/>",
"По скорости примерно так же, как и раньше",
"так че, когда карточки скупать можно будет? :trollface:",
"возможно, вопрос совсем простой, но все же: нужен fuzzy string matching для наименований товаров. Одна часть - база продуктов (точнее записи из нее, остальная часть не известна), другая - результат OCR.  Пример: ищем ""печеньеварианта"", есть 
1) ""печеньевариантб"" - нужно игнорировать
2) ""лечен6еварuаhта"" - нужно найти
3) ""еченьевариа-та"" - нужно найти
4) ""печеньевариант"" - игнорировать

Можно ли это сделать без семантического анализа или наличия всей базы продуктов? Кроме того, еще не решил проблемы, специфичные для ошибки разбиения в OCR(""10"" = ""Ю"", ""11"" = ""П"" и тд). Предлагали сделать свою метрику сходства на основе Левенштейна, где веса для похожих букв меньше. В таком случае 2 и вправду найдется, а 1 проигнорируется. А как сделать, чтобы все 4 варианта работали придумать не могу. Хотелось бы при 100% точности добиться наибольшей полноты",
"<@U0AD1L5NC>: так как в оффлайне можно взять только часть картин из изначального множества, это через forward-only подход делается, мне кажется. Просто для части картин сеть не обучили ещё",
"Никто не знает как в <http://plot.ly|plot.ly> offline режиме в jupyter notebook  python для выпадающего меню сделать так, чтобы отображалось больше 10 вариантов. Мне надо 30, но никакого скролла не появляется, опций для этого не нашел. В примерах есть вариант со многими странами, но там на js, а не python",
я чот увидел классический трейдофф точности и полноты,
он бы какие roc кривые нарисовал,
"<@U0FEJNBGQ>: ты вот смеешься про муму и грабли, а я недавно был на экскурсии в столовой храма христа спасителя (которая трапезная). В интернетах про нее давно как-то писали, мол, пирожки по 17 рублей, салаты по 30, котлеты по 60 и все в таком духе. Решил с друзьями узнать, как такое возможно вообще. В итоге - обед из двух блюд (салат, горячее, компот и кофе) обошелся мне в 500 рублей. Вот это да, хардкор.",
"а что ты там делал? ты не кликик вроде, и не депутат, кто же ты тогда?",
"клирик, маг, воин, депутат, какой там еще класс остался)))",
Добрый день! Кто нить знает готовый датасет и сетку для анализа букв и цифр из шрифтов? Нужно для небольшого проекта по распознаванию автомобильных номеров,
"Но мы ведь на что-то надеемся, когда нейросети тренируем",
к чему и как всякие градиентные бустинги сходятся - вообще наркоманский полет фантазии (учитывая что это все якобы в функциональном пространстве происходит),
"""доказательство сходимости"" так ведь в локальный минимум очевидно что сойдется, куда ж ему деться",
"ну бывают такие, кто в седле застревают",
"Автор vw говорит что на 0.6% его модель хуже работает. Так что почему бы сразу не брать fastText. Ставиться легко, работает быстро. В добавок там еще новый алгоритм построения векторов слов, что-то типа улучшенного word2vec.",
"ну да, я пару его комментариев почитал только, он вроде как в строну примитивности клонит",
"человек 76 года, природа своё берёт, после 40 я бы сказал что очень мало кто способен",
"Народ, кто сможет помочь с theano?:pray: Я сделал проблему воспроизводимой, не считается градиент, сообщения странные. Конечная цель сделать вариационную ARIMA",
"Вопрос по cuda. Меня интересует, можно ли как-нибудь передавать в device многомерные динамические массивы, организованные с помощью указателей, кроме как с помощью костылей вида [x + y*DimX + z*DimX*DimY + …]. Я попробовал выделить память под “корневой” указатель с помощью cudaMalloc, а дальше в отдельной global функции через malloc раскидать оставшиеся уровни, но что-то у меня с копированием не получилось",
и почему для стохастического этого не хватает,
"Ребята, такой вопрос. Наверняка обсуждали уже, но я тут недавно..

Пусть дано множество реализаций некоторого процесса с несколькими выходами (например,  пользователи, представленные статистикой просмотров страниц сайта по дням). Как бы их визуализировать, сохранив схожесть? Типа tSNE для временных рядов с несколькими переменными.",
"<@U2194SMBM> бессовестный самопиар <http://mabrek.github.io/blog/multivariate-mds-tsne> там ссылка на демо в конце есть, как раз tsne для временных рядов",
"цель - примерно такая же, как и при обычной визуализации объектов с помощью tsne - оценить наличие групп, вариативность",
первое пришедшее в голову - это представить обе пачки как матрицы и  frobenius norm,
"Нет сомнений в том, что это работает, вопрос в том, как показать строго математически",
"плохо представляю себе, как все это работает, но есть возможность заюзать кпу для отрисовки?",
"проверь, какое значение у тебя на первой итерации",
не найду где сид прокидывается,
надо какой нить более адекватный приор чем -.9 - .9,
"Кто-нибудь знает, где можно найти данные о зарегистрированных торговых марках в США? Нашел вот такое: <https://bulkdata.uspto.gov/data3/trademark/casefile/economics/2015/>
Но никак не могу разобраться, где же сами торговые марки",
как такие вещи нормализовать бы?,
а почему не nltk или sklearn?),
"а бинг кстати не банит (не банил, когда мы его краулили пару лет назад), его можно и в наглую юзать)",
"<@U24GRPP41> автор группы, расскажи зачем рунету еще одна группа по дл?",
"sovcharenko [10:10 PM]  
А что побудило создать группу? :slightly_smiling_face: есть же <https://vk.com/deeplearning>

rust.salavatov [10:24 PM]  
<@U041P485A> да много информации копилось, собиралась в своих архивах... как и у многих думаю, решил сделать группу чтоб для себя и друзей выкладывать. Не обязательно новости последнии. Начал посты делать, тут как раз вами указанная группа подхватила и начала репосты из моей делать... И получается что сейчас я просто их дополняю, они интересные моменты к себе репостят а я не дублирую если у них это уже есть. Упор делают на свою область (картинки, видео) и стараюсь описывать посты и переводить на русский описание. Так что указанной вами группы для новостей достаточно в ВК) у меня просто форк как бы)",
было бы круто какой структурированный аггрегатор всячины запилить,
А вы с какой целью интересуетесь?,
"Это хочу посвятить этому свой диплом. А сейчас хочу узнать что на переднем краю происходит, чем пользуются на практике, и какие business value получают от результатов.",
даже не представляю какая в этом business value,
"<@U1HHX1QS3> если приложение промышленное, то очень конкретная. есть например газовая турбина, утыканная сенсорами. надо найти ситуации, когда с сенсорами начинается хрень. потом еще надо определить, это с сенсором хрень, или с турбиной",
"А зачем столько групп, если там все равно 70+% контента -- это перепосты с рэддита?",
"на переднем краю чего? если около инженерных вещей, то там всякое, немного простых  методов ml досыпают. но в общем ничего страшного. Если на переднем краю науки, т.е. как задачи слупов максимизации стохастического функционала, то там страшное)) Но на самом деле кажется от статистики Ширяева-Робертса не далеко уехали",
"общая концепция - восстановить модель сигнала и смотреть на отклонения от нее. как восстанавливать - задача творческая, на мейлру митапе игрушечные данные вообще lstm пытались восстановить :trollface:

как смотреть отклонения - тоже задача дизайнерская. можно делать чисто в таком, фильтрационном подходе. можно лайклихуды смотреть, наверняка есть какието гибридные варианты",
но когда группы плодятся как грибы то как то не удобно совсем,
"когда данные не размечены, как происходит в большинстве задач с аномалиями,  ты сам выбираешь сколько аномалий себе отсыпать",
"да я понял, прост фраза выглядела как моделировать отдельно ошибку прогноза модели",
"есть ограниченное, но какоето количество статей про то, что ""давайте посчитаем скользящую дисперсию ряда и зафитим модель на нее, как обычно"". а вот статей как <http://arxiv.org/pdf/1201.5786.pdf> с честным совместным моделированием - почти нет",
А как из нее сэмплить параметры модели? или там немного иначе?,
"&gt; А зачем столько групп, если там все равно 70+% контента -- это перепосты с рэддита?
И вообще arxiv-sanity есть",
"много факторов. кому-то лень каждый день эти чужеземные реддит/твиттер/arxiv/gitxiv открывать и высматривать есть ли там новые материалы, при том, что они не всегда пересекаются, их нужно выбирать среди кучи вопросов, и тд. кто-то вообще читает только  то, что в ленту попадает,  т.е. сами ничего не ищут, а тут в привычный контактик упало, почему бы не открыть, даже если и на английском. одна подписчица предложила в канал телеграма постить, ей так удобнее, я ей предложил создать такой канал, теперь это все еще и в телеграм сливается.
я и не собирался этим специально заниматься, группу создал потому что я на работе так прокрастинирую - читаю рандомные посты, так что решил заодно и кидать в вк на случай если найти снова что-то нужно или кому-то это тоже интересно будет. при этом времени или желания все это дело как-то консолидировать, комментировать или переводить нет (хотя материалы на русском обычно набирают много лайков, так же вводные материалы для начинающих), т.е. я зашел в группу с утра, набросал в очередь вещей из инета, которые показались интересны, после обеда повторил и забыл до след утра. я бы вообще давно забил на это дело, мне кажется, но люди резко начали подписываться - видимо, им это удобно.  в итоге мне хлопот немного, а все сохраняется, доступно через поиск, число подписчиков растет, еще и благодарности время от времени приходят.",
"у меня была похожая идея, чтобы помимо предсказаний какую-нибудь ещё полезную информацию считать
которая потом пойдёт как мета-фичи для стакинга",
"<@U06J1LG1M> думаю за меня уже ответил <@U0952QCHE> . Согласен что нужен хороший единый агрегатор и единая удобная группа/канал информации. Но чем больше мелких нишевых  так по мне удобнее. Каждая новая группа это по сути 1 новый редактор для отбора информации. Чем больше групп тем больше редакторов... Тех кто ищет для вас каждый день что-то интересное (время иногда надо приличное). Вы вправе не подписываться. Тем более вконтакте сейчас удобная разбивка новостей по вкладкам. Я сделал себе вкладку с десятком групп по DL и листаю как одну... не вижу проблем (репосты не мешают, они скрываются автоматически)",
"Ну да, но меня как аудиторию это все не трогает никак",
"я проглядев до картинки дерева, увидел там в каждом листе было несколько плотностей. я все думал, как бы объяснить такую простую вещь - вот есть обычное дерево-пень, допустим, эквивалентное такому правилу: 
if x &lt;5 
then y = 1
else y = -1

все что надо расширить в сознании, это сделать так:
if x &lt;5 
then y = 1, z = 2
else y = -1, z = 4",
"у HDP есть гиперпараметр, который отвечает за то, как много асимпотитически хочется топиков",
"<@U040HKJE7>: как вы строите ансамбли? Делаете сначала распределение разбиения, а потом сэмплите оттуда новые и разбиваете?",
"господа, а кто разбирался с Glove (<http://nlp.stanford.edu/pubs/glove.pdf>), может подскажете по деталям? Никак не доходит зачем в уравнении (6) добавлять bias для контекстного слова чтобы восстановить симметрию при перемене контекстных и обычных слов (bias для обычных слов четко обоснован тем, что он заменяет log(X_i) в правой части (6), а вот про второй сказано ""а добавим ка для симметрии"")",
правда как бы MIT Press на такое не обиделись,
"итак:
1) выбираем рандомную фичу `a`
2) накладываем приор gamma(1,1)
3) считаем наилучшее разбиение как обычно
4) смотрим какая это квантиль пусть это будет наше эмпирическое `p`
5) строим постериор для этого `p` зная то, насколько хорошо он разбивает бинарные классы
6) сэмплим одно разбиение из постериора, двигается вниз
7) шаг 1",
"Тупой вопрос конечно, но все же, а какой профит? Я не до конца всю конструкцию осознаю ",
Ну т.е. для RVM я понимаю зачем так делают ,
я не понял как /:,
"тоесть деревья глубины 20 потребуют огромных датасетов чтобы обучиться чемту разумному. с другой стороны, кто сказал что нужны именно деревья?)",
"но в твоем я все равно не понял, а какой критерий останова этой процедуры?",
"Настройка и прочее как и везде, походу можно затестить изменив метод получения разбиения",
"ну это следующее: 
Каждое построение xgboost-а это будет сэмпл модели из этого семейства, так как разбиение в каждом ноде сэмплится. Поскольку бустеры это какие-то случайные модели, можно не сильно париться и не тренировать ооочень очень долго, гоняясь за тысячными, а останавливаться раньше, делая бустер вычислительно позволительным. Точность и обобщающую способность мы будем получать другим способом, а именно, усреднять по моделям. Таким обрадом можно один раз подобрать приемлемые параметры для бустера в детерминистской постановке, исходя из мысли что нам надо быстрое обучение и годненькое качество, сэмплить модели из семейства с этими параметрами. Кажется сложно написано(",
"Ребят, а кто нибудь занимался поиском сходных по изображению(стилю?) предметов одежды?
В общем история такая, cмотрел вчера футболки в одном из интернет магазинов и решил  just for fun запилить прототип поиска по стилю. Уж очень его там не хватало)))
Скачал ~11000 картинок.
Запихнул их в vgg16 с убранным последним softmax слоем в keras.
Получил для каждой футболки по 4096 фич. Ищу максимльно похожие по  ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’ расстояниям.
Да, набросал вебморду чтоб удонее было тестить - <http://imgrec.ddns.net/> (по умолчанию показывается рандомная картинка и по 5 близких к ней для каждого расстояния)
Результаты вроде не плохие))
Дальше хочу попробовать убрать 2 последних слоя из vgg16 вместо одного и сравнить результаты.

Как вы думаете, какие ещё подходы можно использовать для данной задачи? Что/где посоветуете посмотреть/почитать?
Вроде <@U06J1LG1M> писал в slack что-то по этому поводу? Найти не смог.",
"они взвешивают деревья в зависимости от того как вероятен был сэмпл такого набора данных, а я хочу делать стохастическое разбиение на каждом ноде, чтобы даже для одного набора данных у нас был набор разных деревьев, можно запилить еще и их идею тоже",
:disappointed: я как у них понимаю идею,
когда там pgm уже стартует /:,
"<@U0G29N5U4> тебя в письме когда звали ментором, не говорили когда старт?",
"смотри: у меня следующая процедура…
Для простоты это будет задача бинарной классификации
Настраиваем параметры обучения как обычно, они не отличаются: глубина сколько деревьев и тд, это не главное
Главное состоит в том, как происходит разбиение в каждом ноде, когда мы строим само дерево.
Если бы мы делали как обычно, мы бы находили точку, которая минимизирует энтропию после разбиения и разбивали бы по ней. 
Я хочу иначе: 
Взвесим точки возможных разбиений обратно пропорционально получаемой энтропии после разбиения, если у нас есть деление на ноль, то просто выбираем эту точку, где энтропия ноль(можно выбрать другую процедуру). Отнормируем наши веса так, чтобы в сумме они давали единицу: это будут вероятности для каждой точки разбиения. Из получившегося распределения сэмплим точку разбиения и говорим: «вот она». Двигаемся дальше рекурсивно.",
по какой из фичей разбивать выбирается по результатам семплирования?,
осталось придумать как извлечь профит),
"правда это менее привлекательно, так как не параллелится",
"<@U1CE1A01J> я советую изучить как  работает артистик стайл, понять что за фичи извлекаются для style loss, затем можно делать как ты уже делаешь - т.е. вытягивать например 1000 семантически близких объектов к целевой картинке используя fc6, затем сортировать их по стайл фичам; но есть нюанс, нужно вычислять фичи только от сегментированного объекта, самый простой способ получить грубую сегментацию - это взять выход pool5 слоя, получается тензор height X width X depth, и берем усредняем по depth, получается маска width X height  и экстраполируем до оригинального размера картинки, ее по трешхолду превращаем в бинарную маску, и из этого уже извлекаем фичи стайла",
"я понимаю, что эти два понятния имеют разное определение, но есть ли примеры, где две этих величины существенно отличаются (при условии, что мы ""умно"" подбираем модель)?",
дизайн - как у однодневного лендинга :slightly_smiling_face:,
"как минимум то, что любой интернет-юзер такое инстинктивно закрывает, еще до того, как название прочитает",
"Поймал себя на мысли, что байес это как наркотик. У кого нибудь что-то похожее было?",
А как вербовать непосвященных? Когда пытаешься объяснить фриквентисту что-то.,
"Кстати, недавно думал: А почему бы не разукрасить машину в Razzle Dazzle? Чтобы камеры не детектили.",
да чот не думаю что наше законодательство поспевает за такими деталями,
а что где то незаконно?,
"Даже если будут детектироваться, то по могут быть проблемы с тем, чтобы определить куда машина едет или планирует ехать. Ну, это если по задумке Duzzle",
а где нибудь минимизировали байесовский риск для выбора предикта?,
"еще как вариант алгоритмы типа метафон, хотя это не совсем тот usecase",
"лучше не вербовать. если умный, то априорная вероятность, что сам завербуется, достаточно высока. если глупый - наоборот, слишком низкая, да и зачем они нам нужны?",
"Если бы можно было доказать ""правильность"" байесовского подхода, то тогда понятно, но вроде как нельзя, более того, у него есть масса недостатков.",
"Я сам лично ни разу пока не видел, где бы байесовский подход мог сделать хуже, но есть вопросы, ответы на которые он пока дать не может",
"<@U13E1AWCX>: ну, это как бы шутка у меня была...",
"Вопрос: не знаете ли вы какие либо материалы (блогпосты, статьи stackoverflow) где есть разбор сгенерированного lasagne (theano) C++ кода и попыток его интерфейсить с другим C++ кодом? В контексте CNN-ок",
"посоветуйте что-то способное к RT Map/reduce, объем данных не очень большой около 2k событий в секунду, желательно  что-то стабильное)).  как минимум  нужно фильтровать неуникальные события (когда пользователь посетил страницу более одного раза).",
"пользовался storm до этого, но не как map/reduce",
"с другой стороны, для поставленной задачи (дедупликация), хватит пары экранов кода на Java/Scala без всяких frameworks и Redis как промежуточное хранилище (класть в него текущее окно и искать дубликаты)",
сейчас 3 уже как бы,
вопрос в догонку: а кто имеет опыт работы c clickhouse может даже девелоперы сей штуки есть? насколько она стабильна?,
<@U1G303UTW>  а какой объем данных?,
"<@U1G303UTW> это понятно, поэтому и интересно. JOIN на столбцах операция дорогая, в хане для этого даже бест-практисы есть, где юнионами он эмулируется, но интересно сколько работает нативно",
"Хотелось получить один интерфейс где бы можно было в C++ передовать forward pass, и получать backward в зависимости от условий в виде вектора float-ов, потом закинуть это все в петлю для batch-ей.",
"<@U07V1URT9> скажи, какая информация тебе нужна -- я здесь напишу.",
"про то, как использовать сгенерированный theano C++ код, знает, быть может, только автор того PR, а как вызвать C++ из Python - документации куча",
"Подскажите, пожалуйста, существует ли алгоритм кластеризации, где я могу задать свою метрику и максимальное расстояние внутри кластера?",
<@U0U2ENJ4U>: 1.7 млн точек — закрытия минутных баров по fRTS за 2006-2015? А зачем выводить все это разом на график?,
"Коллеги, есть востребованная задача, возможно кто-то уже решал, поделитесь пожалуйста.

Имеются данные о составе слов корпусов в момент времени t1-tn (есть мера изменения состава  Y2-Yn соотвественно). С какой ошибкой мы можем утверждать о составе слов в корпусе в момент времени tn+1. 
Есть ли примеры решения данной задачи?

Для наглядности, приведу практически пример . 
Выбираем книги 1,2,3,4 по какому то алгоритму, связанному с их содержанием (пока просто слова), можем ли мы сказать, какие слова будут в книге 5, а каких не будет, а в 10-ой книге?",
"почему нельзя для каждого слова предиктить как time-series? или требуется смыслы находить, типа сюжет, все дела?",
"смысл слов пока оставим в стороне, вопрос лишь о том, можем ли мы сказать какие слова будут в принципе, а каких не будет (в общем виде  O[a&lt;p2(x1)&lt;b]&lt;e   о- оценка ошибки, е - значение ошибки при прогнозе вероятности слова в момент 2 при знании параметров х слова в момент 1.  p2(х1) - искомая функция.",
"так задачу можно попробовать решить. 

я бы разделил слова на 3 большие группы:
-общие слова, которые точно будут
-стилистические слова, которые както привязываются к автору
-тематические слова. тоесть книга про биологию не будет иметь (много) слов из астрофизики, а книжка про покемонов будет аналогично будет содержать мало взрослой лексики

надо понимать, что общие слова будут предсказываться очень хорошо, стилистические слова - AR-моделью (тот же автор пишет также), а тематические - не уверен что вообще будут предсказываться (для этого надо знать про что конкретно следующая книга). например, слова из биология для 7 класса и биология для 9 класса не будут сильно пересекаться, так как тема одна, но слова другие",
"Подход понятен, можно говорить о 3-ех оценках допустимой ошибки будет, для разных групп слов.  Как слова разделять не ясно, не руками ведь.",
тип того. можно чисто co-occurence. я делаю TFIDF матрицу и множу саму на себ транспонированную. выкидываю мелкие значения в полученной матрице и работаю с этим как с графом,
А почему ты предпочитаешь именно такой способ? Привычка? Или для каких-то задач этот способ работает лучше? ,
Как п3 может навредить коленям? Можно случайно много всего выкинуть? ,
"не совсем уверен, что вопрос сюда, но вроде это самый близкий по смыслу канал

кто-нибудь может подсказать, как писать статьи (научные) по результатам хакатона? может быть, есть какие-то примеры? мне известна только статья от 5vision <https://arxiv.org/abs/1512.01693>
есть ли ещё примеры работ?

хочется в каком-то виде описать наш результат с последнего DeepHack",
"Имхо, также как и любые другие научные статьи.",
"раздуется, но хорошая реализация ее куда надо правильно сложит. а вот попробуешь по ней apply прогнать... и все, пиздец",
"коллеги, допустим, у меня есть художественное произведение
кб этак на 500 минимум
как из него можно выдрать  фразы прямой речи с  маркером авторства или того, кому обращена фраза?
ну или хотя бы кейсы разговоров, с указанием участников?

идея абстрактная, просто фантазия в голову стукнула
но если есть уже работающие методы или идеи - поковырялся бы",
А вот как данным с телескопов РАН получить доступ? Я бы хотел посмотреть вот на этот вот <http://observer.com/2016/08/not-a-drill-seti-is-investigating-a-possible-extraterrestrial-signal-from-deep-space/> сигнал,
"<@U0H7VBQQ1> чуть-чуть обсуждали это на завтраке. Вот есть автоэнкодер, на внутреннем слое он вроде как должен отображать координаты из входного пространства во внутренние координаты на каком-то манифолде, а сами эти внутренние многообразия кто-нибудь исследовал с  точки зрения какой-то геометрии? есть что почитать любопытного на тему?",
"хммм. у меня другой корпус, там тексты поменьше и временами сильно поменьше. но сам корпус побольше.
а так - как я и предполагал, и чего не очень хотел - ручная разметка да позиционирование по строкам
но как пример и направление - очень хорошо.
спасибо большое!",
"&gt; Проблема осталась такая:
&gt; соотнести ""АМW П/Э ПАКЕТ БИС"" и ""amw п/з пакет био"".
&gt; соотнести ""j1Р VАLIО ТИЛЬЗИТ 45% НАР 1"" и ""сыр valio тильзит 45% нар 1""
&gt; Чтобы такие примеры находились нужно занижать порог, что повлечет число ложных срабатываний.
А в чем проблема-то собственно? Эти строки отличаются на 2 символа. Зачем тут занижать порог? И почему тут триграммы не работают?",
"Такие пространства очень трудно исследовать в статике математическими методами, но их, как ни странно можно увидеть и составить какое-то о них представление в динамике. Я как-то описывал на хабре похожую задачку, про то как увидеть процесс обучения нейросети и представить себе географию фитнесс-функции. Тоже дофига многомерная, по измерению на каждый синапс. Но если каждому синапсу приписать призвольный единичный 2d вектор и показывать как ведёт себя их сумма (произвольня проекция из дофигамерного пространства на 2d) получается довольно интуитивно понятная картинка.",
"ИМХО, очень много что достойно переизобретения чуть-чуть не так как оно было изобретено в прошлый раз. А для этого нужно заглянуть под капот.",
"Можно на своем примере показать, какие были идеальные наименования и как к ним пришли?",
было бы интересно взглянуть как это было реализовано,
а какая тогда целевая функция оптимизировалась?,
и какие преобразования выполнялись на каждом шаге оптимизации,
"если оно конечно работает, как должно",
"Вообще, я видел на ютубе (откуда про эти зионы узнал) как раз такие конфигурации народ собирает",
"Народ, а кто нибудь для работы использует/использовал внешние видео карты, и если да, то есть ли какие нибудь подводные камни?",
Ну как бы без GPU особо толкового ничего и не сделаешь,
"Народ, а кто нибудь для работы использует/использовал внешние видео карты, и если да, то есть ли какие нибудь подводные камни?",
"Народ, подскажите как лучше  решить такую задачу:
1)Есть фотографии для которых размечено много аттрибутов (может быть несколько аттрибутов для каждой. соответственно есть/нет)
2)Так получается, что уверенным можно быть только в тех аттрибутах, для которых стоит ""есть"". То есть если аттрибут проставлен, то он есть, а если нет, то нельзя точно сказать, есть он или нет. При этом есть подозрение, что количество фотографий на которых фактический есть аттрибут, но он не проставлен гораздо больше, чем тех, где он есть и проставлен.
3)Есть подозрение, что если делать multi-label classification, то из-за 2) получится, что сетке будет выгодно говорить, что аттрибута нет

Соответственно, у меня пока в голове что-то вроде semi-supervized multi-label classification на сверточных сетках, но я пока не понял как это реализовать нормально..",
"наверное, имеет смысл распространить метки по некоторой метрике схожести, например как в <http://www-connex.lip6.fr/~gallinar/Enseignement/2009-Papiers-ARI/MM2008-cao.pdf>",
"Народ, подскажите как лучше  решить такую задачу:
1)Есть фотографии для которых размечено много аттрибутов (может быть несколько аттрибутов для каждой. соответственно есть/нет)
2)Так получается, что уверенным можно быть только в тех аттрибутах, для которых стоит ""есть"". То есть если аттрибут проставлен, то он есть, а если нет, то нельзя точно сказать, есть он или нет. При этом есть подозрение, что количество фотографий на которых фактический есть аттрибут, но он не проставлен гораздо больше, чем тех, где он есть и проставлен.
3)Есть подозрение, что если делать multi-label classification, то из-за 2) получится, что сетке будет выгодно говорить, что аттрибута нет

Соответственно, у меня пока в голове что-то вроде semi-supervized multi-label classification на сверточных сетках, но я пока не понял как это реализовать нормально..
немного статей... :disappointed: <http://www.ijcai.org/Proceedings/15/Papers/570.pdf>
<http://jmlr.org/proceedings/papers/v29/Wu13.pdf>
кода не нашел",
Нескромный вопрос. А почему это решили устраивать в будний день в рабочее время?),
"<@U0K61TDUK>: начало рабочего дня у тех кто ходит на завтраки смещено, могут себе позволить иногда на работу в 11-12 прийти.",
"оно же не отнимает пропускную способность, когда не используется",
"Вопрос из статистики c элементами R.
Заеб**ся ломать голову, подскажите, пожалуйста.

Итак, shapiro.test , Нулевая гипотеза, что распределение нормальное. Если получаем p-value &gt; 0.05, то отклоняем Н0 и принимаем Н1— распределение отличается от нормального.

bartlett.test, Н0 опять звучит как “гомогенность дисперсий в выборках соблюдается”? Т.е., если p-value &lt; 0.05, у нас нет оснований отклонить Н0. Так? Или в этом тесте Н0 звучит как “гомогенность дисперсий в выборках не соблюдается”?

Нулевая гипотеза звучит по-разному для разных тестов? Или есть единое правило нулевой гиппотезы?

2 примера выше из курса по Статистике в R на stepic. И формулировка Н0 капец как различается.

Есть ещэ t.test, где Н0 гласит— значимых различий нет.

Спасибо.",
Такое только когда <@U04URBM8V> не приходит,
А почему в превью stepiK? Всегда же stepic’ом был,
"А ведь я отчетливо помню, как еще в первом курсе по статистике Анатолий Карпов говорил о том, что, если p-value меньше 0.05, то у нас *нет оснований отклонить Н0*, потому что “типа, слишком низкая вероятность встретить такое или еще более выраженное отклонение”, иными словами, — отклонений нет и Н0 звучит— средние равны.",
"Это примерно как советовать разобраться в принципах работы двигателя внутреннего сгорания человеку, который хочет научиться водить автомобиль.",
<@U0JHK9001>: а в каком модуле?,
"если на уровне математических абстракций ещё не понял как он работает, то лучше не начинать с кода",
"да, будет не так быстро, как xgboost и h2o. зато кастомизация абсолютная",
"<@U040HKJE7> напомни, почему мы сейчас не стали сами писать бустинг?",
"Мне еще Вольфрам Альфа помогает в таких случаях, когда знаний матана не хватает.",
"<@U0TUTASNR> в общем виде когда все что угодно может быть пропущено, задача, конечно, нерешаемая",
"<@U0AD1L5NC>  я вроде как придумал что делать. Там предположительные пропорции примерно такие : примерно 20% на каждый аттрибут заполнены, остальные 80% записаны в негативный класс, вес которого раз в 100 больше. Получается, что если взять сбалансированный сэмпл : все элементв позитивного класса и в негативном оставить 1% заполненным, а остальное как пропущенное, то в случайном подсэмпле заполненного негативного в основном будут как раз значения аттрибутов отрицательные. Можно будет обучить определять аттрибут тупо по всему позитивному и 1% от негативного, такого же по размеру. Но у меня этих аттрибутов больше сотни... то есть это мультилэйбл однозначно, но с кучей неразмеченных данных на каждый аттрибут. По одному аттрибуту обучать не хватит данных, а иначе непончтно что делать с неразмеченными. Думал написать свою loss function, но ощущение, что перемудрю..",
"<@U0AD1L5NC>  а по идее loss function должна быть такая, что если в векторе из y_true на i-ом месте None, то не важно какой y_predict, 1 или -1 все равно по i разница будет ноль.  Тогда градиент будет считаться от противного, то есть от ошибки первого или второго рода. Может даже сходиться будет :slightly_smiling_face:",
"<@U0TUTASNR> ```y_pred = tanh(NN(x))``` ```U(y_true, y_pred) = &lt;y_true, y_pred&gt; -&gt; max``` где `y_true ∈ {-1, 0, 1}` и `&lt;⋅,⋅&gt;` скалярное произведение, правильно?",
Кто какими штуками пользуется для систематизации работы/чтения кучи чужых статеечек?  Помню не давно в чате проскальзовало что-то про какой-то комбайн где и интеграция с github есть и еще чего-то но не могу найти,
"но я скорее больше про личный. ну тип какая-то хреновина где пишешь ссылку на статьи, свои комменты какие-то выкладки к ним  и подвязываешь папку/гитхаб свою которая имеет к этим статья отношение",
"Я сталкивался с подобной задачей - автоматическое теггирование изображений. Т.к. на каждое изображение могут быть десятки тегов проставлены, а самих тегов тысячи, то люди размечают изображения очень по разному. Поэтому тоже возникала проблема с пропущенными тегами. Лучше всего сработал простой способ с сигмоидой и бинарной кроссэнтропией. Еще очень хороший результат получился с помощью идеи из этой статьи Information-theoretical label embeddings for large-scale image classification(<http://arxiv.org/abs/1607.05691>). Если кратко, то в статье предлагается использовать матрицу pointwise mutual information и SVD для перевода вектора тегов в сжатое представление, а при обучении использовать cosine proximity как функцию потерь.",
Кто нибудь смог достать Megaface датасет?,
Но как же gan для генерации текстов?  ,
Кто то делал распознавание номерных знаков автомобилей? Посоветуйте современные методы/системи.,
"Да, загрузился. Посмотрел на описание данных. Не представляю что употребляли товарищи, сделано как чужими для хищников.",
"<https://opendatascience.slack.com/archives/_meetings/p1472564669000049>

<@U04423D74>: Кстати, я регулярно по субботам выбираюсь из дома куда-нибудь завтракать, часов в 9 утра обычно, когда ни машин, ни людей, ни пробок. Сесть где-нибудь на веранде на Арбате. Или поехать на Даниловский рынок, к примеру. Атмосфера в городе потрясающая, и располагает к приятному времяпрепровождению намного больше, чем пыльная суетная среда посреди недели. Я не агитирую за перенос ни в коем случае, но мне надо было поделиться этим наблюдением.",
"<@U09JEC7V0>: пятничный ужин с крафтовым пивом (а у кого и не пивом, а даже чем-то покрепче) и ранний субботний завтрак с китайским чаем это все равно что карамель ""Взлетная"" по сравнению со швейцарским шоколадом :slightly_smiling_face:",
"<@U0JKYTE4B> я с тобой полностью согласен, но получается, как получается :facepalm:",
"<@U07V1URT9> <@U0AD1L5NC> <@U0ZJV6E5Q>  В случай с сигмоидой и бинарной кроссэнтропией не до конца понятно, почему он будет работать на плохо заполненных данных. У меня, скажем, на 500000 фотках машина отмечена только на 5000, но реально присутствует на 20000.  Если  я не буду брать подсэмплы из негативного класса, то за одну эпоху ошибочный вклад в градиентный спуск от тех 15000 тысяч присутствующих, но не размеченных машин, разве не сведет все попытки предсказать наличие машины на нет? То есть вроде как все равно надо делать с подсэмплами.. Или я не понимаю :slightly_smiling_face: Неужели просто сигмоида и кроссэнтропия с кучей выходов зарешают эту проблему?...",
"Я всё думаю как бы его хитро с Менделеем скрестить (благо тот умеет подключаться к sqlite Зотеро), но пока не пришел к рабочему решению.",
"Мое предположение,  что семплы, которые помечены негативным классом, но им не являются, вносят дополнительную регурялизацию. Вот например в статье Dropout Training as Adaptive Regularization авторы используют неразмеченные данные как дополнительную регурялизацию. Можно еще попробовать методы Positive Learning (<https://www.eecis.udel.edu/~vijay/fall13/snlp/lit-survey/PositiveLearning.pdf>). А вообще надо пробовать. На моих данных метод с сигмоидой и кроссэнтропией работает довольно неплохо. По крайней мере этот метод можно использовать в качестве baseline. Мне тоже интересно, что можно сделать лучше, но я пока не нашел ничего лучше.",
"<@U049HDR2Z> а можно в Mendeley перенести существующую библиотеку с сохранением структуры папок, в которых лежат статьи? Я когда смотрела, такой опции не нашла и это несколько расстроило - получается, что классифицировать литературу нужно заново, задавая вручную категории.",
"<@U0Q3USC0Z> В нём довольно ограниченный круг возможностей для интерфейсинга с ФС: можно настроить какие директории Mendeley будет отслеживать на предмет новых файлов; можно выбрать директорию, куда Mendeley будет сохранять копии при добавлении нового элемента в коллекцию; можно выбрать структуру папок для этой директории (например, вложенные директории с фамилиями авторов). Вот, в принципе, и всё. В недавно вышедшей версии не проверял, но в предыдущей при перемещении этой директории со статьями в другое место и перенаправлении Mendeley на неё всё сломалось. О сохранении тэгов и классификации даже говорить не стоит. Т.е. формат работы такой: создаешь директорию для всей литературы, натравляешь Mendeley на неё, и дальше работаешь только через интерфейс. В нём можно виртуальные вложенные директории строить, можно тэггировать, т.е. удобства достаточно.",
"Коллеги, не подскажете для ClickHouse какая конфигурация серверов лучше подходит (размер хранилища,  оперативки).",
Данные примерно как в метрике),
"внезапно понял, как много я потерял, не зная что такое карамель ""Взлетная"".",
"Пока проблема, т.к. пайплайна работы со статьями я для себя еще пока не нарисовал. Но типичный кейс — когда скачал файл arxiv.2123.pdf, наставил в нём комментов, а потом хочешь переместить в другую папку или автоматически переименовать в формат author-year-title.pdf",
"<@U0VQJFN81>: Я правильно понимаю, что в задаче и база, и образцы находятся в сыром (raw) формате? И все попытки направлены на то, как минимальными усилиями сделать поиск именно по сырым данным, без их предобработки?",
"ну да (если я правильно понял, что имеется в виду под raw) - нет возможности различные tfidf считать или другим способом что-то предобработать (т.к. всей базы нет)

человек может понять, что строка ""БАТОНЧИК МUЛКИ ВР-Й ККУ(IНИЧ11ЫЙ КО"" это ""батончик милки вей клубничный ко"" даже не зная какие еще бывают наименования продуктов (заметьте, что в ошибка не из типа ""1("" = ""К""). Вот хотелось бы что-то такое",
"т.к. неизвестно, какие варианты правильные",
"Вики - это всего несколько Гб текстов, да и стилистика текстов далека от разговорного языка. Про либрусек и флибусту я знаю, что скачать можно с торрентов, вопрос скорее в том, как потом это множество файлов fb2, epub и прочих форматов превратить в текст? например для wikipedia есть утилита Wikipedia_Extractor.",
можно также как на <#C1CEM43TJ|mltrainings_live> сделать,
"GAN для текста -- это интересно, тут сам автор объясняет, почему их к тесту не применяют: <https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/cyyp0nl>",
"После поста первая же мысль про word2vec как инструмент для небольшого изменения вектора — и на тебе, первый же комментарий и ответ на него ровно про это",
"На углу здания как выходишь из метро налево. Короче недалеко от Гамбринуса, чтобы понятнее было",
"Кроссэнтропия вообще адекватна для этой задачи, у вас же классыч не взаимоисключающие.
Когда-то я в подобном случае перекошенных данных рассматривал выходы по отдельности и сделал хук, что за ошибочное указание, что объект есть, когда его нет в разметке наказание было меньше, в моём случае в 1000 раз. Это частично исправило перекос,",
"<@U0TUTASNR>: можно рассмотреть ""noisy"" модель. Пусть f(x) – какая-то нейросеть, выдающая вектор такой же размерности D, сколько атрибутов может быть. Вероятностная модель без ""шума"" была бы `p(y|x) = \sum_{d=1}^D p(y_d|x) = \prod_{d=1}^D Bernoulli( y_d | f(x)_d )` (где `Bernoulli(y | a) = a^y (1-a)^(1-y)` – плотность распределения Бернулли). Однако, у нас есть шум, поэтому предположим, что вероятностный процесс разметки пользователем данных выглядит следующим образом: пользователь смотрит на атрибут и бросает монетку. С вероятностью λ он атрибут игнорирует, а с вероятностью (1-λ) он честно размечает данные. Тогда зашумлённая модель будет `p_{Noisy}(y|x) = \prod_{d=1}^D (λ [y_d=0] + (1-λ) p(y_d|x))` где `[y=0]` – индикаторная функция, равная единице, если событие истинно, и 0 в противном случае.",
"Ну а дальше остаётся только реализовать `log p_{Noisy}(y|x)` в численно устойчивом виде, обучить ту самую нейросеть `f(x)` на максимизацию этого логарифма и подбирать лямбду как гиперпараметр на (0, 1)",
"Обучить на тех лейблах, что есть. Посмотреть, где в training set сеть выдает ошибку, потому что считает, что там есть объект",
"Кстати, а как в таких случаях принято определять конфиденс предсказания сетки? ",
Просто как амплитуду вероятности лейбла? ,
"Публика там была сильно не в теме, как обзор ок",
"Подходил к нам любопытнейший человек, позицию которого можно обозначить как ""слышал звон, не знаю, где он""",
"Я бы не сказал, что это зло, если его оглавление воспринимать как чеклист при проектной работе",
"Ламерский вопрос по Theano: Дано - числовой вектор a и целочисленный вектор b. Нужна оперрация, которая породит вектор с такой же длинны, что и b, так чтобы c[i] = a[b[i]]. То есть переложить значения из первого вектора в третий используя второй как карту.",
"В numpy такое точно можно сделать, а как это в theano сделать нативными методами.",
"<@U04ELQZAU>: pmbok стандарт ведения проектов.Каста людей обладающих pmp  - те кто сертифицировались по пмбоку , часто -формальное требование на крупных внедрениях для проджектов. Очень формализовать любят все...",
"<@U064DRUF4>  О! И правда работает. Как то я не подумал, что это может быть всё на столько буквально.",
"Вкину свои 5 копеек — митап реально оказался спорный. Первый доклад был обзорный по LDA, хотя воспринимать детали использования было довольно сложновато и подавались они как магия (зачем так — непонятно). Второй — просто про Vowpal Wabbit, чуть ли не на уровне флагов при запуске. В принципе местами было прикольно. Про четвертый уже все сказали, а третий был на мой взгляд хоть и странный, но забавный (хоть и был пожалуй самый обзорный и поверхностный из всех).

Больше всего удивило то, насколько много людей пришло — либо с продвижением встречи перестарались, либо реально хайп по данным сейчас супер силен уже и у нас.",
"Вокруг датафеста тоже куча хайпа, только больше внутри тусовки. Коллега с работы переживает, что не попадет на него из-за количества народа, кто подал заявки.",
"<@U0FEJNBGQ> <@U040HKJE7> насчет поднятых рук - я на каком-то РИФе предложил тогдашнему своему шефу задать ""установочный"" вопрос типа ""поднимите руки те, кто обычно поднимает руки на всякие вопросы"". По его словам ЕМНИП получилось чуть больше 2/3. На более гиковских сборищах м.б. еще меньше...",
"<@U0AD1L5NC> не знаю, сам такой задачей не занимался. вообще после этого я увидел сообщение выше от <@U0ZJV6E5Q>, в котором упоминался Positive Learning, это оно же. дружат ли эти методы с CNN надо спросить у <@U0TUTASNR>, когда он почитает какие-нибудь обзоры )",
"итак, краткая история про заявки, и спойлер про то, что с ними будет

1. мы пока провели всего одну волну аппрувов, 400 штук.
2. у меня уже есть свежая табличка с заявками, из которой я к завтра отберу еще 200-250 анкет. новая волна будет завтра
3. вместе с новой волной, завтра будет анонс по всем каналам, что регистрация закончится в понедельник. также я закрою регу на митапе и разошлю всем зарегавшимся там напоминалку о том, что рега считается только на яндексе. и что прийдя на место, зарегаться будет нельзя
4. в понедельник закроется общая регистрация и во вторник будут разосланы последние 150-200 аппрувов. 100 из них я сделаю совершенно рандомными, независимо от каких-либо фич анкет (только удалю тех кто вместо анкеты клаву прочищал).
5. во вторник еще раз пропиарим трансляцию со словами ""мест нет, инвайты разосланы, смотрите трансляцию""",
"для этого мне нужны представители таких площадок, кто не прочь пригласить к себе еще человек 500, лол. или 1000, как пойдет",
"Подскажите, пожалуйста, не смог нормально нагуглить. Я сгенерил датасет из двух картинок и лейбла, как мне все это упаковать в датабатч, которыми ConvNet оперирует?",
или это типа поощрительный приз для тех кто не прошел? -),
да тут много людей кто у них преподавали:),
как жеж я был наивен,
"Есть файлик с 1.5кк коротких(сравнительно коротких, не твиты, файлик весит 2.5гб) текстов, нужно научиться для нового текста находить наиболее похожий в этом файлике, грязно и быстро. Пока хочу отстеммить и конвертнуть в тфидф, дальше тупо считать расстояния между входным текстом и известными, какие подводные камни?",
"Вопрос про fine tuning на mxnet:
Есть предтренированные веса для inception v3, inception bn, inception bn-full 21k, vgg, googlenet. Запускаю fine tuning на другом датасете. inception v3 и googlenet обучаются так, что с первой эпохи точность на трейне и  валидации заметно выше, чем если учить с нуля. Остальные сетки обучаются на трейне со скоростью как будто запущены с нуля и при этом выдают околонулевую точность на валидации. 
При этом я использую один конфиг запуска и просто меняю пути к весам, сам json с архитектурой и бинарники под вход сетки.
WTF? Что я делаю не так?",
"Если так, то почему inc v3 учится?",
"на вопрос: почему так происходит, пока, к сожалению, ответить не могу. Возможно, можно будет сделать некоторые выводы постфактум, после того, как оно заработает",
"там может быть один из оргов ктото кто работает в мейле, но мейл официально организует завтрашнее мероприятие Data Science Meetup",
"это скорее будет упражнение, как по имеющимся gitxiv-ам запитчить ~невдупляющих~ экспертов ФРИИ
``` проанализировать возможный рынок, разработать бизнес-модель, презентацию и план вывода продукта на рынок.```",
"Всем привет! Может кто посоветует простенькие пакеты для винды  с методами обучения с подкреплением  ??   reinforcement learning,  Q-learning",
"Посоны, а кто пробовал cudnn под виртуальной машиной? Оно вообще жизнеспособно? ",
<@U0U2ENJ4U>: а ты какой фреймфорк хочешь запустить?,
"ладно, другой вопрос, как бороться с `A driver of version at least 361.00 is required for CUDA 8.0 functionality to work` при установленном 367 ?",
равно как и не работает хак отсюда - <http://askubuntu.com/questions/672047/anyone-has-successfully-installed-cuda-7-5-on-ubuntu-14-04-3-lts-x86-64>,
"Для n уникальных элементов лексикографический номер перестановки считается за O(n^2) (как и генерация самой перестановки по номеру), а зачем тебе вероятности?",
"<@U06J1LG1M> как тебе на новом месте? Кстати если пойдешь в аквапарк вайлд вади, то есть лайфхак - можно сделать лицо кирпичом и пройти забесплатно в сауну в крутом отеле рядом. Там кстати во многих отелях так можно, делаешь уверенное лицо, прикидываешься гестом и топаешь в сауну",
Какой бренд оперативы - норм?,
"Я что-то не могу понять за счет чего у них это работает. Там даже никаких матричных разложений нет, как я понял, только hashing trick на co-occurrence матрице",
"Сталкивался ли кто-нибудь с multi-output regression для long term forecasting временных рядов?
Суть в том,   когда нужен прогноз по дням на неделю вперед, чтобы модель выдавала предсказания не по одной точке, а сразу вектор длины 7.
Оно вообще работает? Что актуального почитать на эту тему?",
А как обычно это во временных рядах делают? Там разве не добавляют предсказание на завтра и прогоняют модельку на этом еще раз? ,
"это рекурсивный метод называется (предсказать одну точку, потом засчитать, будто она была и сделать след) , он  быстро расходиться умеет.
линейные модели сразу на какую угодно дату предсказывают, т.к. им пофиг",
"Как вариант, предсказывать сразу 7 значений и в функции потерь взвесить ошибку по убыванию",
"призываю спецов по керас. есть такой порт гуглнета в керас. <https://gist.github.com/joelouismarino/a2ede9ab3928f999575423b9887abd14>
там файл с custom layers использует theano. как нужные строчки переписать под tf? (а в идеале в виде абстракции для бекенда) не в numpy же загонять все",
хочу попробовать гуглнет так как он меньше и на устройстве будет быстрее работать,
"Ну, для подобных целей не использовал. Моделировал persistence эффекты а рекламе. Там как раз рекомендуют для коррелирующих рядов. Но по сути получились те же две регрессии. ",
"<@U0AD1L5NC> ну в абсолютных значениях - да, не настолько быстрый, как хотелось бы (для скорости можно PVANET использовать - <http://arxiv.org/abs/1608.08021> - только его обученного вроде нет, но можно и обучить ведь), но вот сравнению с реснетами он получается и точнее и быстрее. А ещё можно гонять картинки не 299х299 (8х8 на global average pooling), а 267х267 (7x7 на global average pooling), 235х235 (6x6 на global average pooling), а то и 203х203 (5x5 на global average pooling). Тогда будет ещё быстрее.",
"Из вакансии ГетТакси *“Develop and constantly improve a model based on machine learning approach to set optimal product settings worldwide (up to 100 settings) to drive our business at the most efficient way. At the first steps, most of the time will be devoted to routing algorithm optimization”*

У ЯндексТакси есть такой же вопрос сразу в форме на вакансию аналитика “Предложите свой алгоритм распределения заказов”.

Эт такая реальная/неизученная/трудноимплементируемая проблема в наши дни? Вроде, есть орграф, есть веса(трафик, расстояние, етс). Звучит не сложно) Или просто так звучит, потому что не занимался графами дальше курса Райгородского. Какие основные челленджы могут быть сделать крутой алгоритм распределения заказов?",
"Вообще, выглядит как bipartite matching ",
Ну или как вариант 7 (s)arima моделек,
"такой вопрос: задача предсказать цену квартиры, в трейне есть только квартиры до 5 комнат, а в тесте даже 9 есть. как решить такую проблему и какими моделями? пока на ум приходит только регрессия :disappointed:",
<@U0FEJNBGQ> а как деревья в регрессии могут за пределами виденного предсказывать?,
"нет, такого не смогут, как я думаю",
"при линейной регрессии есть проблема, когда много ohe фичей. поэтому не очень хотел такое юзать",
"хм, но площадь же весьма важна, как мне казалось, будучи участником этого рынка",
"это типа как продажи предсказывать, но дат рекламных кампаний не дать",
"да, конкурсы они такие конкурсы. Когда показывал данные экспедии турагенту, первый его вопрос был - ""где звезды отеля в поиске?""",
Куда вообще люди датасеты выкладывают?,
<@U0G29N5U4> тоже не понял зачем,
"Как-то слышал такую мысль, что функция потерь является устойчивой к выбросам, если она асимптотически ведет себя как линейная функция. Почему это так?",
ну дак почему так происходит?,
"Почему линейная - устойчива, а квадратичная - нет",
"Потому что L1 оценивает медиану, а L2 -- среднее. L1 является асимптотически линейной, так как она линейна в любой неперсекающейся с нулём окресности.",
"<@U0KQ5M6KX>: во-первых, ты так не запредиктишь число комнат, которое не видел, во-вторых, зачем отказываться от количественной переменной, если она про сути такая (как минимум, монотонность функции от нее очевидна), в-третьих, зачем плодить свободные параметры... Уж лучше набрать разных функций и зарегуляризировать, останется парочка",
hashing trick можно рассматривать как некий аналог random projection,
пример как работает random projection,
"попалась статья <https://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html> , вызвала сильное удивление интерпретация результатов chisq.test , почему он значения test statistics интерпретирует как корреляцию (и она еще и больше 1 у него получается)",
"Я тоже поначалу удивился зачем там вообще хи-квадрат, но потом залип на другом моменте.",
"А какие есть интересные метрики для многоклассовой классификации? интересует когда нужно штрафовать ошибку предсказания по разному, например, если соседние классы , то не так строго, а если дальние, то очень строго",
Всем привет:) А как сейчас делают выделение коллокаций в domain-specific текстах без ручной разметки? Например название компании из трех слов выделить или что-нибудь подобное. Знаком только с решением через Pointwise Mutual Information,
<@U061VLHH7>  есть еще совсем примитивный скор как тут <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>   он же используется в gensim phrases,
"умеет,но лишнии руки и голова не помешают,мб кто прост ченить еще более годное выдаст",
"кстати, без шуток. как раз когда там будет блаблабла про бизнес, у нас будут лекции про АД в когнитивщине, а попозже - секция <#C047H3N8L|deep_learning>",
"а, не, в 17-40
```Анализ сетей передачи информации в мозге (связности мозга или connectivity) стал ступенькой для более целостного понимания принципов работы одного из сложнейших устройств в природе. В докладе пойдёт речь о том, на каких уровнях можно анализировать сети в мозге, какие данные для этого нужны, какие подводные камни могут возникать при таком анализе. А главное — что нового позволили узнать такие исследования.```",
"Подскажите, как используется bounding box regression? Мы предлагаем регион, потом уточняем с помощью bbox? или bbox применяется уже к выходному региону?",
а кто устраивал из завтрака 4chan?,
А какие на практике можно применить бенчмарки для топик моделлинга? ,
"<@U1BAKQH2M>: могу посоветовать хорошую библиотеку (пиар-пиар :joy:), bigARTM, с склерновским апи. вообще народ вроде как LDA юзает, типа mallet и gensim.",
"<@U27MC8J2C> эм, вопрос не в том, как тм делать, а в том, в каких попугаях мерять крутость метода. Вот есть сферический lda и lsi, есть какой-нибудь практически разумный метод численно посчитать, что лда круче лси?",
"отсмотреть темы глазами и посчитать соотношение нормальные/мусор? замесить набор документов на определенные заранее темы, посмотреть,  сколько реальных тем оттуда вынет и какие документы туда отнесет?",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1473148602000161> на практике так обычно и делают, как мне кажется",
"короч я хочу затестить идею, которую <@U040HKJE7>  на митапе в мейле затирал как раз в контексте топик моделинга - долбануть word2vec и кластеризовать вектора оттуда",
"summon <@U04AR6WF0> :slava: 
как сравнивать друг с другом LDA и не LDA модели?",
"Привет народ, по-моему тут еще не писали, но Garage48 собирается проводить хакатон про открытые и большие данные в Тарту (городок в Эстонии) в октябре 21-23. Так как аудитория здесь очень разношерстная то мне сложно судить на сколько вы могли слышать про Garage48. Судя по сайту, то они проводили хакатоны в Минске и Москве, достаточно часто проводят их в Украине (из последних: Gamification 2016 в Одессе и Garage48 IoT &amp; Machine Learning в Киеве) ну и естественно в Эстонии, Латвии и Литве. Идея проста - участники приходят на мероприятие и либо презентуют свою гениальную идею и набирают под нее команду, либо присоединяются к чей-нибудь интересной идее и работают над ней вместе с другими. Еще в этот раз у нас будет несколько компаний, которые предложат свои темы и данные под эти темы - ну и возможно спец. призы для тех кто сделает что-то стоящее. На все про все участникам дается 48 часов от момента формирования команд. После мероприятия можно остаться на несколько дней и посмотреть Тарту, Таллинн и прокатиться на пароме в Стокгольм или Хельсинки, так что присоединяйтесь! Ссыль: <http://garage48.org/events/openbigdata>",
"Интересно, есть ли какой-нибудь чатик closeddatascience, где сидят всякие Ng с ЛеКунами",
"Решил D. Blei с нейросетками поиграться, хоп такой summon @rfergus как мне архитектуру нейросетки придумать и он сразу ответил :slava:",
<@U0XR20SA1> как он в сравнении с vw и другими алгоритмами?,
"не сравнивал. С vw сравнивал сам автор vw. Думаю можно не пробовать, должно быть примерно так же как fasttext.",
"интересно, как на ваших данных покажет себя vw в сравнении с fasttext)",
А как его быстренько попробовать? Есть обертки на питоне удобные?,
"какая реализация factorization machines сейчас актуальна? 
libFM последний релиз 2014, pyFM как будто выложили и забыли.",
"<https://github.com/scikit-learn-contrib/polylearn> еще есть. Но я, как обычно, ничем не пользовался :D",
"а в vw прям класический fm? я не знаю как он в деталях работает, онлайн его же нельзя гонять?",
"Не. Графлаб проприетарный, платный. SFrame они сделали опенсоурс. Но сделан он очень классно, как по удобству, так и по скорости работы. Я как то тамошним GBM уделал XGBoost",
vw -lrq вроде как очень подобен классическому фм,
"Fastfm сделали и оно работает, зачем тебе еще коммиты? Его имеет смысл использовать если нужны факторизационные машины в режиме mcmc, или на скорую руку в питоне. Для режима sgd имеет смысл использовать vw",
Правильно воспринимать это как линейную модель второго порядка,
кто в группу Deep Learning вконтакте сует Левенчука?,
"Нельзя сохранить модель — это особенность режима mcmc, когда модели семплируются для оценки предсказаний из апостериорного распределения. Я для себя вывел, что fm имеет смысл использовать именно в mcmc режиме",
"В режимах, когда параметры находятся в явном виде, например через sgd или als, модель сохранять можно",
"вот я сравнивал сгд и алс - ну разница вроде в перформансе особо не было,  чем мсмс круче принципиально (если что, я толком не знаю, как он работает)?",
"mcmc по сути усредняет много моделей, отличается от sgd/als примерно как rf от одного дерева",
чот вообще разницы не увидел между одной моделью и 20,
А кто может составить ему альтернативу среди русскоязычных блогеров?,
Кто ещё пишет о дл на русском?,
"но в среднем, выбор между жж и хабром про сети на русском - как два стула",
"<@U04BFDYPV> ну как сказать, фм очень сильно економит место, если interactions нужны в моделе. + быстрый линейный прогноз по каждому наблюдению, что тоже сладко.",
Зачем постят Левенчука - не знаю!,
<@U04422XJL>: fine tuning как раз после предобучения происходит,
"ALS гораздо стабильнее и не оверфититься от любого чиха, как SGD",
Я как-то тоже воспринимаю finetune как обычный процесс обучения с инитом от Аллаха. ,
А можно узнать в каких случаях?,
А вот в рекомендашках как-то можно получить векторы для пользователей и продуктов не из трейна? Типа как в обычном svd,
Не сами продукты как вектор представляются. ,
"Ребят, можно ли как-то посмотреть из-за каких фич модель приняла такое решение в классификации?  То что в логистической регрессии можно можно получить коэффициенты - это понятно. В дереве решений можно посмотреть на полученное дерево тоже. Но вот как при работе модели получать не только класс, а также какую-то метку, указание, почему она выбрала именно этот класс?",
"Скажите, пожалуйста - а что такое Gate в нейронной сети и зачем он нужен?
Я правильно понимаю, что он полезен только для LSTM и LSM?",
"<@U04ELQZAU> подскажи, пожалуйста, в статье говорят: предположим, выполнены следующие условия
 - `Psi_x` имеет единственную неподвижную точку
 - Итерации сходятся к ней
Есть ли статья, где доказывается, что `Psi_x` -- сжимающее отображение?",
"Сап чат!

Помните мы с <@U04423D74> както обсуждали возможность лекции про введение в психологию?
Так вот, это обсуждение вылилось в серию мероприятий до конца года про АД в науке *Data&amp;Science*

Первое мероприятие в серии будет про физику высоких энергий и БАК

17 сентября поговорим про Большой адронный коллайдер, распад B-мезона, автоматический отбор полезных данных на детекторе LHCb и многое другое. На конференцию приглашаются специалисты по работе с данными, физики и вообще все, кому интересно, как современные технологии помогают познавать устройство мира.",
"там только для студентов/сотрудников вшэ был анонс, я только после того как запостил увидел",
зачем я в Baby заглянул :but_why:,
почему они выбрали именно baby и hospital? :but_why:,
"<@U0XR20SA1> 
Добрый день
Сейчас как раз работаю по векторизации. Еще руки не дошли - но думаю более интересно было бы сравнить не с vw, а с GloVe. Тем более что обе имплементации есть на C. Эмпирически - fastext тренирует 5 эпох за тоже время что GloVe 15, при этом получая примерно одинаковые результаты на eval. Но это было на text8(тот что на 100м датасет)",
А как ты сравниваешь получение Glove векторов и алгоритм классификации текстов интересно? не понимаю пока,
"ладно, пошел читать всю переписку сверху внимательней, прошу прощения.
А по векторам - просто у fasttext как раз же два направления - одно классификация текстов, другое - построение векторного пространства.
Видно настолько много с ним для векторов работал, что уже забыл про второе предназначение",
"Понял, почему ты перепутал :slightly_smiling_face: По векторам, я честно говоря не понимаю, что люди с ними носится. Они сами по себе не нужны никому для решения конкретных задач, а являются входными данными для какого-то алгоритма. Более того, в случае нейронных сетей можно их считать парметрами нейронной сети и обучать или дообучать во время общего обучения.",
"Ну отдельно их учить выгодней в том случает если используешь их в нескольких сетях. Тогда ты подкидываешь их готовыми - и сетке не надо каждый раз их доучивать. Просто получается быстрей, как по мне",
"Да, конечно. Возможно, когда данных мало для конкретной задачи, то выгодно их уметь предобученными конечно.",
а как это связано с xgboost?,
Кто из чата сегодня на семинар Ламперта в яндекс идет?,
Почему не поднимать вагрантами виртуалки?,
"есть вопрос, не связанный напрямую с NLP, но сопряженный. Есть большое количество текстовых документов (~1 млн), которые представлены в виде векторов латентных фич (размерность=600) - их возвращает d2v. 
С этими векторами надо постоянно проводить какие-нибудь манипуляции: перезаписывать, искать ближайшие и тд.

Вопрос - какое наилучшее продакшн решение для хранения этих векторов?",
"... после того, как статья прошла ревью на топовую конференцию и народ в интернете поразился невероятно хорошим результатам, хотя никто статью понять не смог",
"<@U2194SMBM>: выглядит очень круто и прямо как то, что нужно",
"Ну, по пейперу от GloVe accuracy получается выше, чем у word2vec со skip-gram.
Но вообще я как-то читал работу где они все сравнивались - и при должной настройки параметров результаты получались примерно одинаковые..
Только я эту работу все никак найти не могу - так бы кинул",
"glove/word2vec это примерно как баранина/говядина
одно реже другого, но примерно такое же по своим качествам",
"Да, в glove сильно накосячили к эспериментами, причем возможно что злонамеренно, так как сам Миколов на докладе в России говорил, что автор glove делал доклад у них в фейсбуке, и уже тогда ему указали на некорректные эксперименты, которые он не стал исправлять в статье",
"В общем я не могу детально многие вещи рассказывать, какие штуки используются для нахождения ошибок в текстах. Но в общем можно, но это проще каким-то митапом. Это так быстро не опишешь.",
"Товарищи, меня чёрт дёрнул обновить Ubuntu до 16.04, и теперь у меня не встаёт cuda 7.5. У кого-нибудь такая проблема была?)
Кто и как её решал?",
"Обучал недавно на 3GB русских текстов. Википедия не намного больше. Это часа три для GloVe. Правда словарь на 425 тыс слов был. Тут какой порог отсечения поставишь, такая скорость и будет обучения. Но на википедии в любом случае на обычном компе с 16ГБ обучиться можно вполне.",
"Последние результаты по классификации коротких текстов, лучший результат все же оказался за нейронными сетями. Кто не читал мои сообщения в течении недели, предыстория следующая: есть 23 тыс. текстов, нужно классифицировать их на три категории. fasttext показал accuracy ~81.5, VW ~82.5, Lstm в один слов с предобученными glove - 83.90%",
"Видимо плохо выразился про ""чем больше лейблов, тем лучше"", хотелось бы статьи с более общими тегами, как например на <http://lifehacker.ru|lifehacker.ru>, а в википедии придется все категории для статьи к одной сводить, что тоже непросто.",
"<@U1J5E9HJL> «вообще по заголовку в конкурсе Avito fasttext давал ~70% точности, с использованием w2v 73%»

А как fasttext связан с word2vec векторами?",
"Я начинаю интересоваться железом и поэтому хочу задать пару вопросов чатику :wave:
Какая видеокарта сейчас является золотой серединой для обучения средних задач по дип лернингу?
Стоит ли искать подержанные варианты, например на авито? 
Насколько важны в данном случае качественная материнка, оператива и проц?
Почему все используют NVIDIA? Другие бренды в принципе не поддерживаются или дело в мейнстриме, сложности настройки?",
"gtx1080, поискать можно, но смысла особого нет, ибо цену на нее выставляют почти как за новую",
"Вечер добрый! У меня две проблемы, одна из другой. 1 - начал заниматься оттоком, и встал вопрос А куда вообще копнуть, может кто подскажет ресурсы, статьи и методы по теме, пока юзаю стандартные алгоритмы прогнозирования: лог регрессия бустинг и тп (в machine learning пока junior), а 2- ая соответственно, данных на основе которых строится модель оттока много но они в основном сконцентрированы около нуля (те матрицы признаков сильно разреженны), как тут подойти пока не знаю, то ли их выкинуть и подобрать другие признаки или можно на их основе что нибудь построить? буду благодарен за любые рекомендации.",
А экстрополис в каком подъезде?,
А какой есть легкий способ найти экстрополис?,
Я человек простой. Вижу ребят из топа в фб - спрашиваю как решали.,
"<@U1ULNASSV>: в 12:00, как и сегодня",
"хочу странного, не знаю, бывает ли такое в природе и как называется (да и надо ли мне такого хотеть). Результаты SVD (вектора) заметно искажаются одинокими выбросами в данных. Есть ли нечто, минимизирующее L1 разницу при разложении?
На самом деле, хочется интерпретируемости получаемых при разложении векторов.",
"Кстати почему с выбросами L1 лучше? Я думал что L1 только разреженное решение дает, но в этом чате я уже несколько раз видел про какое-то соответствие L2 со средним и L1 с медианой. В чем суть?",
"""какое-то соответствие L2 со средним и L1 с медианой. В чем суть?"" - ну это если сумму квадратов или модулей чисел от другого числа минимизировать, то ответ будет, соответственно, средним и медианой. Так как медиана робастнее, то поэтому, видимо, и лучше L1. В этом был вопрос?",
"SVD ведь можно рассматривать как обобщение OLS так? Тогда вопрос <@U0FEJNBGQ> состоит в том, есть ли подобное обобщение для least absolute deviations?",
"Всем привет! Подскажите, пожалуйста, какие есть подходы для отличия опечаток и ошибок в тексте?",
"Ребят, есть примерно 12К девочек с тиндера с описаниями, фоточками и ссылками на инстаграм. Можно выкачать ещё больше.
Проблема в том, что как бы сам тиндер несколько против сбора данных для чего бы то ни было.
Какие у вас тут правила на этот счёт?",
"на moscowpython был как-то доклад, где чувак автоматизировал свайпанье девочек там, было забавно",
"Не знаю куда написать, вот обучил твиттер бота <https://twitter.com/neurozavet>,  с помощью char-rnn, но он туповатый, датасет слишком маленький, наверное. Собираюсь набрать побольше и научить его отвечать на твиты",
"<@U1LGHMGPM> Оригинальное слово без ошибки известно?
Потому что, например, слово ""нарос"" (как и многие другие слова) может быть как безошибочным, так и словом с ошибкой или даже словом с опечаткой в зависимости от того, от какого слова ""скакать"".",
Это уже выглядит как некоторый подвид расстояния Дамерау-Левенштейна,
<@U2AD078S1> а какой был объем обучающего датасета?,
"Кстати, интересный вопрос: какой примерно должен быть объем датасета, чтобы char-rnn выдавала что-то адекватное?",
"<@U1BAKQH2M> :troll: что-то, что человеком не будет восприниматься как явно бессмысленное. У Karpathy в небезызвестной статье (<http://karpathy.github.io/2015/05/21/rnn-effectiveness/>) вроде адекватные результаты.",
"На датафесте Ветров рассказывал про мудрецов: каждый следующий использовал постериор предыдущего. Как мне показалось, это очень интересная идея, которая позволяет преодолевать некоторые пороги с качеством данных. Скажем, например, есть два похожих по структуре объекта-модели, но у них есть таки отличия. Мы предполагаем(из содержательных соображений), что часть взаимосвязей в них похожа, а часть нет. По первому объекту у нас мало данных, а по второму много, но нам интересен именно первый. Так вот идея подойти к нему изучая второй, а потом использовать результат(постериор на параметры модели) как априорное знание для модели первого объекта. Таким образом, у нас должна получиться, как мне кажется, более осмысленная модель для объекта, который мы изучаем, нежели бы в лоб строили по первому. 
Основной проблемой будет сам трансфер: стоит ли его ослаблять, насколько сильно и тд.
Применение(которое мне сразу приходит в голову) совершенно не воздушное, у нас по экономике России катастрофически мало данных, но можно попробовать найти похожие страны на нашу и делать макромодели на них(как раз завтра международная экономика у нас, будет возможность это обсудить с перподом). Макроэкономические связи, в целом, не должны радикально быть другими, если экономики похожи, но есть географические особенности и национальные, которые могут вносить коррективы в некоторые уравнения. Если это будет рабочей идеей, то можно попробовать так бороться с нашей чудо статистикой и делать немного более качественные прогнозы. 
Да и в целом не только в макро это можно использовать, а в любой проблеме, где ищется «святой грааль» и нету очевидной оценки регрессии. В связи с тем, что данные используются разные - каждый раз мы получаем какие-то разные модели, не стыкующиеся друг с другом(разные контрольные переменные, методики, исследователи и тд). Что можно попытаться сделать, так это передать то «знание» которое получилось раньше на других данных для последующего анализа.
По «bayesian knowledge transfer» чето не нашлось релевантного в гугле, и я даж чет хз как его формулировать, чтобы выстрелило, плюс хочется услышать какой-нибудь комментарий от более опытных людей в байесовском анализе",
во как раз за несколько дней до датафеста появились,
В и С я понял как параметры наших моделей,
"Разделение на общее и не общее было для того, чтобы понимать, какие верования следует ослаблять, а какие нет.",
"Спасибо большое за советы! Будет, куда покопать :slightly_smiling_face:",
"Кажется я плохо переварил
`p(B|C) = \int p(B, A|C) dA = \int p(B|A) p(A|C) dA`
Это все череда следующих вопросов?
1) `p(A|C)` - «какое у нас общее знание если мы имеем `C`?» Для этого распределения делается стандартный байесовский вывод для нахождения этой плотности?
2) p(B|A) ""Окей, у нас есть общее знание, как выглядит модель `B` тогда?»
3) Поскольку общее знание у нас случайная величина, то выходит p(B, A|C)",
И зачем интегрированием избавляться от A?,
"&gt;  И зачем интегрированием избавляться от A?
Если от него не избавляться, то где его взять? Мы никогда не наблюдаем параметры модели",
Это все просто использование постериора А после С как приора для B или что?,
"Можешь привести пример, пожалуйста? Кажется у меня из-за того, что обозначения похожи(ABC) не выстраивается полная картина происходящего. 
`С` это случайные наблюдения того, где у нас хорошие данные
`A` я так понял это то, что описывает модель для `C`
`B` это сэмплы для того, где у нас плохие данные
Верно?:thinking_face:",
"<@U1LNBRZ29> я думаю, что прежде чем развивать байесовской подход к идее, нужно прочитать про эконометрику панельных данных. И уже отсюда думать, как применить к этой теории байесовской подход.",
"Да, причем в области панельных данных множество работ. И панельные регрессии для исследования экономики России не редкость, а устоявшаяся практика. Все не так плохо, как может показаться. ",
"я кидал сюда вообще вроде бы статью с последнего Диалога, где это обсуждалось всё",
"там как раз проблема основная не в том, чтобы отобрать пул кандидатов, а в том, чтобы понять, кто реально был, но судя по формулировке твоей задачи, у тебя проще это всё, а как отбирать пул, там описано",
"Не совсем понятно, как реализовать, правда.",
Привет! Есть кто не спит? Как ДатаФест прошел? …,
где еще бывают доклады про черного козла?,
"Народ, а подскажите  датасет текстовый, чтоб у него “вложенная” структура была типа : пост -&gt; коммент  -&gt; ответ на коммент….  Или где достать такой на “актуальную” тему типа Olympics / US elections.. English language все",
"<@U1G303UTW> а что с козлом, когда повторы в записи будут?",
"вернемся к датасету: или где б такой накроулить? Мне вообще нужно 3-4 поста, т.е.е там можно и руками все, знать бы где…  Твиттер вроде не подходит, там же только комменты максимум?",
"жж есть, там как раз пост и деревья камментов",
"Ребят, не подскажите, где можно презентацию Ламперта ""Classifier Adaptation at Prediction Time"" найти?",
"Приветствую всех!
Буду благодарен всем, кто поддержит анонс конференции АI Ukraine на Хабре - <https://habrahabr.ru/company/flyelephant/blog/309736/>
И будем рады всех видеть в гости;)",
"А кто нибудь знает, где можно получить таможенную статистику рф по внешней торговле?",
"<@U2194SMBM>: в Росстате для многих вещей есть разбивка по категориям товаров, но в ограниченном доступе. Тут лучше непосредственно с ними связываться и узнавать, что есть и на каких условиях они готовы предоставить.",
"частоту класса в трейне для категории, или сочетания категорий можно заносить как фичи (weight of evidence)",
выглядит как multiclass с предсказанием вероятности каждого класса,
Но у меня он как бы взвешенный,
"Кажется, я придумал, как это решать",
"да, это как раз то, что нужно, спасибо!",
"а как правильно поступать в таких ситуациях, когда в процессе выкладок мы на каком то шаге получаем в формуле вероятность, которая не имеет физического смысла?",
"если как раз такой вопрос имеет место быть в p(A | B, C), но p(A|C) физически смысла не имеет, можно ли положить p(A|B, C) = p(A|B)?",
"Ребят, подскажите куда копать, или хотя бы пару ссылок дайте по такой проблеме: оценка качества ответа пользователю (пользователь задал вопрос, тех поддержка ответила), прогнозирование наиболее вероятного ответа на вопрос (выбор из шаблонов), и автоматическая генерация ответов по запросу.",
"Ребят, подскажите куда копать, или хотя бы пару ссылок дайте по такой проблеме: оценка качества ответа пользователю (пользователь задал вопрос, тех поддержка ответила), прогнозирование наиболее вероятного ответа на вопрос (выбор из шаблонов), и автоматическая генерация ответов по запросу.",
"ну про генерацию тип neural conversational model есть, там как раз про техподдержку пример ",
первые две задачи звучат как типичные задачи поиска,
 и как сбои могут проявляться?,
С каких пор 48гб мало? ,
"А кто может рассказать, когда и где ждать слайдов и записей с ДатаФеста?",
<http://arxiv.org/pdf/1608.04644v1.pdf> как жить теперь если кругом обман?,
"привет коллеги, кто знает можно где-то добыть статистику работы электрооборудования (трансформаторы, ЛЭП и др.) отказы или измерение каких-то параметров... может какое-то другое оборудование из сферы генерации энергии или электроснабжения",
"<@U049HDR2Z>: в смысле что они не написаны рукой человека и сфотканы. Вот если бы человека попросили написать числа так же, как на этих примерах, отсканили бы, и сеть ошиблась - тогда да",
"мне все таки кажется это проблема, которая демонстрирует не способность текущих сетей обобщать, хитрый подгон или не хитрый подгон - не важно, рукой человека написано или нет - тоже не важно, главное что наш мозг очень хорош в обобщении и манипулировании абстракциями, а компутеры в подсчетах; те успехи что мы наблюдаем в сетях связаны с тем, что они перешли на сторону свойственную мозгу, а не машине, и нам кажется что это круто, в том числе от сюда все эти хайпы, человечество давно мечтает что бы его поработил скайнет; так вот,  нам могло на секунду показаться что машины уже глубоко зашли в ту область где рулит мозг человека (не рука которой пишут, а именно моск который обрабатывает инфу), но эти примеры показывают что машины только сделали маленький шажок в это поле, так сказать лизнули травинку на краю поля, и сразу порезали язык, бррр :levenchuk:",
"<@U04ELQZAU> как раз будет солидный возраст, можно будет побрить голову, отрастить усы и завести себе бложек для рассуждений о новой весне :troll:",
"Хотел напомнить, все кто завтра собирается зайти на завтрак, жмите :heavy_plus_sign: под событием в этом канале. Если есть вопросы, пишите мне сюда или в личку. :sunglasses:",
"Привет! Есть сеть стационарных секьюрити камер в торговом центре. Стоит задача: определить по каждой из камер, сколько человек присутствует в кадре.

Есть готовое решение, которое через стандартные алгоритмы CV производит такой подсчёт, но этому решению присущ ряд недостатков: проблемы с перекрытием объектов, ложные срабатывания на тележки и пр. Частично эти недостатки устранимы через постобработку, но не все. Возможно, что имеет смысл зайти к решению данной задачи со стороны DL.

Вопрос: можно ли решить эту задачу с использованием DL и в какую сторону имеет смысл копать?

PS. Есть ли подходы новее, чем Crowd Counting с CVPR 2015?",
"Всем привет! Хотел совета спросить:
какие задачи можно решить с помощью ml в банке? может у кого был опыт? 
знакомые работают в одном и хотел с ними попробовать сделать какой-то тестовый пример",
А почему в <#C0KL2AXD3> ?,
"<@U0QMJJ3SL>: всё (как всегда) зависит от данных.
Обычно в предложении описываются варианты моделей (например, тот же поиск оттекающих клиентов), а по факту уже решается, что можно вытащить, а что нельзя.
Тюнинг и модели это последняя часть, как правило :)
До этого нужно огребать от данных :but_why: ",
"<@U0QMJJ3SL>: 1) какой канал наилучший для взаимодействия с клиентом, 2) какой продукт предложить клиенту далее, 3) сегментация клиентов",
"тут разные подходы есть. зависит от того, как генерировать ""окно"", т.е. веса",
"пространственное положение учитывается. правда, чаще всего косвенно. ""декодер"" запоминает, на какой позиции он сфокусировался.",
первый - как в статье,
"так как ocr, и в speech recognition символы идут монотонно, то мы получаем вот ту диагональ.",
"где мы можем предсказывать конкретную позицию, куда сфокусироваться",
"Всем <!here|@here> привет!

Напомню, что послезавтра мы планируем еще один крупный митап про анализ данных и науку *Data&amp;Science*: Большой адронный коллайдер

Там можно будет подробно разобрать, чем же занимаются на большом адронном коллайдере, кому и зачем это нужно, и как это связано с АДом :slightly_smiling_face: 

Место действия - Экстрополис, с 13 до 18 в субботу.
Ссылка для регистрации <https://events.yandex.ru/events/ds/17-sept-2016/>",
"есть ли среди нас мсье с причудами, гоняющие куду под виндой? чем лучше мониторить dat shiet?",
или как далеко в космос собралась улетать карточка,
"Раз пошла такая пьянка, кто-нибудь знает, как под Mac OS :mac:  сделать reset Nvidia GPU, чтобы память освободить, а то часто когда запускаю tensorflow :tf: , он говорит что памяти свободной очень мало (",
"Привет! Тинькофф в апреля 2016 проводил хакатон «Эра ботов». Кто-то может поделиться или подсказать, где взять датасет от этого соревнования?",
А у кого есть по летней школе Тинькофф банка dataset?,
"Можно учить вектора, а не метки, и решать к каким они классам относятся по ходу дела. 
В метрическое дерево какое-нибудь складывать. И эпизодически решать, не объявить ли что-нибудь новым классом",
"В презентации на дата фесте Воронцов показывает красивый график с интерактивным и иерархическим новостным анализом (<https://www.youtube.com/watch?v=CGpm1ve6VAM&amp;feature=youtu.be&amp;t=25m35s>). И создается ощущение, что есть такой сайт/платформа, где можно поисследовать новости по разным темам с помощью таких графиков. Но найти не могу.",
"Как решать задачу ""классификации"" на много-много классов? Учить вектора, а принадлежность к классу определять по расстоянию до опорных элементов.
Представь себе диаграмму Вороного, все приходящие объекты переводишь в вектора и складываешь по ячейкам. Ячейки пересчитываешь иногда ",
"спасибо, что-то я на ночь глядя немного затупил. Я вот никак не мог понять где еще надо импортировать так как в основном коде есть импорт, Если вдруг конму интересно, то оказалось, что можно просто импортировать скрипте  hyperopt-mongo-worker, который надо запускать вместе с с основным кодом.",
"<@U1FLG6YR1> а я почему то расшифровала как rows river. По таким запросам имплемениации тоже нет)
Будем искать!",
А как с этим бороться?,
"можно проверить без дифференцирования, что AR и MA части норм работают. Надо глянуть реализацию предсказания в какой-нибудь ARIMA модели, подозреваю, они берут последний уровень как константу, к которой прибавляют интегрированное предсказание",
и ввести приоры на параметры как обычно,
"или мысли, как оттуда весь словарь вытянуть",
"Картинка относится к другой программе (не знаю почему грузится она). 
Я скинул, чтобы поржать над недельным курсом (два учебных дня) по DL за 60к",
"Привет всем. С год назад вышел из канала, так как тут офтоп был полный. Сейчас полистал, есть интересные обсуждения и обмен опытом. Есть  которые тем недавно поднимались и по которым есть что сказать - glove, w2v, hash2vec (я написал glove  с нуля: <https://github.com/dselivanov/text2vec>).
1) GloVe vs word2vec. Примерно одно и то же, зависит от downstream задачи. Но если есть co-occurence матрица то glove обычно обучается до сходимости гораздо быстрее.
2) оригинальный glove (<https://github.com/stanfordnlp/GloVe>) содержал кучу багов и поэтому качество порой страдало. Не знаю пофиксили ли это сейас
3) glove-python (<https://github.com/maciejkula/glove-python>) значительно хуже даже оригинала, так он учит контекстные вектора и оригинальные вектора в одновременно в одном массиве.
4) hash2vec вообще показался ересью - в статье куча фактических ошибок.
5) в text2vec я сделал модификацию glove с L1 регуляризацией (думал что можно будет выделить “топики” в векторах и получить эмбединги с семантическим смыслом). По факту я не могу сказать что вектора получаются интерпретируемые. Но как оказалось небольшая регуляризация дает значительный прирост качества на маленьких корпусах.
6) glove в text2vec быстрее оригинального - каждая эпоха ~ 2 раза и сходится тоже быстрее (меньше эпох)

Будут вопросы - пишите",
"&gt; чем это плохо?
я не могу объяснить почему это теоретически плохо, но фактически sgd сходится к более плохому решению",
"копаться везде приходится. Как не откроешь чей-то код, а там ад полнейший :slightly_smiling_face:",
этот код на плюсах отдельно можно запускать? типа как оригинальный glove - через CLI,
"Кстати есть некоторое заблуждение по поводу расхода памяти в glove и по поводу того что word2vec стриминговый и glove нет. В glove само обучение как раз онлайн. Только co-occerence матрицу геморройно строить с маленьким расходом памяти.

<@U0DA4J82H> glove из text2vec можно легко из CLI запустить. Запилю интсрукцию для не R юзеров, давно хотел.",
"я слабоват в  теме,  кто может объяснить чем отличаются  Q  от TD",
"впрочем это личное дело каждого, куда нести деньги",
"народ подскажите на каких датасетах-бенчмарках тестят имадж ретривал алгоритмы? я нашел что вроде во многих статьях юзается Holidays, Oxford 5k  и Oxford 105k",
да и может кто подскажет что там у нас щас стейт оф зе арт в имадж ретривале,
"если бы там было чтото крутое, они бы выставили напоказ как они уделали и Ridgeway gbm и Xgboost",
и конечно какая же статья теперь без криповатости,
"У меня задача  такая,  Есть целевая  функция,  ее вид  точно известен.  Агент совершает скажем 1000 действий,  и по ним я  легко могу построить  реальную функцию.  Вопрос,  каким методом мне  лучше приближать реальную функции к  целевой после совершения  агентом  1000  действий",
"целевая  функция  это прямая,  под углом,  реальная функция  такая же прямая  только с  другим  углом. Весь вопрос  как  подбирать веса чтобы реальная  функция стала максимально близко к  целевой",
и я  точно знаю что я  хочу  получить как раз прямую,
почему вы решили использовать RL?,
"почему же. у вас - прямая, вы точно знаете как должна вести себя функция.",
в  идеале  как я  понимаю  99%  всех  данных у меня  отфильтруются,
"в  общем хочу  разобраться как  мне  правильно менять веса,  чтобы получить желаемый  результат",
"и надо понять, какие действия для этого выбирать?",
"""получаем много действий"" - сетка предсказывает, какое действие выполнить, на основе данных? для каждого действия по выходу?",
победитель получает все ????    какой  выход самый  большой   тот  и есть действие ????,
"как удобно, я  всегда готов",
"Видимо, проблема в том, как сформулировать задачу в терминах mdp. Но мы задачу не знаем, поэтому помочь сложно.",
"вы хотите построить фильтр данных? как вы его оцениваете, что он хорошо работает?",
"Я в универе как-то делал проект по вот этим данным, мы использовали father's education как IV. 

<https://www.bancaditalia.it/statistiche/tematiche/indagini-famiglie-imprese/bilanci-famiglie/index.html?com.dotmarketing.htmlpage.language=1>",
"Сегодня попросили  совета в такой задачке: есть величина, которую нужно оценить. последовательно поступают результаты ее измерения. Проблема в том, что некоторые измерения могут отличаться от истинной оценки в 2-3 раза.  Нужно  дать достаточно хорошее приближение оцениваемой величины. (Точного определения, что такое хорошее приближение не дали) Количество измерений должно быть как можно меньшим, 20 - предельное. Поскольку было раннее утро (12,часов)и я был в полусонном состоянии, то выдавить из себя ничего лучшего, чем на каждом шаге отбрасывать 2 или 4 крайние значения и брать медиану или среднее арифметическое я не придумал.",
"Во-первых, стоит, как всегда, посмотреть на то, что это за распределение. Если нормальное, то классическая статистика подойдет. Если только положительные числа (предположил из того, что ты сказал про возможные отличия в два-три раза), то может быть асимметричным и похожим на какое-нибудь логнормальное. В любом случае, я бы просто брал и фитил это распределение, а доверительные интервалы и остановку брал из правдоподобия. Или из апостериорного распределения, если есть какое-то априорное знание, например, значения вокруг какой-то величины должны, по идее, крутиться или дисперсия у измерительного прибора общая для всех измерений. Или дисперсия может быть какой-то функцией от матожидания (гетероскедастичность) - тогда будет несколько сложнее, но все равно решается через соответствующее априорное распределение на нее, подход общий",
это измерение какой-то детали как я понимаю. Вся проблема в выбросах,
"Господа, а подскажите по TF (r0.10). Хочу посмотреть как работает вычитывание данных  при помощи `eval` и почему-то вылетает `OutOfRangeError: RandomShuffleQueue '_15_shuffle_batch_3/random_shuffle_queue' is closed and has insufficient elements (requested 30, current size 0)`. Код вот такой:
```
filenames = tf.convert_to_tensor(filenames, dtype=tf.string)
labels = tf.string_to_number(labels, out_type=tf.int32)
self.log.debug(""Labels shape - %s; Filenames shape - %s"" % (labels.get_shape(), filenames.get_shape()))
        
input_queue = tf.train.slice_input_producer([filenames, labels])
self.log.debug(input_queue)
data, label = self._load_png(input_queue)
        
<http://self.log.info|self.log.info>(""Reading batch"")
data_batch, label_batch = tf.train.shuffle_batch([data, label],
                                                          batch_size=batch_size,
                                                          num_threads=num_threads,
                                                          allow_smaller_final_batch=True,
                                                          capacity=10000,
                                                          min_after_dequeue=100)

# [...]

batch = train_dataset.read_batch(batch_size=30) # релевантная для ошибки часть метода приведена выше 
coordinator = tf.train.Coordinator()

with tf.Session() as sess:
    sess.run(tf.initialize_local_variables())
    threads = tf.train.start_queue_runners(sess=sess, coord=coordinator)
    
    print(""Evaluating batch..."")
    b = batch[0].eval()

    print(""Requesting stop..."")
    coordinator.request_stop()
    coordinator.join(threads, stop_grace_period=10)
```",
"&gt; Медиана для измерения какой-то величины, по-моему, не айс, если выбросы редкие, а в остальном разброс нормальный. Среднее арифметическое после вычета выбросов будет точнее, т.к. ближе к матожиданию 
Точнее в каком смысле? Среднеквадратичном? А если в среднеабсолютном? :confused:",
"Вот, например, меня давно интересует, зачем в различных исследованиях измеряют средний (а не медианный) возраст – это же глупая и совсем неинтерпретабельная статистика! Она имела бы смысл, если бы мы использовали ЦПТ и последовательно выращивали N независимых людей, тогда средний возраст позволил бы оценить нам суммарное затраченное время. Медиана же поделит пространство на две равновероятные части, тут уже можно какие-то выводы делать из этого.",
"<@U0ZHHV83C> в этом примере еще ярче виден важный пойнт, который поднял <@U0G29N5U4>. а именно, все зависит от распределения
с таким же успехом можно привести кучу примеров когда среднее менее осмысленно чем медиана",
"Смешной контраст с тем, что говорят дилетанты о том, как ИИ всех зохватит и лишит работы. А эта область, наоборот, расширяет когнитивный фронтир, и задач становится все больше и больше. :filosoraptor:",
"Кто-то пробовал следующий приём при обучении глубоких сеток для классификации изображения:
Взять сетку и после процесса обучения/файнтюна сделать предикт по трейну. Дальше возможны разные варианты:
- прогнать только по ложным предиктам 
- прогнать по ложным предиктам, где разныца в  вероятностях между предсказанным классом и истинным меньше трешхолда
- сделать какой-то из вариантов с небольшой частью оставшегося датасета 

Цель - повысить точность. Как думаете, стоит потратить время на такой эксперимент? ",
"Есть глубоко разбирающиеся в  теме, может кто код писал своей нейросети с RL ?",
"<@U04ELQZAU>: если заметил, я посылку о нормальном распределении указал явно в том тезисе. В этом случае среднее не просто среднеквадратически точнее, а лучше приближает матожидание, а матожидание - это лучший point estimate для принятия решений как ни крути",
"Я вот не пойму для каких распределений медиана несет реальный смысл. Что она оптимизирует лучше, это борьбу с выбросами, например - красивая эвристика, конечно, но в остальном физический смысл неясен. В скошенных распределениях медиана кажется настолько же бессмысленной, как и среднее",
"Ну, я не знаю, как помогает знание о 50%. Я понимаю полезность других квантилей типа 95% интервала, но с этим туго. Не сильно информативнее среднеквадратического эстиматора, который, если я правильно помню, минимизирует как минимум дисперсию всегда",
"Особенно в тех случаях, когда последовательные независимые одинаково распределенные испытания невозможны ",
"Понятно, что с картинками проще :Bayes:. Но речь-то не о том, мне не нравится выбор мат. ожидания как меры центральности по-умолчанию",
"Вопрос знатокам. Пусть есть более-менее классическая архитектура CNN. По умолчанию обучение X-&gt;Y. Можно еще где-нибудь с середины форкнуть сетку, добавить деконволюционных слоев и навесить реконструкционный лосс X-&gt;X. 
Почему массово так не делают? Кажется, что это дополнительная регуляризация на ""разумность"" фичей. Или это имеет смысл только если множества объектов для этих двух лоссов отличаются? Мб статьи про это есть?",
а почему ты считаешь это разумной регуляризацией? в каком смысле разумная?,
"1070 как вариант, чтобы было на сколько-то процентов менее боязно :slightly_smiling_face:
Драйвера как-то криво встают, но все трудности преодолимы",
"<@U1G303UTW> ну вот как бы нет, дороже процентов на 10 всего",
он наверное тяжелый как бетонная плита,
"по цене примерно как макбуки, тыща евро за кг",
"тогда помогите мне,  как  мне менять веса  на  каждом  слое  сети в  зависимости  от награды ?",
"Это вопрос по технической стороне? Как в твоем фреймворке это делать?
Или по математической, насколько менять?",
"ну  сетку я сам написал,  хочу теперь к  ней  применить RL. А вот как  его применить не могу понять",
нужен пример  как менять вес  в  зависимости от награды,
"По-сути ты хочешь в качестве входных данных использовать автоэнкодер. Когда данных много разнообразных это наверное и не нужно. 
А на маленьких почему нет",
"Мне кажется, надо сначала понять, зачем тебе тут RL, в чём суть задачи, а потом применить какой-нибудь известный алгоритм ",
"у  меня есть  прошлая награда,  и текушая  награда.  Как веса менять,  чтобы достичь цели ??  вот и весь вопрос -))))",
"Вопросы сами по себе неправильные бывают, и это как раз пример такового",
"А что такое RL и когда он нужен, есть понимание?",
"Поясняю. сделали  действия  получили награду,  поменяли  веса  получили еще больше награду,  поменяли веса  -  цель как можно быстрее  получить максимально возможную награду!!!!!!",
"примеров для  обучения  сети нет и  быть не  может.  А учить надо,  вопрос к вам как учить без примеров. Я знаю RL  вы хотите что другое  применить ???",
"да в моем случае  есть  раунд  теста,  за который я  могу начислить  награду.  Но показать как  нужно себя  вести в  этом тесте  я  никак  не  могу,  это невозможно",
"Получаешь реворд R, тогда тебе надо чтобы Q[s,a] было как можно ближе к R + gamma*maxa' Q[s',a'], т.е. задача сводится к тому чтобы у функции Q, которая аппроксимируется нейронной сетью, настроить веса, где s,a фичи, а R + gamma*maxa' Q[s',a'] - верное значение.",
"Для  тех кто не в  теме,  любой  ленинг  это  всего лишь  задача подбора весов  без  тупого перебора",
"Опишу только для случая `X &gt; 0` (для `X in R` чуть проще). Введем фиктивную случайную переменную `Z` со значениями `{1/3, 1/2, 1, 2, 3}` и построим модель:
```X = Z * m * eps,```
где `eps~LN(0, s)`. Тут `s` -- гиперпараметр. Тогда
```
p(X, Z | m, s, P) = p(X | Z, m, s, P) p(Z | m, s, P) = p(X | Z, m, s) p(Z | P) = LN(X | Z * m, s) * D(Z | P)
```
где `P` -- вероятности соотв. исхода, `D` -- дискретное распределение. Искать оценку `m` можно ММП.",
"чего там писать-то, это просто регрессия, где целевое значение выбирается чуть более хитрым образом",
"тогда объясните как  это работает,   вот  есть  у меня  награда,  что дальше я  делаю,  как вес  поменять ???",
"и какой  вес,  у  меня  же много весов, и много слоев  в сети",
"ну просто кроме каких то рассуждений о том что такая регуляризация прикольная, нет вроде как обоснования что это чем то лучше л2 на веса",
"разберись сначала, как просто сеть тренировать, в обычном supervised случае",
"и конечный датасет, как я понял ",
и как тут рл поможет?,
"проблема например в том чтобы балансировать лоссы между собой, в идеале это какая-то динамическая балансировка как в adversarial",
"<@U04422XJL> раньше инициализацию делали послойными автоэнкодерами, но сейчас вроде как забивают просто потому что это долго",
"кажется, что как раз решая задачу восстановления инпута, ты можешь веса увести в сильно другую сторону от задачи классификации",
а как же диплернинг в полевых условиях,
всем желающим как  минимум  прочту хорошую лекцию  о биржевой торговле -))),
"и  даже у  меня  уже результаты  есть положительные,  как  ни странно",
"Я вижу две проблемы:
- декодер для больших сеток типа инсепшн или реснтов должен быть такой же сложный как сама сетка. И это отдельная архитектура, которую тоже нужно оптимизировать. 
- это означает, что при прочих равных бачт будет уменьшин как минимум в два раза, а это тоже влияет на сходимость и конечную точность",
"Скажите, знает кто-нибудь, как в sklearn сделать проверку качества для multilabel-классификации?",
а можно как набор независимых бинарных задач,
"Приветствую всех! 
28 сентября буду делать вебинар для начинающих Data Scientistов и тех, кто хочет двигаться в этом направлении. Буду благодарен за поддержку поста анонса на Хабре и его шеринг - <https://habrahabr.ru/company/flyelephant/blog/309992/>",
"Всем привет. Нужна помощь в следующей задаче, подобрать наиболее подходящие резюме под вакансию. Обработка текста делается с помощью IBM Watson Alchemy API, после чего нужно проранжировать обработанные резюмешки. Кто может подсказать подходящий алгоритм ранжировки, а также  альтернативу IBM Watson для обработки текста?",
"можно подойти к этому как к рекомендательной системе. вида ""пользователи с похожим резюме чаще попадали на такую вакансию (или похожую группу вакансий)"". а дальше уже зависит что ты считаешь похожими резюме :slightly_smiling_face: от косинуса между tfidf до сколько угодно глубокой дичи (а также извлечением из этого ""ключевых слов"" и делания того же самого)",
"Товарищи, такой вопрос. Есть несбалансированный датасет (99/1). Обучаю на 70% множестве randomForest — на остальной части имею 0.8 AUC, 0.95 Precision и 0.92 Recall. Пытаюсь там же прогнать XGBoost — получаю AUC = 1 как на кроссвалидации, так и на «честном» тестовом множестве. Пробовал на 3х разных сидах. ЧЯДНТ?",
"&gt; Если бы его не было rF бы ругнулся
не совсем понял, это же тестовое множество, какая ему разница, что предсказывать",
"ребята, а не подскажете, какой физический смысл несёт poisson loss? В Keras он определён как 
    Mean of (predictions - targets * log(predictions))",
в каких случаях его использовать?,
"Когда в нём наблюдений одного класса значительно больше, чем наблюдений другого (других).",
"там еще непонятно, как данные сглаживали",
"<@U13E1AWCX>: данные типа mnist  -- это те, где на картинке только один объект? А чего мудрить, взять просто сверток со страйдом &gt; 1, а в декодере тех же сверток, но уже с дробным страйдом &lt;1, получатся деконволюции",
"я думал, что кто-нибудь скажет, что лучше много маленьких сверток, чем мало широких, но есть такое-то эвристическое правило, что общее количество фильтров это f(n), где n - число объектов обучающей выборки",
"я правда давно не серчил по теме, но я не видел каких-то стандартов как делать deconvolution правильно для генеративной модели",
"А у меня 28x28, вот и думай, как правильно сворачивать",
"Так, мне тут пояснили, что в тензорфлоу есть функция pad, так что одна проблема как бы снимается",
Как и попыток этого объяснения. «Очевидно же» — такой себе научный аргумент.,
Всё-таки кто смог в VisDoom ,
"Не подскажете еще датасеты с кардиограммами как эти  <https://physionet.org/physiobank/database/mitdb/>,  <https://physionet.org/physiobank/database/svdb/>,  <https://physionet.org/physiobank/database/incartdb/> ?",
"ну да - с большим я погарячился. Ну гдет 10-20GB. Но нормального текста с человеческими знаками. Поэтому собственно wiki и comon crawl под сомнением..
пока вот подумываю использовать часть от датасета google - <https://cloud.google.com/bigquery/public-data/gdelt-books> - но пока чтот не особо разобрался как делять на него query)",
"&gt;сложить несколько главных компонент
зачем складывать? 

мне кажется, логика объяснения дисперсии при таких махинациях не обязательна. 
но идея что первая объясняет больше чем сумма компонент - согласен, выглядит стремно.",
что есть аггрегированный индекс? как складываете (это ж перпендикулярные вектора),
"таки не пойму, что значит тут PC1+PC2, если это тупо модули сложить, то да, странно, но если это сложение векторов в пространстве, то почему бы и нет, восстановление из уменьшенной размерности получится",
"не пойму, о какой дисперсии идет речь в оригинальном сообщении <https://opendatascience.slack.com/archives/theory_and_practice/p1474376571000734>
если оригинальные данные - это многомерный ряд, то и дисперсия у него не одно число, а covariance matrix",
"Я сейчас, наверное, глупость спрошу, а если мы скажем возьмем первые 10 компонент по какому то набору данных и на этих 10 компонентах еще раз построим pca первая компонента после второй итерации будет объяснять данные хуже чем первая компонента из первой итерации или все же есть шанс выжать из неё немножко больше?",
"<@U0FEJNBGQ> речь идет про ""дисперсию"", как в смысле вклада в ковариационную матрицу данных. это размышления больше на уровне матриц",
"а то че как лохи только две брать, пусть все возьмут :troll:",
просто есть работа в одном из топовых журналов - где складывали,
"я еще могу увидеть следующую логику: просуммировали вектора и увидели, что ""на глаз"" результаты работы с продуктом\сервисом стали выглядеть лучше. только в таком случае математическое обоснование через главные компоненты неприменимо (так как ""лучше"" измеряется в других величинах)",
"в каком это ""топовом журнале"" складывали?",
"не знаю, какой объём, но текст должен быть качественный",
"Как и многие думаю собрать комп для DL. Определился с видеокартой (gtx1070) и с процессором(Xeon E5-2670 с ebay). Сейчас стал вопрос с выбором материнки и вот 2 вопроса:
1) Стоит ли затачиваться под 2 процессора?
2) Вот 2 материнки, разница в 1.5 раза - стоит ли переплачивать?
<https://market.yandex.ru/product--asus-x99-a-ii/13874776>
<https://market.yandex.ru/product--asus-x99-deluxe-u31/12353181>",
<https://opendatascience.slack.com/archives/theory_and_practice/p1474376571000734> о каком индексе речь идет?,
"вопрос в том, как объяснить людям, что скалярное сложение первых компонентов PCA дает шум, а не другой компонент, который лучше объясняет дисперсию данных",
"есть формула (6) которая вроде тот самый индекс для одной страны. а где формулы её составляющих? мутная статья, а самое интересное как-то словами и поверху",
"я вообще не пойму, что они сделали в итоге. Вроде пишут, что FSI составили на основе трех главных компонент. Но потом они дают большие таблички (table 2 и table 3), где оценивают регрессию FSI на эти самые главные компоненты...",
можно для тупых пояснить пример: почему длина + ширина предиктивна на цену?,
"это же как раз контрпример: были двумерные вектора, вы тупо сложили базисные вектора(считай компоненты), получилась дичь, точки (1, 7) и (4, 4) схлопнулись",
"Тренировка по ML 24 сентября (в эту субботу) будет посвящена утечкам в данных и пробиванию лидерборда :slightly_smiling_face: По какой-то причине сразу несколько последних соревнований имели подобные уязвимости, это повод разобраться: как запороть соревнование, будучи его организатором; как обнаружить и грамотно воспользоваться ситуацией, будучи участником. Приходите, или участвуйте в трансляции (гоу <#C1CEM43TJ|mltrainings_live>)!

В программе:
- Победители и участники Data Science Game (<http://www.datasciencegame.com/>)
- Победители Kaggle TalkingData
- Kaggle RedHat Business Value

Мероприятие пройдет в БЦ Морозов, необходимо зарегистрироваться:
<https://events.yandex.ru/events/mltr/24-sept-2016/>",
"я же привел простой пример, почему можно складывать признаки, зачем это обсуждать? :slightly_smiling_face:",
<@U0ZHHV83C> потому что в твоем примере признаки складывать как раз нельзя,
а как же складывать пол (0-1) и возраст :troll:,
<@U1BAKQH2M> ты повторяешь одно и то же. Как так можно вести диалог? :slightly_smiling_face:,
или то что ряд таунхаусов у вас будет как стадион,
<@U0ZHHV83C> а откуда ты получил такие красивые и чистые главные компоненты для примера? Ты же их для этого должен был составить как линейную комбинацию кучи факторов. И получить прям длину и ширину. Какие факторы являлись сырьем?,
"Я этот пример привел скорее для того, чтоб объяснить смысл сложения признаков. Ведь в статье именно этот подход используется как основной. А складывать не сами признаки, а их главные компоненты -- это уже их ноу хау.",
А какие бывают критерии качества индекса?,
"то есть у нас есть многомерный временной ряд, надо из него получить одномерный, но так, чтобы этот одномерный как можно больше информации сохранял из многомерного",
"а если не вариацию суммы мерять, а насколько уменьшилась вариация исходного сигнала, когда из него вычли эту сумму",
"<@U0Q3USC0Z> нет, конечно :slightly_smiling_face: Но я не могу найти, где бы они это утверждали...",
"Как мне кажется -- они не складывали их. Они просто утверждают, что первые 3 дают 70%, вот и все :slightly_smiling_face:",
Как иначе они получили одну цифру из трех?,
"можно вспомнить,  <https://en.wikipedia.org/wiki/Body_mass_index> из медицины, там как раз рост с массой в одну формулу замешивают",
"это надо рассматривать как нелинейную регрессию на некую неизвестную величину. В случае с BMI неизвестная величина - это доля жира в человеке. Что есть FSI у них - неизвестно, поэтому и качество не оценишь",
"а затем делают регрессию, где зависимая переменная - это их индекс, а независимые переменные - это уже совсем други макропоказатели",
"как построить реальную регрессию, если нет зависимой переменной?",
"можно попробовать это рассмотреть как multi-output regression с таким вот bottleneck внутри, что все outputs должны быть пропорциональны какой-то одной формуле от inputs",
как зависимые переменные использовать выходные макропоказатели,
"на VAR  я там что-то потерялся, зачем оно там было?",
Кто-нить знает как зашейрить веса двух близнецовых CNNов  Siameese NN на keras?,
"Это тот же трюк с гистограммами, как в H2O? Должно быть приличное ускорение",
"Интересно, использование GPU для обучения NN накладывает какие-то ограничения на максимальное количество входных параметров этой нейронной сети? Если да, то какое примерно и зависит ли от конкретного GPU/поддерживаемых ею стандартов?",
"&gt;Inspired by human brain
куда ж без этого...",
"О результатах пока рано говорить, так как я просто отслеживал скорость сходимости по первым N итерациям",
"Ну просто интересно, какого порядка оказывается эффект",
"<@U1Z78RL3X> не складываем. При агрегации будем либо брать только первую компоненту, либо уже совсем другие подходы.
<@U0FEJNBGQ> в нашем случае, это не FSI, как было во вчерашней статье, а другой показатель. Для оценки его адекватности можно подобрать ряд критериев. К сожалению, пока детализировать не могу - проект этого на данном этапе не позволяет.",
<@U13E1AWCX> а ты код никуда не выкладывал? Можно на пример в коде посмотреть где то?,
"Кто-нибудь подскажет, где можно взять нормальный токенайзер (как здесь <http://www.nltk.org/api/nltk.tokenize.html> для английского) под русский язык? Для NER'а нужно",
"<@U0E4S5LU9> интересный вопрос. Окей, тогда переформулирую, кто как токениризует текст для задачи NER?",
пропроцессинг где заменяются ненужные символы на пробел. Потом токенизация по пробелу.,
"а зачем объединять вопрос коллокаций и токенизации? Я бы токенизировал по пробелу, потом уже выделял коллокации -- они и будут близки к NE",
как сделать колокации без токенизации?,
"Есть биас-вэрианс трейд офф. Какие алгоритмы (кроме использования регуляризации) выбирают уменьшить вэрианс, пусть и со смещением?",
"Если так, то я не понимаю, как это можно сделать",
"хм, а как на ячейки рубили?",
В эту пятницу на семинаре группы байесовских методов как раз должны эту статью разбирать,
Всё как в лучших домах ЛондОна: <https://twitter.com/gilgul/status/778720911125917696>,
"Шеф, сказал составить все хотелки по фестам, конференциям, митапам  на следующий год по тематике Data Science. Не подскажете может уже есть какой нибудь аггрегированый список или сайт с мероприятиями?",
"Я бы сначала априори посчитал, потом сделал какую нибудь коллаборативную фильтрацию, и все рекомендуемые покупки, которые имеют достаточный уровень поддержки, совместно с товарами из текущей корзиной покупателя вывалил в рекомендации. А там посмотрел бы что получилось и радовался/расстраивался",
А ни у кого нет  w2v или glove модели для русского обученой на новостях?,
Коробка это как раз практика,
"расскажите, как наиболее простым и адекватным образом задеплоить модель на tf?",
"А кто-нибудь слышал про такое понятие, как ""витрина признаков""?",
Где про это можно почитать/посмотреть?,
А плюсовая версия почему не может тогда?,
"Не может потому что без них скомпилена, как вариант",
"<@U1ZAJ4FC4> Условно говоря, это место, где ты хранишь все возможные данные, которые генерируешь@покупаешь@воруешь@убиваешь, и в зависимости от задачи, берешь для моделирования только те, которые тебе могут пригодиться",
"Товарищи, а кто знает способ реализации circular padding на tensorflow?
Это когда мы не нулями забиваем то, что за краями изображения, а числами с другого края.",
"Нет. Это адовая таблица со всеми возможными фичами, чтобы не бегать и не собирать из по куче разных источников. В отличие от озера, как я его понимаю, там все структурировано.",
"Пожалуйста. посоветуйте краткое описание А/В тестінга, что это и как использовать. И чтобы было понятно математику (но ее не было много)",
А где можно грепать поисковые логи в режиме онлайн? Нужно чтобы там был указан UserID или что-то похожее.,
"<@U1CEDPJSU> далеко не все мероприятия планируются за год, хорошо если за полгода, да и то анонсы сильно позже выгладывают, когда программу согласуют. Поэтому просто положите себе в годовой бюджет некое число поездок, насколько денег хватит, а там разберется ближе к делу",
"Немного на правах рекламы и в рамках поддержки курсов <#C074F6E1K|deephack>  по NLP и RL
<https://www.hostkey.ru/>
Кто прошел на курсы или найдет в себе силы дойти до долгопрудного, написать мне и найти меня в лабе :slightly_smiling_face: можем дать купон на 5к рублей на любые мощности",
"<@U1ULNASSV>: останется лежать на ютубе после окончания трансляции, как я понимаю ",
"Добрый вечер. Интересует такой вопрос: как правильно в питоне вызывать модели данных большого веса? Вот есть модель word2vec, около 1Гб. При вызове, она сохраняется в переменную и лежит там до вызова. Проблема в том, что при запуске такой программы в демоне, моделька не загружается и ""пропадает"". Никто не сталкивался с такой проблемой? Есть решения?",
"<@U07V14PC6> 
```[Опрос] Господа, минуточку внимания. У меня с научником возникли расхождения во мнении по препроцессингу и фильтру шумов. Я хочу узнать мнение большинства. Опрос слепой, выборов ответа не будет, пишите, как думаете, как угодно детально. Если ответов наберется довольно много, я ответы разбросаю по категориям и представлю дайджест. Всего 8 вопросов, 4 из них обязательных. Есть иллюстрации)```
<https://goo.gl/forms/YJAyeOLFgT8eGOzw2>",
"<@U07V14PC6> а как изначально стоял спор? потому что один и тот же фильтр использовать для всего подряд, особенно для нерегулярных сигналов, это странная затея",
"Про нерегулярные сетки - этого не было в изначальном вопросе. Но т.к. я, в том числе, обрабатываю и данные ээг, а раньше строил фазовые диаграммы по нерегулярным измерениям, мне показалось логичным включить этот вопрос в опросник. как я делал - расскажу, но в конце",
"Mixar, к слову, не очень интересным показался. Была крутая довольно идея ТАССа - типа создание виртуального мира, где будут геометрические фигуры разного размера или цвета, которые отображают метаданные новости, а в остальном - куча кардбордов и АР с метками. В общем, такое",
"Ребят, а подскажите какими методами в модели сетки устанавливаются w_o (смещения, bias)? Как я понял это делается отдельно от весов ",
"Если у меня уже есть numpy массив весов и смещений для каждого слоя, веса уставливаются функциями типа set_weights, вопрос как установить bias",
"Для баловства i5 нормально будет. Частота ведь больше 2 Ггц?
Тут как показал эксперимент скорее всего можно и про PCIE lanes не заморачиваться.",
в общем я так понял этот год как и 2013 никому не интересен,
<@U06J1LG1M> <@U041P485A> так весь цвет нашего DS  &amp; DL чот лица распознаёт и стиль переносит. Некому ImageNet тащить,
"как поэтично
«кафе библиотека невский»
…бессмысленный и тусклый свет",
Кто за ганжимана в Питере?,
"Интересно, их бустили или как :slightly_smiling_face:",
"сап, чат. возвращаясь к теме покупки бу железа с ebay, кто-нибудь покупал процессоры ES/QS? Я правильно понимаю, что это инженерные образцы? Цена на них привлекательная, но какие подводные камни при приобретении?",
"иногда можно напороться на интересные глюки, на одном E5 у меня с AMD картами вылетает bsod из-за неправильного адреса, когда пытается инициализировать 3d",
зачем там три слеша в урле интересно,
"Пункт `If you care only about the ranking order (AUC) of your prediction` я понимаю как ситуацию на Кеггле, когда всё, что тебя интересует — это максимизация AUC’a, а там хоть трава не расти. У меня же AUC более-менее приемлемый, но вот ситуация со специфичностью/чувствительностью модели немного грустненькая  по части специфичности ибо лучшее, чего я пока добился — это 0.85 / 0.47.",
"Для этого надо знать меры, которые будут применяться. А когда модель почти всё предсказывает в минорный класс, даже и считать ничего не надо. Предложил скидочку — разорил компанию.",
"В общем, сначала хотелось бы разобраться с аналитической составляющей вопроса, прежде чем браться за бизнес-составляющую. Отсюда и вопрос: какая из двух альтернатив, предложенных в доке хгбуста, больше подходит к моему случаю?",
"Это когда надо предсказать конкретно вероятность (churn rate, ctr и тд). Из рабочего примера: можно предсказать вероятность покупки билета на конкретное мероприятие, а можно найти top-k вероятных покупок клиента. В первом случае оптимизирую logloss, а во втором важна оптимизация ранжирования.",
"<@U0FEJNBGQ> скорее рассчитано на тех, кто восхитится этой разработкой и закричит: о боги, я хочу работать с этими ребятами и пилить этот прекрасный велосипед!",
"Доброго  времени суток. я тут на прототипировал всяких моделек в R (классификация на 3 класса обычная). теперь стоит задача склеить это с продакшн с# системой которая будет генерить вектор входных переменных и дергать эту самую модель по  10к раз в секунду (+- порядок). вопрос - чем мне посоветуете обмазываться? переобучить все это добро на python и дергать оттуда? воспользоваться удивительными С# ml либами или положить на ансамбли и xgboost'ы b и пытаться вытаскивать итоговую модель как вектор коэффов при соответствующих  переменных и их interaсtions?
какие практики приемлимы? p.s. нет не трейдинг и не HFT но вот кол-во вызовов predict может быть большим",
"1. конвертация моделек в известный формат (линии, деревья) для того чтобы просто коэффициенты в предикты на C# запускать
2. как раз в браузере была открыта такая вкладка <http://www.slideshare.net/0xdata/h2o-3-rest-api-overview-55224429>",
"дааа не) покерботу нада за несколько секунд обойди дерево а в узлах действий оппонентов сидият печеньки с предсказанием
<@U040HKJE7> а что ты под конвертацией подразумеваешь - есть какие-то готовые решения? както плохо представляю как random.forest условный расковыривать

спасибо",
"я тоже хочу вопрос задать про РХП.  допустим мы получили  0.89(да) 0.8(да) 0.74(нет)  0.71(да) 0.63(да) 0.49(нет) 0.42(да) 0.32(да) 0.24(нет) 0.13(нет)  соотношение классов в выборке 2/3 (4/6 (6 - да(TRUE) 4-нет(FALSE)) ошибка в ту или иную сторону одинакова нужно угадать наибольшее количество, можно взять методом перебора от 0 до 1 с шагом (например 0,1) то лучшую отсечку мы получим 0,8 так как 8 объектов буду правильно класифицированны .  Как можно по другому ? нашел что можно изолинией с угловым коэффициентом ///
Как считать отсечку через изолинией с угловым коэффициентом??",
у меня только один вопрос — как РХП превращается в ROC?,
"по поводу того где ставить отсечку — лучше имхо смотреть на precision/recall/F1, в зависимости от задачи",
"всмысле, как брать отсечку? спросить у того, кто просил эту модель, какое соотношение TP/FP его интересует. 

в зависимости от этого можно резать либо сильно слева, либо сильно справа. либо если ему насрать, гдето посередине",
Как найти эту ебанную идеальную точку отсечения?,
вот  как не перебором взять,
подскажите какую реализацию GloVe лучше использовать? а то начинаешь гуглить - а их дофига разных,
"Вот вроде неплохой туториал по ним, но он не использует InverseLayer почему то для депулинга, в чем причина? 
<https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/>",
<@U04URBM8V> смотри как надо завтраки устраивать,
"коллеги, вопрос по статистике. как оценивается стат.мощность критерия?
то есть, я эпизодически встречаю утверждение ""параметрические методы мощнее непараметрических"" - но из каким образом получен такой вывод?",
"ты имеешь ввиду, размер выборки?
и какой порог в данном случае? p-value не подходит, по идее",
"поставлю вопрос по-другому
допустим, я закладываю 0,05 на ошибку первого рода. допустим, у меня есть выборка определенного размера, и есть понимание, что размер эффекта (судя по другим исследованиям), например, d-cohen равно 0,23
как мне вычислить мощность тестов манна-уитни/т-критерия? чтобы потом сравнить и сказать, что вот у манна-уитни мощность ниже, вероятность ошибки второго рода, соответственно, выше
распределение неизвестно или не факт, что близко нормальному",
"можно, конечно, в лоб нагенерить данных с нужным распределением и размером эффекта и сравнить, какой тест как ошибается на них",
"Спасибо всем кто пришел на завтрак, бывало круто увидеть старых знакомых и ознакомиться с новыми людьми.)",
"Как сексист, предвзято считаю, что в Москве темы были интересней. ",
"bed of nails — unpooling когда раздувают нулями:
array([[1, 2],
       [3, 4]])
…
array([[1, 0, 2, 0],
       [0, 0, 0, 0],
       [3, 0, 4, 0],
       [0, 0, 0, 0]])",
Там где он есть в коробке,
а где он есть в коробке?,
"А, сорри, думал то что ты хочешь -- дефолтное поведение. Не использовал лазанью.
Тогда, кажется можно такой же трюк с индексацией, как в numpy заюзать",
"трансляция и запись самого мероприятия будет

еще там будет онлайн конкурс на месяц, куда мы отгрузили 7М транзакций и неплохой призовой фонд",
"Коллеги, кто использовал данные с Avito, например для проведения каких-либо соревнований, говорили с самим Avito о использовании их данных?",
"Коллеги, кто использовал данные с Avito, например для проведения каких-либо соревнований, говорили с самим Avito о использовании их данных?
Получали их одобрение / разрешение / согласие ?",
А какая может быть причина у неустойчивости?,
Можешь на каждой итерации доставать градиенты и смотреть в каком месте они самые большие,
"В каких весах, имеется ввиду?",
"Еще может быть проблема exploding gradients, какая у тебя инициализация? ",
"Я просто никогда такие глубокие сети не обучал, я мог, наверное, много где ошибиться в вещах подобного рода",
"Думаю, что ответ таков: да, все, кто использовал данные Авито для проведения соревнований получали  одобрение / разрешение / согласие :slightly_smiling_face:",
"А может кто подскажет где можно большие данные по недвижимости найти, желательно по СПб более менее актуальные. Я парсил irr но уж больно это долго, а данных около 1к всего",
"Ну короче я для себя понял, что хорошо работают  как сверточные, так и разверточные резнетыв",
"Привет всем! Предположим у меня есть множество с двумя признаками, в котором первый признак x1 равномерно распределен на [0,1], а второй признак x2 считается как x2 = -1.0*sqrt(0.25 - (x1 - 0.5)^2) + 0.5  + eps, где eps - это какой-то нормальный шум с маленькой дисперсией. То есть все точки расположены вокруг полуокружности (x1-0.5)^2 + (x2-0.5)^2 = 0.25. Мне нужно уменьшить размерность множества до одного признака, логично предположить, что нужно просто спроецировать точки на эту окружность. Вопрос: возможно ли это сделать с помощью автоенкодера, то есть на выходе я хочу получить точки, спроецированные на окружность?",
"Ну это понятно :slightly_smiling_face: Вопрос в том, как это сделать в автоенкодере? Я так понимаю, конфигурация должна быть (2, 2, 1, 2, 2), но я попробовал разные настройки, что-то пока не получается",
"у тебя отображение декартовые-&gt;полярные -- некоторая непрерывная обратимая функция. По теореме Стоуна её можно как угодно хорошо приблизить, например, нейронкой. Т.е. теоретически -- да, это возможно.",
"&gt; По теореме Стоуна её можно как угодно хорошо приблизить, например, нейронкой. 
речь то про ансупервайзед лернинг",
"<@U0E4S5LU9> Ага, точно! На самом деле, задача возникла из аналогии между автоенкодером и PCA в случае, когда используется линейная активация и один скрытый слой. В этом случае, автоенкодер проецирует точки на главные компоненты. Другими словами, можно ли использовать автоенкодер, чтобы проецировать точки на более сложные кривые (не на прямые, как в PCA)?",
Привет all. Кто максимально просто расскажет основные практические кейсы применения т.н. Gaussian processes в ML? Кто что делал на практике?,
"Еще на кафедре, где я учился, это использовали в субд для cost optimisation при построении плана запроса",
печаль :disappointed: хочется для местных кто не поехал сделать митап а материалов мало :disappointed:,
Но стало интересно где еще применяются,
"<@U0FEJNBGQ>  вроде как нет, в этом плане лучше алгоритмы вкоторые в hyperopt использовать",
"В субботу 1 октября будет ML-тренировка-зарешивание. 

Это будет последний день соревнования DCA с CIKM Cup, участники с Димой Дремовым будут дорешивать задачу до победного. Желаем ребятам успешно финишировать и не переобучиться на public-лидерборд! <https://competitions.codalab.org/competitions/11171>

Кто не участвует в DCA, могут собраться и начать решать другое соревнование. Например, очередной контест с фразнцузской площадки <http://DataScience.net|DataScience.net>: <https://www.datascience.net/fr/challenge/28/details>
(есть еще целый месяц, призовой фонд — 6000 eur).

Собираемся в ШАДе, переговорка Принстон. Схема прохода:<https://yandex.ru/maps/-/CZcsM8YJ>

Необходимо зарегистрироваться на мероприятие: <http://events.yandex.ru/surveys/3867/>",
"Возможно можно разложить каждый датасет на главные компоненты, и как фичи датасета использовать параметры сжатого пространства",
"какая польза будет извлекаться из факта, что датасеты X и Y ""похожи"" ?",
возможно кластеризовать и брать какие то характеристики по кластеру?,
"тут, как я понимаю, именно content based сходство нужно",
"а из самих песен какого рода фичи извлечены? ну BPM обязательно, вариация BPM внутри одной песни, как-то ритмический рисунок бы еще вынуть",
"Товарищи, я тут всё ещё с texture network мучаюсь. А когда мы инициализируем шум? Каждый batch? Или один раз при инициализации сети? И у нас 5 шумовых тензоров, которые разного размера, генерируются независимо или это  сжатый в разной степени один и тот же шум? У меня сейчас в каждом батче он генерируется заново, в каждом примере в одном батче свой шум, и 5 тензоров при этом генерируются независимо. Я что-то делаю не так?",
"Я как бы сбоку проходил, но по идее можно всегда разные",
"а какие есть задачки для rnn простые, чтобы можно было бенчмарки погонять? Что-то вроде binary add, чтобы тоже быстро считалось",
"Добрый день! Какие методы кластеризации текстов лучше всего себя сейчас показывают (тексты немаленькие, например, научные статьи)?",
Для меня lsi + kmeans давал очень похожий (или лучше) результат как и многие другие более сложные методы ,
внизу поста ссылка на новый AMI от амазона с этим как раз,
"у нас doc2vec работал не лучше, чем тупо нормализованная сумма word2vec

и да, а где же LDA-щики?) bigARTM и все такое",
"А все, кто не заказчик - вендор. По формату просто не прошел.",
"скормить граф как обычную матрицу в алгоритм обычной (предположительно метрической) кластеризации - так себе идея, если интересуют именно кластера~сообщества в графе",
"я читал статью, где пытались скрестить DeepWalk (вершину графа перевод в вектор) и Doc2Vec (текст в вектор). Там для каждой вершины делали конкатенацию векторов и запускали кластеризацию. Пока я сам не пробовал, есть сомнения насчет этого подохода. Может кто сталкивался с подобным?",
"пытались расхачить deepwalk для графов. получалось так сяк. делал <@U070Y25AS> 
еще <@U0QJG7WBA> наверняка пробовал в ОК запускать такое. как минимум на SNA хакатоне в этом году были пробовавшие выдергивать представление из графа",
"Кстати, а как вы оцениваете качество кластеризации текстов? И как выбираете кол-во кластеров, если оно заранее не задано?",
"Вопрос по статье <http://arxiv.org/pdf/1609.08144v1.pdf>. Кто может пояснить формулу 8  (страница 8) в статье? не понятно, что за множество Y ∈Y к которому принадлежат сгенерированные предложения.",
"<@U1K6XT2F8>, вопрос бесмысленен без конкретизации целей кластеризации и хотя бы минимальной формализации критериев по которым будет оцениватся результат. Потому как кластеризация ради кластеризации это бесконечный процесс генерирующий много фана, но нулевое value...",
кто из нас работал в дропбоксе? :troll:,
"речь, как я понял, не о ранжировании",
"как кстати распределение наблюдений по классам выглядит? более менее равномерно (по 300 на класс), или есть длинный хвост сверх-редких классов?",
"чат, а есть ли какие-то best practices по созданию фичей из времени события? 
например, из дня обычно хорошо работает фича вроде is_weekend или просто день недели. а вот именно время суток как правильнее использовать?",
самое тупое - час дня как категорию. С какой целью фича? Для регрессии можно рядов фурье занести с периодом начиная с суток и кратно меньше.,
"<@U2J47FFPV> именно тренировали?
где можно почитать про результаты?

а то вон даже технические компании мастодонты тренируют-тренируют, и все равно получается фигня",
"а обучение вообще было нужно? можно это как рекомендательную систему рассмотреть, но звучит так, что достаточно быстро было похожести считать и все",
"Т.е. я не предлагаю это как идею решения для Амазона.
Это был мой комментарий к фразе ""т.е. фактически пройти тест Тьюринга"".",
"ну, про нашего бота, обученного и выпущенного на двач, писали :troll: 
```Диалог ведёшь как бог. Я пойду за тобой на край света, анон. Это тот самый Двач, что я искал все эти годы. Я понял это только благодаря тебе. ```",
"С одной стороны -- нет, с другой -- это неметрическое пространство, так как неравенство треугольника не может быть выполнено.",
"<@U14GG4E69>  а вот кстати, расскажи как у вашего DetectNet получился receptive field 555x555?",
какой самый кошерный и шустрый пакет/либа для вычисления dtw? нужно как-нибудь посчитать попарные расстояния для 4х к рядов и не умереть от старости за это время,
У кого есть линки на сравнительно свежие статьи по network pruning?,
"<@U2CPWUFN0> в анализе соцсетей ты работаешь с графами как с интересным и насыщенным источником данных. визуализируешь, статистики клевые считаешь, фичи дергаешь. проблема в том, что алгоритмы работы с соцсетями - тема для отдельного курса",
"да, я смотрел это, но как я понял, он там сам дтв считает честно, он умеет как-то по умному прерывать вычисление при поиске ближайших последовательностей",
"Если кто найдёт torrent на open image dataset, или выкачает , сделайте торрент",
"Если не секрет, зачем нужен DTW?",
"<@U040HKJE7> OK. Я вот задумался, а как один запуск процедуры DTW можно распараллелить. Часть, связанная с применением динамического программирования, насколько я понимаю, не параллелится, т.к. на каждой итерации нам нужна информация из прошлых итераций. Возможно, имеет смысл сэкономить считая растояние между парой любых кадров в отдельном потоке перед проходом динам.программирования.

Или вариант для каждой пары временных рядов считать DTW в отдельном потоке",
"<@U1BAKQH2M> за считалку на куде спасибо, гляну",
"Прибор работает не известное заранее число времени, разное. Получаются временные ряды с дырами, где приробор выключен. Если их просто выкинуть, то возникают ""сезонности""",
"такой короткий вопрос: если я хочу заменить предсказать непрерывную переменную, какую мне архитектуру взять? FCN? Обычная бизнес-задачка, на которую обычно хгбуст пускают",
"Fully connected network, как я понял",
Просто есть и вариант расшифровки как fully convolutional network,
"Собственно первая статья по алгоритму EM называется ""Maximum Likelihood from Incomplete Data via the EM Algorithm - A.P.Dempster et.al.""
В названии написано  Incomplete Data: нам не важно по какой причине у нас данные не полные: мы так построили модель, что в ней имеются скрытые ненаблюдаемые переменные ИЛИ какие-то данные не были получены на стадии эксперимента",
"Чистый EM работает только в очень специальном случае, когда апостеорное распределение на скрытые переменные можно аналитически посчитать (впрочем, само распределение не очень интересно, главное знать некоторые его статистики)",
"<@U04ELQZAU> ""когда апостеорное распределение на скрытые переменные можно аналитически посчитать""

Когда было нечего делать я пробовал сам вывести то, что написал выше в двух примерах. Хотелось самому повторить результаты вот этой обзорной статьи:
<https://scholar.google.com/scholar?cluster=3607839237682679799>

Да, там везде вылезало апостериорное распределение скрытых переменных.",
"Не известно  про апостериорное распределение  какое оно вообще на самом деле, считается только MCMC. Есть ли смысл сначала сделать variational на смеси гаусиан например, чтобы просто посмотреть сколько мод там хотя бы? Что понимать куда MCMC вообще сошелся",
"А как ты по смеси поймешь, сколько там мод? ",
"Ну они, допустим, воткнутся. А как ты это детектируешь? ",
"ну где ""не нулевые"" компоненты смеси, значит там моды у истинного апостериорного, значит MCMC надо устроить так, чтобы мод было хотя бы столько же",
"Всем привет. Есть парочка вопросов по основам. 
Прохожу сейчас курс на курсере с Andrew Ng и вот там была часть в логистической регрессии, где для описания более сложной гипотезы они использую полиномы в степени N. И вот я не совсем понял, почему их полином выглядит именно так. Там они перемножают все составляющие для каждой степени по интересной формуле
```
degree = 6;
out = ones(size(X1(:,1)));
for i = 1:degree
    for j = 0:i
        out(:, end+1) = (X1.^(i-j)).*(X2.^j);
    end
end
```

То есть, например, для третьей степени получим `x1^3*x2 + x1^2*x2 + x1*x2^2 + x2^3`. 
Я задал на форуме вопрос, мол, а почему нельзя использовать `x1 + x2 + x1^2 + x2 ^ 2 + .... + x1^k + x2 ^ k % where k - degree`, на что ответили
&gt; In this exercise, we want to consider all of the quadratic combinations.

Но если честно, то я не до конца пойму, почему так",
"Логика примерно как в вычислении ковариаций.  Т.е. чтобы добавить взаимодействий признаков, надо их поперемножать",
"И второй вопрос по нейронным сетям: там на примере нейронной сети с уже подобранными параметрами они имитируют XNOR элемент. 
До этого они рассматривают создание AND и (!X1 AND !X2). Но там сеть без hidden layer, всё получается довольно просто. А вот в XNOR они добавляют этот hidden layer и в итоге мы имеем x1, x2 {0, 1}, hidden layer из 4 элементов и выходной элемент (unit/компонент, как правильно?). 

При этом c помощью этого hidden layer они имитируют два элемента одновременно: AND и (!X1 AND !X2) а после складывают их через OR. Вот когда смотришь на эти операции, то всё довольно просто и понятно, но как они дошли до такого понимания? Меня интересует, как они проектируют эти сети? Они же не от балды придумывают коэффициенты? То есть у себя в голове я представляю, как эти XNOR сделать, но как выразить это с помощью hidden layer и коэффициентов - хоть убей.",
<@U0AS548A1> а где можно прочитать больше теории на эту тему?,
"это отдельный геморрой, А найти все максимумы это вообще реальная задача? Не представляю как ее решать",
"как правильно посчитать максимальную скорость параллельного чтения с нескольких дисков? 
каждый диск выдает условные 150Mb/s, но когда их 10 воткнуто, то они же должны упереться в PCI шину, и как понять её максимум?",
"Можно взять любой учебник вводный, такой как Introduction to Statistical Learning by James,  Tibshirani, and Hastie и почитать _теории_",
зачем два раза одно и тоже читать?),
Смотря какая задача и сколько ресурсов у тебя есть. ,
"А как сделать так, чтобы в tensorflow во время шага оптимизации дополнительно какие-нибудь значения возвращались, например, функция ошибки? ",
"а расскажите кто что использует для evaluation doc2vec (или аналогичных) моделей, если не знаешь ни слова в языке, на котором тренируешь модель?",
"это кратно усложняет эту оценку чего-то, особенно если там тоже не очевидно какие объективные метрики качества)",
Ну допустим векторы используются для классификации как одна из фич. Если поменять векторы и качество улучшилось - то векторы тоже улучшились (по крайней мере для данной задачи). Мы так обычно делаем,
"Котаны, а кто нибудь визуализировал плотность чего нибудь на карте Москвы (трафик, население, количество шаурмечных, вотэвер) ?
Что нибудь такого типа, но для Руси-матушки
<http://farm9.staticflickr.com/8536/8694183111_c2c80516df_z.jpg>

Как вы это делали? (желательно на :python: )",
поэтому ищу какой нибудь быстрохак,
ну вот на следующем шаге после хексбина советую скачать какой нибудь шейп-файл по москве,
"данке, осталось понять откуда взять эти шейпфайлы и как все соединить :slightly_smiling_face:",
геопандас жрет как шейп так и геоджейсон,
"<#C2JSJ31L2|_meetings_nyc> угадайте, для кого канал",
"а миксить собственно как обычно, я так делаю:
```
fig, ax = plt.subplots(figsize=(15,15))
df1.plot(ax=ax)
df2.plot(ax=ax)
```
например",
"Добрый день. мне необходимо из под .Net дергать xgboost модели с objective = ""multi:softprob"".
Возникла идея реконструировать предикт (вероятности каждого класса)  по xgb.dump(), но я не очень понимаю как для мультиклассовой классификации переводить значение в листьях в вероятности принадлежности классу...",
"Доброе утро.
Я безуспешно пытаюсь обучить глубокий вариационный автокодировщик генерировать лица.
7 сверточных слоев сжимают RGB картинку 128x128 до 16x16, полученное представление подается в два последовательных FC-слоя, которые возвращают параметры распределения.
Симметричная штуковина используется чтобы реконструировать входную картинку. Везде используется BN+ELU. Обучаю  адамом со стандартными параметрами tensorflow.
Как бы  я не танцевал с бубном, оно упорно генерирует только черные квадраты.
Правильно ли я понимаю, что это бич вообще всех автокодировщиков и насквозь обучать глубокие архитектуры невозможно? Или это я что-то делаю не так?",
"Какое распределение у p(x | z)? то есть, как ты моделируешь наблюдения?",
"про wrapper спасибо. смигрируем  :nor:  . Хотя так как вызовов в секунду много,  изначально идея была сразу дерево спарсить чтоб не бегать за ним в python.
Да и в целом интересно - в дампе же все деревья есть а как из одного числа в leaf получить вектор длинной num_classes не ясно мне(.",
"<@U19VD9AQH> Я сейчас как раз его изучаю, среди библиотек элементов нет того, что бы меня на 100% удовлетворило, все под другие задачи просто. Но в целом неплохо, если найти или сделать палитру элементов.",
Да dot хорошо работает пока по-дефолту он сам располагает кружки так как ты хотел.,
"Привет, подскажите какие на сегодняшний день самые лучшие решения в сфере поисковых и рекомендательных систем? При чем имеется ввиду не алгоритм Google или Яндекс, а открытая платформа\движок или коробочное решение, которое продается? 
Ну например MatrixNet. Что еще есть в этой области?",
"привет, у меня есть дата сет с заходами пользователей на сайт, где два параметра session_time и user_id с начала года, чтобы для простоты я ее вынул простым запросом `SELECT session_time, user_id FROM x`. Подскажите пожалуйста, как можно посчитать по августу сумму уникальный айдишников за 30 дней предыдущих дней каждой даты этого месяца?",
я что-то думал-думал как их прикрутить и ни чего не надумал,
"<@U040HKJE7> дык это же весело. Особенно, когда бизнес метрики в проде взлетают после такого на 10-20% :oh-yeah:",
или еще какую экспертную рекомендашку без лернинга,
"&gt;  например, если брать клики и покупки товаров - там очень сильная зависимость, и извлечь пользу из такой постановки очень сложно

а как обычно по православному включают и клики и покупки товаров в рекомендашки?",
а может кто подсказать нормальный датасет по временным рядам?,
xgraph или как там его?,
"Когда мне нужно было делать картинку с архитектурой, я использовал <https://www.draw.io/>",
"Только вот с этим graph-tool стоит быть осторожным, т.к. там какая-то особая атмосфера в архитектуре. Кому-то пришлось всё переписывать на networkx после того, как он зашёл с ним в тупик. Ну и на винду он вроде как не ставится.",
"&gt; Ну и на винду он вроде как не ставится
А в разделе под мак, они советуют ознакомиться с <http://www.defectivebydesign.org/apple>",
"А почему кстати нет? Выглядит он мощно, дока с картинками, интерфейс простой и работает вроде как быстро, вот таки штуки там есть:
<https://graph-tool.skewed.de/static/doc/clustering.html>",
У меня после списка зависимостей всё опустилось. Плюс оно вроде как компилится час.,
и как я понимаю это затравка для <http://deeplearningcourse.ru>,
Так что вроде как apt-get и можно пользоваться,
"учу сетку предсказывать некоторую величину, расположенную от 0 до 1 примерно, лосс - mse, сеть стартует в какой-то сильно невыгодной точке, стартовый mse оказывается овер10к, и спуск к каким-то разумным значениям идет оочень медленно, как можно попробовать инициализироваться удачнее?",
"смотрю в сорс кераса и сходу не вижу на него ответа, но вроде как кажется, что внутри батча суммируется, а пишется средний лосс по всем батчам",
"а кто из авторов твой научрук, уж не Ратти случаем?",
"<@U2J47FFPV> <@U2D4XJL3A> <@U2KC9L5AR> спасибо, думаю, для начала - хватит
<@U2D4XJL3A> пока только начало научки, так что да, ""просто попробовать"" как раз",
"<@U1K6XT2F8>, если KPI при оценки кластера это близость по словам, то почему-бы сразу не строить кластеризацию на его оптимизации? Но скорее всего он, конечно, не единственный и не все так просто. Интересный, на мой взгляд, момент состоит в том, что кластеризация графа цитирований и кластеризация текстов проявляют разные аспекты ""структуры науки"". Граф цитирований хорошо раскрывает структуру коллективов, институтов, лабораторий, показывает лидеров мнений и т.д. Тексты статей показывают более общие области научного знания. Сложив все вместе мы можем потенциально найти группы которые работают в близких областях, но не знают друг о друге. И уже эту информацию использовать для выстроения новых связей.

Если смотреть на задачу в таком ключе, то я бы взял иерархический графовый алгоритм кластеризации (например, hierarchical affinity propagation), а затем рассмотрел кластера как документы и попробовал построить тематическую модель (например, robust LDA). И далее искал бы кластера между которыми мало связей-цитирований, но сильная семантическая схожесть.

Но у вас может стоять другая итоговая задача и тогда метод тоже может быть другим.

Если говорить конкретно про проблему разных терминов для обозначения одного и того же, то есть методы которые к этому толерантны (почти все генеративные модели это норм проглотят), с другой стороны, если выстроить явную таксономию и сделать словарь со связями (типа WordNet, но для науки), качество почти любой модели увеличится. В принципе, генеративную модель (тот же LDA) можно использовать и для того чтобы найти понятия схожего смысла (можно попробовать и автоенкодер сделать для слов, но, скорее всего, будет похуже).",
"а кстати как так получилось что в статье АлексНет 2012 есть дропаут, а статья по дропауту вышла только в 2014?",
"точно, а я чот вот эту открыл <https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf>",
"Друзья, привет.

<http://trec.nist.gov/data/reuters/reuters.html> - а вот этого сета нет ни у кого в виде торрента?",
<@U0QJG7WBA> Интересно услышать как ты предполагаешь использовать таксономию или дерево классификаций для улучшения моделей?,
"То есть определять какие слова в тексте на самом деле имена? И еще можно про канонизацию, а то в интернете только про святых :slightly_smiling_face:",
"ок, не просто имена, а даже к какой категории они принадлежат и замена слова на эту категорию",
"Построение таксономии делается вручную, как я понимаю? Насколько это вообще реально сделать для той же науки, да хотя бы одного направления науки?",
"да, решили провести семинар для начинающих. 
там совсем с нуля расскажем как работает классификатор на модели inception v3",
Инрепритируем как вероятность и дальше методом макс. правдоподобия,
"Стоп, какую вероятность? Ты знаешь, какой оптимизационной задачей решается ядерный свм? ",
"&gt; Можно повторить вывод лог. регрессии с ней
&gt; Нельзя
&gt; Инрепритируем как вероятность и дальше методом макс. правдоподобия",
а еще можно где оставить отзыв? -),
"А, svm как и регрессия это же такой линейный классификатор",
"Ничего не понимаю тогда. Ты спрашиваешь, какие различия между логистической регрессией и свмом с таким ядром, различие в том, что у первой разделяющая поверхность линейна, а у второго уже нет",
"А есть кстати какие нибудь резоны использовать ""лог."" регрессию не с лог. кривой ?",
"Вон чего устраивают: <http://www.magesblog.com/2016/10/notes-from-4th-bayesian-mixer-meetup.html>
Когда и мы начнем?",
"Какой ужас. Сначала пишут что хотят, а потом удивляются почему весь сайт целиком превращается в помойку и его никто не читает",
"А как люди чинят гадкие паттерны, которые возникают при генерации изображений с помощью транспонированных сверток?",
"Это-то понятно, меня смущает то, что ни один из порогов не позволяет мне хоть как-то изменить результаты классификации. Более того, такой результат даёт как одна модель, с параметрами, выставленными вручную, так и модель оптимальный набор параметров которой выбирался из 300 рандомных комбинаций.",
а почему вообще хгб а не что то попроще?,
"Погоди, а зачем тебе пороги?",
"А сейчас получается так, что куда ни выкручивай этот порог, результат классификации не меняется.",
"Это, кстати, к вопросу о том, почему бы не попробовать более простые модели.",
"<@U04ELQZAU> а можно ссылку на записи?
И как часто бывает семинар? Как-то я совсем упустила...",
"подскажите, пожалуйста, как сводить обычную задачу классификации (например, через сверточную сеть) к задаче подсчета сколько объектов какого класса представлено? например, подсчитать сколько мужчин и сколько женщин на изображении, либо сколько раз человек подпрыгнул/присел судя по сигналам с акселерометра.",
"Разные есть подходы. Можно сразу классифицировать число (что несколько утопично), можно через подсветку и подсчет получившихся компонент (считай -- локализация, про это как раз статья по ссылке), а есть еще с attention'ом и lstm'ом красивый монстуозный подход -- <https://arxiv.org/pdf/1605.09410.pdf>",
"с сигналами все проще намного, поидее lstm обычной хватит
как мне кажется картинки и сигнал — две разные задачи",
"посмотри как примитивные подходы для поиска объектов работают, там берут окно и бегают по всему изображению, перебирая все комбинации",
"ладно. скажу, что сигнал трехмерный (в принципе так и есть в случае ""акселерометра""(IMU)).  а если взять несколько IMU, то уже в какой-то мере как изображение получается, засчет большого количества измерений пространства :slightly_smiling_face: как никак близкие задачи)",
"В сигнале есть выделенная ось -- время, а сколько каналов -- как правило не важно.",
"Всем привет! Хотим хранить логи в колоночной базе, конкретно — Redshift. Логи имеют общие поля и отличающиеся. Если кто сталкивался с организацией архитектуры хранения, дайте совет. Логов довольно много, по ним делаем частый анализ данных.",
интересно было бы почитать их engineering blog про то как они решают реальные проблемы связанные с медленной вставкой в индекс и т.п.,
<@U17S9F6KV> а анализ какого рода? поиск? группировки? какой-нибудь count distinct? какое желаемое быстродействие (т.е. время ответа)?,
"(мимокрокодил) видел, как ребята делали широкие таблицы и использовали clickhouse для таких целей, вроде довольны, но деталей не знаю",
"Что обычно делают, когда в регрессии целевая переменная содержит выбросы?",
"Если мало, то после пристального рассмотрения как правило выбрасывают.",
"а оценивать качество в таком случае как лучше? не rmse, а mae?",
"Во, актуальный вопрос, поясните, как оценивать качество регрессии в разных случаях",
"когда r^2, когда mae, mse, rmse?",
в смысле взять логарифм от целевой переменной - когда это обычно используют?,
R^2 — когда ты трепетная девочка-гуманитарий и анализируешь данные опроса по 30 респондентам.,
"логарифм - твой выбор :slightly_smiling_face: с другой стороны, какие перед моделью стоят условия? кто и как ей будет пользоваться и как будут решать, хорошо она работает или нет?

потому что предсказывая логарифм, задача модели - наилучшим образом выучить лог-порядок данных",
"я вот думаю, как оценивать качество модели, если использовать логарифм",
"вообще про оценку результатов хорошо было бы поговорить, вот я например сейчас делаю сорт оф метрик лернинг: учу сеточку предсказывать некое расстояние между двумя обьектами, притом считать это расстояние точно я умею, вот как мне корректнее всего проверить бы качество этой специфической регрессии?",
"сегодня понял, что не понимаю, почему у евклидового расстояния проблемы с градиентом",
"хм, продифференцировал руками и вроде grad(||.||_2) = x/||x|| , ну тогда в окрестности нуля его шатать должно в зависимости от того, с какой стороны подходить (ну в одномерном случае там вообще +-1) получается",
"гайсы, подскажите такую штуку. задача регрессии, есть дата (представлена годами). я хочу юзать целевую переменную предыдущих периодов. в трейне - 6 годов, в тесте - 2. как это правильно сделать? я предполагал, что нужно делать сдвиг на два года, типа если у нас есть данные за 2009-2014, а 2015 и 2016 в тесте, то нужно показатели за 2014 всунуть в 2016 и т.д. но мы тогда потеряем 2009 и 2010 + непонятно как обучаться, потому что теряем 2 года",
"<@U2194SMBM> на фесте даже показали, как такой бот обученный на слаке выглядел",
"<@U0B00V3K2> даже я получаю результаты на glove зачастую лучше чем w2v 
но что значит лучше? это я проверяю опосредованно в задачах классификации или регрессии где среди всего остального есть и тексты
я так понимаю что не все так просто с оценкой качества именно векторизации слов",
"это один из способов. есть рекурсивные прогнозы, когда модель только T+1 умеет, и для шага в T+2 используем прогноз на T+1",
"да, короткий. 
навскидку совершенно непонятно, кто будет лучше - объединение краткосрочной и долгосрочной, или одна краткосрочная рекурсивно, т.к. краткосрочная будет на больших данных обучаться",
"можно попробовать будет три модели сблендить, как вариант) буду все пробовать, спасибо!",
"<@U041LH06L>, а ты, кстати, знаешь, почему 30? я встречал только одно объяснение, но оно... такоэ...",
"Не помню, так глубоко не копал. В те времена, когда это было актуально, бигдаты под рукой все равно не было.",
"Товарищи, а как связаны между собой `scale_pos_weight` в `xgb.train` и `weight` в `xgb.DMatrix`?",
"Когда нужно знать на сколько ты порядков ошибся, а не в абсолютных значениях.",
"как раз хотел узнать, не хочет ли кто в их челлендже поучаствовать, там призы даже есть",
"Как уже выше говорили, для бабла лучше подходит. Оно неотрицательное и с большими выбросами вверх. Как правило, логарифм выглядит как нормальное распределение.",
"А что насчет интерпретации этого дела? как понять, 0.33 это много или нет?",
и вопрос в том как считать расстояния между внутренними представлениями - l1 или l2,
"А с какой целью оно считается? Если для минимизации, то L2 можно в квадрат возвести",
"как такую метрику объясняют людям, далеким от МЛ и статистики?",
чтобы использовать внутренние репрезентации как интерпретируемый эмбеддинг,
"не, не самой метрики, предсказаний и целевой переменной, перед тем, как rmse считать",
"кстати, обычно на практике используют натуральный логарифм или десятичный? мне интересно, как бизнес-люди относятся к натуральным логарифмам",
"Коллеги, 
Есть задача: из текста новости извлекать кратко событие в виде ""Субъект-действие-объект"". Весной мы со студентами собрали набор текстов и навернули эвристик. Как-то это работает. Но не очень. Вопрос, куда можно еще покопать? Есть идея обучить какую-то сеть на результатах работы этой эвристики, а потом искать ошибки руками и дообучаться на исправлениях.",
"Я нашел PubMed датасет <https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/>
В нескольких статьях (напр. <http://www.sciplore.org/wp-content/papercite-data/pdf/gipp15b.pdf>) видел, как кластеризацию оценивают на основе MeSH (кейворды для медицинских статей). Отличаются от обычных кейвордов тем, что их ограниченное количество и существует иерархия.
Кто-нибудь работал с этим датасетом? Насколько эти кейворды адекватны?",
"У меня тут странноватый вопрос про Adjusted R^2. Википедия и мои воспоминания о лекциях говорят, что «Adjusted R^2 лучше, чем R^2, потому что он учитывает сложность модели и позволяет сравнивать вложенные модели» (<https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2>), т.е. не страдает, как R^2, от безвыходного увеличения при добавлении предикторов.
Но `Adj.R^2 = 1 - (SS_residual / (n - p -1)) / (SS_total / (n -1))`, тогда как `R^2 = 1 - SS_residual / SS_total`. Вики говорит, что это можно рассматривать как замену смещённых оценок дисперсии на несмещённые.

Отсюда вопрос: если мы подгоняем линейную регрессию, то как при добавлении предикторов дисперсия остатков от предсказания может увеличиться (чтобы подсчёт несмещённой её оценки продемонстрировал обещанные преимущества Adj.R^2)?
Или по-другому: как может быть так, что у линейной регрессионной модели по подмножеству предикторов Adj.R^2 больше, чем у модели по всему множеству предикторов?",
Привет. Где же можно найти размеченные датасеты на русском? Все никак не могу найти чем обучать модель для кластеризации текста.,
"Но как он может уменьшиться — я не понимаю, ведь дисперсия-то не увеличится.",
"Где те дяди-статистики, которые говорят, что уменьшается?",
"Выше спрашивал, повторю еще раз: может у кого есть датасет от Тинькофф банка с хакатона по чат-ботам?",
"Так это adjusted. Её специально аджастят, чтобы она не росла всё время, а адекватно показывала момент, когда уже хватит добавлять фичи. То есть, исходная R^2 в этот момент упирается в потолок, а скорректированная начинает потихоньку падать за счёт коррекции.",
"<@U22LWR1TJ>: как adjusted r^2 может *падать*, если там стоит (и единственная меняется при добавлении фичей) несмещённая оценка дисперсии остатков от предсказания, а дисперсия остатков от предсказания (и, следовательно, её оценка, даже несмещённая) при добавлении любых фичей *не увеличивается* (а для того, чтобы adjusted r^2 уменьшился, нужно, чтобы дисперсия остатков увеличилась)?",
"Историю про поправочный коэффициент я слышала, но а) не смогла смоделировать такое поведение: какой бы шум или коррелированные данные в каком бы количестве не добавлялись, adjusted r^2 не убывает, и б) объяснение того, что ""поправка перевесит"", противоречит тому, что эта поправка является только поправкой на несмещённость, а не штрафом.
Поэтому и спрашиваю, кто неправ: я (тогда где?), википедия или легенды об исправленном коэффициенте детерминации.",
а библиотека libnn.a/libnn.so где лежит?,
"так как <#C0SGCGB52|career> у многих на mute, думаю, стоит продублировать сюда. Тема - time series и digital signal processing",
"В пятом курсе специализации МФТИ на курсере есть чуток про анализ time series, как раз на выходной",
"Подскажите, пожалуйста, как решать следующую задачу:
имеется набор пользователей (порядка 10^4). Для каждого пользователя есть история в виде: *состояние-&gt;действие*. Всего состояний очень много, но их безусловное распределение ""сконцентрировано вокруг среднего"".
Задача: для заданного (нового) пользователя мне нужно считать степень похожести его на всех остальных по мере получения информации о нем.",
"когда писал свой велосипед, про openregex ничего не знал :slightly_smiling_face:",
"просто знаю человека, который на этой теме защитил кандидатскую уже наверное как более 10 лет тому назад. В том числе с реализацией DFA.. жаль реализация коммерческая. Сегодня они наверное более 30 типов семантических паттернов выделяют. Да и имеют официальный патент США на выделение связей типа ""субъект-действие-объект""",
"не могу найти презентацию, где там целые большие таблицы с примерами таких отношений",
"<@U0DA4J82H>: для себя я тот факт, что логнормальное распределение особенно часто встречается в бизнесе, объясняю так примерно:

1. Оно приближает пуассоновское с большой лямбдой, включая overdispersed, типа бетабиномиального, а те, в свою очередь, хорошо моделируют т.н. count data, т.е. количество событий в единицу времени. Поэтому если у тебя измеряется какая-нибудь выручка, или объемы торгов, или что-то еще, что зависит от числа транзакций, которое при этом достаточно велико, то шансы, что будет близко к логнормальному, сами по себе велики

2. Если же говорить вообще о деньгах, не привязанных к количеству, например, о ценах, то есть такой психологический аспект восприятия: человек мыслит в терминах ""дороже на 10%, дешевле на 10%"", а не в абсолютных цифрах, поэтому бизнесы, наверное, подстраиваются. Почему человек мыслит именно так - ХЗ, наверное, потому что это инвариантно количеству/весу/объёму товара, который может хотеться купить

Соответственно так и предлагаю трактовать RMSLE - на сколько процентов возможна ошибка (правда, проценты вверх и вниз считаются чуть по-разному, что может путать, но можно сразу их показывать для упрощения восприятия)
",
"<@U0ZHHV83C>: для начала надо ответить себе на вопрос: действительно ли важен порядок прохождения состояний или только количества прохождений и значения атрибутов состояний или действий. Во втором случае отлично подходит заворачивание этих количеств/атрибутов в фичи и дальше юзай кластеризацию с любой метрикой, которая понравится, хоть косинус, хоть LDA, хоть разложение матриц и проч. - в зависимости от типа атрибутов, если они есть

Могу назвать два затыка, которые придётся решить:

1. Если у тебя есть циклы в графе, и некоторые состояния у отдельных юзеров, бывает, очень часто встречаются, такие аутлайеры. Тогда помогает их обрезать до какого-то разумного максимума типа двух-трёх сигм, или логарифмировать, или измерять неравными интервалами

2. Если некоторые состояния ""важнее"" других. Ну, типа, хочется, чтобы те, кто сделали покупку, были в одних кластерах, а те, кто нет - в других. Тогда лучше их на этапе предобработки разделить на разные датасеты. Если же хочется как-то помягче, можно после нормализации данных поумножать их на коэффициенты в зависимости от ""важности"" фичи - многие метрики, включая косинус, чувствительны к масштабу

А в первом случае все сильно сложнее... Я пока у себя не нашёл алгоритм, который показывает лучшие результаты, чем если принять предположение, что порядок не важен, но зависит от датасета, наверное, или искать нужно дольше... ",
"&gt;""дороже на 10%, дешевле на 10%""
Говорят, что это связано с тем, что для наших предков опасность от +1 хищник возрастала больше, когда их изначально было 2, а не 100.",
"Кто-нибудь встречал работы, в которых исследовалось быстродействие какой-нибудь распределенной реализации k-means в зависимости от 1) количества процессоров 2) объема данных? Желательно, чтобы количество процессоров исчислялось как минимум десятками.",
"&gt; Кто-нибудь встречал работы, в которых исследовалось быстродействие какой-нибудь распределенной реализации k-means в зависимости от 1) количества процессоров 2) объема данных? Желательно, чтобы количество процессоров исчислялось как минимум десятками.
Кажется, это первая домашка ШАДа по парпрогу)",
"В спарковской статье, к сожалению, нет зависимости времени работы от количества процессоров. Как я понял, там больше про инициализацию алгоритма",
"Привет всем! Кто-нибудь знает как в xgboost вытащить impurity? Или аналогичную свободную библиотеку где это можно сделать? В sklearт можно, но качество обучения не устраивает.",
"&gt; после обновлений
Подразумевает что раньше работало, тогда непонятно почему проблема в железе :)
У меня чтото похожее было, решилось установкой последней версии драйвера",
Ребят -- а как вы выбираете метод оптимизации себе?,
Пробовал ли кто добавлять шум к градиенту?,
"Просто SGD требует графика для lr, а как его собирать? Адам в этом смысле приятнее, что его пустил -- и жди, особенно если учится долго.",
"Есть ли случай (и много ли), когда SGD был лучше Adama?",
"хм, я пихаю в рекуррентные штуки рмспроп(умные дяди вроде хинтонов говорят ок), можно на пальцах, какие подводные камни? чем простой сгд лучше чем спуск с адаптивным лр по параметрам?",
"Подскажите, какие еще есть действенные способы работы с пропущенными значениями кроме 1) заполнения пропущенных значений -1; 2) создание отдельной фичи, в которой 1 указывает, что значений в такой фичи пропущено?",
"Можно заполнять их на основании значений у тех объектов, где они не пропущены: медиана, среднее, и т.д.",
"Когда учил resnet'ы для авито конкурса, sgd с nesterov momentum был сильно точнее Adam's. ",
"А почему нельзя просто менять lr, когда точность/loss перестаёт меняться? ",
"Как я понимаю, это неправославно. Нужно откатываться до предыдущей лучше -- и от нее начинать спуск с новым lr. При этом вопрос -- а сколько итераций нужно сделать ""в минус"", чтобы остановиться? 1 -- неробастно, 10 -- расточительно.",
и поэтому же непонятно зачем откатываться,
"Поэтому имеет смысл откатиться назад, чтобы меньший lr случился когда эти шаги уменьшения train err (но не уменьшения val err) еще не сделаны",
"в предположении оверфита - ок. Но это как-то на грани, когда мы видим, что кривая может и вверх пойти. На практике крутые пацаны нормально все регуляризуют и аугментируют и меняют lr тупо по насыщению. Нахождение минимума во флуктуациях и последующий спуск от него - это бессмысленно",
"да и вроде как умею делать w2v в генсиме, хз насколько полезно",
<@U1BAKQH2M> а где все проходить будет?,
"если у кого есть все 160 гб, я бы может даже по старинке переписал на диск (или кто-нибудь поможет с раздачей :))",
"<@U04BFDYPV> а можешь в кого-нибудь в яндексе ткнуть, чтоб задумались над автофиллом встроенным в форму регистрации на мероприятия? а то уже чот надоело каждую неделю одну и ту же инфу о месте работы и учебы забивать, + если вы собираете данные о посетителях - наверное логичнее иметь одну стабильную анкету  на одного человека и линковать ее к событию, а не вводить данные каждый раз вручную и потом их как-то матчить",
Какими методами кластеризации можно сделать стратификацию выборки?,
"<@U1CEDPJSU> говоря о стратификации, мы обычно понимаем разбить фолды для обучения, чтобы пропорции классов были одинаковыми (очень важно когда есть редкие классы). тут не надо ничего кластеризовать и обучать",
"<@U040HKJE7> с тест сетами было бы круто, но кто ж их даст)",
"Вот как здесь, например — <https://www.kaggle.com/c/axa-driver-telematics-analysis/data>",
"где можно взять словарь частотностей русского языка, построенный на большом корпусе? хочется tf/idf внутри одного документа посчитать",
"можно как rf, только раундов побольше обычно надо",
зачем вообще ET нужен на практике?,
"а расскажите, когда есть смысл вообще пихать дату в рф или рт а не в хгб? я правильно понимаю, что в сильно шумных данных с не очень сложной структурой всякие методы на бэггинге могут быть лучше?",
"<@U1BAKQH2M> думаю, да. еще параллелить RF легко, хотя кого это волнует",
"<@U24FHECDD> наверное в теории можно представить какие-то странные случаи, когда тебе нужен предикт с использованием многих ядер или там быстро обучаться на большом датасете, но на практике скорее не важна",
"сейчас заметил, что он плохо обучается на фичах, где много категорий. xgboost их лучше понимает",
"скорость обучения тоже может быть важна, когда параметры давно подобраны, и надо просто засунуть в модель новые данные (опять же прод)",
Можешь попробовать как в bnp сделать mean target value,
"я до сих пор не понимаю толком, почему это работает, но мне во французском конкурсе помогало, пока я его делал",
"Еще, кстати, можно decision tree на одном атрибуте запустить, оно скажет, как лучше всего данные разложить",
"А MCMC не должен сходиться, если долго-долго крутить? Я к тому, что не получится ли у тебя все максимумы получить с одного сида? По крайней мере, не встречал до сих пор, как кто-то усредняет результаты разных прогонов",
Ну и как совместить два яндексовских  события в одну субботу?,
"А ты запускаешь в несколько потоков, на графики смотришь, как сходятся?",
"Да, смотрю как среднее сходится и какой уровень отказов",
"Коллеги, а никто не поделится скриптом на python, как в bigartm правильно выделять фоновые темы и их тюнинговать?",
"на том же кагле постоянно митапы проскакивают, в европе тоже много где бывают",
тоесть рассказывать некому и особенно не для кого,
"```Катерина 12:39
Я тут в Токио тоже пытаюсь встречи про ML делать)
Только туго идет - уровень как-то гораздо слабее питерского

Катерина 12:41
Да! А представь, когда на встрече ML Society в гугле из 20 человек, к которым подходишь и спрашиваешь чем они занимаются - только 1-2 реально делают ml - остальные потусить пришли

Катерина 12:46
У меня в митапе уже больше 400 человек, да только вот спикеров не найти))
```",
"странно очень, я часто на посты на японском натыкаюсь, когда какие-нибудь адовые штуки начинаю гуглить",
"допустим у нас есть датасет и для него известно как в конечном итоге должны выглядеть кластеры. Есть какая-то метрика, которая оценивает качество кластеризации. Тут скорее хочется узнать, стабильно ли хорошо работает алгоритм или какое-то изменение в данных (например, добавление новых объектов) может ухудшить его работу",
"если известно, как должны выглядеть кластеры, то это задача классификации",
"Все наверное знают, но я чот только наткнулся на курс (и ресурс) <http://videolectures.net/mackay_course_13/>",
"Тут скорее другой случай. Нет жесткого разделения на классы в классическом понимании. Есть некоторые предположения о том, какие объекты похожи, а какие нет. И на основе этих предположений строится какая-то эвристика, с помощью которой можно примерно оценить адекватность кластеризации.",
"можно вообще сделать иерархическую вероятностную модель, где искомый класс будет зависеть от предположенного вашей эвристикой, как и от фич, а эти предположения будут определяться своим распределением для каждого наблюдения",
А почему оно в блоге мейла?,
ну а почему бы и нет -),
"<@U27MC8J2C>, спасибо большое. Я видел эту презентацию и как отдельно тюнить одни топики от других понятно. У меня вопросы в другом. 1) как правильно выбирать кол-во фоновых тем (да и реальных топиков тоже) 2) как их потом правильно тюнить кроме того, что фоновые надо сглаживать, а остальные разреживать. Это я уже и в доках прочитал, но best practice правильного скрипта на эту тему так ни у кого и не увидел (( Буду очень признателен, если поможете с этим",
"У Воронцова где -то звучало, что сперва берём кол-во тем с запасом, а потом после анализа оставляем сколько надо. Но как???",
"обьясните для не очень умных: почему при использовании clipnorm сетку разрывает нахрен и лосс в инф улетает за пару итераций, а если брать clipvalue - все спокойно-годно?",
"Паш, а кто текст редактировал? Или у тебя времени на шлифовку было больше?",
"<@U040M0W0S>  как раз мейл и редактировал, иначе там щас коментах :grammar: набежали бы",
ну как будто и стиль подправили. редактор там новый что ли,
"не, слиль править им нельзя, если это не рекламный пост какой то, только если попросишь",
"орфографию мне вот помогает плагин в хром выправлять, но с синтаксисом и пунктуацией беда пиздец, мне когда док файл присылают с коментами, там в каждом предложении 2-3 ошибки -)",
"<@U2194SMBM> очень странно, что это везде преподносится, как преимущество на тем же Дирихле, но нет ни единого примера нигде, как с этим правильно работать ((",
"Коллеги, всем привет. Прощу прощения за длинный вопрос.
Столкнулся с рядом вопросов при реализации рабочей задачи. Т.к. здесь немало профессионалов в NLP, то думаю, кто-нибудь подкинет мыслишку.

Итак задача:
 - Дано: экспертами заготовлены несколько категорий (пока 3), в каждой - сет из (пока) ~50 пар вопрос-ответ (категории: велосипеды, мотоциклы, авто);
 - Цель: на основе указанного сета построить вопрос-ответную экспертную систему, которой юзер может задать вопрос произвольным текстом. Отвечать, разумеется, нужно одним из заготовленных ответов.

С виду rocket-science'а здесь нет, поисковая задача, тем не менее есть трудности:
 - точность сопоставления начинает хромать при добавлении большого числа категорий и пар вопрос-ответ.
Что уже есть:
 - базовый матчинг: stemming, bag-of-words, косинусной мерой сопоставляем вопрос юзера и вопрос из базы; выбираем максимальный по мере.

Главный вопрос: как можно увеличить точность?

Мои мысли здесь (и тоже открыте вопросы):
 - как-то (?) научиться определять главные слова в вопросах пользователя и вопросах из базы.
 - синонимизация слов. Тоже вопрос - как бы этот процесс автоматизировать.
 - попытаться определить категорию, чтобы заостриться в рамках неё. Для детекции категории приходится выбирать вручную ключевики типа ""велосипед"", ""мотоцикл"", ""машина"" и т.п. Но они не всегда есть.
 - что делать с ""развёрнутыми"" вопросами? В слишком длинных вопросах труднее отыскать ""зерно"".

Что можете посоветовать?",
"и вопрос в том, как уменьшить влияние пропусков",
"Угу. Начало всегда хорошее. :slightly_smiling_face:

&gt; lucene/elastic search взять, там хорошие реализации в коробке есть
lucene большой, какие функции из коробки ты имеешь в виду? использовать как поисковую систему в целом?",
"слушайте, я пока не нагрепал, тут как-то обсуждали как делать демона на TF, который постоянно активен и пушить ему таски",
"как это вообще называется, чтобы загуглить",
"Главное, чтобы особенность пользователей не коррелировала с пропусками. Например, если система предсказывала социофобов, глухонемых или тех, кто сейчас в международном роуминге.",
"<@U2BFSKNDP> для синонимов можно попробовать word2vec близость слов смотреть, плюс есть словари синонимов. Для определения категории можно попробовать натренировать классификатор на текстах данной тематики, если они у вас есть.
Еще, как идея - если цель быстро найти ответ на вопрос, то может быть будет лучше это оформить не как чат-бота, а как поиск, который работает по мере набора вопроса и имеет возможность вывести несколько вариантов? Все равно будут вопросы которые будут неправильно обрабатываться, но когда поиск вывел какую-то фигню, это проще принять чем когда чат-бот ответил не на мой вопрос.",
"&gt; для синонимов можно попробовать word2vec близость слов смотреть
Спасибо, попробую. Мысль была, но не испытывал, отмёл пока, т.к. боюсь, что для w2v маловато текстов будет (у меня их пока всего 150, как я говорил, расширяемо до ~1K).
&gt; словари синонимов
ОК, уже учёл, спасибо.
&gt; натренировать классификатор на текстах данной тематики, если они у вас есть.
И я это делал. Проблема в том, что тексты очень короткие. Это ведь вопрос, по объёму он редко тянет на статью или хотя бы на абзац. Здесь нейронки выглядят многообещающе (от чего я пока весьма далёк), но знаю, что для них текстов нужно нарыть большую кучу. :slightly_smiling_face:
&gt;  вывести несколько вариантов?
О да, это было бы замечательно - перенести часть ответственности на энд-юзера! Однако ж требования к проекту таковы, что надо ответить чем-то одним.",
"Только надо посмотреть в каком виде они в модельке лежат, может быть придется POS метки прибавлять к токенам.",
"Подскажите, пожалуйста — вот есть у меня ключевое слово и описание к нему. Допустим, таких описаний у меня 100, они все разные. Большая часть из них релевантна ключевому слову, но встречаются такие, которые нерелевантны (пример — синонимы). Условно, есть ключевое слово ключ, и в 90 случаях он описан как инструмент (гаечный), а в 10 — как источник воды. Каким безболезненным способом можно отбросить эти 10 описаний?",
Он требует объем данных примерно как w2v,
Пожалуйста. Немного рекламы: у меня будет доклад на HighLoad++ где я хочу обобщить подходы в диалоговых системах. Вопросы-ответы тоже как часть диалоговых систем получается.,
А когда и где это будет? <@U0XR20SA1> ,
"Расскажите, что есть новенького в борьбе с imbalanced dataset? Про какие новые методы стоит почитать/попробовать?",
Ну и до кучи у кого что хорошо работает  из классики? weights/undersampling/oversampling/outlier detection/cost sensitive/ensembles,
С какого метода обычно начинаете?,
я в общем спрашиваю - что нового есть и какой личный опыт,
"Да, мы действительно пытались разработать “рекомендательную систему” для прогнозирования того, какой метод балансировки лучше всего сработает на конкретном датасете на основе свойств этого датасета. Там обнаружились всякие интересные закономерности. Но для решения этой задачи оказалось, что в первую очередь важно подбирать параметры балансировки. В том же SMOTE в зависимости от того, сколько накидывать новых наблюдений можно получить совершенно разные результаты :slightly_smiling_face: Так что эта работа пока подвисла в том числе и по другим причинам.",
"Есть вот такой пакет, если никто еще не скидывал, где реализовано много методов балансировки <https://github.com/scikit-learn-contrib/imbalanced-learn>",
то есть на большом числе задач применение методов балансировки как мертвому припарки :slightly_smiling_face:,
"еще смотрели undersampling и комбинации, когда и то и то",
undersampling тоже иногда работает неплохо как ни старнно,
с трудом представляю где можно взять сотню датасетов что бы ещё unbalanced были,
"это не так интересно. интересно в таких задачах, где это естественный процесс",
"да, как раз робастность достигается за счет то, что на каждом шаге новая искусственная часть выборки",
"У нас его нет. Да и они, как я понял, используют его поскольку постольку. ",
"<@U0AF2AZCM> тебе нужно какой-нибудь экзотики типа Hellinger Distance Decision Trees что ли? :slightly_smiling_face: Есть классная книга, полный обзор современного state of the art (по состоянию на 2013 год) IMBALANCED LEARNING Foundations, Algorithms, and Applications.  Она небольшая, всего страниц 200 (и половина референсы)
Ещё вот тут у чуваков классный обзор, в частности, обзор разных ""видов"" несбалансированности -- когда меньшего класса просто мало, но он обладает какими-то хорошими отличительными признаками и когда он слишком рассеян по датасету и как с этим быть <http://sci2s.ugr.es/imbalanced>
Ещё свежий обзор: <http://dl.acm.org/citation.cfm?id=2907070> (опубликовано в августе, но материал до середины 2015, пробежался по диагонали -- вроде норм)
и такой прямо последний-последний  результат про метрики качества для моделей <https://arxiv.org/abs/1608.08984>",
"Всем привет!
В своем вузе в качестве проекта по защите информации хочу сделать атаку на нейронную сеть, как это делают в этих статьях:
1) <https://arxiv.org/pdf/1412.6572v3.pdf> ""Explaining and Harnessing Adversarial Examples""
2) <https://arxiv.org/pdf/1602.02697v2.pdf> ""Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples""

Есть пару вопросов:
1) Кто-нибудь уже делал похожее? Или может знает, кто делал?
2) В качестве демонстрации метода хотелось бы сломать какую-нибудь открытую сеть. Никто не знает интересный пример, что можно сломать?

Спасибо за внимание :slightly_smiling_face:",
"коллеги такой вопрос. 
вот есть классификатор обученный распознавать печатные буквы.
как его можно обучить так, чтобы он для склейки fW не возвращал 90% confidence что это буква M? 
кроме варианта добавить такую склейку в обучающую выборку.
или все таки добавить мусорный класс к датасету, куда относить все такие аутлаеры?
вобщем чтоб символ потенциально не из словаря вообще не попал на классификацию или для него была очень низкая confidence 
какие будут мысли?",
"Не, не затестил, но оно делает как раз то, что нужно было чуваку, который это всё начал -- автоматом ищет вражеские примеры и заодно может с их помощью доучивать сеть, чтобы она больше не реагировала на них",
я чот пытаюсь запредикить ряд аримой из statsmodels,
"Коллеги, всем привет! 
<@U040HKJE7> <@U0471Q5AU> спасибо за приглашение!
Рад присоедениться к вашему уютненькому :slightly_smiling_face:

Я совсем зелёный в вопросах дата-сайнса, поэтому не судите строго за мои, пока что ещё глупые вопросы. Прошу чуть-чуть подсказать по математике возможными вариантами, куда смотреть порекомендуете начать.

Сейчас мы в нашей небольшой команде устроили внутреннюю игру-соревнование между друг другом. Кто за ограниченное время (этих выходных) реализует более точный алгоритм.

*Задача такая*: на тестовой выборке небольшое количество данных:
– 164 события. Исход: произошло/не произошло; дата.
– Вероятность которых предсказывали наши пользователи (587 пользователей в тестовой выборке),  – Всего 6477 ответов (мнения пользователей о вероятности событий)

Контрольная выборка будет в 3 раз больше тестовой. 

Тупое среднее арифметическое мнений толпы даёт 76% точность предсказания событий.
Эту точность хотим повышать, считая индикатор толпы поумнее чем средним арифметическим :slightly_smiling_face:

*Вопрос*: какие мат.модели порекомендуете зелёному парню, что бы за пару дней выходных их попробовать можно было успеть в игровом режиме?)

Зелёный настолько, что не программировал до вчерашнего дня 5 лет, а опыт гугления есть) 
Вчера за 3.5 часа освоил базовые основы синтаксиса Python, посчитал веса доверия пользователям по их личной точности предсказаний (количество правильных предсказаний/количество предсказаний пользователя) и начал считать общий индекс с применением таких весов, этим повысилась точность с 76% до 77.8%",
"<@U2PL3TD5X> выложи данные в открытый доступ, мб чтонибудь понасчитаем даже. 
какие и где у тебя есть данные? у тебя просто и ""тест"" и ""контроль"" и непонятно как ты 76 и 77.8 получил (как ты делил тренировочные данные, мне почемуто кажется что ты без валидации это все считал).",
Привет! Кто из сообщества сегодня тут?<https://events.yandex.ru/events/meetings/15-oct-2016/>,
"<@U0FEJNBGQ> с общим пайплайном +- разобрался. А в какую разумную модель для предсказания рядов можно пихать дополнительные фичи? Вот есть у меня значения ряда за последние n точек, в каждой точке еще какой-то вектор фичей. Как по науке забустить предсказание чем-нибудь используя эти допфичи? Для будущего я, конечно, их не знаю, так что exogenous в ариме - не то",
"<@U1BAKQH2M> построй для сезонности и влияния других фич регрессию. Для тренда тоже можно, а можно и ариму, тогда из регрессии надо убрать время и это будет что-то типа EM: убираешь регрессией сезонности и фичи, фитишь тренд, далее по кругу до сходимости. Для самих фич, если они неизвестны в будущем, тоже надо построить какой-то процесс, можно аналогичным образом, можно промой, можно простой регрессией по времени и сезонности - как нравится

Если сделаешь байесовскую регрессию, можно самплить из нее и получать доверительные интервалы",
"Мне тут задачку дали, не знаю, как подступиться. Имеются данные о транзакциях, в которых есть id продукта, количество, цена за штуку, время заказа, а так же id покупателя и id заказа. Цель - для каждого покупателя предсказать время и общую стоимость следующего заказа. Есть какие-нибудь мысли?",
"тут бы еще байесианам проснуться с графической моделью сначала вероятности того, когда купит, а в зависимости от этого на сколько денег (или наоборот)",
"память ddr3 можно довольно недорого найти (я дороговато взял, когда собирал)",
<@U0U2ENJ4U> отличный вариант. А как на этих материнках с pci-e ? можно gpu воткнуть?,
"&gt;  А как на этих материнках с pci-e ? можно gpu воткнуть?
Надо спеки читать, обычно все нормально. Можно даже несколько.
Проблемы могут быть с большими карточками (1080 не референсные огромные бывают.), но можно на райзер посадить (судя по тестам на скорость не влияет)",
а кто то юзает всякое водяное охлаждение или это для мажоров только?,
пойти что ли ограбить кого,
какой то сандиск 960 гб ссд за 220 баксов есть,
для двух видюх например какую мощность блока питания нужно?,
"<https://opendatascience.slack.com/archives/theory_and_practice/p1476542678002220> а можно как-то одновременно? :-) у меня проблема другого характера - пока не понял как задачу представить в виде набора признаков, чтобы потом на этом модель тренировать. ",
"А как заказать такое чудо? Они ж только на ebay водятся, да?",
"<@U06J1LG1M> зачем тебе i7, давай лучше двухпоцессорный сервер соберем.",
"Survival analysis я хотел тебе упомянуть, но, на самом деле, это слишком просто для ритейла. Лучше возьми лосс из экспоненциального распределения и построй любую модель, хоть линейную регрессию, хоть xgboost. Хорошие фичи (местами повторяю <@U0FEJNBGQ>): время, которое прошло с последней покупки (очевидно), время с первой покупки (очень важная фича), количество покупок (причем часто лучше выделять тех, у кого была всего одна, в отдельную категорию - вторая и последующие обычно сильно отличаются от первой), частота покупок, средний чек (или даже вектор сумм последних x покупок), какой-нибудь эмбеддинг (хоть TF-IDF) того, что человек покупает. В сезонности не забудь, что бывают еще праздники и сезонность не только по дням недели, но и по месяцам. Составь, кстати, кластера юзеров по наполнению их корзин и частоте покупок. Стоит такие метки тоже как фичи внести. Не забудь, что надо предсказывать лог-сумму и лог-продолжительность",
"Одновременно предсказывать можно, если достаточно точности до такого периода, когда покупки происходят почти в каждом. Тогда фичи те же самые, а распределение negative binomial (или gamma-poisson) будет. Просто Пуассон точно не пойдет, потому что неизбежно у разных групп будут разные дисперсии",
"Количество предыдущих покупок, как правило, очень хороший предиктор в ритейле, т.к. люди покупают со временем все чаще, пока не стабилизируются. Но если покупок много, имеет смысл дискретизировать, чтобы не было оверфиттинга или обрезать сверху. На графике увидишь сразу",
"<@U0FEJNBGQ> еще прав в том, что из всего этого можно сделать иерархическую модель и объединить все, но при всей моей любви к Байесу, когда есть много фич, лучше работает ML, по крайней мере в ритейле",
бери с турбинным кулером (это как на картинке выше),
"Ого, как подробно, спасибо! 
А что за лосс из экспоненциального распределения?",
"Да, из log-likelihood (<https://en.m.wikipedia.org/wiki/Exponential_distribution>). Я, правда, никак не пойму, почему они множитель lambda ^ n отбросили",
"Единственное что, это предположение исходит из того, что количество покупок распределено по Пуассону, а я сам же писал, что чаще всего это не совсем так, там overdispersed распределение. Но вот какое распределение будет также относиться к negative binomial, как exponential к poisson, я не знаю",
"ХЗ... Вообще не пойму, почему они так ограничили y. Почитай вот вики, там очень подробно описано. Но главное, построй график интервалов между заказами, посмотри, насколько он хорошо подходит под это распределение. Если подходит, можно юзать. Если нет (например, overdispersed все-таки), надо гамму, наверное",
"&gt; бери с турбинным кулером (это как на картинке выше)
как раз турбинные невыносимо громко шумят, для дома лучше брать с тремя обычными вентиляторами",
Как из новости размером с твит высосать из пальца целую статью: <https://m.geektimes.ru/post/281518/>,
"&gt;Это очень приятная новость.
&gt; Когда такая поддержка появится в стандартных процессорах Intel, то они могут приблизиться или даже обойти по производительности глубинного обучения графику Nvidia. Разумеется, при условии соответствующей оптимизации программ. 
:facepalm:",
"Ребят, нужна консультация по нейронке. Есть задача регрессии (не сбербанка), есть как категориальные, так и числовые данные; в целом обычная table data. делать нужно прогноз на будущее. xgboost работает, но недостаточно хорошо, не умеет экстраполировать. как правильно подготовить данные для нейронки и как потом ее настраивать?",
"я вот хочу проверить эту теорию, но не знаю как правильно готовить данные для такой задачи. это главный вопрос",
а не подскажешь что мне сделать с данными и как нейронку настроить?,
"дак всё так же как с лин. моделями, только надо отцентровать данные, через то же StandartScaler",
ну а ты сам как думаешь :slightly_smiling_face:,
"ну это понятно. сейчас больше речь про то, как уловить особенность данных",
"<@U14GG4E69> а карты которые под брендом nvidia и те что под другими, но на базе той же 1080, как упомянутая msi, они отличаются только кулерами, цветом и формой, или есть какие то различия?",
<@U0FEJNBGQ> а какой самый кошерный способ?,
"не знаю, какой самый кошерный",
А субботний семинар Ветрова снимался на видео - есть информация куда и когда он будет выложен? А то я не досидел,
Все самое интересное как раз в конце началось:disappointed:,
"<@U0H7VBQQ1> да, правда: я имею ввиду word perplexity (даже если модель на буквах) c достаточно большим словарем (больше 200k) и корпус хотя бы несколько сот миллионов токенов. А какую word perplexity видели вы? :slightly_smiling_face:",
Где в мск хорошие магазины с комплектующими?  Пока нашел только вариант с DNS,
"В эту субботу (22 октября) на тренировке <#C1CEM43TJ|mltrainings_live> будем разбирать соревнования:
— <@U053R9RS6> и <@U214JSBHP> расскажут про CIKM Cup 2016 Cross-Device Entity Linking Challenge от компании DCA, это соревнование зарешивалось на тренировках, в итоге Дима и Иван заняли призовые места.
— Никита Винокуров расскажет про то, как он в одиночку занял 5-е место на Kaggle Grupo Bimbo Inventory Demand.

Регистрируйтесь, приходите!
<https://events.yandex.ru/events/mltr/22-oct-2016/>",
ну такому холивару место как раз тут,
"Тогда не понятно, зачем покупать напрямую из маркета?",
"Точно? Мне почему то помнится, что недавно еще можно было на страницу магазина перейти всегда, независимо от того, можно ли купить прямо на маркете. ",
"но они не оч много рассказывают. 

доходя до занятного, например про ябло-тачку когда спросил (зная что у моего профессора в германии почти всех аспирантов по теме увели именно на это и перевезли в долину), дескать к ним тех мл-щиков не пристыковывали, они ""ниче не видели ниче не знаем, никаких немцев нет""",
А насчет Siri и Deep Learning есть какая-нибудь инфа? Насколько активно используется в данный момент и в каком направлении движется,
"&gt; доходя до занятного, например про ябло-тачку когда спросил (зная что у моего профессора в германии почти всех аспирантов по теме увели именно на это и перевезли в долину), дескать к ним тех мл-щиков не пристыковывали, они ""ниче не видели ниче не знаем, никаких немцев нет""

На самом деле вполне возможно, что они и правда ничего не знают.

В эппле очень плохая в этом плане культура: никто не говорит с коллегами о том, чем он занимается, у них секреты друг от друга даже внутри компании, что, кажется, дико.",
"На ICML его видел. Делал вид, что почти не общается с теми, кто фреймворки в эппле пилят",
"<@U06J1LG1M> это значит что как только ты перестаешь использовать тачку, то с этого диска все потрется.",
"Гайз, а тут такой философский вопрос: когда стоит делать shuffle данных? насколько это важно делать для разных типов задач и какие могут быть последствия?",
"я так понимаю, что лучше делать shuffle, так как ты уменьшаешь вероятность переобучиться?",
"Ну из практики пример: когда сетку допустим учишь на изображениях, то лучше перемешивать, иначе будут сначала идти батчи одного класса, потом другого и тд.",
"как он на это реагирует?  там в принципе, есть параметр `subsample`, который должен брать рандомное подмножество",
вроде никак не должен реагировать. он ведь как раз и берёт случайное подмножество (либо целиком всё).,
А кого тут можно про Correspondence Analysis / Multiple Correspondence Analysis / Multi-Dimensional Scaling поспрашивать?,
а кто-нибудь знает хороший словарь на русском ( или просто не-английском) где была бы указана позитивная/негантивная оценка (sentiment),
"<@U0VQJFN81> а какого рода рекомендации? выкладку на полки особо не соптимизируешь, народ привык брать нечто там, где оно всегда лежит, если только скидки какие-то персональные",
"америкосы любят спам присылать, как на электронную почту, так и на обычную",
"когда регистрируешься и получаешь дисконтную карту, надо адрес обычно указать",
а на убунту 14 (которая на амазоне) максимально можно какую куду поставить?,
"<@U0FEJNBGQ> выкладку еще как оптимизируешь, это называется мерчендайзинг. например, если заметишь, товары достаточно дорогие и интересные с т.зр. продавца выкладываются на уровне глаз. самое дешевое - ниже всего, чтобы не было удобно. фигня, которая тебе особо не нужна (либо, второй кейс, расходники, которые можешь забыть) - у кассы, чтобы пока стоишь надумал. классический кейс ml для ритейла - анализ корзин. так выяснилось, что, внезапно, подгузники часто покупают вместе с пивом. это все тоже используется. люди большие деньги зарабатывают на таких решениях с девяностых годов",
"еще в ритейле последний тренд, ну как последний, с нулевых - оптимизация цен через что-то типа многоруких бандитов, модифицирующих скидки",
"<@U0ZHCGY5T> знаю чувака, который построил большой бизнес на выдаче кредитов по социальному скорингу. он сначала пытался продавать модель как сервис банкам, не получилось, решил тогда сам сделать мфо и обороты ярд, в рублях, правда",
"Это ж какой там медь толщины, на 30А",
"Или такие широкие шины, как на серверных блоках питания. Всё равно кроме 12 вольт им ничего не нужно.",
"Зачем нам здесь два гейта от одних и тех же входов, input/sigmoid и state addon/tanh, против одного tanh? Просто потому что это добавляет capacity в сеть и дает возможность отдельно обучать значения и отдельно избиральность апдейтов?",
Какую убунту нужно сейчас ставить чтобы не мучаться с драйверами\библиотеками? 14.04?,
"У меня на 16.04 всё встало гораздо проще, как ни странно",
"а есть ли такое в  диплернинге, чтоб какой-то нейрон обучался узнавать некий случайный процесс в своем рецептивном поле? Поясню на примере: в зрительной коре есть такие нейроны, которые стреляют спайки тогда, когда видят конкретное движение - скажем, полосочка двигалась слева направо. Этот нейрон не будет реагировать, если полосочка перемещалась сверху вниз, к примеру.",
"Я знаю, что  в своременных сетях нейроны хорошо выдеделают прастранственные инварианты (например, на первом слое это может быть решетки под разным наклонам, а на более верхних - чтото посложней). Это очень похоже на простейшие нейроны коры. Но а как насчет пространственно-временных инвариантов, а не только пространственных?",
"Если на вход приходит не картинка `pixel at (x,y)`, а видео-ряд `pixel at (x,y,t)` и есть свертки вдоль времени, то временные фичи так же закодируются.
Вопрос в том, при каком сценарии это можно будет полезно использовать. (т.е. если в рабочем прогоне анализировать уже записанный ряд -- норм, если вживую -- то не очень)",
"Ребят может кто сталкивался, учу классификатор(sklearn.neural_network.MLPClassifier) на ноуте всё норм время обучения ~2 минуты, на сервере  виснет  и не обучается =(((  конкретно на функции fit(train, Y). В чем может быть проблема? проверил библиотеки numpy и sklearn версии совпадают, сервер по характеристикам лучше ноута.",
"что будет самой сложной частью мне сказать трудно :slightly_smiling_face: в целом вы меня поняли правильно. какую архитектуру нейронки брать - я не могу подсказать. и являеться ли RNN лучше чем, сверточная с `pixel at (x,y,t)` на входе - тоже не знаю",
"<@U0H7VBQQ1>, сценарий -imitation learning (learning by demonstration). Это когда у сети нет ни внешнего сигнала ошибки для фидбека, ни внешнего подкрепления, ни кучи примеров демонстраций. А ей пару раз продемонстрировали как человек что-т сделаел, и она это тут де пытается сделать сама. В биологических нейросетях такая задачка решается при помощи зеркальных нейронов  - зеркальный нерон х активируется и когда обезьяна видит, как кто-то сделал дейстиве Х,  когда она сама делает дейстиве Х (показывает язык, например)). Я хочу это использовать. Для этого надо, чтоб сеть научилсь составлять для каждого видео максимально информативное описание , закодированное в активациях нейронов в ее самых глубоких слоях. Скажем, для разреженных автоэнкодеров информативность описания достигается за счет sparsity penalty (это вынуждает каждую картинку описывать минимальным количеством нейронов глуюокого слоя). Как бы вот то же самое мне провернуть для видео...",
"запусти с `verbose=2` и смотри, где останавливается",
"<@U1D6QAJPJ> то что ты сейчас описываешь очень напоминает этот пост <http://karpathy.github.io/2015/05/21/rnn-effectiveness/> там продемонстрирован способ визуализации ltsm ячеек, и как раз там автор выделяет такие нейроны которые реагируют на простейшие действия во времени",
"Кто-нибудь работал с h2o? как в нем выбрать тип модели (классификация или регрессия)? Выбираю датафрейм, выбираю модель (лес, SVM), выбираю y column, в котором int в виде 0/1. А он мне все время строит регрессию на них. Не могу найти выбор параметра, отвечающего за тип модели",
"А еще, сейчас модно стало непонятные ситуации, когда нет четких критериев сверки, решать с помощью Adversarial сеточек..",
"Звучит как задача, которую особо еще никто не решал и уж точно не решил (по крайней мере я такого даже близко не видел)",
"<@U1G303UTW> поставил параметр и понял, что на сервере как то очень уж медленно высчитывается шаг",
"хз, не слышал о таком, вообще я подозреваю что так то оно на самом деле и есть, и перед тем как задаваться вопросом как это сделать, нужно поисследовать - а что вообще в обычном режиме происходит

проблема что лстм сеть из нескольких многих слоев не так то легко обучить, обычно 1-2

но по аналогии со сверточными сетями, где каждый следующий использует простые признаки для построения сложных, я думаю есть основание ожидать и от многослойной лстм похожего - если на одном уровне нейроны научились детектить примитивные временные последовательности, то на следующем будет детектор “последовательностей составленных из простых последовательностей” ну и тд",
"Добрый день. Есть такой вопрос, по анализу частоты временных рядов пополнений банковского счета. Нужно выделить регулярную составляющую и аномалии. Образование радиотехника подсказывает, что нужно что то вроде Фурье, но прямо в лоб не DFT не помогает -  ряд вида 0,0,0,0,x,0,0,0,0 где x - пополнение за один день.",
"Когда будет понятно с временем и местом для ""ужина"", кидайте сюда за пару дней до, люди подтянутся наверняка",
Так давайте объявим?) Как раз среда = «за пару дней»  :slow:,
"<@U0H7VBQQ1>
&gt;Кажется, что такие вещи можно решать RLем.
Я тут вижу, как минимум, 2 подзадачи. Одна, как вы и сказали, RL. На входе RL каждый раз имеем, грубо, две видяшки (на первой запечатлены старания учителя, на второй ученика). Внутри сети каждая видяшка порождает свою репрезентацию, и по схожести этих репрезентаций мы дальше генерим наш реинфорсмент  - и улучшаем полиси. Но отсюда вытекает вторая задача - сделать так, чтоб внутри сети строилась эта самая “хорошая репрезентация” видяшки. Такая, чтоб из близости двух значений репрезентаций (в некотром пространстве репрезентаций) следовала успешность имитации по человеческим меркам. Тут возникает, например, вот какая сложность: пусть наша нейросеть повторила учителя вцелом правильно, но местами чуть медленнее, а местами чуть быстрей. В результате, если смотреть пофреймово, то разница между “наивными” репрезентациями видяшек может быть колоссальна. А по уму не должна так уж сильно отличаться.",
"Для объявления нужно время, место, и тот, кто туда по любому придет",
"Вообще, с длинными видео, как я понимаю, народ сейчас толком работать не умеет",
"Вообще сличить две видюшки будет не просто. 
Дискриминатор в тех же adversarial networks надо будет знатно обмануть, чтобы она на простых фичах не однозначно угадывала где учитель, где ученик.

Но есть подозрение, что сетап несколько похож на SeqGAN:
1. есть длинная цепочка действий, 
2. есть примеры ествественных последовательностей, 
3. нет критерия, который бы говорил синтетическая последовательность или натуральная.
loss -- правдоподобие семпла по мнению дискриминатора. Но вот вопрос, как сделать чтобы он исключительно двигательные фичи учитывал.",
"казалось бы, у MinMaxScaler могут быть проблемы в случае, когда есть какие-то удалённые от остальных объектов шумы, из-за которых после скейлинга выборка всё ещё будет не распределена в одинаковых масштабах по осям. типа по одной оси всё в порядке, приведём всё в [0,1], а по другой был шумовой минимум и шумовой максимум, а остальные объекты по скейлинга приведутся в [0.5, 0.7], например. то есть разные масштабы у осей. 
могу ошибаться.",
но зачем добровольно отказываться от регуляризации?,
"Еще есть PixelRNN, которая вроде как более успешная",
"<@U04URBM8V> судя по плюсам коммента, ориентируемся на тебя, когда удобо? (Предлагаю воскресенье)",
"Для этого, как минимум, нужен ботнет, кстати, еще",
"ну я думал вдруг кто натыкался на что-то, вдруг кто с гордостью поделился как он фейковый трафик по одному клику  определял",
"<@U0G29N5U4> не только, в рекламных сетях не всегда есть такая возможность, когда дальше клика по баннеру уже не данных",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1476967121002460>
это просто фингерпринт, поведение это когда ты можешь все евенты на странице захватывать",
"Да ты 99% вычислишь на таком примитиве, как IP",
"Я как раз готовлю докладик на эту тему, как часть literature review для PhD, рекурентность внутри генеративно-дискримитативных систем для обработки пространственных-временных данных",
"конечно, инжектить чужой js не везде безопасно как минимум
как и трекать все действия",
"Поэтому те, кто немного думает, в iframe, конечно, кладут. Но подавляющее большинство не задумываются",
Ты про JS-функцию? Ну конечно... И как это Яндекс собирает heatmaps интересно?,
"ну ты путаешь может понятия, яндекс выдает js, который как раз может взаимодействовать",
"странно что, как созреет, а не как придешь на дата завтрак",
"<@U0FEJNBGQ> подскажи, плиз, есть ли какой-нибудь коэффициент в этих твоих временных рядах / последовательностях, который позволяет отличить ситуацию, когда события были размазаны по отрезку времени равномерно v.s. концентрированно в каком-то из моментов?",
"ну я продолжаю учить сетки векторизовать ряды, вектора сопоставляю такие, что дтв между рядами ~ l2 между векторами, ну вот у таких эмбеддингов полчаются любопытные свойства, например они очень хорошо пихаются в хгбусты всякие, но я пока работал только с боль-менее длинными рядами, ну по 120 измерений минимум, вот думаю, какие аномалии мне грозят если я перейду на короткие ряды",
"я заметил это когда в слак зашёл, там при входе обычно пост от barmaley был :sleuth_or_spy::skin-tone-6:",
"А расскажите, как вы отлаживаете theano?",
"`.shape` у theano variables смысла не имеет, пока не вызвана функция, а когда вызвана - уже поздно",
"Я руками это делаю. По мере написания кода пишу комментарии, какого шейпа ожидаю тензор ",
"Ну и пристально смотрю на сообщение об ошибке, пытаясь понять, где этот кусок выч. графа в моем коде",
"Проблема в том, когда начинаешь после lasagne на theano что-то делать",
"у меня в почте такое:
```в качестве спикера, где Вы сможете принять участие в панельной дискуссии, которая состоится 27 октября 2016 года (11.00-11.30):
«Smart Data. Как зарабатывают сегодня на умных данных? Чего ждет отрасль в будущем?»```",
"так делал, когда метрика `MAPE` была",
"Если есть интерес, то можно провести coding sprint on Gensim в Москве в ноябре. Идеально было бы совместить его с coding sprint on text2vec by <@U0E4S5LU9> , потому что R это тоже хороший язык. :slightly_smiling_face: От тех кто недавно в nlp, нужна помощь в улучшении tutorials. А для остальных есть новые фичи и улучшения.",
"это возможно, когда в следующий раз поеду в бангалор, то можно по дороге в Дубаи сделать спринт",
"чат, если предлагают настроить модель, так что бы она в каждом предсказании ошибалась не более чем на N%. После размышлений стало ясно, что например MAPE, и то что я выше написал не одно и то же. Есть мысли почему такая метрика плохо\хорошо? Такие метрики вообще существуют?",
насколько страшна ситуация когда в батч падают примеры только из одного класса?,
"это как с 0/1 loss - градиент почти везде 0 будет, напрямую оптимизировать градиентными методами нельзя. См., например, раздел 8.1.2 вот тут: <http://www.deeplearningbook.org/contents/optimization.html>",
"В порядке бреда можно придумать функцию ошибки на основе сигмоиды с 0 значением при Х до N%. Существование производной это проблемы математиков. А если как бизнес-пользователю подойти к вопросу, почему не следует ориентироваться на такую метрику? Чем это хуже, чем просто RMSE или MAPE?",
"Наверное, это не страшно. Во-первых, вероятность такой ситуации нереально мала,(даже если батч 32, а класса всего 2), во-вторых, как мне кажется, дисперсия градиента функции ошибки на батче недостаточно большая, чтобы такой выброс сильно повлиял",
"Ну, как мала... Примерно 1/K^128, где K -- кол-во классов",
Я это имел ввиду когда про сигмоиду писал.,
"Вопрос не как это сделать, а почему так не нужно делать?",
"(наверно я велосипед изобрел, осталось найти, как его цивилизованные люди делают)",
"Может быть стоит выкинуть примеры из тех классов, где больше, скажем 10к сэмплов",
"Брать больше примеров из тех, где меньше",
Кто-нибудь сталкивался с построением продуктовой матрицы для розничной сети (т.е. какой товар и в каком количестве закупать на сезон)? Где об этом можно почитать? Магазины - спортивные.,
"А объясните, почему тренировка может не сходиться, если батч маленький? Это только из-за того, что полная итерация выполняется медленнее, и за данное время можно успеть сделать меньше проходов по датасету? В deeplearning book вроде наоборот писали, чем меньше батч, тем лучше все должно сходиться",
"<@U040M0W0S> <@U25UPSERX> я тоже очень даже за! Мероприятие буду в трансляции смотреть, поэтому скажу здесь.
Предлагаю ставить плюсы к сообщению <@U040M0W0S> кто хотел бы поучаствовать!",
"Тем, кто ""игрался"" с inception-v3 на tf: вы замечали, что какая-то часть пайплайна сортирует классы по лэйблам (не индексам)? У меня так и не получилось найти причину...",
Всем привет. Может кто подсказать статьи/рассказать вкратце самостоятельно про Spatial Pyramid Pooling?,
Смотрите какая интересная штука: <http://www.gpucloud.io|www.gpucloud.io>,
"&gt; Take off-the-shelf PC hardware, add 2 Titan X Pascal, rack it in a data center of your choice. The only remaining Problem then is the cost due to higher energy bills (300W/GPU). But even with that you should manage to beat spot prices.
&gt; To bring costs down even further we are using an energy recoupment solution, essentially using the GPUs and everything else as heating elements. If you are interested in a trial register here: <http://www.gpucloud.io/>
Написано неделю назад, когда мне в голову пришла та же мысль :slightly_smiling_face:",
"Товарищи, а как между собой соотносятся гиперграфы и тензоры?",
"зачем блас, там просто нужно кусок памяти скопировать",
"Ребят, где почитать, как считать естественную метрику на единичной гиперсфере?",
"<@U0ZHHV83C> а что значит твой вопрос? какая метрика считается естественной, чем метрика на гиперсфере отличается от метрики на какой-то n-сфере?",
"<https://en.wikipedia.org/wiki/Great-circle_distance> из этого я понял, что это только для n=3, так как векторное произведение есть только в нем. Мне надо для произвольного n.",
"это всё да, но как формула выглядит?)",
"пацаны, а какие есть тулзы для разметки изображений под семантическую сегментацию?",
у кого-нибудь есть мб? не могу найти. А 50 фунтов это чот за электронную версию совсем много,
"Вот у меня есть временной ряд с какими-то данными, в которых есть разные id, например, id юзера, id предмета, и нужно классифицировать каждую строку этого ряда. Мне нужно из этого подготовить фичи, чтобы можно было потом это засунуть в модель. 

Я так понимаю, обычно считают что-то вроде сколько времени прошло с первой транзакции юзера, время с предыдущей транзакции, класс предыдущей транзакции, и счетчики сколько всего было транзакций до этого момента, сколько из них были 0 класса, 1 класса.

Вопрос следующий - какой наиболее эффективный способ реализации таких фич? Я так понимаю, нужно для определенного момента времени взять id и уметь быстро посчитать, сколько раз это id уже встречалось. Есть ли какие-то готовые инструменты для этого? Или проще самому написать код, который проходит по данным, для каждого id держит  в памяти все эти вещи, и просто для каждой строки смотрит нужные значения и обновляет их по ходу дела?",
"<@U0DA4J82H> временные фичи типа время\число событий с момента x - отдельными функциями
всякие аггрегаты (среднее число, макс\мин, дисперсия за последние x времени) - раскладываю ряд в траекторную матрицу и считаю уже все это как функции над строками

траекторная матрица для ряда 1 2 3 4 5 6 с окном 3 это
1 2 3
2 3 4
3 4 5
4 5 6",
"почти. это сильно дырявый временной ряд где события происходят абы когда
можно считать как фичи с точки зрения ""фичи со времени события"", а можно - чисто как аггрегаты над полноценным временным рядом без пропусков",
а вот если у меня стрим и я не могу взять и просто отсортировать данные как хочу?,
"да, на каждого пользователя держать набор счетчиков в памяти и обновлять, как приходит событие",
"это можно делать в  sql базе, как многим известный Zabbix делает",
"смотря где — в MSCOCO например хранят координаты полигона, которым обведён объект",
"Или Cairo, как его там...",
крутой у coco сайт какой,
"Слушайте, может тут кто поможет. 

Видел недавно задачку на интервью. Дан лог (отель, день, стоимость) за последние два года. Юзер смотрит на отель в незнакомо ему городе, и хочет понять является ли цена этого отеля на конкретный день ""хорошей сделкой"". Определение хорошей сделки открыто, но я так понимаю, если цена сильно ниже, чем должна быть в такой день. 

Я тут думал построить временную модель, чтобы предсказать цену для данного отеля в конкретный день и сравнить ее с реальной ценой на этот день. Если сильно выше, то можно заклеймить ""хорошую сделку"". 

Что думаете о таком подходе? Какие еще варианты есть решения?",
"Кто знает, что можно почитать про сэмплирование из случайного графа?",
А как ранжировать отели будем?,
А как обьединять все в это одно?,
Ну то есть единный score по разным критериям как лучше составлять?,
Как сэмплирование может быть неслучайным?),
"<@U040HKJE7>  <@U040M0W0S> Да все верно, будем те же статьи разбирать и те же презентации будут. Договорились с Е. Бурнаевым, так как не все из его группы были прошлом митапе.",
"псс, 28го будет бесплатный слэм, который делают как орги science slam, так и intel. по идее будет чтото похожее на наши Data &amp; Science, только в куда более вводном быстром формате
<https://scienceslam.timepad.ru/event/388516/>",
"&gt; только в куда более вводном быстром формате
Было бы прикольно рассказать про специфику коммуникации в комментах YouTube. Выйти, показать на кого-нибудь пальцем, сказать «Ты пидор» и уйти.",
"интересно, почему возраст - обязательное поле при регистрации",
"у меня вопрос к залу :grinning::
как считают знатоки что является релевантным фактором для определения фрода у сотрудников, принимающих клиентов с платежами. Т.е. если есть истории работы работников с клиентами, истории их операций, инфа о сотруднике, какие данные наиболее богаты и как в выявлении воровства?",
"т.е. вопрос не в том какой ML использовать, а вопрос как и из чего сформировать релевантные фичи.",
"ground truth есть? разметка по теткам, кто ворует, а кто нет",
"сходимость кассы при сдаче, у ворующих может быть лучше, чем у тех, кто со всеми реальными ошибками просто кладет бабло в кассу",
"вроде все фичи не особо релевантные. Вот по операциям, как они с клиентами работают. Воруют когда больше устали или наоборот от нечего делать?)",
почему никто еще календарные фичи не предложил? :povar:,
"Я вот думаю, по особенностям операций (нефродовых) за час до воровства и через час после. Есть ли изменение поведения и т .д. Кто-то быстрее стал после как украл, а кто-то ноборот заметает следы и т.д.",
"вот мне интересно, как можно срисовать лям по дороге между клиентом и кассой. Лишней купюрой его по ошибке не положишь, даже пятерками это немаленькая коробочка.",
"сейчас же кассы все должны стать умными и в realtime слать данные в налоговую, а можно этот поток себе заодно оставить, и пустить второй поток событий от CRM, когда договора с клиентами подписывают, чтобы шло событие ""клиент пошел платить X рублей"", их потом сравнивать с потоком от касс",
"речь не о том как уберегаться от воровства, а как его выявлять?",
<@U041LH06L> : а как тут Бенфорд применить?,
"Подскажите, плз, как установить/загрузить Google News word2vec model в gensim для грядущего семинара (Win10, Anaconda, Python 2x)?",
"коллеги, подскажите, на богомерзком маке у меня есть теана, и в теанорц написано цпу и все ок работает; тут значит ставлю лазану, а оно ругается при импорте nvcc fatal   : The version ('80000') of the host compiler ('Apple clang') is not supported

как так?",
"Ну, видимо, оно всё равно что-то для инициализации куды делает",
"с каффе тоже самое кстати, но она по крайней мере требует куду",
"коллеги, никто не знает, где взять тулзину, которую используют для разметки картинок для последующего обучения сегментации? в идеале, чтобы ее можно было допилить для поддержки новых форматов",
"<@U2T5N5UAK> 
Почему бы не попробовать построить модель предсказывающую цену на отель в зависимости от всех этих фич
&gt;День,праздник/событие,звезды, район, условия и т.д.
Далее, если качество предсказания получилось нормальное, те отели у которых цена ниже предсказанной -хорошие.
Степень - отношение предсказанной цены к реальной.
Типа этого <https://habrahabr.ru/post/312842/>, только про отели.

Или действительно, 
<https://opendatascience.slack.com/archives/theory_and_practice/p1477292705002631>
Можно такую модель строить на каждый день и видеть насколько выгодно твоё предложение.",
Как теперь 9кк картинок лучше выложить? я уже пол дня пакую их в архив и пока 9%,
<@U04422XJL> я сюда переведу беседу. еще нубский вопрос про тензоры: зачем они нужны и в каких сферах ds их используют?,
"Но эта сложность, как я понимаю, нигде кроме неё и не используется",
Там ребятами аппроксимировали функции когда это еще не было мэйнстримом),
"Где вы все были, когда я вчера про связь гиперграфов с тензорами спрашивал?",
"написано что 1 гб
но чота подозрительно мало, там же куча фоток, гифок и прочего",
"пример тензора - ускорение в 3D. Если ты напишешь матрицу 1x3 то это не будет тензором, это пока еще матрица. Чтобы сделать это тензором ты должен указать как меняются числа при смене system of reference. Еще можно так разницу указать - тензор это мультилинейная функция, а многомерный массив это  структура данных",
Где можно почитать почему она расходится?,
"<@U2LG87YBF> 
В Саттоне есть раздел (у меня он 5.5 который называется Evaluating One Policy While Following An-
other (Off-policy Policy Evaluation).   Я так понял, что для того чтобы использовать переходы одной политики для обучения другой нужно уметь считать отношение вероятностей прийти в состояние s для этих 2 политик.
В относительно новой статье <https://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients.pdf>
 как то это делают, но я до конца не разобрался с этой статьей.
Также в статье <https://arxiv.org/pdf/1509.02971v5.pdf> используют experience replay для актора критика, но там архитектура актора критика немного нестандартная, она для непрерывных действий и политика там не стохастическая а детерменированная.",
У кого-нибудь есть опыт работы со спектральной кластеризацией (<http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html>)? На какие параметры стоит обратить особое внимание?,
"Привет! Если у меня есть временной ряд продаж продукта, который сейчас растет. Как можно обнаружить точку, когда он пойдет на спад? Более общий вопрос, как делаются долгосрочные прогнозы больше чем на месяц?",
"&gt; Как можно обнаружить точку, когда он пойдет на спад
если он всё время растет, то никак",
"можно смотреть отклонения от твоей предсказательной модели. и когда за какоето окно 
а) накопленная ошибка больше одного твоего порога
б) среднее отклонение отрицательное (и больше другого порога)
...начинай бить в тревогу. может даже застанешь ""пик""",
"как правило, те кто рисует такие циклы и те кто занимаются прогнозами рядов - по жизни не пересекаются",
"но! 
если у тебя много продуктов, большая накопленная статистика по тому, как они себя вели на исторических данных и даже есть какаято структурная картинка типа такой иллюстрации, ...
... то можно эту структурную модель постараться восстановить и пытаться угадать, в какой точке жизненного цикла ряда ты находишься сейчас.",
"&gt;то можно эту структурную модель постараться восстановить и пытаться угадать, в какой точке жизненного цикла ряда ты находишься сейчас.",
А где взять xml-дамп медиавики? Я же этот формат справки ничем не прочитаю :(,
"Котаны, а кто какие тулзы использует  для построения всяких модных user-friendly дашбордов для всяких топчиков и маркетологов?  (а ля google analytics). 

Желательно, open source и которые еще сами умеют в адаптивную верстку под мобилку.",
"друзья, кто-нибудь сталкивался с ситуацией когда при использовании cuDNN время работы форвард-пропогейшн на больших батчах очень нестабильно ? То есть например 5 батчей одно время, потом вдруг на одном батче в 10 раз больше, потом опять 5-6 батчей нормальное и опять в 10 раз больше. После каждого батча я чищу память GPU. На маленьких батчах время стабильно.",
Сколько GPU в корпусе? Как там с температурой GPU?,
"про недостатки v-measure и чего-то там еще мы написали в статье, одна метрика максимизируется, когда все слова относятся к одному смыслу, а другая, когда у каждого слова свой смысл",
"то есть, как делать выводы о том, какая модель лучше, совершенно непонятно, только если кто-от одновременно лучше по всем метрикам, но это крайне редкая ситуация",
а каких именно базовых вещей не хватало?,
"Кстати, кто как word2vec тренирует? Если свои самописные, на чем и как валидируетесь, гиперпараметры подбираете, и останавливаете тренировку?",
"<@U049HDR2Z> SSA использует адаптивный базис, то есть если ряд был — чистая экспонента (в смысле без шума), то у него не будет синусоид и полиномов в элементарных компонентах (элементарная компонента — это ряд, восстановленный из одного слагаемого SVD или используемого вместо него разложения), если чистая синусоида — то не будет экспоненциальной элементарной компоненты и т.д. Вроде бы где-то в теории доказано, какие ряды SSA точно раскладывает, какие асимптотически точно и т.д.",
"Вот как ее улучшить я не знаю, разве что нормы пересчитать как функцию от начальной нормы и вордкаунтов",
"Братцы, может кто сталкивался с xgboost om мультиклассовая, пытаюсь посчитать значение кач-ва на кроссвалидации, а он мне пёс : XGBoostError: b'[18:18:04] src/metric/rank_metric.cc:89: Check failed: (preds.size()) == (info.labels.size()) label size predict size not match'. Было думал не правильно кол-во классов поставил (num_class), но нет, проверил всё правильно. Куда смотреть?",
"Как улучшить вариантов вроде много есть, какой взлетит не известно, но мне нужен какой-то надежный критерий, учится ли сетка или нет :slightly_smiling_face:
На loss смотреть сомнительно.",
"Странно. Этот параметр же как раз и отвечает за то, чтобы такой фигни не было.",
мб он до этого их стратифает sklearn-ом каким,
"Я - тот, кто тебе нужен",
Мы как раз разбираемся с этим случаем,
"Вот, кстати, хотел спросить, часто ли вы сталкиваетесь с такими задачами, где есть много текстов и они очень слабо размечены?",
Слабо размечены -- это метки дурные и непонятно почему они так или когда только часть текстов размечена?),
"Не, это когда &lt;1% текстов имеют метки",
"Ну, не обязательно один процент, интересует случай когда их явно недостаточно",
"<@U043814R6> а расскажи подробнее, если можешь. метки у произвольных текстов могут быть выставлены в этом случае или алгоритм указывает, какие тексты доразметить?",
"<@U0AF2AZCM> случай самый общий, есть миллион текстов, какая-то доля имеет произвольное ненулевое количество меток, остальные не имеют вообще. Требуется добиться как можно лучшей классификации",
"embeddings интересны побочными свойствами. как те, что ты упомянул (king - man + woman, вот это вот все)",
"но я не имею в виду “самые лучшие эмбеддинги”, конечно. не хочется отвечать на вопрос как ранжировать разные embeddings между собой и почему стоит взять именно это отношение :wink:",
"Ну вообщем насколько разных наборов параметров попробовали и посмотрели, какой лучше для определенной задачи",
"а почему во всяких вгг увеличивают в каждом слое число фильтров, но не увеличивают их размер? если я делаю одномерные конволюции по времени, какой интуицией руководствоваться для размеров фильтров? <@U0H7VBQQ1> как у тебя в текстовых задачах было?",
"кто может подсказать, с чего можно подступиться к такого рода задаче: есть большая база вопрос-ответ. нужно вопросы разбить по тематикам/категориям. при этом категории надо получать разной глубины. условно общая тема медицина, потом глубже про какую-то тему и тд.  в идеале хочу прийти к решения задачи такого характера: получил вопрос из этой базы дернул на него ответ. буду рад любым наводкам :slightly_smiling_face:",
"как в sklearn  из обученного дерева глубиной n дернуть предикт из того же дерева, но порезанного до n-1? чот забыл",
когда y_true 0 -  у меня предикт по построению тоже всегда 0,
как бы мне красиво зареплейсить значения в y_true чтоб в nan не попасть и написать в одну строчку?,
"плохо умею в тензоры в тф, чот не пишется",
ты где делишь подставь clip с эпсилоном,
"<https://opendatascience.slack.com/archives/theory_and_practice/p1477405792002756>

Это не про тензоры, а по тензорное произведение. Обычно важно не только само произведение, но и какие именно пространства перемножаются (отсюда растут ноги разговоров о трансформационных свойствах тензоров).",
"<@U2G9Z6ZQQ> а какие данные доступны? Если в разрезе отдельных потребителей или каких-то сегментов, можно точнее - когортный анализ применить, у меня хороший опыт был. Если прогноз на несколько месяцев, а не лет, то эти кривые - без шансов, если не относится к каким-нибудь вирусным аппкам, которые быстро помирают",
В параллельном мире topic models понятно какая модель intrinsically лучше после стати reading tea leaves и потом  AKSW  research group нашла c_v coherence.  <http://www.kdnuggets.com/2016/07/americas-next-topic-model.html>,
"даже по t-sne видно кто вместе, а кто далеко не видно. У нас один студент в инкубаторе делает визуализацию получше - если у кого есть идеи то welcome",
"*Семинар для начинающих по глубокому обучению или КАК СТАТЬ Data Scientist’ом*.
*Спикер: Сергей Николенко,* н.с. ПОМИ РАН, с.н.с. НИУ ВШЭ Санкт-Петербург, автор более 100 научных публикаций, нескольких книг. «Уже десять лет я занимаюсь машинным обучением в разных его формах, в основном -- обработкой текстов, рекомендательными системами и рейтинг-системами; делаю это как теоретически, в виде научных результатов и статей, так и практически, в виде проектов, сделанных с индустрией. Уже десять лет читаю курс машинного обучения».
*О чем:* где и для чего применяется DL, покажет, как с этим работать и укажет конкретные шаги, с которых следует начать. У слушателей будет возможность совместно с экспертом с нуля решить классическую задачу распознавания изображений с помощью глубокой нейросети. Будет возможность задать эксперту вопросы.
*Место:* здание высшей школы менеджмента СПбГУ (Санкт-Петербург, Волховский переулок, 3)
*Время:* 2 ноября, 16:00
*Регистрация обязательна:* <https://goo.gl/forms/KOAuDQ2BP36Z44CG3>",
сорри. не видел такой группы. ща перешлю. а как в календарь добавить?,
"ну ведь по факту замена будет работать как раз потому, что выполняются вещи типа king - man + woman",
"лично для меня это само по себе очень замечательный факт - то, что такая структура появляется автоматически, как побочный результат оптимизации. факт такой же удивительный, как и похожесть откликов нейронов в зрительной коре и на первых слоях convolutional networks",
"<@U0AS548A1> 
<https://opendatascience.slack.com/archives/theory_and_practice/p1477427452002799>
можно вызвать decision_path метод, там будет какая нода предыдущая. или ты хочешь именно predict получить? мне кажется что такое только вручную получится сделать",
"xgboost на tf-idf работает точно так же, как на сумме претренированных word2vec эмбеддингов, кстати",
а как семантическую связь выделяли?,
"<@U0AF2AZCM> да, хотел предикт. Идея в том, что тип зачем для кроссвалидации глубины дерева считать каждую глубину. давайте посчитаем одно самое глубокое, а для остальных будет дергать предикт из него",
"кстати, а как база - только вики использовали?",
"там какие-то мемасы родились про стакание тонн люцен, как оно работало?",
"со мной как раз недавно делились тут, сейчас попробую вытащить",
<@U2N853RGF> в каком смысле? DOE? Если да - то вроде как для других задач.,
<@U0AS548A1> у меня как бы еще и не задача классификации :slightly_smiling_face:,
"тогда лучше ее не сводить к классификации, а решать как есть. Если выборка имеет какую-то структуру - применять спец методы, если нет и данных до фига, то делать как было написано выше правильный дизайн эксперимента (то есть правильно отбирать полезные точки)",
"не помню как именно называется этот метод, но он широко известен",
"Разбиение на уровни - это уже сделали (если кто знает, как называется этот метод - поделитесь, пожалуйста).",
"<@U0FF52P7D> еще один понятный метод, как было написано выше, это SMOTE, но если речь все-таки о регрессии, то там нужно значения выходов как-то для новых точек аппроксимировать локально видимо",
"<@U0AF2AZCM> я говорил о случае, когда распределение целевой переменной выглядит, например, как на графиках выше. Если при этом еще и большую роль играет ошибка на малых по модулю значениях (в примере - около нуля), а количество семплов экспоненциально растет с увеличением значения целевой переменной по модулю - хочется придумать какой-то subsampling, учитывающий природу данных (в частности - важность ошибки на редко представленных значениях).",
"А кто сегодня планирует посетить мероприятие? Давайте, чтоли, знакомиться лично:)",
"Чет я не вижу, как это поможет",
<@U0FEJNBGQ> как в презентации Александр Дьяконова уже все делал :slightly_smiling_face:,
"А как оценивается ""хорошесть"" сэмпла?",
"<@U1CF22N7J> perplexity 30 означает, что в среднем при предсказании следующего слова мы настолько же не уверены в предсказании, как если бы выбирали из 30 равновероятных вариантов, или что для кодирования следующего слова нужно в среднем чуть меньше 5 бит",
Здесь часто произносят как archive,
а зачем мне свертки вообще? я пилю сорт оф feature extraction,
однородны ли сами батчи (как данные внутри батча влияют на скорость его переваривания)? какое распределение времени обучения каждого батча?,
"&gt; какое распределение времени обучения каждого батча?
Лень смореть :simple_smile: Но в нормальность я особо не верю, больше верю в что-то вроде гамма-распределения",
"<@U1BAKQH2M> т.е. ты хочешь взять вещественные числа из ряда, каким-то образом их разложить по бинам, а потом номер бина использовать как токен в предложении и на этом деле тренировать w2v?",
"ну почти, только я хочу номер бина использовать как букву, и окнами как-то собрать эти буквы в слова",
"а такой такой способ, как я написал, уже пробовал? ведь w2v как раз на окошко смотрит",
для года с отсчетами по минутам уже чот так себе идея,
"<@U1BAKQH2M> тоже вынашиваю идею проверить в2в для временных рядов, Засела в голову после прочтения статьи о том, что CNN лучше других их предиктят. Если уж технология ""для картинок"" работает на рядах, то почему бы на них и технологии ""для буковок"" не сработать? )))
Имхо, если брать только сам ряд, то даже если все сделать правильно при его подготовке, а он ""сложный"", например нестационарный как биржевые цены, то получим то, что и так уже знаем - вытащим паттерны, которые можно, например и ассоциативными правилами вытащить.
Т.е., имхо, нет особого смысла усложнять метод анализа.
Могу ошибаться, но м.б. тут лучше сработает:
- использование предложений фикс. длинны, каждое слово в котором из разных рядов
- поиграться с ""точностью"" слов и тега предложения
Вот такое планирую проверить.
Если кому-то интересно, то давайте попробуем вместе. Или это уже в <#C17SUH211|_call_4_collaboration> ?",
"Папаротник, но зачем в <#C04422A5C|_meetings> ?",
"Железо дешевеет, полтерабайта SSD за сотню баксов -- где это видано? <http://slickdeals.net/f/9218879-newegg-app-525gb-crucial-mx300-2-5-tlc-solid-state-drive-105-w-android-pay-checkout-free-s-h?src=SiteSearchV2_SearchBarAlgo1>",
"Привет! Кто-нибудь использовал SyntaxNet для русского языка? Я скачала обученную модель и запустила ее, как написано здесь:
<https://github.com/tensorflow/models/blob/master/syntaxnet/universal.md>
т. е. вот так:
```
MODEL_DIRECTORY=/where/you/unzipped/the/model/files
  cat sentences.txt | syntaxnet/models/parsey_universal/parse.sh \
    $MODEL_DIRECTORY &gt; output.conll
```
Но этот скрипт не токенизирует предложения. При этом, если я вместо `parse.sh` запускаю `tokenize.sh`, то `output.conll` приходит пустой, а в логах вижу:
```
Not found: models/syntaxnet/syntaxnet/models/Russian/char-map
```
Но в той обученной модели, которую я скачала из указанного в документации ресурса, никакого файла `char-map` нет.
Вопрос: как же мне добиться токенизации для русского языка?",
"<@U040M0W0S> Ну, в общей постановке задачи можно, конечно. Но тогда появляется другая, критическая для меня проблема.
Если я подам на вход токенизированные предложения, я потеряю связь между output'ом SyntaxNet'a и позициями токенов (`startOffset` и `endOffset`) - для меня это делает невозможным использование результатов, полученных из SyntaxNet, и результатов, полученных из других морфологических таггеров и синтаксических парсеров в единой фиче-матрице, т.к. я потряю взаимо-однозначное соответствие между токенами.
Я пока не придумала, как обойти эту проблему. :disappointed:",
фиг знает куда экспертом записали?,
"например, для каждого span от syntaxnet искать токены, пересекающиеся с ним. чаще всего это будет один токен, если пересекается несколько, то смотреть, как пересекается. Если несколько токенов полностью вкладываются в некоторый span, то значит syntaxnet посчитал их за один и надо их объединить",
"самое смешное, что я как раз вот перестал быть senior data scientist в deloitte cis :slightly_smiling_face: <@U040HKJE7>, тебе может быть интересно)",
"<@U2194SMBM> спасибо, попробую сделать логику объединения-разъединения токенов, чтобы можно было совмещать результаты разборов разных парсеров.
Так-то это штука полезная, можно во всех проектах потом будет использовать. Правда, есть условие, что парсер должен уметь выдавать позицию токена в тексте. А если парсер не может этого делать, то тут уже не понятно, как выкручиваться. Можно, наверно, по номерам токенов, но тогда надо будет их еще сравнивать посимвольно, в общем, жесть. : )",
"<@U04BFDYPV> вектора для уникальных обйектов из данных.  я скармливаю длинный список пар из айдишников сессий и айдишников объектов, каждый из них может повторяться в другой паре, уникальных меньше чем всего пар, 
когда все получаю на выход, он возвращает для каждой пары а не для каждого id , я думал факторизуется матрица где столбцы и строки это уникальные id для сессий и объектов",
"Я,  может, каких-то тонкостей не замечаю, не знаю. Но как по мне,что gate, что uima возвращают примерно одинаковые xml с примерно одинаковой скоростью.",
"господа, а никто не юзал это <https://github.com/projectmesa/mesa> ? или вообще если кто нибудь сталкивался с агент бэйзд иоделлинг, то может   есть какие нибудь советы по фреймворкам?",
"смотря для чего, если как стивен вольфрам решать дифуры клеточными автоматами - то да ересь",
"&gt;И дальше что? Выяснить, что реальность разительно отличается от этой модели?
яж не социолг какой то",
"<@U04URBM8V> ищи скорее не RL, а bayesian optimization для подробора гиперпараметров (структура сети — гиперпараметр модели). Кажется, я видел статьи, где вполне успешно оптимизировали структуру именно таким образом.",
"а где можно достать датасет с утварью домашней? телефизор, диван, стол, пила и тд, дрель :slightly_smiling_face: для коммерческой разработки",
<@U13QJJR6F> бот еще как напомяналка нужен :slightly_smiling_face:,
иногда он действует как тупая бесчувственная железка - ранит за самое больное ))))),
"Подскажите, на каком ресурсе лучше всего попарсить корпус русской литературы или может датасет уже есть такой публичный?",
"<@U2LHUAC6S>  если будет не лень, можно с того же NOAA взять базу по ветрам на интересующее время (когда был любой ураган) и просеять, разметив координаты, где была скорость ветра более 100 м/с (например).",
"<@U2D4XJL3A> да, это все можно, можно даже было бы ограничиться txt-файлом, приведенным выше
но это только половина данны, т.к. в жизни люди руководствуются в основном не прошлыми данными - они нам будут известны на дату из этих источников,
а еще и прогнозами и у них ""вес"" в принятии решения больше
<http://www.nhc.noaa.gov/graphics_ep5.shtml?5-daynl#contents>
вот здесь для текущих метео-образонваний показана прогнозная траектория
это подразумевал, когда писал в первом посте ""вектор прогноза""
На практике (по наблюдения ""на глазок"") если такая область накрывает определенные области,
то происходят изменения в предметной области.
то....",
Ну вот как раз типа нетронутого ноутбука в пленочках,
"<@U2D4XJL3A> пост был такой:
""Ищется статистика по ураганам в Атлантич. океане северное полушарие (дата, координаты, сила, вектор прогоноза движения, ...) лет за 10 хотя бы. Заране спс""
если не не большой секрет какие еще предикторы собираете? для какой предметной области?",
"Вопрос такой:
Пытаюсь задетектить ботов по веблогам (cookie, ip, url, timestamp). 
В качестве одной из фич хочу использовать неравномерность просмотров в течении суток. У каждого человека есть день и ночь, соответственно часть часов с большим кол-вом просмотров и часть с меньшим. 
Хочется как-то оцифровать вот эту «неравномерность» 
Думаю попробовать IQR для распределения просмотров по часам.
Какие ещё фичи можно извлечь из этого распределения? Возможно я изобретаю велосипед.

Да, буду признателен, если у кого нибудь есть чего почитать по поводу bot detection и т.д.",
"Еще можно посмотреть как именно пользователь передвигается по сайту, какие были предыдущие страницы, есть ли вообще ссылка с прошлой страницы на текущую",
"<@U1CE1A01J>  Вчера в Яндексе рассказывали как они ловят роботов. 
С 32 минуты - ""Роботам вход воспрещен"", Дмитрий Черкасов.
<https://events.yandex.ru/events/meetings/27-oct-2016/>",
"Ага, спасибо
<@U0DA4J82H> Да, в качестве других одной из фич я взял 90 перцентиль продолжительности сессии, 
Про ссылки спасибо, но это не просто в реализации.
<@U0FEJNBGQ> Да, пробую, как оказалось хорошая фича это отношение ширины распределения паузы между запросами к медиане. У ботов часто бывает узкое распределение пауз.
<@U043ZLY35> Да, спасибо, начал как раз смотреть вчера, но показалось что доклад будет вводный, без деталей. Гляну подробнее.

Но всё же, какие идеи по извлечению признаков из распределения запросов по часам?",
еще можно представить время как вектор размерности 24. это можно использовать как 24 фичи,
"еще можно предположить, что в большинстве своем пользователи нормальные, а ботов мало, тогда можно искать аномалии и тоже использовать их как фичи. Например, снизить размерность вектора, затем восстановить размерность и посчитать reconstruction error как фичу",
<@U1BAKQH2M> это я писал в коллаборейшн. как раз формируем датасет (или несколько) для опытов,
когда у людей на сайте стремный яваскрипт - вроде особо и вариантов нет,
а как ты их размечиваешь?,
"Понятно, что видеокарты хороши для жёсткой параллелизации, простых вычислений, таких как перемножение матриц. Но хороши ли они для каких-то сложных операций, которые требуют разложение в ряд тейлора итд",
Параллельно считать много рядов тейлора - почему нет,
"а вы уверены что все правильно сделали? Может HEADER оставили с фантомджиэс? Просто я работал с оберткой над фантомом casperjs и я в принципе не представляю как его можно отличить от обычного юзера за браузером, если только по времени прохода по страницам, но это тоже решаемо",
"Но еще есть аспект локальности данных. Иногда проще посчитать там, где данные уже лежат и где они нужны в дальнейшем.",
"Кстати, а преобразование Фурье на чем сейчас модно считать в реальных задачах:
- на GPU (пишем Фурье как свертку и перемножаем матрички)
- на CPU (кольца, арифметика, рекурсия, БПФ)?",
"Я скорее про то, кто побежает: умножение или рекурсия? :slightly_smiling_face:",
"&gt; на GPU (пишем Фурье как свертку и перемножаем матрички)
Я думал наоборот свертки через FFT реализуют :thinking_face: ",
"Уже через час в Яндексе начнётся вторая конференция Data &amp; Science: нейронауки (<https://events.yandex.ru/events/ds/29-oct-2016/>). Тех, кто не смог присутствовать, приглашаем смотреть трансляцию. 

Будут следующие доклады:
- Мозг и его данные (Александр Яковлевич Каплан, МГУ им. М.В. Ломоносова)
- Методы анализа электроэнцефалограмм и магнитоэнцефалограмм: от модели наблюдения до способов оценки параметров (Алексей Евгеньевич Осадчий, ФКН НИУ ВШЭ)
- Что мы измеряем, измеряя электроэнцефалограмму и магнитоэнцефалограмму: активность, активация и колебания (Татьяна Александровна Строганова, МГППУ)
- Как работать с интерфейсом «мозг — компьютер» (Юрий Олегович Нуждин, НИЦ «Курчатовский институт»)",
"Друзья, помогите очередным советом с кросс-валидацией. 
Какого размера лучше делать отложенную выборку? 
Каким методом выбирать туда элементы для задачи классификации - простым train_test_split с перемешиванием или StratifiedKFold?",
а где взять imagenet 21k?,
"<@U04ELQZAU> ну да, почему нет",
"<@U1CE1A01J> Вот здесь (<http://www.jmlr.org/papers/volume15/oentaryo14a/oentaryo14a.pdf>) описываются как ищут фрод клики, в том числе как создают фичи. По типу данных очень похоже на то, что у тебя.",
"Товарищи, подскажите, а как выглядит glorot_uniform инициализация для ядер свёртки?",
"Я сначала поставил самый последний, 361.57, а куде надо 361.48",
"Кто разбирался с `triplet loss` в статье `FaceNet` <https://arxiv.org/pdf/1503.03832v3.pdf> ?
Обхясните, пожалуйста, почему они отображают именно на сфера и почему в формуле (3) в скобки взято расстояние между одинаковым классом?",
"Кто разбирался с `triplet loss` в статье `FaceNet` <https://arxiv.org/pdf/1503.03832v3.pdf> ?
Обхясните, пожалуйста, почему они отображают именно на сфера и почему в формуле (3) в скобки взято расстояние между одинаковым классом?",
"&gt; Обхясните, пожалуйста, почему они отображают именно на сфера
Чтобы потом использовать косинусное расстояние между представлениями
&gt; формуле (3) в скобки взято расстояние между одинаковым классом
Если ты имеешь ввиду плюсик за квадратной скобкой, то это они так `max(0, L)` обозначили",
"<@U041P485A> да, но тогда почему они везде используют L2? По поводу плюсика: почему лосс выглядет:
sum `[` |f(x^a) - f(x^p)| - |f(x^a) - f(x^n)| + alpha `]`_+
а не
sum |f(x^a) - f(x^p)| + `[`-|f(x^a) - f(x^n)| + alpha `]`_+",
а почему он так должен выглядеть?,
"<@U0ZHHV83C> 
&gt; Ну, так ведь и должно быть (в теории). Что, например, если ввести коэффициент компромисса между первой и второй?
Можно попробовать, но боюсь затруднит обучение. Заметь они и так обучаются только на примерах где ||a - p || &lt; ||a - n || &lt; alpha",
"кстати, насчет triplet loss - у кого есть опыт metric learning в Tensorflow - как это правильно делается, есть ли примеры?",
вот вопрос - как это реализуется в теано - ты подаешь строенные картинки на вход?,
"Ворвусь ненадолго в разговор, в tf мы часто делаем batch-trick:
Когда на дополнительный вход или фиксированно подаётся разметка, по которой я знаю какой семпл с каким в каких отношениях (n, p, вообще не смешивать)",
"А ещё можно какую -нибудь переменную сохранять между батчами, и в неё класть промежуточные вектора, и то что в текущем батче получается, подтягивать или отталкивать.",
"я примерно как <@U065VP6F7> делал: передаёшь большой батч и метки, в loss'e режешь его, считаешь матрицы расстрояний и просеиваешь нужные триплеты",
"кстати да, можно не как я в тупую фиксируя размер батча, а передавать маску с метками (anchor, pos, neg) и в лоссе на неё просто умножать всё",
"Вкратце расскажу о квадруплет:
решается задача классификации, где на вход даются 2 объекта, а на выходе -- 0/1 в зависимости от того, принадлежат ли эти объекты одному и тому же классу. В качестве метрики используется сетка, которая отображает объекты в R^n, затем считаются две фичи: |x-y| и (x+y)/2  (операции покоординатные), эти фичи потом переводят в число с помощью FC слоев и результат подается на loss, который они придумали сами (говорят, обычный logloss не работает). Этот loss описан в формуле (3), коротко: оптимизируем triplet loss в 2 стороны, т.е. как раньше минимизируем между положительным классом и максимизируем между каждым из отрицательных классов.",
"ребята, а есть у кого опыт работы с графовыми БД? можете подсказать, с какой связкой было меньше всего боли?",
"а кто больнее, сам титан, сцилла или гремлин?",
"В районе 23000 на ebay продаются. 32Гб DDR4 лежит купленные, потому как планирую на 2011-3 собирать. Да, картинки хочу считать) Титан за 41000 взял с рук",
Не совсем понял какой проц имеется ввиду(,
"Куда больше времени уходит на отладку, подготовку данных и всего остального вокруг обучения.",
"Я бы брал консьюмерское железо. Как правило оно меньше греется и не нужно заморачиваться с серверным бп, а следовательно и корпусом.",
"Точно, с xeon можно хоть до 128 расширить, благо DDR4 по цене сейчас как DDR3",
"Я не согласен про серверное-консьюмерское, тепловой пакет примерно одинаковый, что там что сям.
БП серверный не на серваках, тоже не особо нужен. Если сокеты совпадают, все ок будет.

&gt; благо DDR4 по цене сейчас как DDR3
Вот только DDR4 у DDR3 вобщем-то ничего не выигрывает на настольных системах.
(я имею в виду, что энергопотребление поменьше, но в десктопах на это не особо смотрят, в отличие от портативных и серверных платформ)",
"А может кто-нибудь сказать примерно, какое соотношение в производительности будет между i7 5720k 6 core 3,3Ггц и xeon 16 core 1,8Ггц при обучении какой-нибудь нейросети и XGBoost, просто если с нейросетями потеряю немного с xeon, зато выйграю много с XGBoost, то вопрос можно закрывать в пользу xeon",
"Гремлин удобный. Проблема в том, что оно вроде работает, а как даешь пусть даже не очень большую нагрузку, начинает происходить ахтунг",
"я не знаю с чем это связано, но на домашнем i7 2600 я учил хгбуст примерно так же быстро, как на каком-то (не помню каком, да, тупо звучит) ксеоне на работе на много-много ядер",
"<@U0G29N5U4> а если данные исходные пошатать как-нибудь, чтоб посжимать гистограмку таргета? преобразования над криво распределенными фичами вроде как помогают ",
Может там какие интересные фичи всплывут ,
"коллеги, может кто видел размеченный датасет с самолетами",
"пока что запустил, как вы написали, получил:
Stopping. Best iteration:
[55]	train-rmse:5.81445	eval-rmse:1.53279",
"да это плохо поставленная задача. плохая метрика для данного процесса. но я одно не могу понять принципиально: почему xgboost не дает бимодальное распределение, а размазывает все вот так вот... фича, которая намекала бы на то, что вероятность нуля велика, есть. однако он берет что-то посередине...",
"а если фич многовато, может, поглубже деревья? как проще всего попробовать оверфитнуться?",
"<@U0G29N5U4> Best iteration 55 намекает на маленький early_stopping_rounds, какой он у тебя?",
"Не так давно на CV семинаре обсуждали статью на nips16 от лаборатории Лемпицкого, где для задачи metric learning растягивали гистограмы положытельных и нешативных пар",
"Парни, есть вопрос! Кто нибудь пользуется Nvidia Inception Program? Сегодня пришло письмо что вы приняты в программу, ждите новостей в ежемесячной рассылке. Очень нужны мощности, но месяцами ждать не могу... Если кто-то пользуется, подскажите долго ли придётся ждать? ",
Привет всем. Как обещал - запилил CLI к GloVe из text2vec для не R пользователей: <https://github.com/dselivanov/text2vec-cli>,
"отладить, чтоб как можно точнее нули определял",
"как жить с таким: посчитал матрицу фишера, а она нехорошая вышла. Мне она нужна для подсчета вероятности получения статистики. В оптах я бы регулеризовал ее да и все, но тут же так нельзя -- получится совсем другая вероятность :disappointed:",
"Кто-нибудь в caffe использовал ""MemoryData""? Почему оно постоянно выдаёт предупреждения ""MemoryData does not transform array data on Reset()""?

В исходниках следующее написано:

<https://github.com/BVLC/caffe/blob/master/src/caffe/layers/memory_data_layer.cpp#L52>

void MemoryDataLayer&lt;Dtype&gt;::Reset(Dtype* data, Dtype* labels, int n) {
  CHECK(data);
  CHECK(labels);
  CHECK_EQ(n % batch_size_, 0) &lt;&lt; ""n must be a multiple of batch size"";
  // Warn with transformation parameters since a memory array is meant to
  // be generic and no transformations are done with Reset().
  if (this-&gt;layer_param_.has_transform_param()) {
    LOG(WARNING) &lt;&lt; this-&gt;type() &lt;&lt; "" does not transform array data on Reset()"";
  }
  data_ = data;
  labels_ = labels;
  n_ = n;
  pos_ = 0;
}",
"Особенно настораживает следующий реквест, где человек год пытвется добавить transform_param в MemoryData <https://github.com/BVLC/caffe/pull/3100>",
"В общем решил проблему не решать: написал руками формулу PDF, где сразу использую матрицу точности, т.е. матрицу Фишера не надо обращать. ",
А зачем нужна матрица Фишера?,
"Матрица Фишера - это, я так понимаю, то, что в линейной регрессии получается как часть решения?",
"МLE оценка распределена как нормальное, где матрица ковариаций это обратная к матрице Фишера ",
Регулеризовать это как прибавить нормальный шум. Тогда уже кажется разумнее в вести приор на параметры ,
Но мне чот лень :),
<@U0G29N5U4>: почему фейспалм?) ты в каждой модели приор на приор вводишь?),
Лень... А как же спортивный интерес?,
<@U24FHECDD> отложенная выборка зачем она вообще? Почему не годится проверка на кросс валидации?,
"ну, переобучаться, конечно, стоит, когда объект нового класса не 1, а какое-то кол-во",
"теоретически, если юзать схему OneVsAll, и каждый из primary классификаторов будет SGD, то можно понятным образом ""дообучиться"", даже когда появляется новый класс",
"вот интересно, кто сталкивался с подобным? как решали?",
"чтобы прям на регулярной основе поялялся класс, у меня такого не было. если эта регулярность = условно раз в день или реже, я бы честно апдейтил модель целиком. если чаще, использовал бы KNN и вообще забыл о существовании такой вещи как “новые классы” :slightly_smiling_face:",
увеличение размера фильтров  в первых слоях вроде как приводит к улучшению качества,
"почему в картинках всегда используют очень маленькие фильтры, а у меня выходит эффективной какая-то дичь с фильтрами по 30 на ряде в 120 точек?",
"А на отложенной выборке, потом нельзя переобучиться еще? Цель моя какая при настройке модели максимальный скор на CV или на отложенной выборке?",
"если временной ряд имеет осмысленные гармоники максимум 10 Гц, а частота дискретизации например 100 Гц, то свертки должны быть как раз тоже порядка десятков точек",
"global pooling как раз для рядов часто используется, в том же spotify-cnn был",
"<@U0M39M6LS> при правильном подборе ядра – может быть, но это очень общее утверждение. Там кроме ядра остаётся только метод оптимизации, это как утверждать ""градиентный спуск при правильном подборе архитектуры сети даст результат лучше, чем любой другой алгоритм""",
"&gt;Популярность по определению ""мимолётна”
люди пользуются тем, что лучше и удобнее работает на практике здесь и сейчас. 

сейчас для неструктурированных данных хорошо заходят нейросетки, а для обычных табличек - бустинг деревьев. когда придумают чтото еще, что можно будет просто “достать из коробки” и будет показывать лучший результат, люди пересядут на него",
"<@U0ZHHV83C> не факт, что я правильно это понимаю, но smv вроде как на основе выбраного ядра проецирует исходные данные. их можно спроецировать очень хорошо и тогда получить супер точность",
"Пространство, вложив в которое данные они станут линейно разделяемыми, может и существует всегда, но svm его не строит. А вот сеточки всякие как раз это и пытаются сделать.",
"а то что можно доказать - бессмысленное времяпрепровождение. теорема об универсальном аппроксиматоре говорит что ты и RBF сеткой можешь сколь угодно точно хоть изображения распознавать. только пока ты будешь пытаться это сделать, ктото впилит resnet и соберет все золото и славу

или на обычном конкурсе, ты можешь играться с ядрами пока время не выйдет… в то время как <@U054DU76Y> сгенерит фич и стакнет 5-слойный xgboost",
"как только ктото придумает нечто, что будет лучше заходить на задачах чем бустинг деревьев, люди подтянутся и за несколько лет пересядут на этот метод",
"ну я смотрел отчёты этих годов, было много RF как раз",
<@U0H8RU34N> да :slightly_smiling_face: иначе тех кто делает модельки заменили бы на жадный перебор и выставили на мороз,
"это как ранг матрицы по её изображению искали, там еще собака в соавторах была",
"Вангую, ближайшее, что может сдвинуть RF с позиции универсального бойца - это градиентный бустинг, но который тюнить было бы проще, чем Xgboost. В этом плане интересно c LightGBM поиграться. Кстати, ходят слухи, что нетюненный матрикснет работает как тюненный  хгбуст, так что вполне возможно, скоро появятся открытые аналоги, с которыми работать будет приятней, чем с хгбустом.",
&gt;&gt;&gt;нетюненный матрикснет работает как тюненный  хгбуст,
"Прошу прощения, что врываюсь в разговор.
Подскажите, какую функцию ошибки можно использовать, если моделью мы предсказываем выигрыш в игре?",
"<@U04URBM8V>, <@U07V1URT9> в продолжении разговора про свёртки — более менее то о чём вы говорили как раз у fchollet в последней его статье про Xception — <https://arxiv.org/abs/1610.02357>",
"<@U040HKJE7> ""в то время как <@U054DU76Y> сгенерит фич и стакнет 5-слойный xgboost» что такое 5слойный xgboost? где почитать? что то я про слои в xgb как то пропустил",
"обычно делают слоя 2-3, но бывают и особенные случаи когда их 5+",
"Всем доброго веремни суток, господа чатящиеся!
Есть ли у кого-нибудь (или то, как можно достать) данные по кредитному скорингу по СНГ? ( Америку не предлагать. Немцев тоже)",
"коллеги, всем привет! хотел спросить, есть ли где в свободном доступе база авторефератов и абстрактов научных статей на русском языке и можно ли ее как-нибудь достать не изобретая велосипед?",
"может быть кто слышал про открытые фтп библиотек, откуда можно выгрузить что-нибудь подобное",
"<@U0E4S5LU9> <@U25UPSERX> не было идеи сделать имплементация adagram в питоне? и какие мысли на счет нее?
<@U0P95857C> знаю что делал - но не знаю чем это закончилось",
<@U25UPSERX> зачем вы кучу людей лично помянули?,
"Ребята, нид хелп! Нужно предсказать продажи по категориям товаров на февраль - июль 2017( т.е на 9 месяцев вперед, начиная с текущего месяца).  Есть исторические данные по ежедневным продажам за 12-40 месяцев, большая часть данных  содержит сезонность. Попробовал персептрон и LSTM -  плохо работают. Сейчас смотрю в сторону классических моделей (SARIMA), ковыряю библиотеку statsmodels. Statsmodels не нравится. Стоит ли попробовать pyflux или какой-то другой инструмент? С каких алгоритмов лучше начать ? Какой минимальной длины должны быть исторические данные  чтобы делать предсказания?",
"кстати, а есть какие-нибудь черномагические трюки, чтобы байесовские credible intervals к классическим ML моделям прикручивать малой кровью? 
например, с простыми линейными моделями можно с нуля написать на pymc регрессию. а как с чем-то более сложным?",
"Помогите расклиниться. Рассмотрим EM алгоритм для p(X, T| theta). На E-шаге мы находим распределение p(T|X, theta) Где X фиксированы и это выборка. На M шаге бы берем по этой мере матожидание от p(X,T,\theta)",
"почему мы пишем совместную вероятность p(X,T,\theta)",
"хотя по факту работаем с ней как p(T,\theta|X=выборка)",
когда байесиане пишут именно плотность вероятности,
а когда туда уже подставляется значение,
не понятно почему мы в X подставили выборку,
информацию которую понаблюдали надо писать как  условное,
А как ты плотность в правдоподобии выборки из одномерной гауссианы будешь писать? `p(|x)`?,
"если модели линейные то ничего страшного, как мне кажется",
"да, согласен, в теорме байеса самой по себе ок, там умо как раз по множеству событий, порожденной сл.в.",
"Выборка там тащится с самого начала из \log p(X), как ты уже заметил",
"T возникает посередине: сначала годится любое значение, а потом махинациями формула разбивается так, что получается, как если бы мы интегрировали по T",
но прикольно посмотреть как люди для проблем формурулируют байесовские модели,
описаны примеры байесовских моделей как люди конструируют,
Когда тебе рассказывают метод и его границы применимости на языке математики без деталей реального мира,
"ну когда первый раз рассказывают, то да. а так как оно в реальном мире мне кажется тоже интересно посмотреть.",
"Ребят, а есть кто разбирается в том как определять размер выборки если исследуемая переменная количественная и распределена не нормально?",
"&gt; если выборка большая и бустрап полный, то достаточно страшно
почему? в статье написано почему? а как же bootstrap aggregating?",
"Нужно не как можно больше, а чтобы достаточно ",
"Как это, надо чтобы распределение в генеральной совокупности было нормальным ",
"Ну и опять же нсли оно у генеральной не нормальное, как оно будет у выборки нормальным",
"Не, это ты про другое как я понимаю ",
"Ну так как переменная распределена не нормально, то тест Манна уитни",
"Кстати, если данных очень много, то ведь не обязательно делать полноценный бутстрап, как я понимаю. Можно просто сделать N выборок с возвращением или без по вкусу",
"<@U0AD1L5NC> какой там принят порядок размерностей (NWHC или NHWC), совпадают ли операции свертки, паддинг такой же, как SAME?",
"если распределение не эмпирическое то не понятно, почему бустрапные статистики должны сходится к истинным",
"если выборки меньшего размера, чем данные, то вроде как нет",
"Была деталь со сверткой, как сейчас помню",
"Есть кто на <http://www.odsc.com|www.odsc.com> в Санта Кларе? Я вот приехал, можно пересечься.",
"<@U1G303UTW> как думаешь, если организовать в Минске DS митап как их проводят в Москве и Питере, и привезти спикеров из Москвы условно, сколько народу бы пришло?",
"на datatalks до сотни приходит, но там есть и те, кто руками вообще ничего никогда не делал",
"кто либо пробовал добавлять свой layer в Keras и свою loss function? были ли какие либо подводные камни? или есть хорошие примеры на эту тему?
навскидку всё выглядит без проблем",
"И не нужно для каждой картинки провозить процесс оптимизации, как в изначальных adversarial examples",
есть ли на питоне какие либо библиотеки построения красивых графиков для инфографики?,
"или может какие то примеры, как из стандартных matplotlib/bokeh/seaborn сделать конфетку, которую не стыдно показывать?",
"Кто там спрашивал про поиск архитектур сетей с помощью рл?
<https://www.reddit.com/r/MachineLearning/comments/5b5022/r_neural_architecture_search_with_reinforcement/>",
"<@U0AD1L5NC> я имел в виду иммунитет, к конкретным пертурбациям (до чего кривое слово), как настоящий иммунитет работает -- убивает только то, что уже видел раньше",
"Много вопросов возникает сразу -- а как оно будет сохраняться при каком-нибудь заблюривании? А что если не классификацию делать, а сегментацию по adversarial семплам -- где оно покажет левый класс? Везде, в виде некоего шума? Тогда может быть можно найти у него какие-то характеристики, по которым можно попробовать отфильтровать или что-то подобное",
Тривиальные вещи вроде как не работают,
"Вот тут интересно (даже в закладках уже лежало, хотя понятия не имею откуда прилетело). Насколько я понял в пять утра после чтения по диагонали обеих статей, то они делают нечто похожее, только верхняя ищет какие пертурбации добавить для обмана, а вот эта -- были ли они к картинке добавлены  <https://arxiv.org/abs/1610.06940>",
"Всем привет, хотелось бы обсудить несколько вопросов/идей по поводу анализа поведения покупателей. Допустим есть логи с площадки типа Avito.

1.Наткнулся недавно на интересный подход Prod2Vec
<https://arxiv.org/pdf/1606.07154.pdf>
По сути это тот же word2vec но вместо слов - товары, вместо предложений - последовательность товаров купленных(просмотренных) одним пользователем. Обучил, результат действительно хороший, схожие продукты рядом. Вопрос что делать с новыми товарами…

Идея навеяна <http://www.slideshare.net/Geeks_Lab/aibigdata-lab-2016> (слайд 8 ), <https://youtu.be/I5gEQ7W3_1g?t=408>
А что, если попробовать извлекать векторные представления товаров из их изображений? То есть - берем результат prod2vec для всех товаров с которыми взаимодействовали пользователи и строим CNN, на вход которой картинка - на выходе векторное представление(prod2vec). Планирую взять что-то стандартное для imagenet изменив последний слой. Обучаем данную сеть. Получаем способ получения векторного представления для новых товаров.
Как думаете, будет работать?
А как в эту схему бы ещё впихнуть title/description на вход (кроме картинки)?

2. В публикациях по prod2vec один из подходов рекомендовать ближайший по косинусной мере товар к последнему. Мне кажется это не логичным, если я уже купил микроволновку зачем мне её показывать ещё раз? Другой подход более логичен - кластеризовать товары в пространстве prod2vec и посчитать вероятности перехода из кластера в кластер по логам. Для рекомендации брать топ K продуктов из N наиболее вероятных кластеров следующих за последним. 
А нельзя ли тут попробовать применить что-то для предсказания последовательностей, LSTM напрмер, на последовательностях категорий товаров?

3. Да, а почему не применяют LSTM на основе векторных представлений слов (после word2vec). Видел char level rnn, напрмер <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>, а вот на основе векторных представлений - нет.
И соответственно почему бы не применить этот подход к товарам. Построить LSTM предсказывающую следующий вектор товара на основе последовательности предыдущих.",
"да, но у меня тут больше про аналог word2vec для продуктов, который они как раз и сравнивают с Co-occurrence",
"Привет! нужен датасет по проданным квартирам в Москве и МО за текущий год, желательно с ценой и локацией. Может у кого случайно завалялось?",
"Да, вот как раз это и нужно. Так как есть данные циана, го хотелось бы понять какая реальная цена проданной недвижимости ",
"это когда ты не просто говоришь что на фотке есть кошка,а еще говоришь где.Например присваиваешь пикселям объекта соотв. лейблы(сегментация),или как-то обводишь его ящиком,на основе хождения скользящим окном по картинке.Или еще как.",
"Чят, привет. Классифицирую тексты резюме, в качестве фич исключильно n-граммы слов. Модель линейная. При этом получается, что короткие резюме с одной-двумя ""правильными"" фичами сразу оказываются в топе. Типа тот кто просто написал ""программирую на джаве"", сразу имеет неразумно высокую вероятность. Более того, другая крайность - длинные резюме, в котором перечислены 100500 технологий тоже оказываются чересчур хорошо оценены, и это тоже нежелательно. Что бы придумать, чтобы штрафовать ""слишком короткие"" или ""слишком длинные"" тексты так, чтобы это не выглядело костыльной эвристикой? Порекомендуйте плз что-нить про классификацию текстов из современного, кроме fasttext  и  RNN, CNN на текстах. Спасибо! Обнимашки из Шотландии.",
"Когда то очень давно работал аналитиком в сфере недвижимости. Такого датасета в принципе не существует, и я даже технически не представляю как его можно собрать.",
"Вот еще какая мысль в голову пришла: можно использовать что-нибудь типа additive smoothing, при сильной регуляризации маленькие документы могут уйти вниз",
"господа, такой вопрос, есть табличка большая с четырьмя действительными положительными колонками, нужно семплировать строки из совместного распределения, как бы такое сделать? 

я такое на самом деле раньше делал, но хочется что то по проще, раньше я просто обучал RBM, затем просто семплировал скрытый слой и выводил видимый, ну или можно начать с реальной точки и затем MCMC до какого то нового семпла

а ну еще можно добавить, что каждая колонка совсем не нормальна, скорее гамма",
"Во первых агентов и контор много, т.е. чтобы реально собрать датасет по Москве и МО придется хорошо побегать. Причем, как выцепить всех ""частных маклеров"" и тех кто делает сделки самостоятельно, я вообще не представляю. Ну а вторая мысль верная, делится этой информацией они не захотят.",
"гаааайз! а многомерный график с помощью ползунков или других элементов UI, как бы сделать проще всего?",
"Ребят, у кого есть табличка с координатами стран? Для визуализации нужны",
"Есть вопрос про классификацию изображений. Есть фотки, довольно много (около 1млн) размечены на два десятка классов. Классы - причины отказа загрузки фотографии. Причины могут быть законодательные (алкоголь, табак, оружие), этические (порно, эротика, просто треш), качество (перевернута, малое разрешение) и т.п.
Вопрос: какой best practice метод по классификации изображений? Сложность в том, что фотографии очень разношерстные, т.е. это не собачек от кошечек отличить. Какой вообще корпус по размеру должен быть, чтобы хотябы 90 F1 иметь?",
"Всем привет, попробую и здесь запостить, хотелось бы обсудить несколько вопросов/идей по поводу анализа поведения покупателей. Допустим есть логи с площадки типа Avito.

1.Наткнулся недавно на интересный подход Prod2Vec
<https://arxiv.org/pdf/1606.07154.pdf>
По сути это тот же word2vec но вместо слов - товары, вместо предложений - последовательность товаров купленных(просмотренных) одним пользователем. Обучил, результат действительно хороший, схожие продукты рядом. Вопрос что делать с новыми товарами…

Идея навеяна <https://youtu.be/I5gEQ7W3_1g?t=408>
А что, если попробовать извлекать векторные представления товаров из их изображений? То есть - берем результат prod2vec для всех товаров с которыми взаимодействовали пользователи и строим CNN, на вход которой картинка - на выходе векторное представление(prod2vec). Планирую взять что-то стандартное для imagenet изменив последний слой. Обучаем данную сеть. Получаем способ получения векторного представления для новых товаров.
Как думаете, будет работать?
А как в эту схему бы ещё впихнуть title/description на вход (кроме картинки)?

2. В публикациях по prod2vec один из подходов рекомендовать ближайший по косинусной мере товар к последнему. Мне кажется это не логичным, если я уже купил микроволновку зачем мне её показывать ещё раз? Другой подход более логичен - кластеризовать товары в пространстве prod2vec и посчитать вероятности перехода из кластера в кластер по логам. Для рекомендации брать топ K продуктов из N наиболее вероятных кластеров следующих за последним. 
А нельзя ли тут попробовать применить что-то для предсказания последовательностей, LSTM напрмер, на последовательностях категорий товаров?

3. Да, а почему не применяют LSTM на основе векторных представлений слов (после word2vec). Видел char level rnn, напрмер <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>, а вот на основе векторных представлений - нет.
И соответственно почему бы не применить этот подход к товарам. Построить LSTM предсказывающую следующий вектор товара на основе последовательности предыдущих.

Да, вот нашел что-то похожее про LSTM <https://www.researchgate.net/publication/292972291_Deep_Temporal_Features_to_Predict_Repeat_Buyers>",
"Ну и да, такие вещи как плохое качество и низкое разрешение - это просто по EXIF",
Как медалхантер поинтересуюсь: а какие призы?,
"Локальные оптимизации, как и в Torch, насколько я понимаю",
"Гайз, кто на скинхак идёт? Хочу в команду! ",
а как можно это контролировать?,
Еще бы узнать какие там призы. Я тогда может сам буду яростно учить сетки на devbox'e с работы.,
"у Яндекса как правило призы - клевая сувенирка и все то, за что не нужно платить налоги
<@U1Z7QM16H> можешь меня поправить если я ошибаюсь :eyes:",
"неее, нам надо чтото более устрашающее. градиентный бустинг где resnet - base-learner :more-layers: :xgboost:",
а чтоб по имени предсказать национальность -- есть где-нибудь датасет? или как бы его собрать?,
"А где в bokeh ползунки прикручиваются? Я документацию к rbokeh смотрел, callback есть, ползунков нет.",
"Всем привет! У меня вопрос новичка)
Есть набор точек и матрица расстояний между ними.
Нужно выполнить кластеризацию. Число кластеров не задано.
Имеются ограничения на количество точек в кластере (не более m) и на количество кластеров (n_min &lt; n &lt; n_max).
Подскажите, плз, какие есть подходящие алгоритмы для данной задачи.
У меня мысли пока две: 1) использовать k-means с подбором количества кластеров по какому-то принципу;
2) написать что-то свое.
Что есть еще на эту тему? Задача типичная, думаю.",
"а как кстати выбирать метод кластерзации то? в к-минс хорошо то, что хотя бы достаточно визуально понятно что он делает",
"Когда дээнку выделял и гены клонировал, а про рэндом форест и не слыхал.",
ну и если данные хорошие - то почему бы и не использовать,
Можно его как иерархический метод использовать - итеративно разбивая наибольший кластер на два,
"Если строчек больше 10к то с иерархическими методами как-то сложнее, как мне кажется ",
"Может кто-нибудь расскажет, как этим делом правильно пользоваться?",
"не могу не прорекламировать) как раз будем обсуждать POMDP
<https://events.yandex.ru/events/ds/29-nov-2016/>",
но какие там обеды в яндексе!,
"Вот анонс 

&gt; В эту пятницу на спецсеминаре выступит Александр Гасников, МФТИ.
&gt; Название: Современные численные методы стохастической оптимизации и их приложения
&gt; В докладе пойдет речь о том как с помощью стохастической оптимизации можно решать задачи математической статистики (агрегирование оценок) и статистической теории обучения.
&gt; Доклад будет носить обзорный характер.
&gt; Начало спецсеминара в 18:45. Ориентировочная длительность доклада - 2 часа.
&gt; Расписание спецсеминаров и информация о пропусках: <http://goo.gl/kluD8m|goo.gl/kluD8m>",
<@U042UQC96> а можешь дать какой-нибудь мануал на тему как сварить nvidia-docker со своими пакетами в питоне и жупитер тетрадкой?,
"для установки питона/юпитера пишешь команды в докерфайле. Как ты это делаешь обычно:
```RUN apt-get install -y ...```",
"кстати вдруг стало интересно как происходить тренировка сетей на нескольких GPU судя по примерам к Tensorflow модели ранятся параллельно, а затем результаты объединяются на CPU… но это  ж пересылка данных от GPU к  СPU и обратно, нет ли более продвинутых механизмов? типа общего адресного пространства на GPU? чтобы результаты можно было объединить на GPU избегая пересылки данных?",
"Образ можно собирать на чем угодно, nvidia-docker -- это обертка к docker-cli, спросит у nvidia-docker-plugin (это демон с rest api) что да как подрубать",
"12го те кто пойдет на зарешку - приходите сразу после нее в DI Telegraph, у нас много интересного весь день и вечер",
"место, где вживую можно потроллить <@U04URBM8V>",
"Ребятки те кто с pymystem дружат, не подскажите? Не могу в нём найти ""Веса разборов и снятие омонимии"", которые в <https://tech.yandex.ru/mystem/doc/usage-examples-docpage/#weight-omonimy> описаны.",
<@U040M0W0S>  <@U25UPSERX>  ребят а запись по генсим спринт будет? потому как в четверг вечером занят =(,
"```
19:00	Борис Янгель: Neural conversational models: как научить нейронную сеть светской беседе	

20:00	Евгений Волков: Goal-oriented диалоговые движки	
```",
"<@U1CF22N7J> ээ а как же продувка, мать не перегреется? :slightly_smiling_face:",
"Товарищи, кто умеет искать по специальным ресурсам - вот такая книга в pdf вам случайно не попадалась?
<https://www.packtpub.com/big-data-and-business-intelligence/learning-probabilistic-graphical-models-r>",
"интересно,  каким образом деревья, которые не умеют экстраполировать, помогают найти области пространства параметров, в которых целевая функция будет больше (меньше), чем они видели раньше?",
скорее наоборот квоты как в ссср,
"<@U13E1AWCX> Постараюсь
Рассматривается ситуация, когда есть не сильно большое количество объектов в пределах сотни. Эти объекты довольно самостоятельны. Можно думать о них как о странах(k объектов), в которых изучается влияние разлчных факторов на экономический рост. У каждой из стран есть своя история и наблюдаемый экономический рост(N_j наблюдений). Так же есть вера, что каждая страна довольно индивидуальна и влияние факторов на рост там везде какое-то свое. 
Помимо веры в то, у стран все как-то по-своему происходит, мы знаем, что некоторые страны похожы друг на друга(ну например какие-нибудь две страны из ЕС) и у них факторы будут влиять на рост похожим образом. А есть страны не похожие друг на друга(США и Нигер) Вряд ли у них одинаковое влияние. Эта мера схожести, конечно, отдельная тема, но, допустим, у нас есть экспертные оценки(можно и распределения, как удобнее) в интервале [0-1] - ""не похожие--похожие"" для удобства(а что делать если в другой шкале и думать пока не хочу). В оценке модели для одной конкретной страны вообще неплохо бы учитывать то, как процесс себя ведет в похожих странах и давать больше значения тем процессам, которые нам кажутся должны быть похожими.
В решении этой исследовательской задачи кажется разумным использовать байесовский вывод, но само использование графа похожестей стран с точки зрения байеса для меня не очевидно, поэтому нид хелп.",
"Тогда когда придет новый объект, то его скрытый фактор можно будет вычислить, проинтерполировав по остальным объектам соразмерно их похожести на новый",
"А для тех, что еще не новые, как получить скрытые факторы?",
"Получается, что страны находятся в каком-то пространстве, где задано какое-то расстояние и оно видно нам через похожести(или какую-то другую меру), и уже отуда берутся параметры для распределения параметров модели",
Как ты вводишь в ГП свою обучающую выборку?,
"Ты сужаешь носитель этого распределения, когда обуславливаешь ГП, задавая ограничения вида f(x_i) = y_i",
"Потом ты спрашиваешь, а какое распределение на значение функции f в точке x_new",
чота митап скатывается в диплернинг. При мне такого не было!,
"Ладно, я спать. Огромное спасибо! Я в таком ключе еще не думал про панель, но это очень похоже на то, куда зарыться в поисках ответа.",
"А ни у кого не остался старый датасет про планктон, с разметкой для тестового множества?",
Присоединяюсь к вопросу. Интересно в сравнении с Tableau как она.,
"Я к тому, что тут  регулярно всплывает хула в адрес таких известных библиотек, как bokeh, plotly и прочее, а тут какое-то безымянное поделие предлагает их функционал да еще и в дэшбордах. Отсюда и сомнения.",
"Ребят, кто с gensim word2vec работал? Такая ситуация: я обучил w2v модель, смотрю близость предложений по метрике wmdistance и получаю какие-то значения. Все гуд. Потом я эту модель сохраняю на диск: model.save(), закрываю ноутбук юпитеровский, открываю, загружаю модель model.load(), смотрю близость wmdistance и она уже не работает. Ошибок не выдает, но всегда возвращает inf. Хотя model.most_similar() работает как обычно норм.",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1478696004003635>
вообще как там реально устроено довольно интересно… а то без деталей выглядит как шаманство",
"ну это скорее как looker, насколько я понимаю",
"в прошлом году пробовал обучить с помощью q-learning машинку ездить, она кое как научилась проезжать круг при этом задевая стены постоянно, но все равно прикольно было, опыт получил) 
Кстати,  кажется, что если придумать хорошие действия и хорошее состояние среды, то для RL будет более выгодно, так как в прошлом году, насколько я понял подход бы такой, что перебирали траектории, зная физику. Здесь же среда такая, что эту схему не применишь уже.",
"&gt;Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. 
Не пойму, а зачем может пригодиться датасет без описания фич?",
"Привет!  
Кто-нибудь *Caffe* использовал?  
Что там вот это значит: 
`  net.blobs['data'].data[...] = transformer.preprocess('data', im)` ? 
Правую сторону я понимаю, а вот куда там Эллипсис ведет на левой стороне? 
Наверное, хорошего понимания Питона тоже было бы достаточно… в общем, помогите, чем сможете. Заранее спасибо!",
"net.blobs['data'].reshape(batch.shape[0], 3, 224, 224)

вот например когда размер батча меняется",
"ага, то что аттрибут data - он тоже какой-то объект, который готов принять, что дадут… т.е. преобразованное изображение, которое как батч двумерных массивов идет…   Как-то так?",
"<@U13E1AWCX> <https://arxiv.org/abs/1611.02731> Уже видел ? Абстракт какой то размытый. В эту среду в ШАД как раз был разбор статьи про PixelRNN. Либо плохо разобрались, либо там очень ""абстрактные"" результаты .",
В рамках похожей задачи у меня как раз все в fe упиралось,
"<@U10HRKG4B> это происходит, когда у тебя в предложениях есть слова, которых не было в w2v модели",
"А кто знает, трансляции завтра с Sberbank Data Day будет какая-нибудь? Или по факту запись.",
как минимум потому что надо организовать 1200 наклеек на бейджики :eyes:,
или может есть хоррошая идея как это извлечь из настоящих картографических сервисов?,
а как бы дергать оттуда пару: снимок/слой разметки?,
"<@U040M0W0S> есть вот такой проект <https://github.com/trailbehind/DeepOSM>, там есть ссылки где данные взять можно",
"Я когда смотрел пришёл к выводу, что проще купить отдельный комп или ноутбук.",
"как и написано — третий подъезд. У нас в этот день Субботник для дизайнеров, не перепутайте)))",
"Сообщество, такой вопрос. Есть два :seat: класса :heavy_plus_sign: :heavy_minus_sign: . Точнее, есть только :heavy_plus_sign: и штука из которой можно сэмплить :heavy_minus_sign: . Хочу сделать коробку которая по входу из н насемпленых :heavy_minus_sign: будет говорить какой из них больше похож на :heavy_plus_sign: . Казалось бы, ранжирование, но меня интересует топ 1, и не интересует порядок. Не проще ли обучить какой-нибудь классификатор с хитрой метрикой и потом брать argmax по предикшенам?",
"Мне кажется, или можно переформулировать: ты хочешь сделать коробку, которая говорит, какое правдоподобие у семплов?",
"Если рассматривать сетку как feature map, то да (см., например, логистическая регрессия — там сигмоида)",
"это фейк, как вы знаете",
интересно где они используют reinforcement,
Ты никому не расскажешь из какого офшора они пришли?,
Это где же так сурово?,
"С поисковиками всё непросто, там, как правило, в топе магазины, плюс правообладатели пользуются теми же поисковиками, находят сайты со ссылками на всякие файлопомойки, а потом пишут письма счастья последним, угрожая судебными исками, поэтому в интернете ссылки зачастую дохлые",
"&gt; а кто-нибудь когда-нибудь подписывал NDA на _пять_миллионов_долларов_США_ на хакатонах?)
&gt; Skinhack
Кто там его проводит? Youth Laboratories? Создаётся ощущение, будто у них ничего, кроме тех 8 500 размеченных картинок, и нет",
"<@U0AS548A1> когда мы начнём приём заявок, нужно будет пройти тест и написать эссе, почему нужно тебя взять",
"Может кто знает, есть ли для русского языка корпус для обучения определения семантической схожести двух текстов? Для английского есть соответствующее задание на SemEval <http://alt.qcri.org/semeval2017/task1/> , а для русского нашел только workshop RUSSE <http://russe.nlpub.ru/>. Но там задача определения семантической схожести только для слов.",
"Ребят, а по яндексовским зарешкам что-то выкладывают? не попал сегодня, а как раз хочу решать allstate...",
"ребята, кто завтра собирается на gensim coding sprint - коворкинг ""Белый лист"" - это вот это?
<http://www.belylist.ru/>
<@U25UPSERX> , не подскажете?",
"друзья, а есть у кого DRIVE ретинопатийный?",
"пацаны, а я вот смотрю в htop, пока теано сетку обучает. и так вот периодами CPU подскакивает. это вообще в какой момент? я тут понял, что не очень себе пайплайн представляю в контексте ресурсов. думал, что по-максимому грузится напрямую в GPU, ну и все :slightly_smiling_face:. а CPU-то чего делает?",
"как я понял электронной версии нет, по крайней мере так написано в самой книге",
"вроде ни у кого без :xgboost: не обошлось, а разница была в фичах и том, что перед ним стакали",
ну вот как перед этим стакали поподробнее,
"и наградить толстовкой того, кто опишет эти решения их плюсы и минусы в статье",
Зато какие ковры на фотке!,
"<@U07V1URT9> <@U0PEXP9PH> с успешной медалькой :slightly_smiling_face:

а че выиграли то? много участвовало? много кто дошел до конца и запилил решение?",
"Это третьи выходные подряд, когда участвуем в хакатонах. И 3й раз побеждаем (если считать победой питерский, где решили только одну номинацию из двух).",
то есть выиграли а как называется ты хз?),
"<@U0M39M6LS> Мегафон. но нам нельзя было в нем участвовать, так как у меня тест сет был и сбамит-машиной выступал я",
"как сабмит отправять train, y_train и test",
"И кто больше годных фичей сделает, тот выйграл",
как определить годные ли фичи?,
"у кого скор выше, то больше годных фич нагенерил",
А почему мы это обсуждаем на <#C04422A5C|_meetings>? :thinking_face:,
В твоём случае понятно почему долго,
"а не, 40 минут - это просто проход по всему дата сету) то есть эпоха, как я понял",
"Когда запустишь xgboost, то в топ-10 ворвешься))",
"Как говорится ""Он наступит скоро, надо только подождать""",
А какой там размер трейна?,
как в python посчитать значение функции распределения многомерной нормальной?,
А так как он многомерный — Монте-Карлой его!,
"последний раз когда я смотрел многомерный нормальный CDF, у него была “почти” аналитическая формула для произвольной ковариационной матрицы
перобразовать св с произвольной ковариационной матрицей к диагональной линейным образом скорее всего нельзя. поворот типа главных компонент сделает ее с минимальной дисперсией, но ковариации\корреляции между компонентами все равно останутся",
"Надо лишь понять, как в `A x &lt; b` с положительно опредённым `A` перебросить матрицу направо",
"я так и думал сделать, но чот блин я просто хотел посчитать куда у меня точка попадает мимоходом а не развлекаться :disappointed:",
"<@U1CEDPJSU> это был кустарный конкурс чисто под этот хакатон. достаточно аскетичные маленькие данные, надо предсказать какой клиент “приоритетный” тоесть чето из услуг купит. участвовало правда всего 3 команды и не факт что конкурс появится когданибудь еще",
<@U040HKJE7>: а когда он был? Просто работая в мегафон о нем не слышал.,
в очереди в столовой баталии - споры про обратную связь и как устроено зрение,
"А мне нравится как Nvidia троллит Интел: 
&gt; So it’s understandable that newcomers to the field may not be aware of all the developments that have been taking place in both hardware and software.",
"там дают пообщаться с кем-то онлайн? и нужно определить где человек, а где бот?)",
"Господа, дайте знать, если увидите ошибочные данные в блоге NVIDIA. Мы прилагаем много сил, чтобы не было никаких ошибок, но кто от них застрахован?",
"Там дают написать бота. А потом жюри будет оценивать их. Возможно, как раз на сцене",
"В эту субботу (19 ноября) на тренировке <#C1CEM43TJ|mltrainings_live> будем разбирать следующие соревнования:
- Алексей Носков расскажет про то, как он вместе с командой Another Haunted and Leaked One участвовал в соревновании Bosch Production Line Performance на Kaggle и занял там 6 место.
- Михаил Горкунов расскажет про первую задачу с контеста Сбербанка <#C2LJA6VP0|sberbank_contest>, в которой занял 6 место.
- Александр Желубенков расскажет про вторую задачу с контеста Сбербанка <#C2LJA6VP0|sberbank_contest>, где занял 4 место в общем рейтинге.

Регистрируйтесь, приходите. Будем ждать!
<https://events.yandex.ru/events/ds/19-nov-2016/>",
mnist - это рукописные цифры?  1) почему именно логистическая регрессия 2) зачем её в keras делать?,
"ну вот у меня тоже была мысль про краулинг
но вдруг кто видел уже готовое %)",
но почему бы и нет :slightly_smiling_face: это же учебная задачка,
"<@U04423D74> А какие ещё параметры нужны кроме цен?
Есть только цены. 
<http://priceanalytic.com> - могу выгрузить нужные категории по состоянию на какой-нибудь день.",
"Вообще <@U0U2ENJ4U> мем выдал - кто рассуждает про AI, в жизни не программировал линейной регрессии даже. ",
Блог их радует. Intel к xeon phi делал какой то маркетинговый ужас. Порой были замеры по производительности с предыдущими поколениями их CPU. Все ради фразы эта штуковина в 10 раз быстрее.,
"Всем привет. У меня такой вопрос. Есть векторное представление некоторого объекта (напр. предложения). Я его хочу раскодировать в последовательность слов, что обычно делается рекуррентной сеткой. Но при этом мне важно уметь задать первые несколько слов, а чтобы сетка продолжила предложение. Вот я не совсем представляю, как можно это начало предложения запихнуть в декодировщик, чтобы он именно продолжил предложение?",
А как вообще _обычно_ это делается rnn?,
"если нужно просто текст сгенерить - как тут, то мне вроде оль-мень все понятно",
но в случае с подаче семплированного входа на выход -  не совсем понимаю куда вектор исходного предложения условно девать,
но чтобы начало было как я сказал,
"Это не по статье совсем. 
Вектор исходного ты можешь кучей способов загнать в декодер.
Для начала в hidden, на вход свои токены, когда свои кончатся, брать семплированные",
там же где прямая трансляция была,
Особенно когда фуршет начался :trollface:,
А по какому слову гуглить обсуждение? Я давно тут ничего не читал уже.,
"коллеги, столкнулся с типовой задачей, но с которой никогда не работал
есть поток данных (грубо говоря, кликстрим), хочу научиться оценивать ""нормальность потока"" (все ли данные забираем, не теряется ли что и проч.)
в идеале - еще и предсказывать ситуации, когда будет высокая/низкая нагрузка
куда смотреть, как это в мире белых людей делается?",
"<@U1BAKQH2M> какие у них были проблемы с locf? кинь ссылку, любопытно",
ты лучше скажи какое отношение пришедших до зарегистрированных :slightly_smiling_face:,
Открываю первый аукцион где меньше 100,
"Привет. Есть вопрос: нужно из предложения, например ""would like to visit London tomorrow and spend 1000$"", извлечь дату (завтра), место (Лондон) и бюджет (1000). Каким образом это лучше сделать? До этого с NLP не работал почти.",
"можно звать только тех, кто есть в этом чате. и пригрозить: ""если вы зарегались и не пришли, то мы вас кикнем из ods"" :putin:",
Я так думал. Поправьте если не прав. Решается задача понижения размерности. А как вычисляются значения пикселей на пониженной размерности это просто параметр: avg/max/decimation.,
"А, извини) Отличное название. Если сможешь выговорить ""субдискретизирующий"" без запинок, то все сразу поймут какой ты умный.",
"Занимаюсь задачей классификации предложений(по сути коротких текстов из 2-10 слов, длина у всех конечно разная). С вопросами по этой теме писать сюда?
С чего посоветуете начать?
Пробую word2vec, tfidf - далее сверху xgboost
Какие еще классификаторы могут сработать? Что еще попробовать - может lstm? Можете накидать статей годных?",
"Как можно посчитать корреляцию между категориальными признаками? Между численным и категориальным? Был вариант для каждого значения категории смотреть расстояния между распределениями численного, но вдруг есть какой-нибудь норм метод. ",
"&gt; Как можно посчитать корреляцию между категориальными признаками
<https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>",
"Так как предложения короткие, w2v тоже хорошо должен пойти",
"Хз какая у меня регрессия, возможно буду писать обжектив для хгбуста, может в сетки пихать ",
"Заносить как фичу, либо прямоугольную, либо затухающую",
"Всем привет, подумал над обсуждением на завтраке про LSTM для предсказывания музыки, осталась неясной следующая вещь: если на вход LSTM при обучении подаются батчи вида ""100 нот на вход - следующая на выход"", то как тогда LSTM может ""выучить"" всю песню целиком? При такой схеме все скрытое состояние при обучении определяется только для последних 100 нот + при семплинге нужно пихать, опять же, 100 нот, добавлять одну и т.д.
В моем понимании оно должно работать так, что семплировать можно просто подавая одну следующую ноту на вход вместо каждых последних 100, а все остальное поддерживается скрытым состоянием на протяжении всей последовательности (песни, временного ряда, ...) - правильно ли я понимаю схему? И как тогда следует проводить обучение, т.е. каким должен быть формат батчей?",
"Да, у меня как раз вопросы в том, что:
1) Если у меня последовательности длиной 1000 (и генерить я хочу тоже длинные), то как сеть научится помнить больше, чем 100 символов? Там же какие-то long term dependencies вдруг.
2) При семплировании тоже нужно пихать 100 последних символов, хотя предсказываю я только один - не логичнее пихать таки один последний символ (предсказанный), раз у сети есть скрытое состояние?",
"на practice лично я генерацией музыки не занимался, но мне кажется, что
1) .. как сеть научится помнить больше, чем 100 символов? - stacked lstm (например, верхнеуровневая сетка отвечает за прогрессию и условно выбирает ""аккорд"", а низкоуровневая отвечает за мелодику внутри такта и обыгрывает выбранный аккорд)
2) .. раз у сети есть скрытое состояние? - если некоторое скрытое состояние уже есть, то предыдущие символы нам уже не нужны, чтобы сгенерировать один новый символ, нам нужно подать на вход только один символ",
"ну в игрушках ничего не придумают за 3-4 года такого, чтоб сделать текущие карты медленными грелками, а это все же основной рынок вроде как",
"Процы/память точно не будут медленными грелками, так же как и сейчас 3-х, 4-х летние процы. E5 первого поколения и последнего отличаются по производительности на ватт на 80%, по цене в 30. Купив  за 50$ проц, который стоил несколько лет назад ~1500$, я с удовольствием потрачу разницу на электричество.",
"<https://docs.python.org/2/library/profile.html> - и сразу станет понятно, где узкое место",
"А по каким соображениям можно делать из word2vec -&gt; sentence2vec ?
Брать среднее, максимум?",
"В частности вот эти легендарные уже E5-2670 по 60$ появились в декабре прошлого года, когда фейсбук списал какие-то бешенные десятки тысяч серверов",
"Так же очень дешево сетевое оборудование, можно собрать себе state of the art сетку пятилетней давности за три копейки. 10Гбит -- легко, как насчёт infiniband 40Гбит?",
"постю куда хожу, так что можно оценить посещаемость",
"<@U0M39M6LS> нет, только туда, где кормят",
"Касательно меня у меня задача, где всего много:
классов несколько сотен
объектов под миллион, причем представленность классов убывает по 1/х
объекты - текст или пара текстов (короткие как я описывал)
И в общем я бы хотел для всего этого получить поменьше признаков,потому что считается ну очень долго",
Тепловыделение титана примерно как у двух секций батарей :slightly_smiling_face:,
Как новый дум допройду :smile:,
"В конце октября-начале ноября можно было 4$ с одной 1070 снимать в день, когда zcash запустили, сейчас чуть меньше двух",
"Как соберешь комп, сделай, пожалуйста бенчмарк Титанов. не ясно, на сколько проблема с PCI критична.",
"Я что-то поздно про эту проблему узнал, когда уже практически заказ сделал. Но думаю норм будет, 8Гбайт/сек ""должно быть достаточно для всех""",
"Кстати, почему решил 1080, а не Титан на Максвеле? //вот думаю, не упрусь ли с 8ГБ по памяти",
"Но это прошлые титаны же, кроме как памятью они вроде не блещут",
"На практике, мне показалось, что если обучать большие сетки, то на с 24 Гб GPU она сходится куда быстрее, чем с 8 Гб GPU",
"А кто-нибудь знает как нормально в несколько потоков батчи в keras готовить? Вот есть свой генератор, в нем идет считывания с диска, ресайз и преобразование в матрицу. Если делать как по ссылке выше (через thread), то там GIL все портит. При использовании процессов там возникает проблема в обмене данных, т.к. они сериализуются pickle, а это очень медленно.",
задача deep learning как раз и состоит в поиске `локального` минимума,
"Что-то не согласен, но объяснить почему -- не могу вот так навскидку",
"в ссылке выше (ну на статью про spacy) есть ссылки на научные статьи; там везде получалось, что линейный svm + униграммы/биграммы + может быть tf*idf работает для классификации текста примерно так же, как все новомодные CNN, LSTM и тд (иногда чуть лучше, иногда чуть хуже), а усреднение word2vec векторов работает стабильно хуже. fasttext работает чуть лучше, но тоже примерно так же. Сильно лучше - bidirectional LSTM/GRU, особенно если attention добавить, и вот это они в spacy пилят.",
"Я понял, что глобальный минимум будет == оверфиту только в том случае, если у сети будет капасити равен или превышать количество информации содержащейся в тренировочном наборе. Что для текущего положения дел ещё сильно далеко от возможностей железа (и в будущем наверное так и останется -- по мере роста производительности датасеты тоже будут только расти, и терабайт имейджнета поди будет вызывать усмешку через 10 лет, как сейчас мнист или cifar)",
только вроде этой статье лет примерно как мне,
"Я, кстати, именно на той статье основываю свои утверждения. Там как раз пример того, как двуслойная свёрточная сеть с одним полносвязным выходом находит глобальный минимум",
"&gt; Ну... 1080 быстрее, чем Титан Х процентов на 20

Ну это пока, свежая куда прибавляющая десятки процентов на старый Титан уже не придет, а на 1080 вполне может.",
"Но в целом, мой вывод сохраняется, только нужно держать в уме, что в картинках может быть куда меньше информации, чем при грубом посчёте умножая пиксели на штуки",
"В то время, как любой более адекватный метод должен использовать больше 30 пикселей картинки",
"про прямоугольную фичу понял, спасибо. А как правильно учитывать взаимное влияние разных айтемов друг на друга? Т.е. если акция на какой-то айтем из какой-то категории, то остальные айтемы из той же категории должны упасть",
Саймон как всегда спасает ситуацию врываясь с тредом из кружков :slightly_smiling_face:,
"Точно, Бенджио же! Вот же видео где он обсуждает много всякого про диплёнинг на летней школе в стэнфорде, в частности по ссылке про локальные минимумы и т.п., надо бы пересмотреть и законспектировать <https://youtu.be/11rsu_WwZTc?t=25m4s>",
"привязать фичу к конкуретному айтему (да, так может получит дохулиард фич). и при предсказании других айтемов учитывать к каким айтемам че прикручивали",
"Хм, неясно, про какой *the* global optimum он говорит. Их, вообще говоря, много",
"Разве что есть вырожденный случай, когда они ровно одинаковые",
Ты в смысле SGD или в каком смысле?,
"Хм, а как его определить можно?",
"Где можно долго идти по градиентому спуску и убедиться, что ты реально в локальном минимуме, и смотреть их кучность",
Как они оценивали именно глобальный - не знаю!,
"В общем, если они говорят, что локальные минимумы близки к глобальному в том плане, что **веса** глобального минимума близки к весам локального, то нужно как-то аккуратно бороться с симметриями пространства весов, чтобы идентифицировать глобальный минимум, соответствующий данному локальному
А если близость определяется по близости к loss'ов, то это не меняет суть предыдущего обсуждения – глобальный минимум может быть где-то далеко в стороне, куда ты простым sgd в жизни не доберёшься, но он идеально запоминает выборку, а твои нейросети просто хорошо решают свою задачу",
Почему мы вообще можем предполагать что такой глобальный минимум имеет право на существование?,
Ну вот как в примере с 30 пикселями уникально идентифицирующими изображение и т.п.,
"Не знаю! В общем, мне кажется, надо прочитать в статье Bengio-сотоварищи, какие они делают утверждения про глобальный минимум",
<@U2KRCH9EU> как и во многих других фреймворках: получает вектор интов выдает вектор векторов из матрицы.,
"Кто-нибудь занимался извлечением признаков из спектрограмм / классификацией спектрограмм? Например, для fourier transform infrared spectrometry. Текущая работа в лаборатории связана с анализом ИК-спектров на предмет содержания функциональных групп и химических элементов. После этого делаются выводы о составе сложного образца - по сути задача классификации. Идентификация этих функциональных групп довольно условна, так как образцы материала содержат очень много различных веществ. Есть мысли что из фундаментального почитать на этот счёт?",
"Если учишь с нуля, инициализируется как скажешь, по умолчанию малыми случайными числами.",
"если делать trainable=True, то тогда работает как fully-connected? какие особенности?",
"Почему w2v/glove противоречит пониманию эмбеддингов?
trainable=True -- они будут обучаться, нет -- будут фиксированными, градиент дальше не потечет.

Как тут fully-connected получается?",
Всем привет! Хотел спросить есть ли здесь  кто то с Израиля?,
"<@U053R9RS6> сейчас в Тель-Авиве, думаю он в теме кто еще здесь :eyes:",
так эти фичи как раз и забирают эффект от акций,
<@U04ELQZAU>: у меня на работе стендап в 12. Обычно в 9:30 прихожу и часов в 11 ухожу. И как раз именно сегодня вроде как генеративную музыку обсуждали :y_u_no:,
"Если добавлять фичи, то все эти два шага объединены. Да и выглядит это логичнее, как по мне. Только вот меня смутили рассуждения, которые я однажды услышал. Потому решил тут в чатике и убедиться.",
"Ребята, если бы у вас была возможность получить бесплатно с Амазон книгу по ML/DataScience, какую бы вы выбрали? :slightly_smiling_face:",
"Бумажные книжки ещё просто не очень практичны, редко когда получается посидеть и полистать их",
"Ну исключение может быть, когда нет электронной версии",
"Бумажные, мне кажется, удобнее читать, когда есть изображения",
"Между пространством loss-функций и  f-расстояний есть однозначное отображение (<https://arxiv.org/pdf/math/0510521.pdf>).
f-расстояния (например, расстояние Кульбака-Лейблера) определяют разницу между двумя распределениями (в данном случае, между реальным распределением данных и параметрическим распределением, заданным построенной сетью). В этом смысле параметры сети рассчитываются через (квази)правдоподобие.

Поскольку данные случайны, то и функция правдоподобия случайна. И поэтому найти глобальный минимум loss-функции очень сложно.
Можно рассчитать матожидание правдоподобия - это тоже будет случайный объект, но более удобный для оперирования.

`w0 = argmax L(w)`, где `L(w)` - функция правдоподобия, а `w0` - идеальное значение параметра
`w* = argmax EL(w)`, где `EL(w)` - матожидание правдоподобия, а `w*` - достигнутое значений параметра

А дальше есть теорема, которая доказывает, что `|w0 - w*|` - мало с экспоненциально большой вероятностью.
Более того, можно даже определить множество таких `w*`.

В общем, есть много локальных минимумов, которые не только по значению близки к глобальному минимуму, но и по расположению к нему близки.

Подробнее на эту тему см. <https://www.youtube.com/playlist?list=PLXqezZ11Vm4p7Qf38OuHXYBqxT8i4LDab>

<https://opendatascience.slack.com/archives/deep_learning/p1479322661002599>",
"если кластера как фичи потом в другой алгоритм заносишь, то, как обычно, кроссвалидацией подобрать",
"да, если бы потом как фичи — я бы и не задавал, наверное, вопрос :slightly_smiling_face:",
"Мне тоже бумажные книги приятнее читать. Удобно иметь сразу оба варианта, так как в электронном можно быстро поиском найти нужную информацию.",
Я вот не представляю как взять с собой куда-нибудь вот эту книгу — <https://www.amazon.com/Speech-Language-Processing-Daniel-Jurafsky/dp/0131873210/ref=sr_1_11?ie=UTF8&amp;qid=1479376002&amp;sr=8-11&amp;keywords=speech+recognition>,
Главное чтобы не получилось как у Кнута :slightly_smiling_face:,
<@U04URBM8V> а где ваши фотки с завтрака?),
"А нет предположений, когда DLBook уже таки выйдет?
Думаю, просить деда мороза или нет?)",
Предзаказы принимают уже пару месяцев как,
"народ, здесь есть кто-нибудь, кто разбирается в Automated Planning? те кто знают, что означают аббревиатуры PDDL, HTN, ICAPS и т.п.",
"<https://arxiv.org/abs/1611.00847>
&gt; Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at <https://github.com/iPhysicist/CNNDesignPatterns> ). We hope others are inspired to build on our preliminary work. 

Про то, как строить архитектуру свёрточных сетей",
"Где вот FractalNet, натренированный на ImageNet?",
А еще где Xception за пределами tf?,
"Гайз, не уверен, что в тему, но вдруг кто-то шарит и может помочь.

Такая ситуация: хочу себе домой собрать десктоп. Хороший, и чтобы надолго. 1080, конечно, вряд ли буду покупать, но :more-layers: хочется делать, поэтому думаю взять 1060, например. Интересует следующее:

1) Как в принципе стоит придумывать конфигурацию? Есть где-то какие-то готовые сеты сборки, чтобы посмотреть, прикинуть во сколько обойдётся? Возможно, получить какие-нибудь осмысленные комменты, а то я не шарю?
2) Где имеет смысл покупать? Деньги у меня в евро, могу перевести в рубли и купить здесь, но, может, имеет смысл покупать из Европы/Штатов с доставкой сюда? Нет ли каких-то проблем вообще с доставками? Меня напрягает, что технику постоянно коцают, да и гарантии, наверное, не будет. Но вдруг есть какие-то удобные способы это сделать и не получить битые платы и мониторы.
3) На 1060 можно обучать что-нибудь достаточно глубокое? Я не планирую каждый день на ImageNet'ах фититься, но иметь возможность порешать на каггле всякие конкурсы с картинками (вроде недавнего <https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring>) очень хочется.",
"2. Покупка в штатах -- возможность хорошенько сэкономить, но все риски берёшь на себя. Имхо, риски не большие, я так беру уже много лет железо, часто б/у или refurbished (так как гарантия туманная всё равно, а цена может быть сильно ниже), пакуют обычно хорошо (особенно наши посредники, которые знают как почта ведёт себя), консолидация посылок экономит деньги на доставку, но крупные-тяжелые штуки всё равно смысла таскать мало. Процессоры, видеокарты, ссд, мелочёвку и т.п. -- норм, мать, жесткие диски -- я бы посмотрел у нас, корпус, кулеры всякие,  -- однозначно у нас.",
И в каком фреймворке его сложно выразить? :slightly_smiling_face:,
"Хех, там 1070 как старый титан по скорости, надо же",
"Всё-таки поразительно, как кипит рисёрч в этой области. Кажется вообще над каждой темой работает в параллель по нескольку команд. Пару недель назад всего другая работа про lip reading пролетала",
"не подскажите как в Keras реализовать полносвязный слой с общими весами с условием, что нужно применить как обычную матрицу, так и транспонированную? Можно ли обойтись стандартными примитивами или свой слой нужно писать?",
"то, что слой является обычным полносвязным, но есть место, где он должен быть применен с транспонированной матрицей.
грубо говоря, если записать как 
dense = Dense(10)
x = dense(x)
...
x = dense(x)
 - будут использована одна матрица весов
Вот мне нужно, чтобы была возможность использовать ту же матрицу, только транспонированную",
"Добрый день. Занимаюсь задачей применения ml в text mining для определения эмоционального подтекста предложений или групп предложений. Предполагаю, что тема далеко не новая и изобретать велосипед не хотелось бы в части первичного анализа важных переменных. Текст использую только русскоязычный, мог бы кто-нибудь посоветовать материалы по данной теме для изучения? какие-либо наработки, на какие вещи обращать внимание (количество эмоциональных знаков, использование нецензурной лексики, использование уменьшительно-ласкательных суффиксов и прочее) и т.д.? Заранее благодать помогающим :+1:",
"Коллеги, всем доброго! Не нашел в этой теме. Подскажите пожалуйста, есть ли сейчас годные синтаксические парсеры русского языка, кроме АОТа?
Для чего: требуется матчить короткие тексты QA-системы, но заказчик хочет все понимать, почему так или иначе что-то сработало внутри, а значит матчить не ""магически"" (типа doc2vec). Представляется, что синт. анализ  позволит находить главные слова, и ""плясать"" от них уже.
Что посоветуете?",
"<@U2BFSKNDP> Когда был молод и юн(то есть пол года назад), я использовал связку tfidf + SVM + lime для того что бы выделить слова в текстах на основе которых идет  классификация. В многоклассовой это выглядело не столь убедительно, но всё равно позволяло помочь ""человеку не знающему"" понять основной подход.",
<@U2BFSKNDP> могу скинуть код как пример,
"Для приведения слов к стандартной форме я использовал pymorphy2
<http://pymorphy2.readthedocs.io/en/latest/user/index.html>
Но с сокращениями как я понял там не очень",
у меня встречный вопрос: почему свм для текстов часто используют?,
<https://events.yandex.ru/events/ds/22-nov-2016/> ребят а кому нибудь интересна эта тема и пойдет ли кто нибудь сюда?,
"<@U04CH4QBD> а malt есть обученный для русского? есть как бы синтагрус, но его тоже раздобыть постараться надо",
"он ведь на эмбеддинги завязан, как там со специальной лексикой?",
"Добрый день. Занимаюсь задачей применения ml в text mining для определения эмоционального подтекста предложений или групп предложений. Предполагаю, что тема далеко не новая и изобретать велосипед не хотелось бы в части первичного анализа важных переменных. Текст использую только русскоязычный, мог бы кто-нибудь посоветовать материалы по данной теме для изучения? какие-либо наработки, на какие вещи обращать внимание (количество эмоциональных знаков, использование нецензурной лексики, использование уменьшительно-ласкательных суффиксов и прочее) и т.д.? Заранее благодать помогающим :+1:",
"хочу быть первым кто будет троллить друзей видеозвонком с лицом путина, или с их собственным лицом =\",
"Коллеги, снова всем доброго дня.
Решил попробовать pymystem3, долго облизывался, ибо он ""performs POS disambiguation"".
Но почему он такой медленный?! Беру пример из документации
```
&gt;&gt;&gt; import pymystem3.Mystem as m
&gt;&gt;&gt; import time
&gt;&gt;&gt; text = ""Красивая мама красиво мыла раму""
&gt;&gt;&gt; t=time.time();  m.lemmatize(text);  print(""%f sec"" % time.time()-t)
['красивый', ' ', 'мама', ' ', 'красиво', ' ', 'мыть', ' ', 'рама', '\n']
2.2829999923706055 sec ```

Это что, такая зверская расплата за disambiguator?",
Кто-нибудь занимался тюнингом параметров `fasttext` для классификации или знает где почитать?,
"Ну там в шапке то написано: ""This fork of BVLC/Caffe is dedicated to improving performance of this deep learning framework when running on CPU, in particular Intel® Xeon processors.."", я поэтому и спрашиваю как дело с этим в реальности обстоит",
Звучит как серьезный такой вброс,
А нагрузка на GPU какая при этом? ,
Нашёл где подробно обсуждают результаты (ооочень длинный тред) <https://github.com/soumith/convnet-benchmarks/issues/59>,
"Задумались мы сделать матрицы переходов из одного состояния в другое, где конечным результатом будет отток.  Что-то типа марковских цепей. Может это они и есть. Встал в вопрос по библиотекам. Какие есть популярные библиотеки с достаточным функционалом? У кого может есть опыт или полезные ссылки в этом направление?",
"Навеяно обсуждением в <#C047H3N8L|deep_learning>. Почитал про производительность интеловских процессоров, оказалось много интересного происходит там, видимость того, что прогресс встал -- просто потому что он идёт где-то глубоко -глубоко, куда редко заглядывает большинство разработчиков.",
"Например, процессор Xeon E5-2699  v4 имеет, как оказалось, пиковую производительность порядка ~1.3TFLOPs SP, т.е. двухпроцессорная машинка будет около 3-х. Не сильно далеко от GPU с их 8-10TFLOPs надо заметить.",
<@U04ELQZAU> а когда будет отбор в Байесовскую школу?,
Вот там они как раз и начали считать теоретически возможные флопсы процессоров и сравнивать коэффициент полезной утилизации.,
"Там ещё капец как всё сложно стало в последних процессорах, они могут до 32 инструкций за такт выполнять, но частота avx блоков совсем другая чем обычных, а обычные могут работать начиная от базовой до турбо в сложной зависимости от числа активных потоков и т.п.",
"архитектура все равно разная, gpu это реальная параллельная обработка
для dl сложно эффективно cpu утилизировать, как я понимаю",
"То, что архитектура разная я ещё понимаю, и даже немного как именно она разная, а вот насколько она в каждом случае утилизируется и в чём ограничения уже как-то выше меня",
"вот это видео даёт понять насколько большая разница между gpu и cpu, я когда его увидел — реально прозрел
<https://www.youtube.com/watch?v=-P28LKWTzrI>",
"<@U1CF22N7J> Но только эти два процессора потребляют столько же энергии, как и GPU, и стоят больше",
"Профессиональные решения, наверное, как один такой прой или чуть побольше",
"Т.е. разрыв не так велик как может показаться, вот и весь мой вывод был для себя",
Но не на два порядка как многие думают,
"а как там со специальными железками для dl, получилось у кого что-нибудь стоящее?",
"интел как раз nervana купили, они железки обещали",
"Амд что-то совсем сдулись, хотя как ни странно, те же криптовалюты на них считаются лучше",
"<@U0J1U64FK> ну вот публикации два года, а где конкретно они это используют?",
"Что делать с такими ресурсами? Вернее, какие новые интересные задачи они могут позволить решить?",
"Да понятно, что крузис как тормозил, так и будет. Но что ещё позволит делать эта неожиданно свалившаяся мощь?",
"Вон тут не успели на десяток терафлопс выйти, как искусственный интеллект попёр изо всех щелей :slightly_smiling_face:",
Только не знаю о каких шла речь :slightly_smiling_face:,
"<@U1BAKQH2M> если есть какой-то адекватный способ оценить качество итоговой системы, и раз так мало данных для тренировки, то лучше попробовать разные варианты: с лемматизацией/без, окно 2-3-5-8, cbow/skip-gram, размерность (она как раз имхо не так сильно влияет) 100-300. Т.к. от задачи зависит что лучше работает. Тренировать имхо лучше с gensim.",
или как раз боьше эпох помогут мне при маленьком датасете?,
а какая размерность использовалась при таком размере датасете?,
"да, пробовать в любом случае есть смысл, у меня в одной ситуации 2048 было лучше 1024 и заметно лучше чем 512 или 256. Мне кажется оптимальная размерность зависит не только от размера, но и от того как вектора дальше используются",
А для каких задач размерность 2к лучше 256?,
"<https://opendatascience.slack.com/archives/nlp/p1479499843001427> не знаю, я не уверен что эмбеддинги 2k качественно отличаются от 300-500 - и 2**2k и 2**300 сильно больше размера словаря, и при этом 2k всего в 4 раз больше чем 500... не знаю как это можно было проверить?",
"Как правило качество получается лучше, также важно, что по дефолут умеет работать с NaN.",
"&gt; Как правило качество получается лучше
А за счёт чего?",
"Блин, даже теряюсь как ответить",
"Одно дело трансформация изображения, когда ты в фотошопе правишь фотки, елозя мышкой и понимая в голове какой результат тебе нужен. И другое дело, когда ты просто задаёшь смысл нужного тебе изменения и изображение транформируется. Что-то я не помню, когда это стало банальностью и тривиальным",
"Там не накладывают одно лицо на другое, там задаётся семантически требуемое изменение, типа ""состарить"", ""добавить бороду"" или там ""сделать улыбающимся"" и оно ищет в пространстве возможных фоток нужную точку, где ""возраст"", ""бородатость"" или там улыбка больше выражены",
я не понял где тут семантика,
А фильтр старения ты как замутишь? :slightly_smiling_face:,
"Да это всё можно в фотошопе сделать, вы чо, какой ECCV'16?",
Это где они глаза заставляли смотреть всегда прямо?,
"Котиков от собачек отличать даже и фотошоп не нужен, а эти дураки целые сетки городили, глупость какая :)",
"Ну и есть наркоманские идеи, как сделать вообще на binary",
"Я просто смотрю, портируют же предобученные сетки на ARM какой нить",
"Когда я смотрел, там было не очень понятно",
Ну вот это какие то примочки :) они не могут быть идеальными,
"Мне вот интересно, почему хороших попыток на телефонных GPU нет",
"Там вообще есть доступ к ним как к числодробилкам или только апи для 3д? Помню вечно всякие разработчики сторонных прошивок под андроид страдают что никакой ни документации ничего не доступно просто так, людям со стороны",
Они же какой то фреймворк запилили с прицелом на андроид,
"Т.е. совсем никого не знаю, кто ее пользует",
"Норм, как раз говорят, что раз в пару лет нужно менять компанию :smiley:",
"Надо убить всех, кто мог прочитать этот чат",
"Привет :) А кто-то встречал базу рисованных от руки скетчей, как в Google quick draw? А то может там какой то свой MNIST есть, а я и не в курсе :)",
Там вроде как матрицу со всеми весами можно вытащить,
"&gt; Now Intel is saying that by 2020, Intel infrastructure will enable training of neural networks at 100 times the performance on today’s GPUs, said Diane Bryant, executive vice president and general manager of Intel’s data center group.
Т.е. как раз петафлопс и к 2024 году оно подешевеет достаточно, чтобы ставить домой, даже интел со мной согласен!",
"возвращаясь к тормозам CUDA - увеличил размер батча в cifar10, как и ожидалось - никакого роста производительности это не даёт:
```2016-11-19 18:36:04.293516: step 20, loss = 4.44 (884.9 examples/sec; 1.808 sec/batch)
2016-11-19 18:36:22.204475: step 30, loss = 4.41 (894.9 examples/sec; 1.788 sec/batch)```",
"где еще крутить, чтобы её поднять?",
"Любопытное дело, как сильно укоренилось в людях поверие, что xgboost может продолжать тренды в задачах forecasting, где есть четкое разделение по времени и предсказывать надо будущее.
Какой бы такой пример собрать, чтобы это было действительно наглядно?
Взять линию y = a * x + b, обучить на куске x = [0, 10], предсказать кусок x = [10, 20],  но как-то слишком просто, т.к. обычно какие-то суммарные статистики в фичи затаскивают",
"короче, нужен пример, где кажется, что xgboost работает согласно поверию, для препарирования",
У меня генсим работал лучше на 600. И нескольких эпохах - кажется как раз 5.,
"Всем привет, подскажите пожалуйста куда можно выложить датасет на пару сотен Гб и при этом как можно меньше заплатить?",
"Привет. Есть такая задачка: есть 2 матрицы A, B размера m*n, Как найти матрицу поворота, такую чтобы построчная сумма углов между строками была мининальна? или построчный косинус был максимальным",
"Можно найти какую-то квадратную матрицу S, такую что ||A-BS||^2 минимально
Кажется, аналитическая формула выглядит как S = (B^T B)^(-1) A^T B (возможно, здесь S транспонированая)",
Спасибо за ответы. Я понял что какого решения которое лежит на поверхности нет (я думал может я что-то очевидное упускаю)… буду копаться в математике. Вопрос навеян <http://stackoverflow.com/questions/40697463/how-to-align-two-glove-models-in-text2vec>,
"коллеги, а кто знает как правильно считать recall @ K ?
допустим у меня есть 5 классов, по 500 примеров, для каждого примера я переранжирую остальные, далее, для например recall @ 5, надо посмотреть сколько экземпляров верного класса в первых пяти и поделить на 500?",
не понимаю тогда как вот тут : <https://arxiv.org/abs/1511.06452> (Секция 7.1) они получили recall @ 1 50 % для датасета CUB-200-2011 в котором в среднем по 60 изображений на класс,
Блин вроде как мне показалось что два раза одно сообщение отправил. Удалил  одно пропали оба ,
"да, процесс обучения не детерминированный, но видимо не настолько случайный - сам я не пробовал так делать, сейчас скажу где читал",
но как бы тоже нет гарантий что они в примерно одинаковое пространство сойдутся,
"Помню когда они выпустили версию один над ними все ржали (и я тоже :disappointed: ). Дескать, ""какой колхоз, посмотрите на энтерпрайз коробки, их же не дураки делают, а они в сто раз дороже!""",
"Представьте, что вы хотите обучить модель предсказания лайка, но отрицательных примеров у вас, разумеется, нет (неизвестно, лайка нет потому что пользователю объект не нравится, или он просто его не видел). Какие есть подходы кроме negative sampling?",
"Я знаю про: <https://arxiv.org/pdf/1205.2618.pdf>
Но это, кажется, как раз сэмплинг (хоть и хитрый)",
"Типа на какие группы подписан, на каких людей и тп",
"Всё же, мне интересно, как извлечь побольше из имеющихся данных",
"Если делать негативными пары user-item, которые были созданы каким то алгоритмом, то работает это хуже сэмплирования. Лучше сэмплирования у меня работало взвешенное для юзера сэмплирование. Да так, чтобы все пары брались из большего пространства путем оценки скора.",
"<@U0K47J3HR> а вы в курсе, когда определят списки участников?",
"В SEO может подобное пригодиться, я думаю, чтобы знать, какой стиль использовать для каких запросов",
"Добрый вечер.
Что можно прочитать про one-shot-learning и semi-supervised learning в приложении к сегментации/классификации изображений?
Как там вообще дела обстоят?",
"У меня есть задача классификации на 100 классов. Хочу попробовать ансамбли алгоритмов, какие тут могут быть хорошие тактики?
Что подавать на новое обучение?
У меня такие мысли:
1) 100*n признаков - вероятности каждого класса, предсказанные n классификаторами
2) n признаков - просто номер класса, который предсказали n классификаторов
3) Еще что-то?",
"Можно тут посмотреть <https://www.kaggle.com/c/expedia-hotel-recommendations>, там как раз 100 классов было
Например, <https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21466/final-ensemble-method> или в скриптах покопаться",
"<https://corp.mail.ru/ru/press/events/282/>
странно, почему этого события нет в календаре",
"Завтра Артём Бабенко из Яндекса рассажет на ФКН, как искать ближайших соседей в пространствах высокой размерности, что бывает полезно для различных векторных моделей
<https://cs.hse.ru/announcements/196683388.html>",
Как выиграть The International за три простых шага?,
<@U041P485A> <@U09JEC7V0> а кто себя пиарить будет?),
"а я расскажу как с помощью CNN получать представления которые ~возможно~ будут хорошо работать для поиска ближайших соседей с помощью алгоритмов, про которые на ФКН расскажет Артём Бабенко",
"как лучше, при генерации батчей дублировать слои в картинке, или на уровне сетки вставить какой-нибудь lambda layer, который там уже будет конвертить?",
<@U041P485A>: это когда и где? Какие именно картинки считаются похожими?,
"<@U2194SMBM> переведи чб в RGB, как тебе такой ход? :slightly_smiling_face:",
"вот думаю, какой вариант лучше",
это когда все приходят и смотрят в свои ноутбуки,
<https://opendatascience.slack.com/archives/_meetings/p1479734860002137> как-то проскочило мимо меня. какой там нынче контест решают?,
"у вас в москве/питере преимущество, что есть нормальные универы, где делают интересные доклады и проводят семинары + людей больше. а у нас маленькие качественные мероприятия - редкость. особенно регулярные",
"<@U30Q72KLJ> 4 часа как правило хватает, чтобы обсудить, обменяться идеями, разобраться, что не получается, если упёрся во что-то",
"О, про стайл трансфер, а может кто-нибудь боль-менее интуитивно рассказать, какие эффекты дает использование разных архитектур (vgg, resnet, inception) для вычисления стайл лосса? Интересно в контексте выбора сетки, лучше всего улавливающей стиль текстуры",
"<https://opendatascience.slack.com/archives/deep_learning/p1479738957003005>
Я тут недавно сгенерировал идею, как можно экономить на покрышках для шасси :slightly_smiling_face: Боинг заинтересовался, и уже, поди, патент подал. Осталось подождать, когда увижу первый самолет с ""крылышками"" на колесах для раскручивания шасси потоком воздуха при посадке и как следствие, снижения износа от трения",
Тут еще не известно где денег больше,
"Денег больше там, где можно создать осязаемую интеллектуальную собственность.",
"<@U30Q9Q8MQ> лопасти такие, как у вентилятора :slightly_smiling_face: или в диске колесном, или в покрышках",
это  кстати оффтоп. Но я чето хз где это в тему будет. Во флуде наверное....,
"Табло хорош, что можно дешборды настроить и потом на сервер выложить, добавить автообновление и дать доступ многим пользователям. С R можно кластеризацию и тренды делать, например. В 10 это по дефолту есть, но не совсем понятно какие алгоритмы там и настроек не очень много",
"я видимо, недостаточно разобралась в табле, но когда пыталась там что-то сделать, то попытки сделать какие-либо чуть больше чем элементарные операции с данными для перестройки графиков начали сильно напоминать эксель. Ну, то есть, как аналитик, я что-то в R посчитала, запихнула в Табло, поняла, что вот здесь бы перепосчитать, тут подвывернуть, посмотреть, потом еще что-то сделать. И снова нужно ижти в R.
А в R я весь этот процесс делаю относительно быстро и не отходя от кассы, но, наверное, теряю в клевости итогового отчета\дашборда, но лучше ли это, чем бегать из Табло в R\Python и обратно, вопрос.
Но спасибо. Стало ясно, что стоит потратить на Табло чуть больше времени, разобраться.",
Когда еще можно будет шутковать про жирных в такой компании,
"вчера добрался до неё как раз, ожидал большего
она больше как справочник для рисерчеров, где хорошо прослеживается история и есть более 500 (?) ссылок на статьи",
там как раз очень много отсылок на эти книги,
"Не знаю как с книгами, но заказы стоимостью более 200 баксов сейчас в Россию не пускают, если там за доставку амазон отвечает",
а какой там лимит на лавко-стулья-места? вроде около 8? до разделения на two parties не доходило?),
"Не подскажите где искать трансляцию про pml чат боты сегодня в Яндексе? На сайте: <https://events.yandex.ru/events/ds/22-nov-2016/> не видно ссылки, хотя написано, что будет трансляция.",
"Я бы начал с кластеризации. Потом посмотреть на распределения в кластерах и дендрограмму, думаю можно будет понять каким классом метить кластер",
"<@U1BKBLY4X> Тут есть две главные подзадачи:
- четко сформулировать ЗАЧЕМ вы пытаетесь разделить людей и нелюдей  (потому что от цели будут меняться методы и даже ваше определение ""людей"" и ""нелюдей"")
- дать четкое определение понятию ""поведение"" (потому что сейчас у вас есть отрывочные сведения об отдельных временных метках)

А кластеризацию можно делать только когда вы хорошо понимаете, как у вас устроено пространство, в котором вы кластеризуете.",
"т.е. не любят, когда их парсят?",
"типа того) ладно, будем парсить, значит.
Надеялась, что, может, кто собрал уже и готов поделиться :slightly_smiling_face:",
"чудеса какие то, раньше работало что поменялось не понятно
делаю `import theano` получаю такую ошибку
```
usr/include/string.h:652:42: error: ‘memcpy’ was not declared in this scope
   return (char *) memcpy (__dest, __src, __n) + __n;
```
менял версию GCC (на 5, по умолчанию 4.9) через переменные окружения, скачал самую последнюю версию из исходников ничего не помогает, кроме костыля с -DFORCE_INLINES

кто с подобным сталкивался?",
"<@U0KQ5M6KX> 
- четко сформулировать ЗАЧЕМ вы пытаетесь разделить людей и нелюдей  (потому что от цели будут меняться методы и даже ваше определение ""людей"" и ""нелюдей"")
находить в дальнейшем не людей в новых или старых источниках пользователей.
- дать четкое определение понятию ""поведение"" (потому что сейчас у вас есть отрывочные сведения об отдельных временных метках)
это сколько определенных действий на время с момента установки приложения сделал пользователь
как то так...",
<@U041P485A>  а как его чистить?,
"<@U1BKBLY4X> Пока это все очень нечетко

Отвечая на первый вопрос, вы говорите, что вам нужно отделить людей от нелюдей, чтобы отделить людей от нелюдей.
Однако так и осталось непонятным, зачем вам их отделять.

&gt; это сколько определенных действий на время с момента установки приложения сделал пользователь
Когда вы говорите ""_определенных действий_"" вы имеете в виду вполне конкретное единичное действие строго конкретного вида А или просто любое действие
И ""_время с момента установки_"" тоже бывает очень разное
Если пользователь А установил приложение год назад и с тех пор совершил 5 действий, а пользователь В установил 1 минуту назад и тоже совершил 5 действий - у них одинаковое поведение?

Изложу проще, существо из мяса и костей, которое открывает ваше приложение (возможно даже не зная вашего языка), щелкает на 5 кнопок за минуту и закрывает приложение, чем-то принципиально лучше бота, который также щелкает причем те же 5 кнопок? Или они все-таки  для вас оба нелюди?",
"&gt;И ""_время с момента установки_"" тоже бывает очень разное
допустим ограничим его месяцем с момента установки
действия, конечно, конкретного вида
&gt;Если пользователь А установил приложение год назад и с тех пор совершил 5 действий, а пользователь В установил 1 минуту назад и тоже совершил 5 действий - у них одинаковое поведение?
если это было как и у пользователя В в первую минуту то да.
&gt;Изложу проще, существо из мяса и костей, которое открывает ваше приложение (возможно даже не зная вашего языка), щелкает на 5 кнопок за минуту и закрывает приложение, чем-то принципиально лучше бота, который &gt;также щелкает причем те же 5 кнопок? Или они все-таки  для вас оба нелюди?
думаю на начальном этапе да мы не сможем их отличить, в дальнейшем наш органический товарищ либо не будет заходить(удалит приложение) либо его поведение будет меняться, а вот бот скорее всего будет по прежнему нажимать на 5 кнопок.",
тут есть кто-нибудь кто отобрался на приблидающийся хакатон вк?,
"&gt;  в дальнейшем наш органический товарищ либо не будет заходить(удалит приложение) либо его поведение будет меняться
либо случится какой-либо другой из 100 миллионов возможных вариантов

&gt; а вот бот скорее всего будет
или не будет

не стоит гадать и уж тем более приписывать другим агентам поведение, на которое они перед вами не подписывались... а значит могут вести себя как угодно, причем не как угодно вам.",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1479824792004047>

решаемая, только сначала надо сформулировать в чем именно заключается задача и какими объектами/субъектами надо оперировать",
"да такие вещи никто никогда не отслеживает. даже если озвучивает запреты. но такие запреты как реализовать? от интернета отключить? так что всегда можно. 
но кому ты хочешь помогать?",
"я почему спрашиваю -- придуманная в попыхах идея не зашла(. но хочется поучаствовать, даже пусть в чужой команде.",
"Про источники тут все сложнее. Я не настоящий сварщик, но если стоит задача оценивать каналы трафика, но я бы вообще не так подходил. Есть такие вещи как LTV, retention и т.д. По идее, оценивать канал нужно, исходя из подобных величин.",
"Сколько денег потрачено на 1 пользователя, какие показатели получились.",
"А мне важно кто покупает товары? Если бот покупает подарки, тратя деньги и у него лучшие показатели - я счастлив же.",
"<@U13PBBLLW> Собирал как-то различные отзывы, может пригодятся

Отзывы с Кинопоиска
TOP250 <http://goo.gl/nGsBeN|goo.gl/nGsBeN>
BOTTOM100 <http://goo.gl/4ojcMw|goo.gl/4ojcMw>

Отзывы о ресторанах <http://goo.gl/WVl4CO|goo.gl/WVl4CO>

Отзывы о ""Больницы и клиники"" с сайта otzovik
Формат имени файла: id_оценки, где оценки это 5 цифр по пятибалльной шкале по аспектам: качество, сервис, аппаратура, питание, размещение.
<http://goo.gl/U1ha9Q|goo.gl/U1ha9Q>

Отзывы про различные мед. заведения с сайта spr ru
Формат имени файла: id_общая оценка (0 - отрицательная, 1 - положительная)
<http://goo.gl/5sgsvs|goo.gl/5sgsvs>",
Кто нибудь ставил Caffe (CPU only) на MacOS Sierra? Может быть где-то внятная инструкция есть...,
opencv можно поставить через brew install… и пожалуй это самое простое решение… потому как компилить самому из исходников может быть сложновато,
ну да ты же собираешь с Кудой а тебе она не нужна,
нужно править конфиг выключать куду,
"ну я там дальше использую лосс как в статье про триплет нет (ты то ее точно читал), там экспоненты расстояний считают, так что проблем с малыми расстояними вроде как быть не должно",
"прост если корень извлекать то может nan возникнуть, а где ему еще браться я не представляю",
"Можно взять сетку, где получился nan и посмотреть на каком этапе лосса он возникает",
"Т.е. какая операция конкретно получает не-nan, а выдает nan",
"да, как всегда дошло как начал кому-то обьяснять, у меня не совсем корректная имплементация оригинального лосса",
"интересно, почему в стандартном tripletnet с такой реализацией проблем не возникало",
"Если у тебя керас на tf, порекомендую `add_check_numerics_ops()` -- с ней быстрее найдешь где косяк",
"Всем рекомендую его заценить либо вживую, либо как выложат",
"<@U35FDTFED> звучит как тема для диплома :slightly_smiling_face: типа “все модное смешать”
про GAN часто рассказывает <@U073056E4> , в том числе в эту пятницу будет в мейле на митапе (но про картинки). вообще GAN для текста мне  пока не встречался, не говоря о том чтобы к и без того исследовательской вещи прихреначить RL",
"и да, это все еще звучит как желание собрать вместе все услышанное, а не для решения чегото более менее конкретного",
"<@U0H7VBQQ1> <@U1BAKQH2M>  Спасибо большое! <@U040HKJE7> Мне неважно, как это звучит, главное - результат)",
"&gt; главное - результат)
результат есть там где есть задача",
"Кстати, кто нить с Томитой еще балуется?",
"Когда будет минутка, если кто изучал недокументированные функции томиты (а их дофига), то прошу поделиться :yum:",
"<@U040HKJE7>
(Питер)
Как, где, во сколько, как попасть?",
А где анонс завтрашнего питерского завтрака,
"<@U04URBM8V> (читается ганджиман) будешь пилить анкетку, чтобы собрать мнение о том, куда стоит перенести завтрак?",
".
..
...
все :slow: кто не успел зарегаться на DS митап в пятницу и D&amp;S по биоинформатике в субботу, можете написать в личку с просьбами вписаться",
"а кто пробовал свертки по текстам, поделитесь опытом",
"А, ну, я зону как раз имел в виду. Просто name никогда не видел.",
"Я не пытался никого тролить, я обсуждал, так как это обычно происходит",
"Я бы воткнул два титана х или две 1080 в синие слоты, а в черный воткнул какую-нибудь мелкую типа 710, чисто рендерить рабочий стол и чтобы видео не тормозило, когда сетки учатся",
"<@U0DA4J82H> <https://opendatascience.slack.com/archives/theory_and_practice/p1479820293004043>
Java использую для оптимизации, когда нужна производительность и многопоточность.
Для анализа использую Python и его стандартные библиотеки.",
"Ну таки когда надо какие-то вещи, связанные с рядами, перенести в джаву, какая-то библиотека используется? Или все ручками? ",
"В предположении независимости курсов модель сводится к наивному Байесу, как я понимаю ",
"Можно моделировать K_j как ""полезность курса"", тогда по данным можно будет их обучить, а потом использовать для предсказания полезности курса",
"Непонятно, как это относится к проблеме локальных и глобальных минимумов",
"Без своего фреймворка сидишь, пилишь клауд как дурак!",
"подскажите, у кого есть такой опыт: имеются картинки + к каждой из них несколько чисел-фичей, независимых от картинки. хотелось бы это всё учесть для классификации, вот и вопрос, как обычно лучше получается - обучить отдельно сеть на классификацию по картинкам, взять из неё фичи, объединить с другими имеющимися и натравить xgboost, ИЛИ сделать хитрую архитектуру сети, чтобы на вход принимала пикчи + отдельные фичи и где-нибудь делала мёрж*",
"ой понаставили молотков, у меня вот жена спрашивала куда я так часто хожу, однажды привел ее на тусу, с тех пор она больше ничего не спрашивает",
мы вот как то сидя в говно где то разбирали стайл трансфер,
и люди смотрели как на прокаженных),
"Коллеги, а кому-то приходилось заниматься topic modeling для довольно коротких текстов? С чего стоит начать? Пока играюсь с NMF, так как у LDA судя по слухам проблемы на коротких текстах",
"коллеги, а <https://github.com/Microsoft/CNTK> или <https://github.com/Microsoft/LightGBM> кто то юзает вообще? может если вдруг такие есть то поделитесь впечатлениями?",
"Ну, как минимум в условиях кэггла имеет смысла сделать модель с его использованием и подмешать, я думаю.",
:bishop: он как святой дух там витает,
"надо попробовать лосс такой же использовать, как в lightgbm 
в xgboost другой, причем его упростили зачем-то",
"Очевидно, у тебя там где-то `int` там, где ожидается другой список",
но все параметры выглядят как будто бы в норме,
<@U1BAKQH2M> а как я понял можно еще просто объединять объекты в батчи определенной длины?,
"понятия не имею, я до сих пор разобрался как зеро-паддинг на иннер стейт влияет, батчи разной длины наверное тоже что-то внесут",
и нигде не написано почему он точней,
"&gt; и нигде не написано почему он точней
<https://github.com/Microsoft/LightGBM/wiki/Features#optimization-in-accuracy>",
"я не понял только, как такое растить можно",
"Тут новость ходила, что были слиты базы гитхаба, кто - нибудь видел?",
"непонятно, почему некоторых комментаторов интересует тепло из ДЦ Яндекса - их ватники должны согревать достаточно",
<@U06J1LG1M> ты с какого района?,
ну у меня просто уже все это сделано и хочется как в том же tfidfvectorizer просто список файлов скормить и сразу с моделью разбираться,
"<@U1X5Q0S1L> Вот тебе код загрузки:
```import artm
from sklearn.feature_extraction.text import CountVectorizer

corpus = MyCorpus(docs, lowercase=True, lemmatize=True)
vectorizer = CountVectorizer(min_df=1, tokenizer=corpus.get_tokens, vocabulary=corpus.dictionary.token2id)
x = vectorizer.transform(docs)
n_wd = np.transpose(x.toarray())
invert_vocabulary = {value: key for key, value in vectorizer.vocabulary.iteritems()}

batch_vectorizer = artm.BatchVectorizer(data_format='bow_n_wd', n_wd=n_wd, vocabulary=invert_vocabulary)
model = artm.ARTM(num_topics=num_topics, dictionary=batch_vectorizer.dictionary)

где MyCorpus наследник от gensim:

from gensim import corpora, models
class MyCorpus(corpora.TextCorpus):```",
Просто 3тб я даже не представляю как тянуть,
"ты не представляешь, какие непредсказуемые суммы и тарифы бывают иногда в калькуляции управдома :slightly_smiling_face:",
"Это не тарификация для предприятий же, где мощность отдельно считается, потребление отдельно и всякие дикие коэффициенты за превышение плана и за недобор тоже",
"<https://opendatascience.slack.com/archives/deep_learning/p1480000932003213>
высчитать примерно можно, но тарификация кстати разная в зависимости от объёма потребления населением
<http://index.minfin.com.ua/tarif/electric/electric2016-09.php>

ну и главное можно ж топить с помощью GPU когда прохладно… а с погодой сложнее :slightly_smiling_face:",
"подскажите есть готовые предобученные (в идеале на ImageNet) модели с поддержкой rotation-invariance из коробки? и где их взять?
а то статей особенно за этот год полно, но где бы найти это добро, чтобы потестить? с нуля обучать как то желания мало, нет подходящих мощностей",
"Привет пандиты. Если какое то правило для начального подбора количества слое, нейронов в них, количества эпох в зависимости от входных данных ",
"Но куда удобнее, когда они все уже собраны в одном месте",
"<@U065VP6F7>: Да, будет записан и выложен на ютубе, как и все остальные",
"выглядит как tsne
также объясняюще :troll:",
<@U04ELQZAU> А во сколько и где завтра семинар? Есть возможность прийти?,
"Ага, там примерно на этой модели и дают основы. И есть пример с двудольным графом оценок за курсы как раз",
"<@U041P485A> именно похожести, да, ошибся в терминологии. Задача у меня такая - существует около 10 тысяч очень схематичных картинок (буквально как детские рисунки черным карандашом) 300 категорий. Особенность сета в том, что картинки внутри категории могут сильно отличаться и в том что мы точно верим сету, считаем что он 100% вручную провалидирован. Необходимо понять к какой категории относится изображение.

Обучать сеть по категориям, мне кажется, не очень хорошей идеей здесь, так как внутри категорий картинки могут сильно отличаться. В связи с этим и с недостатком опыта у меня есть мысль, но она настолько мне кажется дикой, что нужен мудрый совет чтобы я эту идею спокойно бросил и пошел по какой-нибудь классической стратегии. Вот эта мысль:

Можно сделать последний слой сети размером = кол-ву картинок. Обучить сеть, после этого, подавая на вход картинку, получать на выходе информацию о наиболее похожей на нее, вспоминать ее категорию и считать ее правильным ответом. Так ведь правда не делается?)

Спасибо заранее за мудрые советы :slightly_smiling_face:",
<@U1VJNTV50> выглядит как обычная мультиклассовая классификация (CNN+dense с softmax). То что картинки отличаются в одной категории не проблема если есть достаточное количество таких картинок (а на таких картинках вероятно можно хорошую аугментацию сделать к тому же).,
"<@U0ZJV6E5Q> спасибо, там видите ли как, если в примерах есть 30 более менее одинаковых и всего 1 уникальная для этой категории то нужно ее успешно распознавать. Но я сейчас как раз CNN + softmax и мучаю)",
"Если вдруг не сработает классификация с softmax (по идее должна) - можно попробовать magnet loss <https://github.com/edgarriba/tf-magnet-loss> или PDDM <https://arxiv.org/abs/1610.08904> (они должны хорошо справляться со случаями, когда внутри категорий бывают сильно различные картинки)",
"Residual блоков там, наверное, как раз 14",
"а зачем нужно это ревью, если можно саму книгу почитать/полистать?",
"Ну это как ревью фильма - читаешь, чтобы решить, смотреть кино или нет",
блин интресно канешн как получена картинка,
Привет всем :hand: кто идет завтра на тренировку по машинному обучению (зарешивание)? (выше ссылка),
"прост на зарешку первый раз иду, пока не поняла чего ожидать от нее :eyes: может кто поделится впечатлениями от прошлых зарешек?",
"Тут вопросы по xgboost в питончике)
У меня бинарная классификация и я хочу отслеживать качество  PR-AUC и ROC-AUC.
ROC есть в стандартных метриках, `param[‘eval_metric’]=‘auc’`. Если передавать метрики таким образом, то несколько передать можно, передавая список строк вместо строки.
PR в стандартных метриках нет, чтобы его отслеживать я написал функцию и сделал `feaval=pr_auc` в аргументах `xgb.train`
Но как отслеживать сразу и  PR и ROC на каждом дереве? Если передать `feaval` то первый способ игнорируется",
"можно конечно все в один флоат засунуть)))
Типа 0.pppp00rrrr - где p и r - 4 цифры после точки у этих величин",
"Предсказываю, через 200 лет в Москве будет такая же средняя температура, как сейчас в Сан-Франциско! :slightly_smiling_face:",
А objective как я понял должна быть дифференцируемой?,
Прошляпии регу и непонятно с кем даже связаться сейчас,
"<https://opendatascience.slack.com/archives/theory_and_practice/p1480071114004191>
а можно просто открыть исходники обертки для питона и понять как надо несколько листов подавать :slightly_smiling_face:",
Как корректнее по русски сказать multiseasonality?,
"2 декабря к нам приезжает профессор Натан Интратор из Тель-Авива, расскажет про новый математический метод анализа ЭЭГ. Все, кто угорает по анализу данных в нейронауках, — велкам в московский офис Яндекса и в трансляцию! <https://events.yandex.ru/events/science-seminars/2-2-dec-2016/>",
то есть там скидки ровно такие же как и до этого были?,
"по поводу upscale увеличивающего размерность — сначала пэдишь pool только с той стороны где не хватает, потом после upscale втыкаешь slice, чтобы вместо 4 -&gt; 8 получить 4 -&gt; 7",
"Смотря для какой сетке, очевидно",
я образал вывод когда пробовал трансп и не исправил,
`mode='dilate'` почему обычно он дефолтный?,
потому что под капотом transposed convolution работает как upscale dilate + conv,
всем привет. Может у кого то в закромах есть линки  с чего начинать знакомство c nlp подходом? книги ресурсы что угодно. Буду благодарен если поделитесь . Заранее спасибо,
"друзья, а кто томитой парсером пользуется и зачем?",
"Вчера <@U07V1URT9> говорил, что иногда можно взять обученную нейросеть, отрезать у неё голову, поставить туда решаюшее дерево, и работать будет лучше. Соответственно, вопрос, нельзя ли тренировать сеть с деревянной головой end-to-end, вдруг лучше получится.
У меня есть такие соображения: решаюшее дерево задаёт кусочно-постоянную функцию, которую можно рекурсивно построить по дереву в виде `выражение-для-корня = [условие в корне] * (выражение-для-левого-поддерева) + (1-[условие в корне]) * (выражение-для-правого-поддерева)` где `[...]` означает индикаторную функцию, которая обращается в 1, если условие выполнено, и 0 иначе. В деревьях обычно условия простые вида `x_i &gt; a`, поэтому индикатор можно переписать в виде функции-ступеньки (функции Хевисайда) `[x &gt; a] = H(x - a)`. Такую функцию всё равно нельзя дифференцировать, но можно приблизить её чем-нибудь дифференцируемым вроде сигмоиды c температурой `sigmoid(t (x-a))`. Эту штуку уже можно дифференцировать, правда, есть подозрение, что градиенты будут плохо ""протекать""",
а как функцию потерь считать? MSE ?,
"<@U0JKYTE4B> ""А почему именно яндекс-карты? Встроенные карты не подходят?"" &gt;потому что встроенные карты для России при увеличении до города содержат только условные очертания местности, хочется больше делатей как на Я.Картах или Google.Maps",
"&gt;  что иногда можно взять обученную нейросеть, отрезать у неё голову, поставить туда решаюшее дерево, и работать будет лучше. Соответственно, вопрос, нельзя ли тренировать сеть с деревянной головой end-to-end, вдруг лучше получится.
поясните за дерево, не понял как его обучать или откуда правила берутся?",
"А какие сетки не влезают в 12 ГБ, мне действительно любопытно",
"часов 7-8, скорее, так как 2-2,5 часа только у меня будет",
"<@U35FDTFED> не, я как раз не шарю вообще. и не понимаю зачем оно нужно. хочу узнать",
"Тогда почему обучение не влезает в 12 ГБ, если веса копейки занимают? Зачем model parallelism? Где-то я туплю.",
"Потому что когда мы изображения прогоняем через сеть, у нас временные тензоры огромных размеров получаются, как я понимаю ",
"Добрый вечер. Недавно потребовалось написать сверточную нейронную сеть для распознавания рукописных символов. Руководствуюсь <http://cogprints.org/5869/1/cnn_tutorial.pdf> . Сегодня сеть закончена, только не работает :grinning: . Возникло два вопроса, подумал, мб тут кто-то сможет помочь с этим.

1. Если не использовать аддитивные веса для вычисления реакции слоя, как сильно это сказывается на работоспособности сети? Пару недель назад был сделан многослойный перспетрон, который прекрасно работает (или не прекрасно, но работает) без аддитивных весов.
2. Есть ли какие-нибудь методики по отладке таких сетей? На этапе обратного распространения ошибки становится совершенно непонятно, правильные ли считаются ошибки и градиенты.

Буду очень признателен любому совету :sweat_smile:",
"Да я как-то гугловский корпус хотел в генсим, а он мне весь комп убил

<https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/Document%20classification%20with%20word%20embeddings%20tutorial.ipynb>

В общем, я делаю как здесь, только на русском, и вот ищу данные",
"<@U040M0W0S> я думаю примерно так же тоже самое, как и в своей статье они сравнивают дропаут с половым размножение; всегда легко найти общие черты, но они могут быть очень generic",
<@U14GG4E69>  это в каком фреймворке?,
"Это я в своём делаю так, но я наивно предполагал, что в популярных так тоже можно, потому как вещь полезная и легкая в реализации.  ",
"кароче либителям ганов и трепанации нейросетей, <https://arxiv.org/pdf/1605.09304v5.pdf>
я раньше думал, ну слепо верил скорее, что в процессе обучения выучиваются только дискреминирующие фичи, а переобучение будет проявляться как раз в запоминании деталей картинок
они тут вот генерят картинки, которые вроде как из не переобученной сети, вроде как активируют только нейрон одного класса на выходе, но такие картинки не содержат только признаки объекта интереса, но всегда еще и осмысоенный фон
мягко говоря, я в недоумении, господа; имхо эта статейка вторая по крутизне в сфере открываня черного ящика, после статья Зейлера и Фергуса",
При использовании дропаута мы как будто бы обучаем ансамбль нейросетей. Хинтон как-то так объясняет эту тему в своем курсе.,
"поскольку исходные картинки часто содержат стереотипичный осмысленный для людей фон (животное на фоне травы/листвы, птица на фоне неба и т.п.), то нет ничего удивительного в том, что сеть генерирует изображение с осмысленным для нас фоном (изображение птицы на фоне неба).
В этом смысле наличие неба такая же фича птицы, как и наличие крыла/клюва.",
"У нас была задача — понять, как сменить у большой авиакомпании тариф с плоского (все платят одинаково) на повесовой (больше весишь - больше платишь)",
"Какие нужны данные и как на основе их правильно сделать тариф, чтобы у компании был профит от смены",
"В целом, тут по-моему довольно обширные задачи — типа, как детектить ботов в соцсетях и их мочить или как закупать фильмы для онлайн-кинотеатра",
"<https://market.yandex.ru/product/8309868/spec?hid=91020&amp;track=tabs&amp;deliveryincluded=1>
но как я понимаю туда если и можно вставить GPU, то с большими плясками, и там всего два PCI",
"Всем привет! Для тех, кто работает с текстом, хочу посоветовать метод Structured Topic Modelling, есть пакет для анализа в R: <https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf> По сути дает возможность вытащить темы из корпусов, в которых не проставлены теги. Пробовал на твитах, работает неплохо. Плюс дисклеймер конечно, я ходил на класс одного из авторов :slightly_smiling_face: + Оф. сайт: <http://www.structuraltopicmodel.com>",
"если изрядно раскуриться, можно что угодно анализировать как текст :eyes: 

например <#C2LJA6VP0|sberbank_contest> вогнал в LDA для тем среди транзакций - <http://ods.ai/lda|ods.ai/lda>",
"это да. в <#C0KL2AXD3|datasets> упоминают <https://www.kaggle.com/c/kdd-cup-2013-author-paper-identification-challenge/data>, прямо неплохо, но это немного старое. в идеале нужно не с kaggle, чтобы можно было как финальный проект потом одобрить для курса в магистратуре",
как раз стык социально-экономического получился,
"А кто-нибудь знает, с какой скоростью рассматривают заявки на <http://www.image-net.org/>? Это вообще обязательно, или можно с торентов натырить, а потом дать на них ссылку?",
"&gt; Gradient boosting -- это такой алгоритм ансамблирования моделей.
для простоты, в первом приближении можно и так сказать. скорее это подход к аппроксимации функций
&gt; Чаще всего им ансамблируют деревья. 
да
&gt; Тогда это еще зовется Gradient Boosted Trees или Gradient Boosted Regression Trees.
впринципе да, но редко кто про это запаривается. это скорее название для отчетов
&gt; XGBoost -- это конкретная высококачественная реализация данного алгоритма. 
да
&gt; sklearn.ensemble.GradientBoostingClassifier -- это другая питоновская реализация того же самого.
в целом да, но в xgboost отдельные навороты на сам процесс построения деревьев. это не то же самое, чтобы взять и переписать один алгоритм более качественно, а скорее “прокачать” стандартный алгоритм

&gt; Gradient Boosting Machines
общее название моделей на основе градиентного бустинга",
"Wait a sec, scan в ""compile-time"" строит граф, как ты его в процессе обучения крутить собираешься?",
"т.е. когда ""градиентно-бустятся"" что угодно -- хоть деревья, хоть не деревья,, то можно сказать, что это у нас GBM? 
а какие эстиматоры кроме деревьев, например? линейная регрессия?",
"Видимо, и символьные переменные можно, как тут
<http://deeplearning.net/software/theano/tutorial/loop.html>",
и надо как то заставить теано это проглотить,
"пацаны, а почему нейронные сети часто (почти всегда) изображают графически, а другие модели редко (почти никогда)? даже какой-то нибудь простой персептрон обязательно с картинкой идет. 
но ведь с этими бустингами и стекингами результат тоже из слоев получается? :filosoraptor:",
картинка перцептрона как раз таки хорошо смысл передаёт,
так же как и смысл lstm :trollface:,
"Привет всем, 
Какую GPU посоветуете сейчас брать для обычного стационарного PC? На уровне поиграться с keras, TF, может быть потрогать CUDA programming, для саморазвития. Бюджет ~$500",
а в конце еще с жирухой какой то,
"Некоторую систему координат на поверхности например человеческого тела. Как численно понять что пятка - это пятка, как относиться к частям сложных деформируемых форм.",
non stock это non reference карта где по 2-3 куллера?,
"Тут nvidia прислала
&gt; Приглашаем вас на семинар по глубокому обучению или как стать Data Scientist’ом
&gt;
&gt; Приглашаем жителей Москвы на открытый семинар по глубокому обучению (Deep Learning), на котором вы узнаете о современном машинном обучении, глубоких нейросетях, и о том с чего начать путь datascientist'а. 
&gt;
&gt; Когда: 30 ноября 16:20 – 18:30
&gt; Где: 2-й учебный корпус МГУ, ауд. П14
&gt; Регистрация на семинар обязательна и закрывается сегодня в 17:00 по <https://docs.google.com/forms/d/e/1FAIpQLSeaAB9LDNm8tbbQTa9tPbFobQvEJAkEM5prnoL97FTOL2rowg/viewform?c=0&amp;w=1>",
"да, это практически все non-founders карты, ибо чем же еще производителям выделяться, как не кулерами",
а в смысле какие конкретно?,
у тебя же строки пронумерованы как то,
"<@U04ELQZAU> не помню, где именно",
"ну пронумерованы. но непонятно на картинке, кто где",
"ну кагбэ я про это и намекнула выше, просто думала, может чего поумней есть, особенно когда куча точек это не очень удобно (тут только часть)",
"&gt; кто находится ближе к единицам (красного цвета)
если тебе интересно кто ближе к единицам, то вот тебе ответ :slightly_smiling_face:",
"<@U04ELQZAU> да, похоже весьма на тсне. Я это к чему: хочу подумать на тему того, как этим прокачать метрик лернинг, чтоб не обьекты  одного класса стягивать, а сразу кнн оптимизировать, тогда можно будет разбивать данные на произвольное число кучек",
"Господа, такая проблема.
Есть последовательности векторов и в каждый момент времени определенное событие либо происходит, либо нет. То есть `n` последовательностей разной длины, и в каждой последовательности в каждый момент времени либо происходит либо нет событие - задача это событие и предсказывать.
Соответственно хочу попробовать тут Many-to-Many LSTM. 
Собственно я пробовал подход, когда подается одна последовательности и предсказывается событие после последнего его элемента(а не в каждой точке), это вроде  Mane-to-One?
Many-to-One прогал на керасе,а вот Many-to-Many  - нет. И поэтому хочу найти пример такой сеточки, чтобы понять как спроектировать слои
Нашел что-то похожее на стэковерфлоу, знающие люди, подскажите это правильный пример должен быть?
<http://stackoverflow.com/questions/38101143/implementing-a-many-to-many-lstm-in-tensorflow>",
"Не уверен, что правильно понял, но звучит как seq2seq ",
"Хм, а если подавать время на вход как ещё одну фичу?",
"В керасе у RNN слоев есть параметр `return_sequences=False`, его на True и будет как на картинке",
а как оно в текущих роботах делается?,
"Зачем считать на много тактов вперед, если внешняя среда (по отношению к контроллеру) слишком неопределенна.
Ну. выдал он плавную функцию на 10 шагов вперед, а толку если колеса все равно не крутятся. потому что рука уперлась в стену, или батарейки садятся и мощности не хватает.
Наоборот, надо как можно быстрее измерить реальный отклик и быстро пересчитать команду (не можешь заехать в крутую гору - развернись, спустись и объедь вокруг)",
"Коллеги, пытаюсь зафитить простую байесовскую сеть в pymc3 и не могу понять, как описать зависимость дискретной бинарной переменной от ряда непрерывных величин",
"оно в принципе и понятно, что как исправить не очевидно",
<@U36TUUCGG> Какой движок стоит выбрать и почему?,
"коллеги, как бы вы визуализировали структуру прихода-расхода бабла?
в смысле, приходит из нескольких конкретных точек, уходит в несколько также конкретных точек
какой-нибудь sunkey?",
<@U36TUUCGG> Что читать в качестве стартового материала? Какой набор устройств и софта необходимо купить?,
"Он показывает, сколько сначала притекло и куда оно все потом исчезло.",
"вообщето это классическая визуализация Минарда, и куда делись отряды оно не показывает. оно показывает интересную связку количества людей, географии и времени. но для задачи <@U04423D74> от нее даже вдохновения не получить :confused:",
"Если погуглить building mining rig, то можно видосы, где народ на десятки карточек делает сетапы",
"Коллеги, подскажите куда копать для решения такой задачи:
С течением времени приходят точки в многомерном пространстве. Для каждой новой точки надо понять принадлежит ли она одному из существующих классов или новому, пока неизвестному классу.
Т.е x_0 это первая точка -&gt; принадлежит классу 0
х_1 может либо принадлежать классу 0 либо классу 1
х_2 может принадлежать классу 0 либо классу 1, если х_1 была выделена в отдельный класс
и так далее",
"боюсь сложно будет длинну хеша подобрать...
сейчас делается просто по порогу расстояния, худо-бедно работает, но как только в класс попадает 1-2 ошибочных примера его очень быстро раздувает",
"Просто задача звучит как хрестоматийная, хотя я сам такую никогда не решал, поэтому банальная гуглежка должна помочь сдвинуться с мертвой точки",
"<@U0FEJNBGQ> касаемо построения нейросетью плана движения, мне попадалась одна статья ( <https://arxiv.org/pdf/1606.04695v1.pdf> ) от DeepMind. 

Они взяли рекуррентную сеть, и заставили ее писать на ""внешнюю"" память две вещи - ее внутренний план действий + когда он должен быть пересмотрен. Сетку обучают с подкреплением (игра лабиринт или типа того) и дополнительно -  наказывают за частые пересмотры плана. Фид-форвард вычисления проводятся только при плановом пересмотре плана. Между пересмотрами план просто выполняется конроллером, никаких вычислений в сетке нет. 

Профит на лицо - если сетка видит статический лабиринт перед собой на три клетки без всяких поворотов, и знает из опыта, что никаких роялей в кустах тут не бывает, она может запланировать три шага вперед, и только после их совершения оценить обстановку. А не проверять, что там изменилось через каждое мгновение.

Недостатки этого подхода вытекают из способа представления плана. Он -матрица, где столбцы это такты времени (читай горизонт планирования маленький), а строки - это доступные действия (читай они дискретные и их мало). У ""настоящего"" же робота пространство действий можно считать непрерывным.",
"3-4 хакатон по нейронаукам
9, 10 - R митап + тренировки
16, 17 - hadoop митап, data&amp;science

вторая половина это выхи 23-24 когда идут корпоративы, и суббота 31ое",
"было бы бесплатно, можно было как альтернативу рассмотреть",
"хм, вот куда завтра идти? точнее, уже сегодня. в Я, где про RL + кулуары, но будет запись или на art of science, где много всякого по мелочам, но записи не будет?",
"Интересно, а почему PyData в России нет? :filosoraptor: <http://www.meetup.com/pro/pydata/>",
как выжать из максимум из theano на cpu? он загружает всего ничего,
"а есть у кого что-нибудь перегоняющее строки разной длины в то, что схавает XGB?",
"судя по картинке, нграммы как раз могут зайти",
а CountVectorizer их как раз может извлечь,
"ну и xgb тут не особо нужен будет, как мне кажется",
интрересно сравнить кстати. Потому как кросс-валидация glmnet для разных значений лямбда очень быстрая ибо он использует начальные значения от предыдущей лямбда,
или какой там у меня,
"Коллеги, а если брать материнку на z170 чипсете, в них 2 gpu вообще как себя ведут? Везде пишут, что для нескольких gpu нужно pcie-3.0 40lines а таких процессоров под LGA1151 сокет вообще как я понял не бывает?",
Как висят GPU на дереве PCI-e. Если для передачи данных от одного GPU  к другому приходится через CPU проходить - это хуже,
"Тогда берите любую, где есть 2 PCIe слота и, чтобы они работали в 8-линий на каждый слот  тогда",
а насколько единый? или как в шутке с xkcd?,
"Я было подумал, что тебя удивило как 8 GPU на одной машине сделать, а не где их взять.",
"ребята знают, как презентовать результаты",
"мы даже проверяли, на наших данных page rank как фича почти роли не играет",
Какие фичи обычно добывают из временного ряда? Есть какой-то обзор? Хочется никакие не забыть. уповаю на  <@U0FEJNBGQ> :slightly_smiling_face:,
"ээ, там много всего может быть, смотря с какой целью, и какие сами ряды в оригинале",
что за ряды и какая цель?,
"<@U1CF22N7J> В смысле, в каком фреймворке? Если не хочу брать tensorflow, потому что для моей задачи он оверкилл, а построить данные красиво и интерактивно хочется.",
"Цель чтобы экстремальное поведение сконструированного ряда, соответствовало экстремальному целевому событию.  События: землетрясения. Конструируется как сумма экспоненциально взвешенных по расстоянию силе и принадлежности к класетру числа землетрясений до",
"Как посоветуете разобраться с категориальным признаком, который очень неравномерно распределен?
Представленность различных значений убывает как 1/x",
"привет! подскажите, где взять координаты кафе/ресторанов в Москве?",
"МГУ как режимный объект, блин. Ужас :disappointed:",
"well, как правило, в вузики не пускают кого попало",
"<@U14BPHDK6>: Спасибо, но немного не то. Как бы это странно не звучало, но нужны сообщения не такого характера. Чтобы поменьше адеквата, умных тем и умных слов. Но двач эта другая крайность.",
"Может кто-нибудь подскажет, где можно взять датасет с текстами и тегами к ним? Пробовал medium, но там довольно широкие теги. Надо что-то подобное, чтобы было много тегов и текстов.",
"Ребят, подскажите, кто знает:
1) где можно взять корпус двача (комментарий - ответ), который был на датафесте
2) есть ли какие нибудь еще подобные корпусы? было бы вообще весело, если (комментарий - ответ-картинка)",
"На ICML была забавная статья, как оптимизировать AUC",
"7 декабря в 16:00 состоится День открытых дверей в Центре Анализа Данных при Департаменте информационных технологий г. Москвы.

Приходите, если хотите узнать:
- о мировых практиках работы с данными в медицине,
- о том, почему с такими данными стоит работать именно в Москве,
- о проектах по интеллектуальному анализу данных и машинному обучению, которые выполнила наша команда,
- о том, кто влиться в нашу команду, даже если вы еще студент!

Для участия в мероприятии необходимо пройти регистрацию:
<https://ru.surveymonkey.com/r/B895NTF>

Всех участников ждет много интересного, неформальное общение и даже турнир по кикеру!",
"Как избежать зависаний даже не знаю чего? Точнее в чем дело вообще может быть? Поставил на сервере из sklearn gridsearch  ходить  svm 'ом, через 9 часов: [Parallel(n_jobs=5)]: Done  31 tasks      | elapsed: 145.1min из 75 А остальные 7 часов он что делает? Видимо там все умерло.",
"<@U2ACJG607>
По поводу доменов
<https://opendatascience.slack.com/archives/theory_and_practice/p1480410352004297>
Возможно полезными фичами стали бы: частота смены IP (<http://viewdns.info/iphistory/>), наличие МХ, text, AAAA записей, поддомены из стандартного словаря (типа www,)
Из Whois - регистратор, дата регистрации, продлялся ли ...
По IP - geo, название хостинга, с кем пересекается по подсети
По поисковикам - кол-во страниц в индексе
Наличие robots.txt и sitemap.xml",
"<@U30Q72KLJ> ""zero shot translation” - это когда обучали одну модель, например на двух парах языков: Английский-&gt;Испанский и Испанский-&gt;Португальский. А потом переводят сразу с Английского на Португальский, хотя модель не обучалась на этой языковой паре",
только объясните мне почекму они питон+либы поставили на ось а дл библиотеки в докер? почему не сразу все в докер а там уже при необходимости virtual env создавать?,
как бы в этот раз такое же не произошло,
"убей не знаю. кто такие, ни одной знакомой фамилии в списке участников
то есть, это не психологи-когнитивисты, а какие-то технари-саентологи-etc, которым понравилось слово cognitive",
"кто нибудь собирал Caffe под Ubuntu 16.04?
```
.build_release/src/caffe/proto/caffe.pb.cc:48295:35: error: ‘class caffe::PReLUParameter’ has no member named ‘_has_bits_’
```
такие ошибки не встречались?",
"ну я скорее про самих оргов спрашивал, я просто не в курсе, кто это",
"Ребята, простая задача, если слать больше писем, больше продажи, но и больше отписывающихся от рассылки. Как померять сколько стоит каждый отписавшийся, чтобы понимать сколько оптимально слать писем ?",
А можете статей по выделению тем в тексте покидать? Или хотя бы где про это искать,
"<@U0KQ5M6KX> а на кого больше ориентировано данное мероприятие? Судя по форме регистрации, вы, в первую очередь, студентов ждете?",
"Про реакт не понял. Но выглядит как создание проблемы, а не решение) ",
"не, я как полный бекенд дев с data science - давайте без react))",
"сейчас еще внимательно пересмотрел данные и примеры kernels. все таки это supervised learning задача получается, так как там известно на каждом шаге правильное действие",
а как проще всего скормить бигартму список текстов? он какой-то uci хочет,
".
.
.
сап чат :tada: 
вопрос к московским площадкам - нет ли у кого открытой свободной площадки, хотябы на 60 человек (а лучше до 100) на субботу 24 декабря? ориентировочно с полудня до 15, в разумной близости от центра

есть план сделать ODS елку :christmas_tree: , с отобранными кулстори за этот год, планами на следующий, и почти настоящим дедом морозом с ништяками. 
если площадка найдется, проведем. если не найдется, просто пойдем кудато пить :christmasparrot:",
"<@U040M0W0S> а как проще всего скормить бигартму список текстов? он какой-то uci хочет (edited)

Я недавно сюда даже код кидал с использованием CounterVectorizer",
"т.е. когда мы пулим свертку по картинке фиксированного размера, мы, например, 2х2 пикселя миксуем в 1, и вот размер пула как-то задаем, типа k = [1, 2, 2, 1]. Мне же хочется взять матрицу размера NxM и вернуть KxM, где будут взяты K максимальных векторов",
"<@U1CF22N7J> Да, когда же они закончатся уже!",
"Привет! кто-то может рассказать почему в facebook research часто любят обучать с hogwild training on one example at a time? Это все на CPU. В чем преимущество по сравнению с мини-батчами на GPU, если в конце у нас softmax на кол-во классов стоит?",
"погоди, кто с потолка берет вероятности?",
"Плюсую к вопросу, так как часто вижу, как выбирают регуляризацию по кросвалидации, например лассо, что немного читерно с этой точки зрения",
"Модель задаётся из соображений о том, как устроен вероятностный процесс, но при этом выбирается достаточно гибкой, чтобы подстроиться под данные. А в примере по ссылке есть только какие-то рукомахательные утверждения в духе ""часто-нечасто"" без данных",
"Ну вот тебе пример Байесовской регрессии: считаем, что целевая переменная `y` линейно зависит от признаков `x`, т.е. `y = w^T x`, только мы `y` наблюдаем с шумами, поэтому на самом деле `y = w^T x + ε` где ε – какой-то шум. Можно счесть его нормальным, если поверить в то, что есть несколько слабо связанных и примерно одинаковых по вкладу истоичников ошибок, либо взять какое-либо распределение с более тяжёлыми хвостами, если это предположение нарушается и может быть много аутлаеров.
Кроме того, неплохо бы задать априорное распределение для `w`. (Для `x` мы распределение не задаём, т.к. мы его уже пронаблюдали и уже не важно, из какого распределения оно пришло). Самый простой способ  – задать равномерное распределение во всём пространстве (на самом деле, такие распределения запрещены с точки зрения теории вероятностей, но всем пофиг :good-enough:), либо пользоваться соображениями о важности признаков: если какие-то признаки более полезны (или имеют меньший разброс), то элементы ковариационной матрицы, соответствующие этим признакам, можно сделать побольше. Распределение, скорнее всего, хотелось бы взять симметричным вокруг нуля, если только у нас нет каких-то соображений на тему того, что какая-то компонента должна быть скорее положительной. Стандартным выбором является нормальное распределение.

После того, как ты задал модель и пронаблюдал данные, настало время делать вывод, т.е. находить апостериорное распределение на интересующую тебя переменную `w`: `p(w | y_train, X_train)`. Это делается по правилу Байеса `p(w | X_train, y_train) = p(y_train | X_train, w) p(w | X_train) / p(y_train | X_train)` (`p(w|X_train) = p(w)` так как веса в нашей модели не зависят от признаков). В случае, если априорное `p(w)` и правдоподобие `p(y_train | X_train, w)` нормальны, апостеориорное тоже будет нормальным. Иначе оно будет каким-то более сложным.

После того, как мы сделали вывод, можно будет предсказывать распределения на `y_test` для новых наблюдений `X_test` по формуле `p(y_test | X_test) = \int p(y_test | w, X_test) p(w | y_train, X_train) dw`",
"ФИО - в разных, но фиксированных местах, или просто где угодно в тексте?",
"<@U30Q72KLJ>  Начали с собственного велосипеда: повернули криво отсканенные документы, разбили на строки, строки на слова. Как выделять куски пока не понятно",
"грустно, хотел посоветовать написать ручную выделялку кусков, которая передает в тессеракт строки и сравнивает с разметкой ) 
опенсорсовых систем, которые делают весь процесс целиком, уверен, что нет
у нас всегда случаи, когда все варианты нахождения полей более-менее фиксированы - ID документы, потому что",
А на хакатончик по нейронаукам кто пойдет?,
"<@U040M0W0S> почитай главу про линейную регрессию у Мерфи. Там все то, что написал <@U04ELQZAU> и еще побольше, главное, есть вывод формулы, как это дело считается",
"Там их еще несколько разных же. Я использовал только обученный на web, а есть какие нибудь бенчмарки  между ними?",
А почему обновляют далеко не все веса?,
"Давно читал, детали не помню. Вся суть в разделе 3, остальное -- анализ.
На процессор дают и семпл и случайный набор весов, какие он должен обновить.
```
Importantly, note that the processor modifies only the variables indexed by e, leaving all of the
components in ¬e (i.e., not in e) alone
```",
"<https://postnauka.ru/talks/70559>
""Представьте себе, что на плоскости есть несколько точек, представленных как точки комплексной плоскости, которые неоднократно фотографируются с ошибками. При этом на двух точках из разных фотографий нам неизвестно, являются ли они результатом фотографирования одной и той же точки или разных, и нам нужно оценить истинные точки.

Идея Сидорова состояла в том, чтобы по каждой фотографии построить полином, корнями которого были бы комплексные точки с фотографии, затем усреднить все полученные полиномы, корни чего и будут оценками неизвестных истинных точек""",
"А, то есть он говорит еще, как конкретно градиент к апдейту весов применять",
"хогвайлд крутится вокруг того, что один вес обновить -- атомарная операция. А на какую величину -- это любой оптимизатор подойдет",
"Например есть два конкурирующих набора априорных суждений и хочется понять, какое из них менее/более строгое",
Подходит ли каким то образом дисперсия наблюдаемых переменных под приором для оценки этой сложности?,
"Раз уж про метрик лернинг и <@U041P485A> в подходящем настроении:
Все эти фишки с метрик лернинг изобретены от безысходности, когда у тебя нет достаточно данных и(или) классы сильно не сбалансированны? То есть, если моя задача связана с классификацией большого набора поступающих картинок и я могу собрать 1000+ сэмплов на каждый класс, который могу придумать, то метрик лернинг не нужен?",
"Но это после вывода, а хочется понять заранее какая модель сложнее или проще",
"но на самом деле натренировать вектора на русской вики - это не так страшно, как кажется",
"Наверное идея в том, что в полиноме корни можно как угодно переставлять, соответственно это решает основную проблему задачи - в какую точку переходит каждая точка на другой фотографии",
мне кажется это можно юзать для прикидки какая из гипотез сложнее,
на 11 странице снизу есть COMP(H) вроде как матожидание L(H) если я их правильно понял,
"а как зовется теорема о том, что n точек можно аппроксимировать полиномом n-1 степени?",
звучит как что-то из хаскеля ,
"<@U1Z7QM16H> где можно посмотреть запись <https://events.yandex.ru/events/science-seminars/02-dec-2016/> ?
или у кого лучше спросить?",
"Вопрос по гипероптимизацию, хочется использовать, но есть вот такой нюанс.
На входе гипероптимизатору нужно задать параметр и диапазон его изменения.
Перед запуском оптимизатора диапазон значений параметра известен.
Как быть если диапазон значений этого параметра меняется случайным образом после каждого шага оптимизации? Т.е. каждый шаг оптимизации приводит, например, к увеличению правой границы. После каждого шага оптимизации или в начале очередного этот новый диапазон известен.
Понятно, что плохо:
1. задать только начальный диапазон – не будут исследованы варианты, появившиеся в процессе оптимизации
2. задать макс. возможный диапазон – может вылететь по ошибке, например при использовании еще не созданного  объекта с номером большим чем размер начального диапазона
3. использовать диапазон от 0 до 100, считая, что это относительный номер в диапазоне возможных значений на данном шаге – размеры диапазона могут сильно меняться, и найденное относит. смещение в диапазоне может означать совсем разные значения на каждом шаге, т.е. “запутаем” алгоритм оптимизации.
Есть какие-то решения данной задачи?",
"<@U09JEC7V0> распределение на чем? на известном на конкретном шаге диапазоне?
имхо, это 3-й описанный выше вариант, или в как применить?",
"<@U2LHUAC6S> я не очень понимаю, чего вы хотите, но в случае задания распределения, ваш диапазон будет от -inf до +inf и смысл куда-то что-то смещать пропадает, т.к. сможете искать оптимум где угодно",
"<@U09JEC7V0> м.б. сначала стоит понять )))
имхо, нельзя от -inf/ +inf, т.к. диапазон на каждом конкретном шаге конечен и границы его известны, если -inf/ +inf - то есть вероятность получить значение параметра ЗА границами диапазона.
или объясните плз как применить распределение на конечном диапазоне, избежав 3-го варианта.",
"Зато интересно с религиозной точки зрения, хотя написано конечно давно, когда ещё этот подход считался маргинальным и поэтому много внимания уделено тому, чтобы обосновать свой подход и погнобить частотный.",
"<@U13E1AWCX> <https://arxiv.org/abs/1611.04273>
Есть что почитать в самолете. Я даже не знаю куда постить, сюда или в <#C047H3N8L|deep_learning> )",
"напомните плз, как называется та огненная тулза по генерации признаков для рядов",
"как я понял, он берет TS и тольк по ней все делает",
вроде нашел чет там как,
"чятик, помогите, кто делал Belief Propagation в pgmpy?

у меня есть грязная краудсорсинговая разметка картинок (бинарная), и я хочу спрогнозировать истинные лейблы с помощью belief propagation, как предлагается в этой статье: <http://jmlr.org/proceedings/papers/v48/ok16.pdf>

Если кратко, то нужно сделать то, что у Дафни называется Bethe cluster graph: для каждой картинки свой factor cluster, а каждому человеку соответствует factor cluster со всеми картинками, которые он разметил (пусть будет N).

Проблема в том, что кардинальность для каждого человека получается 2 ** N, что быстро вылазит за максимальное значение для numpy.int64, и при инициализации граф падает.

Это косяк реализации pgmpy, принципиальное ограничение на объем данных для BP, или я где-то туплю?",
"Два вопроса про краудсорсинговую разметку.

Допустим, у меня есть грязная краудсорсинговая разметка картинок (бинарная), и я хочу спрогнозировать истинные лейблы. 

Вопрос №1: Какие есть хорошие методы для этого?

Majority vote не годится: для почти половины картинок есть всего две метки, и они часто противоречат друг другу — их все придется отбросить.",
"Я пытаюсь это сделать с помощью belief propagation, как предлагается в этой статье: <http://jmlr.org/proceedings/papers/v48/ok16.pdf>

Там нужно сделать то, что у Дафни называется Bethe cluster graph: для каждой картинки свой factor cluster, а каждому человеку соответствует factor cluster со всеми картинками, которые он разметил (пусть будет N).

Кардинальность для каждого человека получается 2 ** N, что быстро вылазит за максимальное значение для numpy.int64, и при инициализации граф падает.

Тут возникает вопрос №2: 
Это косяк реализации pgmpy, принципиальное ограничение на объем данных для BP, или я неправильно что-то делаю?",
"<@U0L2F9PJT> легко - у тебя есть один длинный ряд, представляешь каждое окно в нем как отдельный ряд с айдишником и запихиваешь их все в один дф",
"кажется, проще руками нагенерить всяких скользящих средних и календарных индикаторов, а отбор пусть xgboost делает, или кто там",
"я что-то не очень верю в предсказательную способность фич типа ""число пиков в окне"", а если хочется много признаков, скользящих средних можно бесконечно много набирать с разными параметрами, как у товарищей из фейсбука: <http://www.slideshare.net/seanjtaylor/automatic-forecasting-at-scale>",
"боролся ли кто-нибудь со странным font rendering в reveal.js, когда при переключении слайдов сначала рисуются мутноватые шрифты, а потом нормальные?",
"Можно подождать до конца нового года, когда будут распродавать кучу возвратов :slightly_smiling_face:",
"Когда скидывал, прайс был 36-38 если не ошибаюсь",
"<@U1R8X3V54> я в прошлом году ездила на зимнюю школу бизнес-информатики, в магистратуру не пошла)) но вообще они рассчитаны на тех, кто собирается поступать в принципе",
"Вообще это как 4 дня датафеста, но без бара :but_why:",
"Или можно просто использовать данные как есть и смириться с тем, что в них присутсвует какое-то количество шума",
"Вопрос есть задача классификации причем параметры могут сильно коррелировать с друг другом, требуется построить классификатор который по части параметров будет определять принадлежность классу.  Например в обучающей выборки три параметра x,y,z а для классификации может придти запись у который параметр y отсутствуют.  Есть опасения что брать классический подход то он может присваивать малый вес важным параметрам которые сильно коррелируют с уже учетными, как подойти к такой задачи?",
Кто-то пробовал ковырять чекпоинты моделей в тф? Как от туда можно достать веса слоев? Хочу конвертнуть inception-resnet-v2,
"<@U07V1URT9> загружаешь граф, пишешь `session.run(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))` -- это будет список из numpy.ndarray, его сохраняешь. У тензора есть поле `name` -- это чтоб понять где какие веса.
Типа `for t in tf.get_collection(...): wesa[t.name] = session.run(t) # wesa -- slovar' iz wesov`",
А имена слоев как получить?,
"А как сделать так, чтобы tensorflow сжирал не всю память при запуске, а 4 гига, например?",
"<http://striphack.ru/> (топ-приз – 40000 golden money, внутренней валюты, что эквивалентно 4 килорублям, плюс нужен паспорт для регистрации, плюс ваши персональные данные могут быть использованы как заблагорассудится организаторам)",
ну как по мне девчули там страшные на этом видео,
"<@U09BY2N3X> ты участвовала дважды в одной и той же школе? хочу поехать, но я вроде как в прошлом году на другую ЗШ ездила..",
"Хотя кстати легко придумать варианты когда тесты будут интерферировать, тогда конечно их надо разделять, проще всего разделять по времени. Например, если у тебя 3 теста, и каждый добавляет большой блок на одну и ту же страницу. Тогда, те пользователи у которых сразу 3 блока будет видно, будут перегружены информацией, что заденет целевую метрику. Ну это довольно ясная ситуация конечно",
"Странно как то, нужно тех кто на семинаре был спросить",
"Ну как введение сгодится, наверное",
"книга не может быть интерактивной, зачем смотреть учебник :simple_smile:",
Как теорию вероятностей Колмогоров без интерактивных учебников изучал ума не приложу.,
"а вот тут, не знаете, будет запись?
Митап «Прогнозирование преступлений и открытые данные в правоохранительной системе»
или если нет, то отзовитесь, пожалуйста, кто пойдёт)",
"&gt; Прогнозирование преступлений
Недавно в twitter разразилось бурное обсуждение этичности подобных исследований после того, как какие-то китайцы (?) обучили нейросеть выявлять преступников по фотографии",
"Если что, <@U040M0W0S> как профессиональный сталкер поможет тебе",
"У нас тут нет знатоков с опытом антифрода? Есть ли примеры, когда надо выявлять или противодействовать фроду, намеренно усыпляющему бдительность антифрод-системы, т.е. не скрыться от нее, а усыпить? ))",
"и вообще когда фрод принимает во внимание саму систему защиты, а не просто изощренно вредит",
"Да не, это важно. Ни разу не видел, чтоб в картах кто-то пытался усыпить бдительность антифрод системы. Там имхо нет на это времени. Надо слить трафик как можно скорее, т.к. карты всегда могут заблокировать.",
"интересно, можно ли подстраиваться под антифрод, не важно какой пока",
"Или даже так: можно понять, какие примерно правила у того или иного платежного провайдера настроены в его собственной антифрод системе",
"Какими способами можно построить модель на временном ряде, если авторегрессия не работает?
Синусоида с определённым уровнем шума видна на лицо.
Применяется ли фурье в таких случаях или есть что-то покруче?",
"просто если там синусоида с шумом, тогда почему не работает авторегрессия?",
"Ребят, посоветуйте, пожалуйста, как подступиться к задаче. Есть много рецептов блюд, написанными разными людьми. Хочется посмотреть на дубликаты и почистить их, если ну совсем схожи. Речь, очевидно, не только о копипасте, но и близких по сути текстах: и там, и там берутся те же ингредиенты (но мб в разном порядке), и там, и там с ними производятся те же манипуляции, просто они описаны разными людьми.",
"Расскажите, в какую сторону копать, какого рода алгоритмы могут помочь?",
можно ввести время как фичу,
"Допустим. А вот тексты в описаниях, когда по шагам рассказывают, что брать и делать, - с ними то же сработает?",
"Так сходу не могу даже сказать. Но, вероятно, зависит от того, что именно мы хотим понять. Мы хотим понять, пропускает ли конкретный провайдер транзакции с 07 ECI для VISA к конкретному мерчанту? - мы берём карту, пытаемся заплатить и смотрим результат. Как этому противодействовать? - никак. Мы хотим понять, какую сумму максимально можно слить с карты в конкретный магазин за одну транзакцию - ну тут сложнее, но если ресурсы неограниченны то и это можно сделать. Понятно, что если мы будем платить с одной карты сначала 10 баксов, потом 20 баксов, потом 30 и т.д. то антифрод система нас заблочит рано или поздно, причем причина будет не известна - количество транзакций с одной карты за период или количество транзакций с одного фингерпринта - не ясно. Но если мы будем брать разные карты и разные адреса, то рано или поздно мы  все равно поймем. 

Я бы сказал так, запретить ничего нельзя, кто хочет, тот поймет. Но можно усложнить жизнь, сделав эти действия максимально дорогими. 

Ну и я реально не могу представить случая, чтоб кто-то тратил силы на то, чтоб вникнуть во все правила антифрод системы или подстроить её под себя. Дорого это выйдет.",
"У нас есть мелкий мерчант, у афски порог для 3DS (если 3DS  пройден полностью - мерчант не несет ответственности в случае чарджбека) 250 баксов, допустим. То есть можно платить картами транзакции до 250 баксов и потом теоретически их откатывать, т.к. ответственность лежит на мерчанте в таком случае. Мы берём вливаем миллионы бабла в этого мерчанта, чтоб его афска начала считать транзакции до 1000 долларов за относительно безопасные, т.к. на основе данных возвратов по ним не было. Теперь мы можем засылать в него транзакции по тысяче долларов и затем их откатывать. Аналитик Пётр замечает это и вручную опускает планку снова до 250 долларов. Какие риски и какой профит от этого?",
"Для небольших мерчантов, да и для больших иногда, умники, которые понимают, когда и как можно возвращать деньги за покупки в онлайне, доставляют проблемы",
"<@U1ULFPM0U> а как такое работает, он жалобы строчит? я про ссылку на вк",
Можно научиться парсить рецепты как эти ребята <http://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/> ,
"А классная ведь идея! <https://habrahabr.ru/company/lenovo/blog/126384/>
Экран только маленький... Зрение и так не ахти, а тут ещё мельче всё будет.
Я вот свой y550p собрался так же наворачивать, а тут вон какая моща возможна.
Прям уже думаю о втором ноуте, раз маленький и катать везде.",
Где надежно можно купить б/у ноут?,
<@U1CF22N7J> x220 - это же старая модель? какая сейчас актуальная в этой серии?,
"<@U27HU84H4> вообще да, но можно и нужно ещё пробить серийный номер на сайте леновы, где-то там есть чекалка, можно посмотреть из каких компонентов состоит конкретный ноут",
"Я так пару ноутов взял, когда просто пролетали классные варианты за копейки, потом тут пристроил среди друзей/родных.",
"Я с ебеем ""на Вы"", где там ставят поиск по ключам и  снайпера?",
"и зачем тебе ноут для зарешек? если браузер  не лагает есть смысл сделать белый айпишник и разворачивать дома юпитер, я планирую после нового года как раз это сделать",
"<@U27HU84H4> там нужно в поиске найти то, что тебе нужно и добавить этот поиск в отслеживание или типа того, я хз как описать. Тогда на главной будут показывать варианты при каждом заходе и могут на почту падать. Пишешь что-то типа x220 i7 &lt;250$ и смотришь варианты. В конце концов погугли и про снайпера тоже (это отдельные сервисы)",
"Кстати, <@U040HKJE7>, а что ты думаешь про градиентный бустинг с подбором шага как в Adam'е?",
Наверное уже задавали но все таки какие есть простые методики прогнозирование timeseries прогнозов для большой базы клиентов?,
"<@U0JHK9001> я могу предположить только, т.к. ни в одном из представленных магазинов не работаю и об их refund policy могу только догадываться. Скорее всего пишет, что с его карты в счет этого магазина сняли деньги, а о покупке он ничего не знает. Чтоб предотвратить чарджбек, магазин возвращает деньги. Международные платежные системы (visa, mastercard etc) могут приостановить обслуживание конкретного продавца, если чарджбек рейт превысит 1%. Плюс ко всему, если клиент выставит чарджбек, а мерчант не согласится с ним, и в итоге будет принято решение в пользу клиента, то за рассмотрение чарджбека заплатит опять же мерчант. Поэтому иногда лучше сделать возврат сразу. 

Но я не могу сказать, понимает ли этот чел весь процесс. Он пишет, что 50% - шанс успешного рефанда. Он не выставляет никаких требований к карте. Возможно, он просто делает так, как у него когда-то получилось.",
"<@U1CEDPJSU> ну совсем простой - это наивный прогноз, когда берется среднее/медиана за аналогичные прошлые периоды",
не совсем понял вопроса. аналогичный прошлый период - тут как раз прошлый сезон и имеется в виду,
<@U0FEJNBGQ>: а какую метрику качества лучше использовать при этом для начислений? Rmse или rlmse ?,
"<@U1CEDPJSU> <https://www.otexts.org/fpp> тут про метрики есть раздел. еще бы понять, какого рода ошибки прогноза хуже для бизнеса, оттуда и подбирать метрику",
"Это просто градиентный спуск. Итерационный метод (много методов точнее), где ищется оптимум функции многих переменных. Такой функцией может быть правдоподобие (Likelihood), точнее его логарифм, кроссэнтропия и т.д. Применяется в нейросетях например, а процедура вычисления градиента в спуске называется backpropagation)",
"Добрый день. Я новичок в машинном обучении и анализе данных, поэтому сильно не бейте. Занимаюсь прогнозированием спроса на стройматериалы, работаю в SAP. Там заложены только экспоненциальные модели плюс линейная регрессия, ко многим товарам подобрать модели не удается.  С какой стороны лучше подходить к таким задачам - использовать модели типа ARIMA (а не их частные случаи), или использовать какие-то алгоритмы МО?",
"вообще нужно попробовать юзать не как обычно фичи из одного из скрытых слоев, а например пообнулять топ активации и юзать выходной слой как фичи",
Какой в твоей функции наименьший период?,
"Я просто не понял, как периодичность помагает в аппроксимации. Вот и всё :slightly_smiling_face:",
"Добрый день. Продублирую свой вопрос здесь.Я новичок в машинном обучении и анализе данных, поэтому сильно не бейте. Занимаюсь прогнозированием спроса на стройматериалы, работаю в SAP. Там заложены только экспоненциальные модели плюс линейная регрессия, ко многим товарам подобрать модели не удается.  С какой стороны лучше подходить к таким задачам - использовать модели типа ARIMA (а не их частные случаи), или использовать какие-то алгоритмы МО?",
"Ну периодичность ясно как помогает. Просто когда ты записываешь значения взятые с шагом, несколько в X и последний в Y, то у тебя много представителей одного набора X, Y отличающиеся лишь шумом.",
"Т.е. X будет повторяться через несколько точек, НОК(shift, T) типа, где shift - шаг, T - период",
"кто о чем, а я о нейронных сетях. Просто подумай, если у тебя линейные методы работают плохо, то возможно стоит переходить к нелинейным. Тут нейросетки, все деревья (xgboost и т.д).",
"Ребят, такой вопрос. В случае с обучением нейронных сетей, когда мы считаем метрику loss, как мы ее из массива loss считаем в один результат? Для вывода и для определения, когда останавливать обучение. 
Говоря по русски - если, например, для XOR у нас MSE для активаций составляет `[0.001, 0.001, 0.001, 0.9]`, т.е. имеем медиану 0.001, арифметическое среднее 0.22575, другие средние тоже посчитать можно. 
Как мы считаем вот это среднее в популярных фреймворках типа Keras?",
"нене, вопрос именно про ""средний лосс"", он считается тупо как mean для всех результатов из последней эпохи?",
"средний лосс считается как mean что по батчу для вычисления апдейта, что для вывода на экран",
"<@U064DRUF4> а как это выглядит? Пользователь показывает документ, говорит ищи похожие, а кравлер потом сам пытается понять по каким урлам ходить чтоб похожие найти?",
"Ну ладно, можно не вычищать, просто смотреть как растёт количество",
Когда дойдёшь до десятков тысяч -- можно начинать планировать годы на пенсии когда это всё получится прочитать и какой интересный будет исторический документ ))),
"одно спасение - некоторые табы устаревают до того, как до них добираюсь",
не оч. понимаю где у вас RL задействован,
"Кто-нибудь гонял Pymorphy2 распределенно под Ярном? Не соображу, какой ему путь до словарей на воркерах передавать. С теорией всё понятно - руками кто-то делал?",
там как раз ругались про эту тему - а потом вышла дизайнерша которая для 538 эти штуки и нарисовала,
"а кто вообще ругался на эту тему?  и как? не понимаю. выглядит что все делают примерно одинакого и сами себя ругали, что ли? :thinking_face:",
"стратегия объяснения интересует, методологмческие подводки к темам, и прочее. то есть, как учить этому",
"<@U32J3CRP1> про a3c - openai выложили вариант базовой версии, как можно реализовать <https://github.com/openai/universe-starter-agent>",
для несведущих - что это такое и какое отношение имеет к NLP?,
"О каком нлп идёт речь, если документ такой только один? Нам тут бигдата нужна ",
"ну Пашу тоже поймите, приедет он на НГ, ему же нужна акклиматизация, а на площади будет ощущение как будто и не уезжал из своей пустыни)",
ты просто не знаешь какое у меня сердце :kukluxklan: ,
"что же делать, если нет биг-даты? из последнего видел, как некто туда word2vec (это является частью nlp-подхода?) прикрутил и якобы получал интересные результаты :youknow:",
"Ребят, а кто-нибудь знает где можно достать датасет новостей для обучения классификатора?
Интересуют новости на русском с уже присвоенными рубриками (одной или несколькими).
Конечно можно попарсить yandex.news, но может есть способы проще?",
"есть периодические факторы разной длины, влияющие на поведение, и их нельзя заложить в модель как повторы минимального цикла.",
"<@U064DRUF4> 
А где именно выкладывают?
<http://opencorpora.org/?page=faq#.D0.9A.D0.B0.D0.BA.D0.B8.D0.B5_.D1.83_.D0.B2.D0.B0.D1.81_.D0.B5.D1.81.D1.82.D1.8C_.D0.B4.D0.B0.D0.BD.D0.BD.D1.8B.D0.B5.3F>
Тут что-то не вижу ничего похожего.",
"Первая ссылка как раз то, про что вначале упоминал
Вторую посмотрел. Дело в выборе варианта транслитерации может?",
"Я правильно понимаю, что их Grouped convolution это что-то среднее между традиционными свертками через все каналы и Separable convolution как в eXepction?",
Какие есть популярные маленькие датасеты с бинарными / категориальными признаками типа зоопарка и грибов?,
чот не понятно что ты делаешь ),
"вот например в недавней статье, где визуализируют фичи, получилось добиться значительного успеха только благодаря тому что обучили ган который из fc6 генерит картинку

раньше брали производную по картинке, что бы получить визуализацию фичи, а там берут по представлению из которого генерится картинка

суть статьи в том что ран выучил не явное априорное распределение данных, и теперь картинки стали сильно реалистичнее",
"это тиап пример того, как почти такой же процесс как у тебя полностью меняет ситуацию",
Там где убрали топ5 активаций?,
"там в картинке 2 есть дип генератор, который как раз выйчивает приор",
Тогда как мы понимаем какая область пространства соответсвует конкретному классу?,
"если просто брать производную по картинке, то получается deep dream, где одни глаза и хвосты",
"а тут сеть генератор выучивает априорное распределение реальных картинок и выступает как бы регуляризатором, мы выбираем нейрон для исследования, и говорим типа найти такое представление фц6 которое его максимизирает",
"Мужчины, от моего товарища, который по какому-то недоразумению ещё здесь не зарегистрирован, поступил вопрос. Цитирую дословно, уж простите, если что не так: как запустить модель, которую обучили в Torch7 на CUDA, под OpenCL? И где взять модель, обученную на торче на куде (типа AlexNet)?",
"Насчёт запуска под OpenCl хз, там есть вообще какая-то зависимость между тем где модель тренировалась и где запускать инференс?",
"<@U1CF22N7J> извини, понимаю, что это всё выглядит странно, но опять же цитата:

там вся идея что мы должны обучать на куде а запускать на чем угодно вот поэтому мы и говорим что можель обучалась на куде а запускаться будет на open cl, спасибо за модели вот теперь осталось понять как настроить торч для запуска их на pen cl
я правильно понял что модели обучались на куде?",
"Можно подождать ещё кого-нибудь, кто с этим работал, можно caffe попробовать -- самый большой зоопарк, точно есть и куда и опенсл, можно погуглить/стэковерфлоу/issues на гитхабе полистать",
"Бэкэнды бэкендами, но реализации слоев не являются независимыми. 
Иными словами, модель, обученная на куда (с использованием cudnn) будет иметь специфичные слои, которые выполняются только на куде.
Есть конвертер, которые позволяет в основном конвертировать такие слои в cpu-выполнимые (в модули обычного nn)
Можно ли после этого сконвертировать их в соотвествующие cl-слои -- вопрос открытый, на который я не знаю ответ.

Рассчитывать, что обязательно будет работать на чем угодно из коробки -- весьма оптимистично.",
"Ну, естественно с алгоритмом, потому что просто веса -- это просто цифры и без информации, что с ними делать -- бесполезны. Просто я думал, что там нет какой-то сильной специфики между кудой и cpu/opencl, которую конвертеры моделей не осиливают.",
"Т.е. грубо говоря конволюции -- они всегда конволюции, разве нет? Какая разница какой конкретный девайс их считает -- cpu, gpu или ещё что-нибудь",
"&gt; Просто я думал, что там нет какой-то сильной специфики между кудой и cpu/opencl, которую конвертеры моделей не осиливают.
Да, сконвертировать как-то можно
Вопрос в том, написал ли кто-нибудь уже реализации основных модулей под opencl и конвертер cpu-&gt;opencl",
"Почитал чуток про cltorch и всё такое, вроде нет проблем выгрузить модель с куды и потом использовать с cl, если она не использует какие-то штуки, которые не реализованы в cltorch (большая часть обычных модулей реализована)",
"Здравствуйте, подскажите подходы решение - занимаюсь задачей классификации музыки. Решаю так: разбиваются треки на отдельные короткие фрагнаменты, беру их спектральные представления и использую их как отдельные семплы, на которых впоследствии обучаюсь. Получается, что один трек разбивается на n отдельных семплов. Хочется их как то объединить. Вопрос такой - какие есть более хитрые способы их агрегирования, кроме простого усреднения?",
"а причем тут лстм? как я понял, вопрос в том, как можно предикты обьединять сложнее чем усреднением",
А кто хорошо разбирается в Auto-Encoding Variational Bayes-ианстве? Чем это лучше стандартной архитектуры VAE?,
Это как частный случай получается или вообще одно и то же?,
"Из introduction'а
&gt; How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? [...] For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto- Encoding VB (AEVB) algorithm. [...] The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.
То есть, AEVB – это подход к баесовскому выводу, т.е. нахождению приближённого апостериорного, а VAE – это частный случай такого подхода, когда наша модель (декодер) `p(x|z)` является нейросетью",
А вся эта радость в проде где нить живет?,
"Сначала появляются первые результаты, потом люди применяют те же идеи для других задач, постепенно подход становится common practice, и тогда уже инженер какого-нибудь гугла говорит ""вот Inverse Autoregressive Holder Normalizing Flow – то что нужно для нашей задачи, есть куча статей, где он хорошо работает""",
"<@U04ELQZAU>: вот чот все статьи подряд не хочется) хочется чтобы кто-то уже это переработал) Пока я понял только как фигачить VI стохастически и что для этого нужен reparametrization trick иногда. Ну и ок, несколько моделей (слава Кропотову) было рассмотрены с ним. Но ощущения что я возьму составлю вероятностую модель и быстренько выведу все не появилось /:",
"Я сейчас занимаюсь _некоторыми моделями с дискретными скрытыми переменными_, где мы хотим рассмотреть последние достижения в этом деле – Discrete Variational Autoencoders и The Concrete Distribution",
"Например, никто не знает, как репараметризовать распределение Дирихле (и Бета, соответственно), хотя в статье утверждается, что это сделать можно (на самом деле, нельзя, видимо, это ошибка)",
"клево было бы расшареную папку на канал, где тематичные статьи, но как организовать папку, чтобы она не превратилась в свалку - хз",
А как в lifelines предсказывать вероятность целевого события на несколько периодов вперед. А то что то не нашел э. Только строит экспоненту на lifetime и все?,
"не знаю как с точки зрения ДЛ, но у нее заводской разгон и охлаждение лучше, чем оригинальное от нвидии. Плюс менее шумная. Видеопамять такая же, как на любой другой 1080",
"ну хз, с виду как новогодняя елка. Оригинальная смотрится более взросло :sir:",
все это записано как попало,
"ну дёргай `\d{9, 11}` или как там",
"Ольга, да не как попало
вот если бы цифры из номера договора были разбросаны по всей строке вперемежку с фамилией, а код был бы написан задом наперед, вот это было бы как попало
а тут все просто",
"<@U1CGKK865> Имхо, сейчас уже пофиг, карта сама ставит частоту так, чтобы не перегреться. Разница только в том на какой частоте будет в итоге работать с данным конкретным охлаждением",
"основная то масса полностью сойдется и до оператора будут долетать единицы, десятки в день (хотя какой там поток еще бы узнать)",
"делал похожую вещь, там был примитивный матчинг в стиле <https://opendatascience.slack.com/archives/theory_and_practice/p1481186333000330>, и UI для оператора, где строки красились в цвета по уровню совпадения и были варианты того, с чем примерно совпадает и возможность ткнуть ""да это оно""",
"Но тут, как мне кажется, есть один важный момент
Все-таки эта книга относительно новая (81ый год) по сравнению с Войничем
Если верить, та книга из древнего и мрачного средневековья",
"Лазанья не так популярна как эти ваши мхнеты, тф и керасы",
"<@U07V1URT9> твоя ревность тебя погубит, вопрос был не что мне делать, а почему никто не сделал",
"Если серьезно, то даже в мхнет этот конвертер работает не для всех слоев. Ну и как правильно сказал Сережа, лазанья не такая популярная.",
"возникла идея на этот счет, трейдеры одно время парной торговлей занимались, потом оно перестало работать, но информация о том, как искать cointegrated series осталась, может это вам поможет искать товары-каннибалы",
"А вообще, какой подход верный , если нужен долгосрочный  прогноз: объединить товары в группы или по каждому индивидуально?",
"не знаю. есть еще тема с иерархическими прогнозами, когда и группа и деление внутри группы предсказывается",
"посоны как думаете такой вот 3 месяца юзанный за 1700 бачей ок или чел припух <https://dubai.dubizzle.com/classified/computers-networking/computers/desktop-computers/2016/11/28/reduced-price-for-the-high-end-pc--2/> 
? мне для дл типа",
"Не очень понимаю, почему продавец там припух. Зион такой вроде 600+ стоит, 1080 тоже 600, а там еще миллиард свмстоперделок (их нужность - вопрос другой, но вроде не выглядит как какая-то накрутка цены)",
"Почему продавцу продавать дешевле, чем такое же можно купить в другом месте?",
Лучше взять бу шное серверное железо как это делал <@U1CF22N7J> и поставить две видюхи,
"<@U0U2ENJ4U> они сильно дешевеют, когда у них выходит ""срок годности"" и их сбрасывают тоннами большие ребята типа фейсбука. v3 ещё не протух и поэтому цены на него даже близко не падают",
Какая связь между количеством ядер и памятью? Типа будет больше запускать всего на более мощном железе и памяти нехватать станет?,
"<https://opendatascience.slack.com/archives/deep_learning/p1481194520000246>
да их как то занесло в кулинарную тему :slightly_smiling_face:
<https://github.com/casperkaae/parmesan>
Keras гуглится легче(меньше ключевых слов нужно) и выглядит удобнее",
"Сдается мне, лучше не вскрывать эту тему и не расшифровывать эту книгу. А то будет как у Маркеса в ""Сто лет одиночества"".",
<@U3AK3EFD0> а какой период вы считаете долгосрочным?,
"Немного бесстыжего само-пиара. Я сегодня выложил на архив нашу новую статью <https://arxiv.org/abs/1612.02192> про то, как строить адаптивные вариационные автокодировщики, которые можно быстро до-обучить на новых данных",
спасибо. туда как раз и смотрим. RAISR показался интересным (<http://papers.ai/1606.01299>),
"*Стоял ли перед кем выбор беслпатного OLAP инструмента: kylin, druid, …*
*Из чего выбирали? Что в итоге выбрали? Пожалели?*
ответить, обсудить, почитать - можно в <#C0804BS5Q|big_data>",
"Пусть есть несколько факторов и целевая переменная. Мы хотим протестировать влияние только одного из них на целевую. Корректно ли делать это однофакторным дисперсионным анализом? Чот мне кажется, что нет, но придумать контрпример не могу",
"А почему нет?!
Все равно некоторые влияющие факторы неизвестны. Какая разница будет их K или K-P?",
"недавно был похожий вопрос, но ответов было мало, поэтому еще раз (задача многоклассовой классификации):
есть объекты, у которых есть 2 источника признаков (то, из чего считается набор признаков), причем первый источник есть у всех объектов, а второй только у части, при предикте у объекта есть либо только первый источник, либо оба. Причем второй источник при предикте зачастую будет полезен тогда, когда первый неинформативен (но при этом в обучающей выборке есть объекты, у которых второй источник аналогичен тому, который пришел на предикт, но при этом первый источник информативен, то есть второй получит небольшой вес). Есть ли идеи как быть, кроме как строить 2 классификатора?",
"Подскажите, как можно визуализировать карту (EMEA, APAC, NALA и тд) на matplotlib (seaborn)? Кажется вроде нашел, <http://basemaptutorial.readthedocs.io/en/latest/projections.html>",
"+ Еще вопрос, какие популярные и наиболее информативные визуализации для статистического вывода между 1-й фичей (x, univariate feature) и зависимой переменной?",
narx любопытная тема. что они умеют запоминать в рядах? для каких рядов лучше подходят?,
"<@U28LJS926>: Я не могу подсказать как лучше заенкодированную информацию улучшить путем добавления похожей информации с натренированной моделю, но могу сказать об информации (не данные, а информация #information_theory). Если подойти к сути дела, то информация передается через спутники, как она кодируется (Source coding + Error-correction coding).. насколько известно, современный Source coding информации - это Arithmetic Coding, а для фильтрация шума и тд (канала) то Gallager (LDPC) coding используется.",
может кто нить подсказть курс или лекцию по не ронным сетям с чего начать (желательно на R),
валидация и тест как отрезались?,
"В идеале, нужно будет сделать полуоткрытую сеть, чтобы постоянно дополнять сеть реальными данными для улучшения прогноза, но я пока не разобрался как это сделать.",
йоба даже в превью видно где макет :rage4:,
"я бы трешачка добавил какого-нибудь, как на наклейках было
в виде облака какого-нибудь, а то скучновато выходит :thinking_face:",
"<@U14BPHDK6> можешь сказать, когда точно будет известно? заявку надо оставить и т.д. могу встретить 25-го :slightly_smiling_face:",
<@U040HKJE7> у тебя самого какого размера толстовка?,
Ну на тебе и текущая уже почти как на рэпчике,
"А я вот так и не понял, если я очень хочу футболку, оставил заявку, где ее можно будет забрать 24 декабря?))",
а когда будет мерч штаны?,
"Сорян, опечатался, толстовку имел ввиду. Где забрать можно будет, ребят? :take-my-money:",
"В Москве, либо найти того, кто тебе её привезёт",
"Конкретный адрес в Мск пока обсуждается, как я понимаю",
Почтой кто нибудь сможет отправить? ,
"кто их знает, чем они на работе занимаются. может Каггл решают...",
"ребята, а как вы по-русски переводите one-shot learning?",
а transfer learning вы как переводите?,
"Так круто же, почему не заимствовать новые слова с нуля для новых сущностей?",
"Зачем ему базовые идеи, если вся литература на английском?",
"да я то лично не против, но есть случаи когда надо говорить по-русски",
"Тогда можно бахать как угодно, всё равно будет требоваться обратный перевод на нормальные термины для понимания )",
"Хз, какая согласованная терминология, но я бы так прикинул: 
&gt; one-shot learning
обучение с единичных примеров
&gt; transfer learning 
обучение путём переноса представлений (или что-то типа того)",
"Хотела спросить, есть ли кто то на RMoscow, но огляделась и увидела людей, сидящих в слаке в  osd:grin:",
<@U3A79PDFV> ну <@U04423D74> ты думаю увидела как ведущего :slightly_smiling_face:,
"понимаю что нужно найти локальные максимумы в распределении, но никак не соображу как именно",
Можно еще рассматривать задачу как обычный gaussian mixture.,
"Видно же где локальные максимумы. Можно просто руками значения записать, а потом для каждого элемента выбрать ближайший пик",
"Я все думал как бы формулой эту трансформацию записать, но k-means подойдёт ",
"правда, эта структура непонятно как с университетом соотносится",
посовейтуйте по непараметрическому байесу какую книгу?,
"посоны, хз в какой канал написать, да и вообще сомневаюсь решаемо, но все ж, вопщем завтра у меня будет комп (системный блок), но у меня нет ни моника ни клавы, но есть ноут, можно ли на это дело поставить линупсы? -)",
"С обычным компом может проблемнее оказаться, чем с малинкой. Биос может быть не настроен на загрузку с нужных девайсов, сетевые интерфейсы хз как будут называться и прочая, прочая",
"думаю нет, за ram и cpu хорошая переплата выйдет, проще сервер арендовать и мак как тонкий клиент использовать",
"если ты собираешься на нём стакать xgboost’ы, то ты в любом случае проиграл, потому что с 16 гигами оперативки ты бы умер на недавних конкурсах с кеггла. А если ты его будешь юзать как терминал — тебе относительно плевать.",
"вроде только юрлица могут, и если будут плюшки, то только после того как станешь резидентом сколково",
"<@U0AS548A1> <@U0JHK9001>  Я так смотрю с моей конфигурацией трудно будет найти бу...  15"" Как-то по размеру не совсем удобен мне кажется.. Я читала вроде бы советуют i7,  Как вы думаете может быть всё-таки брать i5 , но 16гб и тд?... ",
А где будет трансляция Бенджио?,
"<@U34UL690R> че-то звучит пессиместично. может в НН можно найти людей, которые не здесь.  так и хочется спросить. какое количество достаточно образует критическую массу? и оценка тяжестью-на-подъем на каком датасете сделана?",
"<@U040M0W0S> оценить количество можно по вовлечённости даже сюда, я думаю. Вижу пока немного, только мск/питер в основе. 
Оценка тяжести сделана на личном опыте организации движухи по другой теме и обмене опытом с теми кто что-то тоже делал в этом плане.
Если что-то получится - ок. Вакансии в НН обсуждали недавно, вроде бы две с половиной нашли ))",
"<@U040M0W0S> на этом учатся. Безопасность + если кто хочет - соревновательная часть тоже есть. Деньги не зарабатывал, самому интересно было, только чтобы площадку отбивать сбор был. Но там мелочь - кофе попить один раз :slightly_smiling_face:",
"не. я имел в виду ""мотоджимахана"" -- это на что люди тратят деньги или то, на чем зарабатывают? :slightly_smiling_face: я думаю это как бы важная фича в контексте тяжести-на-подъем.
впрочем, уже оффтоп начался. 
нужно вернуться обратно. ""митинги"" в НН.",
"""Science Café про Internet of DNA в Москве. Что такое ""интернет ДНК"" расскажут докладчики: David Haussler из University of California, Santa Cruz, один из основателей Global Alliance for Genomics and Health и Андрей Запарий, ведущий разработчик в области биоинформатики R&amp;D Центра Dell EMC Россия в Сколково.
Модератор дискуссии - Юрий Пеков, директор Бластим (а какие эксперты примут в ней участие и почему MIT Technology Review считает интернет ДНК одним из важнейших технологических трендов современности – читайте на странице мероприятия)"" - мероприятие пройдет уже ЗАВТРА",
А есть кто юзал lifelines в pythone?,
"Я тут забацал видео туториал как прогать на d3.js 
<https://www.youtube.com/watch?v=_xvkNo989qY> 
Чтоб нескучно было",
(я хз почему бигдата - но полезная штука),
"раз уж тема зашла... много красивой фрактальной графики можно найти у <http://crist-jroger.deviantart.com/gallery/> . + к тому, есть какой-то challenge, где народ осваивает вот эту программу для генерации - <https://syntopia.github.io/Fragmentarium/>",
"а когда у документов в пространстве LDA-топиков большинство координат равны и близки к нулю, а  соответственно парочка координат в сумме дают 90+% -- это хорошо?",
"в немалой степени магия артм как раз в том, чтоб получать разреженные репрезентации надежно",
"Там как раз небольшие размерности, как правило",
"мне прост не понравилась интерпретация лда-координат как принадлежности к кластерам, это тупо звучит",
"Идея LDA как раз в том, что у тебя есть скрытые группы (= кластера), которые ты ищешь.",
"была статья где lstm autoncoder предолжений что-то немного улучшал. Но такой красивой картинки конечно не будет, документы это же не 10 цифр :slightly_smiling_face:",
<@U041P485A> почему вашей компании нет? не отправляли сабмишин еще?,
"<@U041P485A> а кто эти люди, которые в победителях?",
"Если могут, то каким образом они это делают?",
когда распределение размазанно по классам?,
"<@U06J1LG1M>, настоящая - это когда мы не строим n классификаторов для каждого класса, а классифицируем все одновременно",
почему ты называешь это настоящей?,
Хинтон вот говорит что вообще нет такого понятия как классификация,
"если да, то строй такие регуляризированные деревья где в листах будет большая вариативность семплов и просто выдавай голоса за класс",
"Я уже плохо помню, как он работает, но да, кажется подходит",
"Они как раз делают то, что ты называешь ""настоящей""",
"Мне вот вверху ответили про софтмакс, но я, честно говоря, не понял, как это выглядит",
"И не очень понятно, зачем он нужен",
"Всем добрый день. Выбираю ноутбук для работы и учебы c анализом данных, серфа интернета и фильмов/ютуба. Планируется использовать mySQL, Python, пакет офисных программ. Объем данных в работе с Python по части ML планирую использовать до 300-500 тысяч наблюдений, по в среднем по 30-100 признакам, признаки соответственно могут быть любые, предположим, что все количественные небинарные. Одни из требований - удобство переноса; 15 дюймов; легкий вес; приятный экран, т.к. планируется использовать после основной работы, когда глаза уже хотят выпасть; срок жизни батареи по заводским заявлениям от 8 и вверх часов. Также одним из требований является то, чтобы это был не MacBook. Присмотрел несколько вариантов: LG gram 15, Samsung notebook 9, Acer swift 7. Есть ли у кого-либо опыт использования какого-либо из данных ноутов, был бы благодарен за совет выбора.  Также, если есть иные варианты, тоже готов услышать, бюджет 60-80 т.р.",
"<@U0FL6RNHM>  ну несколько пунктов, если сравнивать указанные мной устройства и макбук с аналогичными характеристиками, разница в цене может достигнуть более чем в 2 раза. Далее это операционная система, когда читал про макось удивило, что в официальном апп сторе нет скайпа, это для примера. также файловая система ближе всё таки от винды. К преимуществам мака можно было бы отнести время работы от аккумулятора, ретина дисплей и тач бар с мультижестами, однако у указанных устройств также указаны характеристики по работе от аккумулятора и присутствуют мультижесты.",
"т.е. всегда, когда я спрашиваю про макбук у владельцев, одним из главных достоинств является ретина дисплей, дизайн/вес и удобство работы. Каких-либо преимуществ в непосредственной работе , к сожалению, не слышал.",
"а удобство работы, это как правило время работы батарейки и мобильность девайса",
Вот ведь как скатывается в холивар эппл неэппл. По изначальному вопросу тишина...,
"плюс мака, который я не изучал у LG,samsung и acer, это его ремонтопригодность. Действительно, когда смотрел б/у ноут за данные средства без проблем можно найти комплектующие",
ультрабук с бюджетом в 60-80 тр? это где такие водятся вообще?,
@ruster а какие хорошие инструменты исключительно никсовые на память приходят?,
Как будто дл фреймворки хоть где-то ставятся без костылей,
хотя зачем хадуп на буке :thinking_face:,
<@U0DJEEXE1> а почему здесь постоянно люди с проблемами в xgboost на винде? :thinking_face:,
"не думаю что в tf настолько много человеко часов, чтобы хорошую многопоточную работу на cpu поддерживать на windows и unix одновременно, когда всеравно 95% на линукс будет работаеть",
"зачем придумывать себе геморой, если всё работает",
<@U0U2ENJ4U> питон у тебя какой версии?,
это как про кернинг узнать,
"друзья, чего-то не могу сообразить. вот у меня есть аудио с видео. там почти везде музыка, но середине есть небольшой момент, когда она стихает и меняется.",
но как мне по этому месту разрезать файл?,
"как в смысле как :slightly_smiling_face:. что может по ""тонкой желтой полоске"" спектрограммы сегменты мне назвать?",
"Да тут если прямо видно, зачем окно терять ",
"если один раз делать, то почему просто пальцем не ткнуть на график?",
"<@U040M0W0S> я недавно где-то спрашивал как нарезать временной ряд по промежутку нулей и там мне адовых магических однострочников понаписали, попробуй поискать",
"Как посоветуете предобработать признаки в следующей задаче:
есть признаки, многие из которых one-hot. 2 класса. Класс 1 возникает зачастую когда несколько признаков принимают значение ""1"" ВМЕСТЕ. Но данных много, отловить все такие комбинации очень сложно.
Вот может есть какое-то юзабельное преобразование признаков, чтобы помочь алгоритму читать такие ситуации?",
"если у тебя есть априорное знание, какие это признаки, можно попробовать сделать отдельную фичу - and по всем (питоний `all`)",
И взять как признаки новые в каком листе,
"Здравствуйте, скажите пожалуйста, какая метрика предпочтительнее для оценки качества модели в задаче прогнозирования оттока клиентов? Выбираю между ROC AUC, PR AUC и log loss.  Какие ключевые различия у этих метрик? Почитав обсуждения о выборе между ними на <http://stats.stackexchange.com> понял следующее- log loss лучше площадей под ROC и PR кривыми если нашей целью является не просто классификация, а точно предсказанная вероятность, а PR AUC лучше ROC AUC в тех случаях когда класс 1 для нас имеет большее значение чем класс 0. Отсюда делаю вывод что PR AUC для этой задачи лучше. Но почему тогда  судя по соревнованиям и статьям  ROC AUC используют чаще в этой задаче?  Что я понимаю не так?",
"лог лосс довольно сложно поддается интерпретации, лучше всего AU ROC как мне кажется",
"Нашел еще статейку <http://www.chioka.in/differences-between-roc-auc-and-pr-auc/> тут пишут что PR AUC лучше подходит для работы с выборками в которых класса 0 сильно больше чем класса 1, так как не учитывает TN ответы.",
У меня как раз такая ситуация - roc абсолютно бесполезен. Смотрю pr auc по маленькому классу и f-меру,
"Такая ситуация. Обучаю в Keras lstm по последовательностям разной длины. Я их группирую в батчи по размеру последовательности и подаю на вход. Как правильно - по каждому батчу обучить n эпох , или n раз обучить по батчам с одной эпохой ?",
"в задаче оттока еще часто пользуются следующим: на holdout множестве берут top-N кто уйдет на основе предсказаний модели, и считают сколько из них реально ушло. чуть ли не как ранжирование это решать

потому что оттоком пользуются напрямую так: предсказывают кто ушел и передают этот список специальным людям, чтобы они провели работу с клиентами. письма счастья написали (работает на тех кто тупо забыл), ништяков подкинули (работает лучше и в том числе на тех кто может к конкурентам уйти), позвонили (иногда может спровоцировать уход к конкуренту) и тп",
"&gt;  берут top-N кто уйдет на основе предсказаний модели, и считают сколько из них реально ушло. чуть ли не как ранжирование это решать
так это precision получается ведь",
"<https://arxiv.org/pdf/1509.05009v3.pdf>

выглядит как что то крутое, но я не могу понять ничего дальше абстракта",
"libfun, а как это правильно сделать? Евгений Соколов вот здесь <http://www.machinelearning.ru/wiki/images/1/1c/Sem06_metrics.pdf> тоже говорит про неадекватность ROC AUC на несбалансированных данных  : ""Таким образом, если положительный класс существенно меньше по размеру, то AUC-ROC может давать неадекватную оценку качества работы алгоритма, поскольку измеряет долю неверно принятых объектов относительно общего числа отрицательных.""",
"В то время как если перебирать батчи, в среднем будет что-то хорошее, SGD же",
"фиксировал бы эпоху даже не как полный проход по датасету, а просто прогнанные через сетку n сэмплов, генерил батчи рандомно каждый раз из датасета",
"Если существенно - это где-то 1 к 100 - то да, AUC не катит. 
Еще есть такая штука как model lift. Тоже в случае, когда надо упорядочить по вероятности оттока.",
"я как-то генерил случайный батч с юниформ распределением таргета, как раз помогало боль-менее против вот этого &gt;&gt;&gt;в среднем у разных сэмплов будет разный ""приоритет""",
А почему текст отличается от картинок в этом смысле?,
"А как сделать чтобы у Kerasa вывод ползунка с loss постоянно делал stdout flush , то есть не начинал с новой строки а переписывал старую ?",
а какую убунту ставить 14 или 16?,
О! <@U06J1LG1M> расскажи потом на сколько было просто завести куду под 16й убунтой,
ее пока и установить как то не просто -),
какую то дичь пишет edac sbridge ecc is disabled,
а 1080 на убунте 16 у кого нибудь работает?,
а какой прирост в скорости куднн дает?,
Где можно почитать про теорию vowpal wabbit? у них <https://github.com/JohnLangford/vowpal_wabbit/wiki> в алгоритме только функции потерь описаны. хочу знать как оно работает,
"я просто раньше с vowpal wabbit никогда не работал, хочу знать что внутри, какие его преимущества",
"Приветствую всех! У нас в Одессе начала понемногу раскочегариться ситуация с ивентами связанными с Data Science, запустили проведение Kaggle тренировок совместно с <@U070Y25AS>, а также семинаров на базе двух наших вузов (ОНУ и Политеха). Теперь за активностью можно следить здесь: <https://www.meetup.com/Odessa-Data-Science/> и здесь <https://www.facebook.com/groups/343424956019333/> Приглашаю всех присоединиться и по возможности также участвовать;) Тренировки и семинары будем проводить регулярно раз в месяц, следующие попробуем уже с трансляцией и записью. 

Вчера я первым выступал на нашем семинаре в ОНУ и чтобы сделать запись, решил устроить еще вебинар, буду благодарен всем за поддержку анонса на Хабре <https://habrahabr.ru/company/flyelephant/blog/317690/>, а также распространинию анонса среди знакомых начинающих дата сайентистов или тех, кто ими планирует только стать. Это будет базовый обзорный доклад.",
"я просматривал их, но в явном виде, типа как мнк или как формируется дерево в random forest, не нашел",
там третий что ли есть? он как то именем отличается,
"Он, как я понимаю, таки сын Йошуа",
"чот Яша от корней уходит, ну может жена француженка",
представил Пашу в простыне как на касетах с требованиями террористов),
"а подскажите плз работы (или ключеывые слова),  где нейросеть структурно меняется по ходу обучения. Т.е. не как обычно -  только веса меняются -   а и сами нейроны или соединения или слои добавляются/удаляются",
<@U04422XJL> какой внутри сидит алгоритм?),
"Можно смотреть на vw как на либу, содержащую пачку алгоритмов, объединенных идеей online-обучения и высокими требованиями к производительности",
"<@U0JHK9001> дай какие-то стартовые рекомендации по тому, как начать изучать и тюнить, что бы добиться наилучшего результата",
"погугли по словам  “mlwave  vowpal wabbit”, там чувак много примеров показывал, как для kaggle использовать
не только в блоге, но и на самом kaggle писал
как тюнить — перебирать все параметры, я не знаю других способов, снижаешь learning_rate и вперед
всё очень быстро будет, mlwave вроде тоже писал на какие параметры внимание обращать",
"чем проще всего графы рисовать? Я нашел только graphviz, но он насколько я понимаю не позволяет отрисовывать прямо в jupyter, либо оно как-то сложно готовится, потому, что в прошлый раз когда я остановился на нем у меня это сделать не получилось. Поэтому и вопрос, есть ли способ отрисовывать графы непосредственно в jupyter?",
"ну базовое как раз от mlwave посты, а дальше все очень просто, остаётся только ручки крутить",
"<@U2FJHNWCS> Я как то рисовал с помощью graphviz:
<http://nbviewer.jupyter.org/urls/dl.dropboxusercontent.com/u/16539116/DecisionTree_demo_tmp.ipynb>",
Была статья где в RL режиме подбирали архитектуру сетки ,
"А если в bigartm одна модальность норм работает, а во второй у всех топиков одинаковые токены, где может быть косяк?",
"Народ, а если коротко, кто знает что такое сигма-пи нейройнные сети и с чем их едят?",
Где ты это нашёл и зачем это нужно? Гугл выдаёт какой-то богом забытый древний материал,
"Или пока просто эти сети не стали популярны, а как станут, так прибежит какой-нибудь шмидхубер и скажет, что 100500 лет назад это уже делал.",
"коллеги, помогите нубу. как можно из картинок в папке получить их какие-нибудь параметры цвета? rgb, баланс белого, что-нибудь еще. 
хочу сравнить картинки, которые постят две разные группы в инстаграмме",
"ну звучит как бред, у тебя же параметры должны описывать ментал хелс, для которого у тебя нет разметки",
"да у меня 1,5к размеченных картинок есть, так-то
но фичи должны быть содержательно интерпретируемы. ну хоть как баланс белого",
"угу, спасибо
осталось понять, как все это из картинки выцарапать %)",
<@U04423D74> а у тебя какого рода разметка? (типа два лейбла: норм и ненорм или что-то сложнее) И что ты из нее хочешь получить из анализа изображения?,
"офф: мне кажется, что я так часто советую взять фичи с предпоследнего слоя сетки, что меня скоро будут воспринимать как ""тот парень с инверсной графикой"", но в моем случае ""тот парень с фичами сетки"".",
Для меня это звучит как multi-label classification.,
тогда как <@U30Q72KLJ> сказал — в HSV и там плясать,
а как это расстояние интерпретируется?,
"чат, а есть у кого опыт самостоятельного составления расписания для изучения новой темы/тем?
какую часть скудного свободного времени получается выделить на это?
какое соотношения изучения теории/практики считаете оптимальным?
нужно ли железно ботать основы или лучше пытаться воспроизводить state of the art и набивать шишки?
эффективно ли раскидать 2-3 направления на неделю или лучше one step at a time?
ну и вообще ваши мысли интересны, дискасс короче",
Тоже бы послушал как сильные люди составляют себе список тем для изучения и потом ему следует ,
"на первый взгляд там никаких ограничений для теано нет, но вдруг какие подводные камни",
"ну и да, у меня подход как раз наоборот, сначала практика, упереться в грабли (что-то не получается, не работает, выдает кривые результаты), а теория для разбирательства с граблями",
"а тут много кто может прямо сейчас сесть и запрогать какой-нибудь бустинг, например?) или другой метод, который на практике хоть раз использовался из фреймворка",
"я вот тоже от бессоницы практикую, но опасаюсь, что организм может привыкнуть и воспринимать чтение статей, как снотворное :slightly_smiling_face:",
"кароче куда 8 и куднн норм встают на убунту 16, только поебаться пришлось часа 4 изза того, что я поставил зачем то сразу после инсталяции систему дрова на карту, что разрешение было, потом начал питонные либы ставить, и как то забыл про то что дрова поставил, потом скачал куду и вот тут начались проблемы, какие то там конфликты куды и тех официальных дров что поставил до этого, в итоге не не логинится в иксы, помогло удаление всего что связанно с нвидией и установкой куды включая дрова которые с ней идут",
"ребят, посоветуйте какие методы попробовать. 
задача:  есть несколько временных рядов и несколько рядов режимов, нужно прогнозировать временные ряды в зависимости от режимов
я уже пробовал смотреть xgboost, LASSO, что еще можно попробовать?",
"<@U32SCRR1R>  подробностей в студию. какой природы ряды, какое разрешение, какая длина, сколько рядов, что есть режимы, есть ли переходы между режимами в одном ряду",
<@U0FEJNBGQ> режимы - тоже временные ряды такой же длины как и финансовые ряды,
"Как правильно сэмплировать time series? У меня такой отстой получается, что ппц. Метрополис застревает в локальном максимуме плотности и до свидания ",
"пробовали когда то решать задачу с вотермарками, делали в лоб: берём маску (маску получили загрузив чёрную картинку на сайт который ватермарки генерит), потом ищем её положение на картинке и вычитаем маску из картинки
это очень долго и ресурсоёмко",
"В сценарии либо сценарий развития ряда какого нибудь, либо ничего, тогда просто безусловное распределение",
"а... сообразил. мы путаем дисперсию с translation distance. дисперсия остается константой, а translation distance растет как sigma * sqrt(n)",
"Дело походу в том, что то, как я это делаю сейчас - плохой способ, надо через разности идти, они стационарными должны быть",
Привет! Кто сегодня на hadoop в mail?,
"Почему один и тот же код может работать с тангенсом в качестве нелинейности, но не работать с другими функциями активаций (релу, сотфплюс)?",
"тоже не дошло почему одна и та же сетка с релу не сходится вообще никак, а с тангенсами - вроде збс",
а как тогда тф-идф делать?,
"Хорошо, спасибо :hand: Попробую поделать, если что, ещё спрошу какие вопросики потом.",
какое максимильное значение выбирать пришлось подбирать,
"Никуда не деться, надо увеличивать дисперсию переходного ядра. Ну либо гамильтоновское, но там тоже параметр надо будет тюнить. Если ты семплируешь из апостериорного, оно тебе известно с точностью до нормировки. Хороший прием тогда в переходное ядро как матрицу ковариаций засунуть матрицу обратную к гессиану апостериорного в точке его максимума",
"<@U1ECHTLQZ> я, когда работал с поисковыми запросами, так и делал - запросы пользователя считал как один текст",
"<@U13E1AWCX> гамильтониан из коробки не заработал, настроить его не получилось у меня, там падало обращение не помню какой матрицы:(",
А зачем она может понадобиться? Списки книг составлять? ,
"в статье Generative Models от OpenAI есть ссылка на DCGAN network, как пример - <https://arxiv.org/pdf/1511.06434v2.pdf>
и красивая диаграмма. я что-то не могу понять, что подавалось на вход этой модели. цитата из статьи на arxiv: *A 100 dimensional uniform distribution Z is projected to ...* но что это за вектор - не говорится",
"каждый вектор мапится на уникальную картинку? то есть, он используется как идентификатор, и задача модели научиться находить сжатое(кодированное) представление картинки, а так же декодировать в реальное представление?",
"И конечно же векторы не мапятся на уникальную картинку - вы подаете на вход что то случайное, на выходе тоже получаете какую то случайную картинку, которой могло и не быть в тренировочной выборке",
"Идея этих сетей как раз не в том чтобы выучить представление конкретных картинок в виде векторов меньшей размерности, а в том чтобы научиться семплировать случайные картинки похожие на то что было в тренировочной выборке",
"как я понял, в процессе длительного обучения, модель учится понимать входной вектор именно как кодирование характеристик",
"с третьей стороны – информации о том, что _можно_ прочитать, и так уже тонны. вот бы кто ее фильтровал и самое лучшее схоронял в списочек :disappointed:",
Мне murphy приехал. Чот больше всех нравится пока,
этот пример не показывает зачем дл для этой задачи,
"<@U06J1LG1M> вот по тому и удивительно, что в DCGAN задача кодирования не ставилась. на вход давался шум. но, чтобы для каждого вектора на выходе получалась реалистичная картинка, алгоритму пришлось научиться интерпретировать этот вектор как низкоразмерное представление. по этому, как приятный бонус получили кодирование и возможность векторной арифметики, как в word2vec. вот здесь Ian Goodfellow об этом говорит: <https://youtu.be/HN9NRhm9waY?t=7m45s>",
"я думал, что подача шума на вход будет вредить процессу обучения. а вышло вон как :smile:",
"<@U0ZHHV83C> ну имхо ты ставишь вопрос из серии “а почему бы и нет”? ну проблемы нет, и действительно почему бы и нет

у тебя вроде есть какая то вероятностная модель, есть так же какие то априорные представления об f, а дл обычно юзается когда ничего нет",
"А ещё DL работает там, где у нас есть основания полагать, что данные как-то особенно лежат на маломерных многообразиях, вложенных в очень многомерные пространства",
"Считаю, надо заканчивать дискуссию, когда аргументы докатились до универсального аппроксиматора",
"<@U06J1LG1M> как раз наоборот: эта теорема (в теории) тут не работает, но на практике работает.",
"Есть свёрточная сетка и датасет с картинками на классификацию; в датасете есть ошибочные лейблы и какие-то левые сэмплы; как понять, что какая-то картинка не улучшает, а ухудшает модель?",
"Шаг номер один - натренировать и посмотреть, где модель наиболее явно ошибается",
"господа, а кто на нипсе был и посетил бенджиовский доклад про “Towards biologically	plausible	deep learning”, поделитесь впечатлениями? это жидоканадские фантазии или стоит потратить время на вникание? или и то и другое одновременно?",
"я понимаю, просто любопытно какая доля прироста получилась изза чистки трейна, а какая изза чистки валидации",
"коллеги, скажи, а как репортить результаты байесовского анализа?
например, я сделал линейную регрессию в stan, mcmc, несколько цепей и так далее
на выходе вроде как имею зафиченные значения intercept и коэффициента при x (еси грубо, то так: y ~ intercept + b_coef  * x)
собственно, непонятно, как это дело репортить - нужен аналог p-value или еще какие правила инференса
куда смотреть?",
"боюсь, редакторов журналов это не очень устроит
что-то было про bayes factor, но я плохо представляю, что это, и как его вытащить из выхлопа stan %(",
"упаси бог от гельмана, он злобный тип, а я слишком нуб для него
я общался с ними, да, хорошие ребята

просто мне казалось, что уж этот-то вопрос должен быть уже проработан и очевиден для тех, кто давно работает",
"Господа и коллеги, если чат выдаёт в ответе почти все слова с повторами по 3-4 слова, то это означает какую то определённую ошибку?",
"Конечно, интервалы везде возьмут. Аналог p-value только Bayes factor, это, грубо, отношение likelihood гипотезы к null. Null - это в твоем случае если нулевой коэффициент, надо зафитить только intercept, как среднее, если у тебя нормальное распределение",
"Уже хуйти когда писали, что 1080ti будет",
ну pvalue на какую гипотезу то?,
а какой смысл оценивать без привязки к модели?,
<@U0BLCTAJK> чисто интуитивно кажется что совершенно непонятно как считать что-то вроде mut inf т.к. в таком случае сохранение информации не всегда профитно (когда выкидываешь мусор),
"Не знаю, в каком канале лучше спросить. А кто-нибудь участвует в каких-нибудь Citizen Science проектах?",
"Чота не находит аттачи, только сообщения - в этом нет смысла",
"могу, а смысл какой от этого практический?",
кто тут спец по временным рядам? <@U0U2ENJ4U> <@U0FEJNBGQ>,
"что за ряд, какие фичи?",
"<@U1CF95PP1> какого рода ""мякоть"" имеется ввиду?",
"Какие фичи, сколько точек в ряду, какой временной интервал :slightly_smiling_face:",
тут уже много моделей и советов перебрали и многое из них вполне может работать. Как готовятся данные для модели? подозреваю там что-то совсем базовое забыто,
"&gt; подавались сами ряды-фичи
что в матрице фичей по строкам и колонкам?
Как в стационарный вид привели, как нормировали? Есть ли даты, есть ли сезонность?",
мне не известно каким образом,
"нет, это по моей работе, могу спросить как ряды делались, но придется ждать до след недели",
"хорошо бы почитать про то, как предсказывать один ряд, имея только даты и этот ряд.",
"могу представить себе, как это сделать через bash/sort -m/truncate, но места таки 100Гб понадобится на диске",
"да, он streaming, но на диске нет места для записи отсортированного файла, не стирая оригинала, как я понял",
"А подскажите тоже про ряды. Я вот хочу поисследовать паттерны дневной активности человеков в соцсетках в городе. В готовом продуктике должна быть возможность описать как-то паттерн и показать на карте какие места наиболее ему соответствуют. Пока нужно какие-то типовые устойчивые паттерны найти, посмотреть как они вообще могут выглядеть. Планирую аггрегировать активность по квадратам, строить нормированные гистограммы активности, где одному бину отвечает один час дня, кластеризовать эти гистограммы по какой-то метрике (симметричный kl, например). Звучит разумно? Может у кого-то есть фантазии на тему того, как такое можно было бы сделать?",
"например, есть человек и его история хождения в инете. я хочу агрегировать инфу про него. например, сколько раз он заходил на какой-то сайт, какое время самое популярное для него и т.д. то есть из условных 1000 строк, я хочу сделать одну для каждого юзера",
"у меня задача немного о другом - вот есть перекресток где народ бегает сильно в 10 утра и в 12 вечера,а в 16 дня провал сильный, например",
я просто не чувствую интуитивно какую метрику брать для сильно коротких рядов,
"Важно только для pretrained, сделать также, как было во время тренировки",
"Средняя картинка - это по всем каналам, да? Это получается ""попиксельно""? А ""поканально"" - это когда среднее считается по каждому каналу отдельно?",
"А какие самые простые и эффективные способы аугментации данных? Покрутить на 90 градусов несколько раз, зеркально отразить? Еще есть какие-нибудь простые? ",
В задачке про Ultrasound Segmentation хитро массировали картинку чтобы было похоже как оно может быть в человечеком теле,
"вопрос (скорее технический по pymc3, но может я чего-то более глубоко не понимаю) -- зачем он там перед тем как лейблы предсказывать, делает `ann_output.set_value(Y_test)` ?",
У нас возник спор. В статье Blundell2015 описан трюк с перевзвешиванием KL во время обучения. Но про autoencoding variational bayes там естессно ничего. Как взвешивать в случае aevb? Ссылка на обсуждение <https://github.com/pymc-devs/pymc3/pull/1600#pullrequestreview-13474980>,
"<@U1QN13664>, в том случае это нужно было только для accuracy. `sample_pcc` только постериор(trace) имеет, когда делает предикты ",
"<@U04422XJL> спасибо, можешь добавить немного подробностей? какие параметры нужно передать , чтобы построить FM второго порядка?",
"<@U1LNBRZ29>: у Blundell'а две статьи за 2015-й, а из PR мне не очевидно, какую из них ты имеешь в виду",
"я тут немного объединил базы фоток с эмоциями  SFEW + emotionet + KDEF + ar_faces + osu_feed получился примерно такой набор:

happy 24780
angry 21752
neutral 14036
disgusted 7883
sad 4955
surprised 4752

щас запустил детектор лиц и кроп, ссылку наверное сюда кидать не буду, таки все базы не публичные (там какие то нелепые правила по использованию), но если напишете мне на почту, то завтра утром пришлю ссылку на гуглдрайв",
Как оказывается много мелочей воспринимается как данность с современными железками,
"Первые впечатления пока отличные, всё шустро, тихо и холодно. Самая громкая часть -- кулеры на CPU, если кто будет ставить в спальне или просто любит тишину, то вентиляторы стоит поменять, родные какие-то трещотки у CoolerMaster Hyper EVO 212",
насчет FM - а <https://github.com/scikit-learn-contrib/polylearn> кто-нибудь пробовал? как по сравнению с конкурентами?,
"там правда про подводные камни, но есть ссылка на лекцию Karpathy как раз про бэкпроп с подробностями",
"в почти всех статьях в интернетике просто рассматривается слой, состоящий одновременно из полносвязного и активации, а по отдельности нихрена не расписано, а меня интересует, как обрабатывать ситуации Dense-Dense-Activation или Dense-Activation-Activation",
"мне больше интересно, где сейчас используется Dense-Dense-Activation и Dense-Activation-Activation",
<@U0JHK9001> а почему тогда не сделать это с нелинейностью?,
"Ребят, я ж не собираюсь такое на практике применять, я изучаю как нейронки обучаются просто, поэтому и смотрю такие упоротые случаи и как они должны пережевываться.",
"Товарищи, подскажите, как пользоваться википедией, пожалуйста. А вернее, её дампом. Почти наверняка где-то есть подробная иструкция, как устроен дамп вики, но я не нашел. Если у вас есть -- поделитесь, пожалуйста.
Есть 2.5 вопроса.
Первые 1.5 вопроса: как лучше всего искать статьи крупными кусками, например ""история"", ""естественные науки"" и как лучше всего сопоставлять статьи на разных языках(одна и та же статья на русском и английском)?
Например в этой(<https://en.wikipedia.org/wiki/Protein>) статье внизу есть несколько плашек с различными классификациями и тегами предмета статьи. Как это называется, и отражено ли в дампах?",
"короче мысли независимыми объектами, я что-то не припомню где важно знать что будет перед или после",
"А есть какие-то сложившиеся тулчейны для питона? Я нашел pywiki набор, но он как будто для работы с вики, а не ее обработки. Еще нашел медиа вики парсер фром хелл. Он мне похоже будет сильно нужен? ",
"К прошлому -- после прошивок всего и настройки вентиляторы стали гораздо тише (в процессе правда послушал как они умеют на полных оборотах -- турбина, жуть). 
Из розетки в простое берёт смешные 90Ватт :slightly_smiling_face: При полной нагрузке: ~550Вт.",
"а как размер батча выбирать? чем больше, тем лучше - главное чтобы в память помещалось?",
"<http://russianaicup.ru/profile/Parilo2>
загрузил своего RL агента в песочницу RussianAICup, можно посмотреть как играет",
"кто был на моднейшем нипсе, расскажите про tensorforest, помню постер такой был ",
"Кто был на нипсе, там такая статья была <http://approximateinference.org/accepted/RoederEtAl2016.pdf> 
Как заставить theano это проглотить? Я в ступоре. Вопрос в модифицировании градиента.",
"А когда этих деревьев в форесте 1000, то становится совсем все просто!",
"Спасибо, <@U2194SMBM> 
Да, про генсим я помню.
Я смотрел тот код где-то год назад, мне тогда показалось, что он чисто ознакомления с прочей функциональностью генсима. Поэтому генсим я оставил на случай если ничего специального не найду.

Я такую табличку не находил, да, она интересная. И да, вопросы индесации стоят остро, когда работать приходится с дампом. 

Пока что я, доверившись <@U0DA4J82H>, расковырял их апи. Там вроде есть и поиск(еще не до конца разобрался), и межязыковые связи, и тексты(которые еще надо пропарсить от разметки) и категории(которые нужно от меты почистить).
Сейчас пример покажу.",
"Мне фактически осталось разобраться с двумя вещами: 
*1.* Не нарушу ли я этики использования их ботоапи, если вытащу несколько десятков тысяч страниц
*2.* Как работает их поиск, потому что попытка искать только в названиях порождает странный эксепшн",
"у меня как у тёзки, плюс я храню время апдейта отдельным полем (если результат для входа может меняться) - удобно потом выбирать примеры, которые были валидны на конкретную дату, чтобы воспроизвести модель на случай полного крэша всех бэкапов.",
"Плавно перенесу судя разговор про LDA из <#C044B7CSQ|lang_python> и задам вопрос: а какие эвристики существуют для выбора количества топиков? У нас есть лосс у модели, но как-то я не уверен что для этого он лучший показатель. Есть так же возможность проинтерпретировать топики как кластеры (по наибольшей вероятности отнесения) и посчитать какой-нибудь silhouette, но кажется при этом мы совершенно не учитываем кучу информации о самих словах в топиках, насколько они разные и т.д.
Замутил ещё метрику: отношение среднего косинусных расстояний w2v представлений топ слов внутри топика и между топиками — но пока она не дает мне четкой картины о том, сколько же топиков лучше выбирать.",
"в постгресе есть тип данных hstore для хранения key:value, как бы встроенный nosql; в нем и храню фичи",
"<@U1D4RRA7K>, можно еще когерентность считать... По количеству тем -- если есть какой-то лосс(видимо лда куда-то как фича потом едет), то его оптимизировать -- вполне хорошая идея.",
<@U26KV8W3D> а там какая из иерархических?,
"Я не знаю, к сожалению. Просто недавно у меня поломался стенд на работе с моделями из-за обновления ARTM, и я заметил, что там новая модель. Поискал статей новых по этой теме -- не нашел. По документации и коду о содержании догадываться было лень, и я отложил.
Там и до этого был способ сделать псевдоиерархию: там был регуляризатор, который прибивает информационно слабые темы. Можно было постепенно выкручивать его альфу и смотреть, какие темы в какие склеиваются. Есть подозрение, что эту схему просто по уму реализовали на уровне библиотеки.",
"А поделитесь опытом, есть какой-то более удобный способ какого-то централизованного хранения данных для маленьких команд чем тупой фтп? Даты мало достаточно, ну гигов 500 например пока, но она вся в каком-то совершенно хаотичном виде хранится в папках на сервере. Данные в основном текстовые разных сортов (цсвшки, жсоны), добрая половина болтается где-то в монге. Человек в команде в дату смотрит мало, разжиреть на порядок данные вроде не планируют. Как проще всего такое организовать?",
"Какой смысл хранить модель в редисе, вроде проще на диске?",
"я не понял, по какой размерности ты собираешься время разворачивать?",
можно придумать эти матрицы параметризовать как нить хитро наверно,
"А ещё не понятно, почему LSTM тебе даст более гибкое распределение",
"Говорят в BigARTM как то темы можно отбирать, настраивая регуляризацию",
"И у него, как и у всех регуляризаторов в BigARTM есть коэффициент влияния `tau`. Если его постепенно затягивать, то темы постепенно будут уходить лишние. Например, можно поставить 10000 тем, и добавить такой регуляризатор. В простых кейсах у меня +- работало.",
"да пусть просто нормальное, зачем над выводом париться)",
это же можно считать как initial space который потом преобразуется,
"Да prior может быть каким угодно, всё равно на выходе из RNN'ки будет какое-то месиво",
"Можно накрутить иерархию из кучи гиперпараметров, когда со временем гиперпараметры перестанут оказывать какое-либо влияние :simple_smile:",
"&gt;Deep Learning for 3D, Inverse Graphics
У меня есть пару догадок о том, кто может про такое рассказать",
"ждешь пока варвывод сойдется
@
понимаешь что KL распределений не симметричен по параметрам
@
натуральный градиент, изи сделать
@
представляешь как публикуешься на нипсе
@
находишь кучу статей за 2010-11 годы
@
долго плачешь о потраченной молодости",
"Всем привет. Нужно визуализировать Decision Tree которая показывает значимость независимой фичей Х от на зависимую переменную У. Когда у меня У - numerical, то с этим как то просто, но что делать когда у тебя это классы на выходе? Я тренирую через Gradient Boosting Classifier, получается он предсказывает только 1 из всех классов, что не совсем показывает всб картину. *Вопрос*: какой алгоритм вы используете для Feature Selection , чтобы можно было визуально показать границы/условия каждой фичи?",
"<@U3GKWDBFU> Олег, привет! Во время своего недавнего выступления на DS Meetup ты рассказывал, что вы производили оптимизацию решений по размеру (freezing и квантование весов). Если с первой техникой всё более-менее ясно, то по поводу второй есть вопросы. Знаешь ли ты и мог ли бы подсказать какие-н содержательные статьи, посты, etc на тему того, как эта процедура реализуется и какие есть трюки? Вводили ли вы какие-то изменения в процесс обучения, или просто пост-фактум квантовали веса и смотрели на результат?",
для тех неудачников кто не был там,
"<@U2194SMBM> не очень понял, а почему Жаккар растет с ростом числа топиков — ведь интуитивно кажется что топики должны становится менее похожими друг на друга (более узкие тематики) ну и соотвественно близость должна уменьшаться",
это типа как у Николенко <http://logic.pdmi.ras.ru/~sergey/papers/N16_SIGIR.pdf>?,
это как один LDA оценивать по другому LDA,
"но вообще как раз она мне интуитивно видится наиболее интерпретируемой: берутся же тоже не все слова в топике, а топ — и дальше мы оцениваем насколько узкотематическим получился кластер и насколько он непохож на остальные, притом при этом имеем возможность честно сравнивать слова",
"то, что w2v нам позволяет понимать, сильно зависит от того как мы его настроили, а также насколько частотным было слово в корпусе, на основе которого строился w2v",
"Вопрос: а есть какой-нить простой и не геморройный сервис, кроме амазона, где можно какие-нить cnn быстренько и недорого посчитать ?",
"Как правильно сделать batch variational  bayes? Т.е. у меня здорова выборка, я каждый раз беру из нее с возвращением случайный батч размера N. Т.е. итерация всех параметров проходит по одному батчу, а потом опять новый. Но ELBO получается не монотонно возрастающим. Что конечно понятно, но как-то грустно",
"Привет! А есть у кого-нибудь датасеты с позами людей, где записано видео?
Примерно такие: <http://vision.imar.ro/human3.6m/description.php>",
и как долго обычно надо ждать? а то у меня лосс как-то медленно совсем уменьшается,
"мало информации. ""медленно совсем"" это как количественно? какой learning rate? какая сеть?",
ну тогда поговорим о медленной сходимости когда 277000 итераций пройдёт :smiley:,
"а как тогда вообще тюнить параметры, если надо 30к итераций ждать?",
"как у тебя выглядит переход от свёрточных слоёв к полносвязным? какой выход последнего свёрточного слоя, какого размера первый полносвязный, есть ли между ними pooling?",
Кто разбирается в градиентном бустинге?,
Почему не подгонять его просто под разность предсказания и лейбла?,
"А каким должен быть переход от сверточных слоев к полносвязным? Я взял vgg и убрал последние 2 сверточных слоя, а в полносвязных слоях уменьшил размерность в 10 раз",
"вообще, если брать продвинутый бустинг, который в xgboost, то там по остаточному значению только направление вычисляется (собственно, как в обычном градиентном спуске)
сами значения уже в терминальных нодах по другому считаются",
"поэтому надо учитывать, как балансировать ошибку",
"Ровно потому же почему при обучении классификации используют logloss, например, а не MSE :slightly_smiling_face:",
"Ну и когда у тебя pairwise ошибка, то непонятно какую ты разницу будешь приближать.",
в xgboost для бинарной классфикации как раз разница :slightly_smiling_face:,
<@U0DA4J82H> зачем ты вообще полез менять архитектуру? ,
Там почти сразу видно куда идет текущее обучение,
Почему? Ты как раз можешь и с этим экспериментировать,
"а как можно мерить качество полученных тем в topic modelling, кроме как глазками? Например, никто не пробовал использовать словари синонимов для этой задачи?",
"Привет всем
как удобней всего изобразить архитектуру сети?
под .md или latex
или библиотека какая на питон есть?",
"Всем привет, я тут разбираюсь с TD(lambda) и что-то недопонимаю как там веса обновляются. Можете посоветовать где подробный псевдокод или простой пример кода на питоне?",
а о каких весах речь идёт?,
Речь идет в первую очередь про то как там в backward view поддерживаются-используются веса eligibility traces,
"кстати псевдокод по твоей ссылке и во втором издании отличается как раз тем, что во втором издании eligibility traces для каждого эпизода переинициализируются",
но написал как считал верным — с инициализацией в эпизоде,
"я в общем-то решаю ту же самую задачку, только ту часть где надо оценить value-function для случайной стратегии",
<@U25K076RH> Из какой статьи картинка?,
"Именно, это первая причины почему Danny Tarlow (<http://www.cs.toronto.edu/~dtarlow/>) крутой , вторая причина его работа по интеграции program induction и вероятностных сеток. Его на московские конфы надо за любые деньги привезти.",
"<@U0QPYUM5M> Вот выше windj007 статью Николенко кидал — там как раз про это. Впрочем, как и сама беседа выше",
Мы с Мишей Фигурновым как раз обсуждали что супер глубокие сетки - раскрученные RNNки . Шмидгрубер тоже созрел <https://arxiv.org/abs/1612.07771>,
"Тут не про это совсем. А про свойства ""фич"" что они не различны на разных уровнях, а то что они действуют как интерактивные уточнения оценки",
"ребята, а кто как использует сжатые векторные представления слов (word2vec, например) вместе с вручную заинжиниренными признаками (которые разреженные чаще всего)?",
походу я знаю кто это был)),
"<@U2194SMBM> я w2v как первый слой CNN использовал и ручные признаки  потом добавлял к dense слою после всех свёрток. в принципе, работало, но PCA выглядит обоснованее и там есть график :slightly_smiling_face:",
а как в таком случае с регуляризацией?,
"Я для текста использовал w2v как эмбеддинг для лстм и для cnn. А спарс фичи, типа тфидф хорошо классифицируются баесом.",
"Я читаю вместе с курсом Воронцова в качестве подготовки к ODS Course)
Считаю, что именно такой должны быть вводная книга по ML, а не когда начинают писать функции-обёртки, чтобы сделать model.fit()",
<@U1ZAJ4FC4> а какая связь то,
И куда ты попадёшь при LogLoss?)),
ну тип градиент описывает куда катиться,
и зачем сюда логлосс приплели?,
тогда можно еще раз почему тот способ может НЕ сработать,
"То есть сделаешь как раз то, что хочешь сделать",
"Я знаю, это контринтуитивно, но не во всех функционалах потерь оптимум достигается в том случае, когда предсказание равно истинному лейблу",
"<@U1ZAJ4FC4> <https://opendatascience.slack.com/files/anotherbugmaster/F3HHWJE0G/pasted_image_at_2016_12_23_02_34_pm.png> - это сам logloss или какая-то функция от него, или другое определение? И для какой задачи этот loss работает? Я думал что для бинарной классификации это `-(yt log(yp) + (1 - yt) log(1 - yp))`",
только напомните плиз зачем вторую производную,
почему вот в хгбусте для mse дефолтная эвал метрика - rmse?,
"Правильно ли я понял, что лучшего способа обучаться кроме как взятие градиента по аргмаксимуму для дискретных величин нету(не предлагают)? Это про статью с гумбель триком.",
"это где вы учитесь, что вам такие вопросы задают? Не сами же одновременно решили разобраться :trollface:",
"Кто нибудь знает где можно найти тексты что то типа о бизнесе или бизнес переписка. На аглийском и французском. Нужно сделать классификатор емейлов на бизнесс/не бизнесс. Попробовал разметить письма руками, но очень много похожих текстов и мало уникальных людей кто их писал, поэтому жуткое переобучение, хочу попробовать tf-idf на каком нибудь корпусе, только вот не знаю где его взять.",
"ну и всё остальное, как противовес.",
"Поищи гайды о том, как вести деловую переписку и обучи тфидф на этих гайдах",
"Всем привет. Кто-нибудь знает как зафиксировать состояние генератора псевдослучайных чисел (random seed) в  sklearn.model_selection.GridSearchCV? Я только понял что в самих классификаторах можно зафиксировать, но ка к сделать так чтобы он stratified KFold всегда одинаково делал? (я сейчас просто ставлю cv = 10,в документации говорится что если там целое число, то используется stratified KFold, но как его зафиксировать?) Будет ли толк если я пропишу просто в той-же ячейке numpy.random.seed(1)  ? И если да, то где прописывать, в ячейке(использую Jupiter notebook), в которой инициализирую GridSearchCV, или в той где его обучаю?",
"пробовал, так и не понял как это правильно сделать, 

делаю так:

optimizer_1 = GridSearchCV(estimator_1,param_grid,cv=StratifiedKFold(random_state=1),scoring='average_precision')

выдает 

TypeError: 'StratifiedKFold' object is not iterable

optimizer_1 = GridSearchCV(estimator_1,param_grid,cv=StratifiedKFold(random_state=1).split(X_sc,y_t),scoring='average_precision')

выдает

TypeError: object of type 'generator' has no len()",
"Ну и так как value function распространяются, то ошибки тоже растекались",
"утро вечера мудренее :slightly_smiling_face: кстати, как я понял могут быть разные стратегии обновления eligibility traces для повторных посещений: инкремениторовать (как в псевдокоде) или сбрасывать в 1",
"это в целом соответствует монте карло разновидностям где тоже можно либо учитывать один раз, либо каждый",
"Это я верно понял, или просто эту деталь опускают как очевидную?",
"по-моему это зависит от того как MDP задан. Терминальное состояние можно рассматривать как ""бесконечный"" цикл без награды, а награду давать за переход в терминальное состояние",
"Там вроде самое важное, что V(s) у терминального состояния по определению равен 0. Поэтому если награды отрицательные, то агент будет стремиться как можно быстрее  дойти до терминального состояния, а если положительные - то как можно дольше его избегать. Ну и поэтому, в частности, значение терминального состояния не обновляется.",
"Я еще хотел спросить мнение опытных людей. У меня есть полуучебный проектик. Я хотел сделать бота для абстрактной настольный игры, и по ходу дела разобраться с RL подходами хотя бы на базовом уровне. Желательно чтобы итоговые нароботки были обобщаемы и на другие абстрактные настольные игры.

У меня есть текущее понимание-план как к этому подойти:
1. Смотреть лекции Сильвера и по ходу делать задачки предлагаемые в курсе дипхака (в процессе)
2. Обучать бота начиная со случайной стратегии давая ему играть против самого себя и используя какой-нибудь из более-менее продвинутых методов предлагаемых в лекциях.

Вопросы:
Может есть что-то существенное что полезно добавить в этот план?
Реально ли подходить к задачке с многоядерным CPU или без GPU продвинутые подходы будут неподъемными?",
"на мой взгляд с нейросетками но без GPU это будет оочень долго. У меня есть задачка с полетом 2хмерного дрона, с на 8 CPU надо ждать ночь, а то и больше, а на GPU час, когда между попытками час это еще более менее. Ну и еще бы неплохо раздобыить датасет для супервайзед лернинга, для хорошего старта",
"а какой самый простейший и быстрый бенчмарк есть? имею в виду ""датасет""",
"<@U32J3CRP1> датасета нет, так как в игру играло дай бог 10 человек и никакой базы игр нет",
"<@U0JJ69UB1> это смотря сколько и какие слои, игра типа понг, да наверное можно взять небольшую сеть",
"я вот еще что думаю, когда учишь RL агента, то есть особенность такая, что надо много раз делать forward pass с батчем равным 1, чтоб действие выбирать по текущему состоянию. И иногда бывает проще делать просто все на cpu. Либо проходить эпизоды и набирать выборку на cpu, а потом обучаться на gpu батчами, если алгоритм позволяет так обучаться",
а какое они отношение к rl имеют? :eyes:,
"так что тут не так уж очевидно какой бенчмарк будет лучшим, я вот почти уверен, что тот же cart-pole будет быстрее на cpu, так как там состояние это вектор размерности 4 и сети обычно для него строят маленькие",
"rushter просто протестить насколько быстро на твоем железе работает сетка, примерно можно прикинуть как будет потом в RL",
и даже можно посмотреть у кого как получилось и как они это делали,
"Там довольно мало дискретных сред, к сожалению. Как правильно заметили выше, там в основном непрерывные задачки. Из бордов там frozen lake и пара задач рядом + три задачи с настольными играми, как раз го вроде, в полном и урезанном варианте.",
"Я тоже про afterstate ничего не знаю :-) кроме общей идеи о том, что если два пути приводят к одному и тому же конечному положению, то это надо учитывать для оптимизации policy. Но как это делать корректно хз.",
"Если хочется просто потренироваться, то можно попробовать взять какую нибудь борду, у которой более менее известно хорошее решение и тренировать свои модели против нее.

Я в свое время смотрел, но мне были интересны игры в которых присутствует случайность. Известна вроде эвристика для Yahtzee и can't stop.",
"<@U34Q3KU8H> а можно по-подробнее, как это использовать и что за задача была?",
"Надо было поднять CTR у email'ов.

поэтому задача сводится к binary classification.

Каждому email template сопоставляется некий score, ну и потом надо выбрать какой-то email используя это score distribution.

Все бы хорошо, но специально обученная девушка создает новые email templates, может не каждый день, но довольно часто. 

Чтобы избежать ситуации когда новые template'ы не посылаются потому что нет достаточного количества исторических данных про успешность этого нового template'a сделан postprocessing который ногами растет из задачи оптимизации про bandit algorithms.

Идея в том, что хочется посылать либо email'ы про которые мы знаем что они хорошие (exploitation), либо те, про которые мы мало знаем (exploration).

Чтобы",
"где p - предсказанная вероятность, N - сколько раз послали",
подскажите где достать речевые датасеты чтоб были указаны спикеры (для speaker identification),
"Народ, кто что думает про соотношение важности точности и полноты в задаче прогнозирования оттока? Допустим мы используем F-меру как дополнительную метрику, но какое значение β выбрать?",
"Это зависит в первую очередь что важней для бизнеса точность или полнота, чтобы понять в какую сторону двигать. ",
Я в смысле использовать как бейзлайн,
Может быть можно нагенерировать по аудиодорожкам к фильмам с субтитрами (там вроде часто указывается кто говорит),
а то как то смущает что тема одна и количество картинок почти одинаковое,
"Всем привет, занимаюсь вопросом выбора ноута, остановился на hp envy, вариант на процессоре amd a9 стоит на 20 к рублей дешевле варианта на i5, какие минусы у амд а9 по сравнению с i5 может кто подсказать , очень поможет определиться с выбором!",
"Пацаны! А у ни у кого не осталось дампа Моего круга, когда он ещё был не просто типа :hhru:?",
"Кто мне тут расскажет, как ML под показ рекламы оптимизируют?

Вот то, что мне пришло в голову после прогулки по пляжу:

[1] модель, которая предсказывает вероятность клика на рекламу P(click)
[2] модель которая предсказывает купит пользователь зеленый холодильник после того как кликнул на рекламу зеленого холодильника. P(покупка | click)
[3] Откалибровать произведение P(покупка | click) P(click)
[4] домножить на цену зеленого холодильника,

То есть реклама ранжируется по P(покупка | click) P (click) $цена

Так вот вопрос, как оно по факту делается в Яндексe, Avito и прочих площадках?",
посмотрите первую ссылку. там как раз похоже на вашу задачу.,
<@U34Q3KU8H> я не знаю как в яндексе но с моей колокольни (мы относительно крупная контора в этой области в штатах) примерно так и есть,
"конечно есть нюансы (тм), например P(покупка | click) напрямую выучить тяжело так как данных мало, и цену холодильника ты можешь не знать",
"<@U1QN13664> <@U34Q3KU8H> у кого как конечно, но в принципе более менее полный спектр задач и статей по рекламе можно найти здесь <https://github.com/wnzhang/rtb-papers> . Но опять же стоит помнить что там никто ничего из продакшена не раскрывает, только намеки.",
"хм, не знаю, как это померять, ибо частота там динамическая",
"Самую дешевую, какая по цвету подходит",
вот я на это натолкнулся и сюда решил написать. потому что убегаю от компа. а может когда вернусь кто-нибудь копи-паст решением поделится))) верю в команду! :hugging_face:,
"не референсный это типа как по ссылке выше, от msi например?",
"Может это только у меня матлаб отторжение вызывает, но
Матлаб - это несовременный язык, в котором нет модульности и все функции попадают в один namespace, не поддерживаются перегрузки функций для базовых типов данных (array, cell array), да ещё и денег бешеных стоит.
В питоне `import matplotlib` и строй любые графики графики, q-функцию, например. Визуализация того, как ездит машинка тоже есть (`env.render()`), но это не особо поможет в понимании алгоритмов.",
"Референсный как раз для того, чтобы ставить пачку карт. И они забирают воздух из передней части корпуса и выплевывают горячий через заднею часть.",
А какие сейчас есть хорошие способы заэмбедить короткий текст? (Среднее по word2vec не предлагать и даже с ft-idf весами ),
"а для чего хочется поменьше размерность? t-sne как мне кажется не очень хорошо делать, т.к. он локально сохраняет расстояния, потом замучаешься вывод его классифицировать",
А если например натравить lstm и взять её скрытое состояние как эмбединг?,
"есть статьи где выход отдельно пред-обученной lstm оказывается полезен, например <https://research.google.com/pubs/pub45729.html>",
"Ну я вижу два варианта, можно сначала забить на остальные фичи и обучить классификатор, который будет градиенты протаскивать через слой с регрессией в матрицу, где хранятся эмбеддинги. Это попроще, есть готовое решение и не одно. Или упороться и сделать сетку с двумя входами, на одном ~пики точеные~ эмбеддинги, на другом допфичи, но это безумие, на самом деле.",
"Embedding layer - штука, которая каждому токену сопоставляет номер и ищет соответствующую строку в матрице, используя ее как представление слова. Разница в том, что изначально все эти вектора рандомные и учатся специально для твоей классификации, а не для предсказания слова в середине окна как в w2v. А архитектура та же самая.",
"<@U053R9RS6>   SARSA табличная или action-value функцию с помощью нейросети  апроксимируется? если табличная или с помощью линейной функции, то по идее не должно такого быть, тем более в +inf. Если с помощью нейросети, то впринципе может все что угодно произойти, так как никаких теоретических гарантий нет, но все равно странно. 
У меня бывало, что action-value функция сильно возрастала, могла и до бесконечности дойти, но там была такая среда, что она могла длиться постоянно и постоянно получать положительную награду, когда политика хорошая обучится, а гамму я по ошибке сделал равной 1. Но я так понимаю с frozen lake это не тот случай.",
"MSI, как на фотке выше отличная в плане шума",
но при этом я не вижу логическо возможность как функция получается выше чем 1,
"Да, нет причины почему eligibility trace не будет 1 если мы на пример засели в петле 2 одном состоянии на некоторое время",
"Проверьте, пожалуйста логику:
В какой-то момент мы будем можем получить дельта отрицательную за счет перехода из состояния с большим value в меньшее. Таким образом появится отрицательное value (по модулю меньше 1)
В какой-то момент мы может перейти с отрицательного по модулю меньше 1 на положительное по модулю меньше 1 и в этот момент полученная дельта сможет по модулю быть больше 1.
Выходит, что никакой гарантии нет когда gamma, lambda  и alpha достаточно большие",
"Господа, есть вопрос. В некоторых статьях, где предсказывают шанс выигрыша команды А у команды Б в каком-нибудь спорте используют difference-признаки: статистика/признак команды А - статистика/признак команды Б (к примеру, разница в среднем весе/росте). Вопрос следующий: разве использование подобных признаков не делает модель ""асимметричной"", т.к. разность не коммутирует: предсказания для A vs B будут отличаться от B vs A? Вообще, корректно ли использовать подобного рода признаки и какие подводные камни могут под этим крыться?

Пример применения difference-фичей есть вот тут <https://arxiv.org/pdf/1511.05837.pdf>",
"Нет, наличие примеров роли не играет. Линейная модель может этот признак взять как с положительным весом, так и отрицательным ",
"господа, вопрос по теане и лазане, накидал я тут ГАН, а теана ругается на что-то, может у кого было похожее

все довольно таки банально:
generator_loss = T.mean(T.log(fake_prob))
generator_params = lasagne.layers.get_all_params(generator, trainable=True)
generator_updates = lasagne.updates.adam(
    generator_loss,
    generator_params,
    learning_rate=0.0002, beta1=0.5, beta2=0.999)

ну так вот, если вызвать это, то все будет ок
generator_loss.eval({x_noise_sym: noise})
&gt; array(-7.383454537326672e-21, dtype=float32)

далее создаем функцию для тренировки
f_train_generator = theano.function(
        inputs=[x_noise_sym],
        outputs=[generator_loss],
        updates=generator_updates)

и вот это валится 
f_train_generator(noise)
что то там про dimension mis-match, но странно после работающего generator_loss.eval({x_noise_sym: noise})

если же удалить из функции обновления
f_train_generator = theano.function(
        inputs=[x_noise_sym],
        outputs=[generator_loss])

то внезапно это работает 
f_train_generator(noise)
&gt; [array(-7.383454537326672e-21, dtype=float32)]

если заменить адам на что то другое, то ошибка все равно вылазит, получается я что то где то накосячил, но вот куда глянуть хз ваще",
"я такого никогда не прогнозировал, но почему бы не использовать сами признаки (вместо их разности)?",
"Какие либы вы юзаете с готовыми слоями под tf? До этого использовал Theano и там все понятно (есть Lasagne), а под tf есть и tflearn, и prettytensor, и много всего еще. Интересны только слои и чтобы было максимально неотдаленно от самого tf. Спасибо)",
Просто без deconv непонятно как GAN фигачить,
<@U0FEJNBGQ> а для чего fuel используете? и какие проблемы с ней?,
"<@U0QPVMAFR> как по мне они все похожи, рекомендую tflearn, он сейчас часть тф, и максимально оптимизирован (керасовская реализация будет в проде медленней)",
Спасибо. А к prettytensor как относитесь?,
"Какой готовый Dockerfile порекомендуете основанный на nvidia/cuda:8.0-cudnn5-devel c набором примерно tf, theano, keras, jupyter, python 2, что бы можно было быстро начать играться с примерами? 
Уже попробовал <https://github.com/saiprashanths/dl-docker> но у него python 2 kernel не запускается..",
"Если серьезно, хз. Мне сложно представить, что видюха потребляет 200 Вт и греется до 85. При том, что у нее есть куда отводить тепло.",
"хм, я просто хочу 1080 к 980 приткнуть, интересно какие подводные камни",
"а для генератора текста на марковских цепях кто чего на питоне бы посоветовал?
я вчера нашел <https://github.com/jsvine/markovify>
и выглядит норм и заработал сразу, но интересен ваш опыт :hammer_and_wrench:",
"<@U040M0W0S>  из одних и тех же текстов
`markovify` генерирует предложения короче и адекватнее
```но речь-то не об этом, не задавайте себе вопроса почему , не поняла вообще, кому куда они пропали нахуй че это там такое?

хорошая песня, но не будем делать.

да, вы правильно поняли, там связи я хочу умереть как будто я один спидов объебался.```
а `pymarkovchain` поток сознания адский выдаёт
```и тебе. -

бамп

мне 23, ему 23 еще только один случай, который был вынужден учиться готовить все, даже некоторые блюди ресторанной кухни по причине - дизайна, его материала называют здесь его есть. в 24 года, девственник, безработный, простатит с 18 лет подруги разбежались и совершеннолетие я не пытаюсь назвать их действия не поддаются..ощущение, что его чувства мало интересуют, но это сложно и если штамп ничего не выходит в первую же неделю знакомить свою девушку с родителями. думаю что и где найти эту леру```",
"*вечером, когда я не буду пьян",
как я узнаю когда ты перестанешь бухать?,
"<@U1BAKQH2M> Или марковская модель с вероятностями [[0.999, 0.001], [0.999, 0.001]], сам угадай, где какое состояние :trollface:",
"попробовала markovify на текстах естьестьесть, чот неинтересно, почти целые предложения конкатенирует",
как бы быстро раскидать точки по полигонам?,
а какие подводные камни ждут меня в предложенном мной подходе?,
"да какой низкий уровень, там 10 строк же",
<@U041P485A> а какие еще есть варианты кроме усреднения градиетнов от разных копий?,
"подумал что еще можно сделать распределенное обучение с помощью синтетических градиентов (<https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/>) но подход с простым усреднением выглядит сильно проще, кто что думает?",
Тут можно полистать как data parallelism в caffe впиливали <https://github.com/BVLC/caffe/pull/2114>,
<@U19VD9AQH> как впечатления от курса?,
"<@U049NHC4X> я посмотрел пока 2 лекции. Как intro курс хороший, тем кто только начинает в DL разбираться. Понравилось, что сразу можно на kaggle тренироваться.",
<@U043814R6> как раз ближе до коллоквиума ехать,
"ssh x forwarding, как вариант.
Но xrdp ставил, работает без особых проблем.",
"Эээ, когда для OpenCV, с трувээм говно какое-то)",
"Дропаут я пробовал на MLP сети, разницы не заметил. По моему опыту большое значение имеют почти все параметры, скорость обучения, reward функция, дискаунт фактор, сама формулировка состояния, размер батча, надо примерно понимать как именно должна обучаться сетка, то есть как она должна использовать состояние, желательно нормализовать состояние. Мой агент для RAIC это небольшая сетка Inx192x128x128x128x128xOut, везде relu, на последнем слое tanh, с чистым relu расходится в inf. Скорость обучения 1e-4..1e-5, ну и надо прилично подждать) до примерно 500К train операций. батчи по 128, можно меньше, но не меньше 32, по графику наград примерно можно делать выводы как меняется поведение, ну и смотреть что делает сетка. Может что-то еще забыл, но в общем, ощущение что все параметры важны. Размер реплай буфера, если он мал для задачи, тоже не будет сходиться нормально (граница примерно на 400К опыта), также хорошо говорят прикрутить приоретизацию опыта, я вот хочу попробовать, но еще не пробовал",
"reward не должна сильно превышал 1 для редких наград, для наград за каждое дейстиве должна быть намного меньше, но это быстро становится понятно, глядя график loss во время обучения, при больших наградах там начинают фигурироват сильно большие значения. Опять же по моему опыту лучше когда loss где-то в районе 0.1 … 0.001",
"<@U053R9RS6> тоже соглашусь с <@U3JJE4HC3>  если обучаются на играх атари, то все как правило берут архитектуру DQN.  Можно наверно порекомендовать просматривать архитектуры, которые использует знатоки в этой области из deepmind например и для примерно похожишь сред копировать архитектуры.",
"Прорекламирую также наш репозитарий, там <@U0K7Q625Q>  одно время собирал какие архитектуры сетей используются в RL в разных статьях. Правда сейчас уже вроде забил, но  информация мне кажется там неплохо собрана: 
<https://github.com/5vision/deep-reinforcement-learning-networks>",
"Я, как доделаю, прорекламирую свой репозиторий с A3C, там можно поглазеть и попробовать что-то для себя вынести :simple_smile: 
Только у меня не совсем asynchronous, у меня поддерживается несколько параллельных сред и агент в случайной из них шаг делает",
<@U0RV376MC>  а почему асинхронного не стал делать? например с distributed tensorflow?,
"<@U0JJ69UB1> Я в нём просто не разобрался ещё)
Но там код практически написан для асинхронного обучения, надо только понять, как до рабочего состояния довести. Я в distributed tensorflow покопаюсь, спасибо!",
чот я когда юзал казенные мейлрушные карты никогда не задумывался об этом ),
"ну вот как раз диапазон 74-78 щас, fan 67%, ну наверное ему виднее, я бы уже на 100 сделал хехе",
"только зачем, в комнате у тебя холоднее от этого не станет, будет только орать как вертолет",
а вообще имхо лучше не париться и гонять как есть,
"<@U0P95857C> хотелось бы немного больше смысла, хотя чего ожидать от гали.ру и двощей
```в этом возрасте держат в черном районе , куда люди едут со всего размаха бью в челюсть борьке и бегу вприпрыжку дальше с хорошим настроением 
```",
почему я не видел генеративной модели на свертках (дырявых например)?,
плюс вроде как machine translation in linear time тоже на свёртках была,
<@U06J1LG1M> а твой код для дцгана у тебя в гитхабе или где то есть?,
в общем когда буду делать хабра пост то сделаю гифку забавную ),
"<@U06J1LG1M> на частоту лучше смотри, чипсет начнет частоту скидывать, когда жарко станет, может не дойти до предельных значений температур. Поэтому лучше скорость вентилятора выставить на 90%, если не мешает, и если не хочется следить за появлением обрывов в графиках частот.",
<@U1BAKQH2M> зачем ты первым слоем такие дырки сделал? У тебя же задача рецептивное поле более сделать) ,
зачем тебе заоблачных размеров рецептивное поле? у тебя сэмплы какого размера?,
"я вот хотел спросить, а если пытаться, как пример, шахматы упихнуть в openai интерфейс",
Вопрос во многом именно как это сделать чтобы это было ок для алгоритма,
как в классических шахматах — откуда куда?,
а тут еще можно куда угодно пойти откуда угодно (чем угодно),
"И более того когда кодируешь координатами в этом есть большой смысл, как мне кажется",
"ну не куда угодно, не откуда угодно и не чем угодно же всё-таки",
А вот как в шахматах закодировать чтобы и смысл был и адекватное количество вариантов мне пока неясно,
а action space у тебя будет кодироваться просто двумя матрицами — куда и откуда,
"Логичным кажется возможность получить от среды список позиций, которые могут быть получены после всех твоих возможных ходов, и чтобы ты как действие выбирал ту, которая тебе больше нравится, и ход автоматически делался нужный",
"<@U065VP6F7> А как думаешь, в ход надо включать “тип"" фигуры?",
"у тебя же есть state с положением всех фигур, есть action space с “откуда, куда” и есть подмножество action space всех возможных по правилам ходов, то есть во-первых ограничение “откуда"" на координаты где есть твои фигуры и “куда” для каждого откуда по правилам хода твоих фигур",
"А почему action space будет большим? Оценка сверху 16*64, по факту же существенно меньше.
Или пара сотен -- это уже много?",
Надо посмотреть как в го реализовано,
"Моя цель попробовать использовать готовую библиотеку keras-rl для настольных игр
Как сформулировать окружение понятно. Я вот думаю теперь как заставить играть его против самого себя. Пока придумал итерационный вариант, фиксируя одну сторону как окружение, а вторую обучать.",
"Гайз, а тут По-моему неоднократно упоминали где можно скачать много художки разом. Которая типа классика всякая. Но не только отечественная. И язык тоже не принципиален. ",
"можно посмотреть как в alphago это делали, насколько я помню они примерно так и сделали. У них там получались лучше результаты , если для второй стороны они брали не текущую политику, а случайно выбранную политику, которая была несколько эпох назад. типа противники были более разнообразные для первой стороны.",
"Кто-нибудь знает какие-нибудь книжки/документы/статьи, где описаны best practices для проектной работы deep learning команд? Ну или в целом ml команд, как например в этом документе: <http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf>",
почему так сложно собирать тф из сорса под старую куду :but_why:,
"в чем логика не держать адекватно устанавливаемые в одну строчку старые версии, почему я должен танцевать с бубном вокруг установки базеля а потом забивать и накатывать теано?",
"содержательный вопрос: вот у нас на карте крутятся какие-то рендерилки визуализаций что-то считают на куде, каковы шансы случайно им навредить запуская на той же карте свои считалки, как обезопаситься? помню тф позволял жестко лимитировать количество сжираемой памяти, теано так умеет?",
"Надо профилировать как четкие пацаны, а потом прописывать cnmem открыто и смело прямо в конфиг :epta:",
"&gt; должен танцевать с бубном вокруг установки базеля
WutFace
Это на какой системе нужно ""танцевать с бубном вокруг установки bazel""?",
почему бы и не оттуда,
Я как бы на своем примере,
"На работу могут взять и без этого списка, как мне кажется ",
"&gt; в чем логика не держать адекватно устанавливаемые в одну строчку старые версии, почему я должен танцевать с бубном вокруг установки базеля а потом забивать и накатывать теано?

Дык там же все по инструкции, как по рельсам... :but_why:",
"я не столько жалуюсь на свою лень и тупость, сколько немного удивляюсь логике, что мешало просто при апдейте оставить ссылки на старые сборки, при том что достаточно очевидно много народу еще пользуется кудой 7.5? ",
Какой размер эмбеддинга в center loss стоит задать для 1-2к классов?,
"если у кого под рукой есть керас+тф, можете запустить по быстрому?",
я подумал когда увидел падающий лосс что все ок,
"mxnet, ты был мне как брат",
<@U07V1URT9> как ты это сделал?,
и какой у тебя сетап,
"Возможно вопрос слишком общий, но я все же попытаюсь. Цель понять как поправить коэффициенты. На что обращать внимание в процессе обучения модели RL чтобы это понять?
Имеет смысл смотреть на loss или только на средний реворд эпизодах?
Как понять что lr слишком маленький или слишком большой?
Как понять каким примерно поставить гамму?

Я понимаю что вопрос сильно перекликается с моим вопросом про архитектуру сетки (из которого я уже много почерпнул), но я надеюсь будут дополнительные комментарии",
"Гугловское, поэтому всем, кто так спрашивает отвечают, что они его неправильно готовят",
может это в керасе лажа какая,
"вообще, конечно, понимаешь насколько крут mxnet, когда нету кучи слоёв абстракций
в tf что-то поменяли и приехали",
"Пока китайцы держут марку. Как правило, если в mxnet что-то поменяли, то скорость работы возросла",
"Лосс мало чего показывает, как показывает мой опыт
Надо тестировать агента на отдельном запуске (желательно несколько раз) среды с жадной стратегией, чтобы проверить обучение.
Гамма обычно от среды зависит, надо смотреть по задаче, но мне кажется, 0.99 вполне универсальное решение, кроме специфических случаев, так как обычно мы хотим максимизировать именно суммарную награду до конца эпизода. Но гамму не берут единицей, так как в таком случае нельзя утверждать, что value конечна.
Lr, мне кажется, надо под архитектуру и задачу подбирать, ну и как-то пробовать менять, если расходится или не обучается",
И куда с тф последние?,
"Как говорится, каждый релиз - новая жизнь. Обновил до 0.12.0.

```
50000/50000 [==============================] - 13s - loss: 2.3036 - acc: 0.1014 - val_loss: 2.3030 - val_acc: 0.1000
Epoch 2/200
50000/50000 [==============================] - 12s - loss: 2.3036 - acc: 0.0948 - val_loss: 2.3029 - val_acc: 0.1000
Epoch 3/200
50000/50000 [==============================] - 13s - loss: 2.3034 - acc: 0.0983 - val_loss: 2.3030 - val_acc: 0.1000
Epoch 4/200
50000/50000 [==============================] - 12s - loss: 2.3034 - acc: 0.0997 - val_loss: 2.3029 - val_acc: 0.1000
Epoch 5/200
50000/50000 [==============================] - 13s - loss: 2.3034 - acc: 0.0966 - val_loss: 2.3027 - val_acc: 0.1000
Epoch 6/200
50000/50000 [==============================] - 13s - loss: 2.3033 - acc: 0.1005 - val_loss: 2.3033 - val_acc: 0.1000
Epoch 7/200
50000/50000 [==============================] - 13s - loss: 2.3034 - acc: 0.0970 - val_loss: 2.3032 - val_acc: 0.1000
Epoch 8/200
50000/50000 [==============================] - 14s - loss: 2.3034 - acc: 0.0984 - val_loss: 2.3029 - val_acc: 0.1000
Epoch 9/200
50000/50000 [==============================] - 14s - loss: 2.3033 - acc: 0.1028 - val_loss: 2.3031 - val_acc: 0.1000
Epoch 10/200
50000/50000 [==============================] - 13s - loss: 2.3035 - acc: 0.0981 - val_loss: 2.3028 - val_acc: 0.1000
Epoch 11/200
50000/50000 [==============================] - 12s - loss: 2.3033 - acc: 0.0994 - val_loss: 2.3033 - val_acc: 0.1000
Epoch 12/200
50000/50000 [==============================] - 13s - loss: 2.3033 - acc: 0.1001 - val_loss: 2.3030 - val_acc: 0.1000
Epoch 13/200
50000/50000 [==============================] - 12s - loss: 2.3033 - acc: 0.1013 - val_loss: 2.3028 - val_acc: 0.1000
Epoch 14/200
50000/50000 [==============================] - 13s - loss: 2.3032 - acc: 0.1002 - val_loss: 2.3038 - val_acc: 0.1000
Epoch 15/200
50000/50000 [==============================] - 12s - loss: 2.3034 - acc: 0.0988 - val_loss: 2.3033 - val_acc: 0.1000
Epoch 16/200
50000/50000 [==============================] - 12s - loss: 2.3034 - acc: 0.0978 - val_loss: 2.3032 - val_acc: 0.1000
Epoch 17/200
50000/50000 [==============================] - 12s - loss: 2.3034 - acc: 0.1005 - val_loss: 2.3033 - val_acc: 0.1000
Epoch 18/200
50000/50000 [==============================] - 13s - loss: 2.3032 - acc: 0.0993 - val_loss: 2.3030 - val_acc: 0.1000
Epoch 19/200
50000/50000 [==============================] - 13s - loss: 2.3033 - acc: 0.0984 - val_loss: 2.3030 - val_acc: 0.1000
Epoch 20/200
50000/50000 [==============================] - 12s - loss: 2.3034 - acc: 0.0985 - val_loss: 2.3028 - val_acc: 0.1000
```",
"надо вычленить где лажа,мнист я расклассифицировал норм вроде ",
"соглашусь с <@U0RV376MC> на loss тоже не смотрю, гамму обычно все ставят 0.99, кроме особых случаев. learning-rate вроде просто подбирают во время оптимизации гиперпараметров, я ставлю обычно adam и lr=10e-4, вроде работает и на атари и на classic control gym. Еще для q-learning может быть полезно на q-values  смотреть во время обучения. Так как для некоторых сред известно какая должна быть примерно суммарная награда (с учетом гаммы) и q-value должно к ней сходится",
"насчет гаммы и особых случаев - ее можно уменьшать, если хочется, чтоб агент “пожаднее” был, больше внимания обращал на близкий reward. Т.е. гамма ограничивает “горизонт”, за который смотрит агент - начиная с какого-то момента реворд далеко в будущем (или прошлом, смотря как смотреть) становится неотличимым от шума.",
"кстати на нипсовском RL туториале в 2015 Саттон сказал, что для эпизодичеких задач гамму надо брать равной 1. Типа нет смысла там дисконтировать, так как бесконечных сумм не будет все равно, но почему-то все все равно продолжают дисконтировать.",
"Зато быстро, нетривиально, маленькие размерности и есть везде. Очень удобно, когда нужно попробовать какую-то новую библиотеку.",
"Сама идея предсказывать кто умрёт на Титанике такая же уебанская как предсказывать на kaggle какую собачку убьют, если её не возьмут из приюта.",
"а почему Dense, а не Convolution2D?",
Мне не совсем понятно почему здесь Convolution2D должен быть полезен. Стоит попробовать?,
Я был бы очень благодарен какую-то интуицию понять почему такой слой нужен,
"<@U0DJEEXE1> А можешь подсказать про керас + конволюшн. Я что-то не уловил как там с размерностями
Я смотрю на пример и там:
model.add(Permute((1, 2, 3), input_shape=input_shape))
model.add(Convolution2D(32, 8, 8, subsample=(4, 4)))
model.add(Activation('relu'))
model.add(Convolution2D(64, 4, 4, subsample=(2, 2)))
model.add(Activation('relu'))
model.add(Convolution2D(64, 3, 3, subsample=(1, 1)))
model.add(Activation('relu'))
model.add(Flatten())

И дальше dense слой",
"<@U07V1URT9>  а ты когда сравнивал <https://opendatascience.slack.com/archives/deep_learning/p1482963834001207> менял dim_ordering на привычный для тф? 
Кажется для теановского dim_ordering keras с тф в качестве бекенда работает гораздо дольше. ",
"Так, я кажется начал понимать почему там был Permute :slightly_smiling_face:",
В общем Permute как я понимаю как раз и занимается выкидыванием “ненужной” размерности,
"Но я пока не сообразил как именно это сделать, для моего случая и почему он отличается от атари",
А какая размерность сэмплов (до закидывания в .fit)?,
"Если я правильно помню архитектуру DQN, то там сэмпл -- это 4 кадра. Их переводят в градации серого и складывают как каналы -- получается, что один сэмлп -- это 4-канальная картинка",
"так. там почти 500 гб. я качнул несколько файлов из коллекции выше по ссылке. но пользоваться этими архивами напрямую неудобно.
есть программа MultiLib для винды и MyRuLib для линукса. но последняя не обновлялась 3 года и нет пакетов для 16.04 
не получается поставить в общем :but_why: 
как вы справлялись с этим полутерабайтным обилием?",
Я скорее про то как оттуда извлечь нужных мне авторов. Я думал через прилагаемый индексный файл это можно сделать ,
а картинка доски с фигурами как представлена?,
Так же как Go в опен аи,
"<@U1BAKQH2M> а почему именнт магнет лосс, а не какой-нибудь центр лосс? Сергей Овчаренко рекомендовал его, потому что проще всех завести.",
посоны а ведь у линейной регрессии с л1 рег нету аналитического решения изза того что там sign? как в случае с л2 где w = (X^T X + a E)^(-1) X^T y,
"Если ты имеешь ввиду считать реворд за ошибочное действие не хуже чем реворд за проигрыш, то это я как раз сделал только что",
"Ну типа съел одну фигуру не значит что обязательно хорошо, но победа это когда съешь все фигуры :slightly_smiling_face:",
Прав тот кто выиграл ;),
Соответственно сравнивать буду по времени когда медленный досчитает 500000,
"а как это работает вообще, если в двух словах? для чего lstm?",
"кто там говорил, что на линукс проще всякие фреймворки ставятся с кудой и прочим? Капец геморой!",
"было соревнование на каггле, где объем камеры сердца меряли",
"на маке заморочка с Clang(из коробки) и GCC(который надо доставлять порой, чтобы собрать какую нить либу замороченную)",
"Откуда проблемы с gcc? Ни разу не напарывался, даже когда кафу руками собирал",
Это к авторам статей. С точки зрения разворачивания готового это как раз наоборот очень легко и удобно,
"попытаюсь ответить на свой вопрос, вроде как да - <https://en.wikipedia.org/wiki/Marching_cubes>",
"Ничьи условно не бывает, это выглядело бы как бесконечная игра но такого пока не случалось",
"<@U053R9RS6>  а каким методом обучаешь? если q-learning с experience replay, то кажется что с редкими выигрышами, а следовательно наградами может помочь prioritized exp replay",
Какой из двух стульев - iteration или gradient?,
"гайз, а какой простой бейзлайн можно взять, чтобы топик моделлинг с ним сравнивать?",
"привет. Помогите пожалуйста разобраться с Keras и tensorflow. 
Я решил немного переделать код асинхронного актора критика от openai (<https://github.com/openai/universe-starter-agent>)  - вместо lstm вставил fc слой и на вход подаю 4 последних кадра. Сделал это сначала в керасе, а потом просто в tf. И почему-то у меня получается, когда я создаю агента в tf (класс FCPolicy), то он обучается; а когда создаю агента в keras (класс FCPolicy2), то он не обучается. Хотя архитектура одинаковая. Делал по 2 запуска каждый из них по несколько часов. 
Может быть я как-то не так использую keras? может кто-нибудь подсказать. Я пока вижу, что отличие может быть в инициализации весов, но кажется, что это не должно быть так критично, и дефолтная инициализация в керасе должна неплохо работать. Код ниже.",
"Господа, а кто что думает по поводу Titan X Maxwell vs 2 * GTX 1070? На сколько сложно модели параллелятся?",
"Да я вчера как бы абстрактно жаловался. Думаю, те, кто давно и активно пользуется линуксом не считают, что это геморой, просто норма жизни (""на выходных поебусь ради интереса"", ага). И это не наброс, на винде/маке тоже свои заморочки, и я _их_ должно быть не замечаю. Но когда я там ради интереса ставил TF и вот это всё (когда кто-то тут сказал как это трудно), это было очень прямолинейно -- запустил инсталлятор куды, скопировал туда же cudnn, `pip install tensorflow-gpu`. 
А на линуксе, куда начинает ставиться, хочет поставить дрова, дрова ставятся только с выключенными иксами, после установки куднн надо прописать каких-то переменных окружения куда-то, во всех инструкциях из инета дальше какой-то базель ставить для TF и т.п. И это ещё повезло, что всё относительно зафиналилось и не попал на пляски с GCC, которые выше описаны. 
В целом это всё делается, просто это муторные многошаговые действия, где всё может развалиться от одного неверного движения (нет, собрать обратно всегда можно, но это ещё больше действий).",
"Титан у чувака на Caffe оказался быстрее при прочих равных. С другой стороны, 22 или 30 часов— какая разница для не продакшена.",
"Ну, и там 2*1070 ровно столько же времени показало как 1 титан паскаль. Not bad. По цене вдвое ниже. А четыре штуки вообще обгоняют два титана.",
"На теано в таких же условиях -- 17 секунд. Кто там говорил, что TF медленный?",
Я бы брал 2*1070. По скорости 1070 почти как старый Титан Х. Ну и в некоторых фреймворках реально получить ускорение почти в два раза по сравнению с одной карточкой.,
"Ну да, как я понял это как часто апдейты скидываются в модель используемую для выбора ходов",
"Какой разумной интуицией можно руководствоваться при подборе размера фильтра для одномерных сверток? А если проходим по ряду свертками разного размера (ну вроде 3,4,5), насколько разумно вектора на выходе усреднять, а не конкатенировать?",
"но я не знаю какие у меня в ээг явления, тем более что с ними происходит после того, как я ряд заресемплю",
Ты из каких соображений ресемпл делаешь?,
"На спектр ~ведь~ должен был бы посмотреть ты ~смотришь, на какие частоты ориентируешься~? Сколько тактов соответствует нужной тебе частоте?",
как смотреть на спектр 24хмерного ряда?,
брал 970 как раз для пробы и понять что оно такое,
какой кстати оверхед при разрыве графа на две карты?,
"а то когда я случайно считал свертки на одной карте, а денс-&gt;софтмакс на другой - почти не тормозило",
"если хочешь разобраться как всё работает и планируешь экспериментировать что-то своё и наркоманить со слоями то изучай theano/tensorflow
если будешь только примеры запускать то кераса хватит :nabros:",
"Некоторое время назад читал про интересное использование SVM для ранжирования:
- модель тренируется на по-парных разницах
- предсказание делается на самих элементах, а не на разницах, и на основе этих предсказаний происходит ранжирование

Но сейчас никак не могу найти, где именно я это видел. Может кто-то знает?",
"<@U0JJ69UB1> да, я планирую статью на хабр по DDPG и выложу еще код на гитхаб, там как раз на примере моего агента будет, я думаю, что оно выше 800 места не поднимется. У нее плохая стабильность, то в деревьях застрянет, то воюет хорошо. И есть еще некоторое переобучение на поведение союзника, я тренировал на игре 1 на 1, то есть сетка училась сразу пятью управлять, поэтому если союзники тупят или как-то нестандартно действуют, то и мой агент может затупить",
"А кто-нибудь может подсказать интуицию, как понимать что алгоритм быстро сходится к некоторому хорошему состоянию, а потом начинает постепенно двигаться в сторону увеличения длины партии / уменьшения средней награды на ход / уменьшения награды на эпизод (при гамма = 1)?",
"<@U0JCGHU9H> 
Это что по чему растет линейно? seq2seq энкодер-декодер как раз пример не раскидываемой нахаляву модельки. С N картами только модель сможешь пожирнее сделать.
Или я неправильно понимаю сетап?..",
"Я тут подумал, возможно у меня немного смешались 2 вещи: статья Гугла про их переводчик, где они каждый из 8 слоев клали на отдельную карту (Tesla k80).",
"И была отдельная статья возможно опять же ребят Гугла, которую я уже не помню, где показывали, что для небольшого числа карточек - потери были не очень большие, не более 10% на новую карту, в пределеах 4-х карт, дальше - больше. Ссылку на эту статью - сейчас вряд ли найду, и возможно там были свертки",
"Кстати, и почему сек2сек сложно раскидывается на несколько гпу?",
В репозитории как раз она и залита,
"&gt; производительность растет почти линейно
Я не понял, почему она расти может",
"у меня возник такой вопрос: приходит время писать бакалаврский диплом, хотелось бы взять тему ""байесовское машинное обучение"", ну или что-то похожее. Никто не может подсказать, какие работы можно почитать или подсказать примеры, чтоб посмотреть, а то совершенно не понимаю какой уровень должен быть у бакалаврских дипломов.",
Скажи лучше какая из них поддерживает видюху,
как вообще может в голове родиться идея собирать дл на винде?,
"<@U07V1URT9> нет, мне реально интересно какой юзкейс, пока  я считал только на кпу - старался собирать все под винду чтоб можно было гамать на одном ядре пока считается, но если гпу занята - зачем винда?:but_why: ",
"всегда казалось главным, чтобы машина ехала, а какого цвета у нее колеса - дело десятое",
"Коллеги, кто мне напомнит, где лучше втыкать BatchNormalization перед или после функции активации?",
А кто-нибудь знает как посмотреть утилизацию PCIe в линуксе?,
(не спрашивайте зачем нам это нужно),
<@U0L4KM9R9> когда я чуть смотрел с GPU passthrough и CUDA все было достаточно грустно,
"Кажется, когда я смотрел единственный вариант был VMWare с PCI passthrough",
<@U0AS548A1> а научруков зачем смотреть?,
"В общем, погонял TF-бенчмарки отсюда <https://github.com/soumith/convnet-benchmarks>
На винде, так как хз как в линуксе мониторить загрузку шины. 
Бенчмарки гоняются хорошо, загружают GPU на 100%, в CPU (E5-2670v1) не упираются, в шину -- не упираются (пики загрузки ~70%, возможно вдвое более быстрый GPU может и забить шину, если будет прожёвывать данные достаточно быстро), MSI Gaming X -- смело рекомендую, стоит прямо под ухом, не слышно вообще. 
""Народный комплект"" можно считать отличным вариантом, ""недостатки"" в виде старого PCIe в реальности  не являются узким местом.",
"Одна, планирую расширять, когда 1080ti выйдет",
"Это тут конечно оффтоп, но помню как удивлялся, когда какие-нибудь фичи, которые были во встроенной в сраный ворд IDE vba с 97 года подавались как невероятное достижение в новых идешках",
"А не значит ли это, что когда поставишь вторую, то она и забьет все PCI-e лайны?",
Причём на матери половина портов работают только когда второй проц установлен,
"Да, помню такое. Но как всегда -- нужно тестировать производительность на конкретной задаче",
"В общем я так и не понял как выходит, что при продолжении итераций у меня все метрики с какого-то момента падают вниз. Я на всякий случай подождал пока ситуация стабилизируется с количество ходов, но это ничего не изменило — реворд на шаг и на эпизод продолжили падать. Лосс и mean_q при этом постоянно росли",
Попробовал на всякий случай гамму уменьшить до 0.99 как в оригинальном алгоритме,
"пишут что q-learning, а особенно с нейронной сетью нестабильно обучается. В статьях видел графики, когда сеть обучается, а потом расходится. Может как раз на это и попал.
Сейчас вроде советуют использовать policy  gradient методы (типа REINFORCE, различные модификации актора-критика), обучается стабильнее, очки выше и реже расходится. Сильвер в своих лекциях про это говорит и Karpathy тоже в своем блоге про это писал.",
"Если запущу, поделюсь как результаты в сравнении",
"А как бы капчу ""I'm not a robot"" обойти? :slightly_smiling_face:",
"Вопрос по Keras:

У меня выход с сетки матрица (10, N, N) с числами [0, 1]

Хочется посчитать апроксимацию jaccard index в каждом из каналов и усреднить.

Если бы  у меня на выход был один слой, а не 10, я бы сделал так:

```
def jaccard_coef(y_true, y_pred):
   smooth = 1
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    union = K.sum(y_true_f) + K.sum(y_pred_f)
    return (intersection + smooth) / (union - intersection + smooth)
```

Как  мне это обобщить на 10 каналов?",
"Поставил точку на варианте с Q-Learning.
Более-менее устойчивый вариант получился с такими наградами:
проигрыш / неверный ход -1
за ход -0.05
за выигрыш +100

Доля выигрышей держится где-то на уровне 80% - 85% против случайной стратегии, длина ходов расти перестала и сходится примерно к 30 ходам (а не как раньше к 100)

В общем и целом такой подход выглядит как рабочий если тюнить награды, но все довольно хрупко",
вопрос на понимание: есть ли в постановке методов раскладки графов какая либо связь с “коммьюнити”?,
"…
метрические методы дают тебе проектор на основе расстояний между точками. и все. что получится в результате проекции и какая будет связь с структурой кластеров - никаких гарантий не дается и не должно быть. 

те из них которые дают тебе 2д проекции только дают тебе твои точки с двумерными координатами (и часто еще модель для проекции новых точек полного пространства в это 2-мерное). 
раскладка графом дает тебе расположение узлов на основе совместных связей (всякие force directed, atlas, etc), что можно использовать как разложение точек-узлов на плоскости. и все",
"…
получится у тебя похожий результат, или нет, тебе никто не скажет. как не скажут это и для метрической кластеризации (могут получиться, но это зависит больше топологии данных чем от метода). и если тебе особенно повезет, раскладка на графе даст тебе тоже полезный (визуально) результат. без гарантий и обещаний",
"Гайз, потерял ссылочку на сайт с разного типа графиками (начиная от bar plot и заканчивая violin plot), по каждому указано, как называется, где обычно применяется и зачем. Подскажите, пожалуйста, кто помнит",
"Не-а, тут, как я вижу, про text visualization, а там общая солянка",
"А кто какими ибп пользуется? Если компьютер потребляет ~800 ватт, то офисные дешёвые не подходят уже.",
а кто-нибудь смотрит на распределение после tdf-if? есть какие-нибудь наблюдение на тему как выглядит распределение которое дает хорошие топики?)) :thinking_face:,
"Подскажите, где можно найти словари разговорного английского и русского? Мне бы по-простому отделить научную лексику от общечеловеческой.",
1. Variational Inference используется для нахождения апостериорного распределения. А зачем нам нужно апостериорное распределение? Используется ли как-то VI в нейросетях (может это имеет отношение ко второму вопросу)?,
"2. В вероятностном подходе по машинному обучению задача обучения (MAP) ставится как W = argmax(p(D|W) * p(W)), где p(W) - априорное распределение весов, D - обучающая выборка, p(D|W) = p(x_1|W)p(x_2|W)...p(x_N|W), p(x|W) - вероятность появления x, т.е. p(x|W) - не произвольная функция, а положительная и суммируется к единице, Если f(x, W) - нейросеть, то p(x|W)=f(x,W)/integral(f(x, W))dx. Но по факту при обучении никакого интеграла не вычисляется. Как это происходит и где подробнее об этом почитать?",
"<@U0WFHMB4L>: точечная оценка ничего не говорит о том, насколько модель уверена в своём ответе. А вот в байесовском (см. <#C1P8YT7C7|bayesian>) подходе к выводу ищется как раз распределение или какое-то к нему приближение. Кроме того, это самое апостеорное полезно иметь для оптимального (с учётом неопредённости) моделирования новых данных. Однако, это всё непростые вещи. т.к. в процессе возникают неберущиеся интегралы или экспоненциального размера суммы",
"Касаемо второго: надо смотреть на конкретную модель, но
1. В случае стандартой supervised задачи у нас есть набор пар (xᵢ, yᵢ) и моделируем мы `yᵢ` при условии наблюдения `xᵢ`, т.е. ищем `argmax p(W | D) = argmax ∏ᵢ p(yᵢ | xᵢ, W) p(W | xᵢ)`, где `p(W | xᵢ) = p(W)` поскольку нейросеть не зависит от генеративной модели `x`'ов, а только от `y|x`. То есть нам нужно брать интеграл не по `x, y`, а только по `y`, что мы и делаем, ставя в выходной слой softmax, sigmoid или L1 / L2.
2. В случае генеративных unsupervised задач вроде вариационного автокодировщика, у нас есть 2 сети: `enc(x)`, генерирующая код `z` (который является случайной величиной, зависящей от `x`) и `dec(z)` (тоже случая величина, зависящая от `z`), генерирующая восстановленную картинку. Для время обучения мы не можем точно посчитать интеграл в знаменателе `p(z|x)` (распределение для `enc(x)`), поэтому и пользумся вариационными методами для его приближения с помощью какого-то ""берущегося"" `q(z|x)`",
"То есть, как только у нас есть какое-то наблюдение `x`, нам уже неважно, откуда оно пришло, ""мы"" в его формировании не участвовали, поэтому и подстраиваться под него не надо (что совсем не так для `y`)",
"Понятно, спасибо. А где можно подробно почитать на тему вероятностного подхода для нейросетей?",
":wat:, `p(y|x)` параметризовано W, ещё как зависит",
"Хм, маловероятно, но а вдруг... Есть ли кто в Ростове в ближайшую неделю?",
"Я же говорю — заморачиваешься, как будто покупка компьютера, это на всю жизнь и нельзя переиграть",
Какие расчеты на пять лет вперёд? За это время всё абсолютно изменится и старое железо только выкинуть будет,
"Б/у серверы -- это для тех, кто 1) хочет заморочиться 2) понтоваться двухпроцессорной машиной 3) ещё обучать всякое на cpu 4) сэкономить чуток",
"Ребят, а если кто шарит, расскажите плз как в комп ставят 4 видюхи, так чтобы они не поубивали друг друга отводимым теплом? Обязательное водяное охлаждение, или есть еще какие-то варианты?",
"Стоковое охлаждение как раз сделано так, чтобы карты забирали воздух из передней части корпуса и выплевывали назад",
проблема только была когда помпа встала :troll:,
"<@U06J1LG1M> вот примеры. размер графиков одинаковые, но количество бинов/колонок разное. поэтому там где больше, там и колонки уже. а хотелось бы, чтобы размер колонки был фиксирован, а менялась высота графика",
"привет всем. у меня просьба: расскажите, кто пользовался, про tableau.  Плюсы, минусы и основные фишки. Буду весьма благодарна",
"А почему нельзя в качестве истории взять лучшие из сыгранных игр? За несколько миллионов шагов наверняка было сколько то хороших партий. В принципе можно даже что нибудь типа генетических алгоритмов запустить - выбирать подмножества сыгранных игр, на нем обучаться и запускать таким образом обученные алгоритмы друг против друга.",
"Ну, это все на уровне идеи конечно, может быть там какие то проблемы, которые в голову не приходят, пока не попытаться это сделать.",
"Ну там если кто внезапно будет в Москве 7го и захочет выползти из дома, то мы с <@U2LGNCP2S> можем составить компанию",
"что там значек бана делает, может тот кто поставил просто не понял что :motokozak: имеет в виду",
"Всего минус двадцать — повод, чтобы не выползать из дома в городе, где есть метро :scream: ",
"да, где шумно и тесно)",
"господа, мне вопщем тут для лекции нада было проиллюстрировать оверфиттинг, ну я взял и сделал кучу полиномиальных регрессий, все как и ожидалось, чем больше степень тем больше кривулина интерполирует данные, проходя через точки и странно извиваясь, но вот на 13ой степени что то пошло не так и она вообще укатила, какое есть этому объяснение?",
"график выглядит как будто он фитит точки, но его потом сдвинули вниз",
"давайте накатим бургеров в фарше в центре (который рядом с кузнецким мостом/лубянкой) а потом что-то решим, звучит как план ",
"Уверен, что в настолке найдешь с кем там сыграть",
"<@U041SH27M>: ты как будто не знаешь, чем в Питере нужно заниматься",
"Больше всего меня забавляет активное голосование <@U040M0W0S> и <@U06J1LG1M> в этом обсуждении, при том, что ни тот, ни другой, как я понимаю, в ближайшие дни не в Москве",
а вам какие настолки нужны? есть скрабл и наполка твистер,
я уже забыл про какой день мы говорим ,
Так что какая разница  :good-enough:,
"2) когда мы вдруг начинаем хотеть интерпретируемую модель и начинаем оптимизировать в пространстве объектов, то хотя бы без простейшего априорного распределения на X получается лажа",
"Как там пишут, MMD это такое расстояние между двумя распределениями, как дивергенция кульбака-лейблера, только более вычурное. Оно требует взятия супремума по всем функциям из какого-то набора F, чего в общем случае аналитически сделать нельзя",
"Зато у нас есть концепция RKHS: это пространство функций вида `k(⋅, x)` (на самом деле, чуть больше этого) для всевозможных `x`, где `k` – какое-то ядро",
А как задается все же это пространство? Что ограничивает эти функции f?,
"Да, скаляр, причём не обязательно 0-1. Для ""хороших"" ядер есть требования, означающие, по-сути, что ядро ведёт себя как скалярное произведение _в каком-то пространстве_",
"Да, ядро как двух-аргументная функция порождает пространство функций одного аргумента",
Может кто сталкивался с подобной проблемой и знает как лечится?,
"Только строго говоря, к RKHS ещё есть всевозможные линейные комбинации таких точек, как если бы мы считали ожидаемое расстояние не до одной точки, а сразу до нескольких, а потом взвешивали",
"(а ещё там есть замыкание, как если бы мы делали то же самое с бесконечным количеством точек)",
"Хм, а как это получается?",
"Т.е. как выглядит формальное определение, которое это позволяет?",
"привет, похожая проблема была, когда запускал агента по ssh. Сперва проводил всяческие манипуляции с `xvfb-run -s ""-screen 0 1400x900x24"" python agent.py` по мотивам <http://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server>",
"Пойнт в том, что в седловой точке есть направления, которые меняются часто когда ты по ней туда-сюда мотаешься",
"А есть которые маленькие, но в постоянную сторону - это то, где у седловой точки вторая производная максимум, а не минимум",
"В статье дается другое интуитивное объяснение зачем усиливать сигнал по направлениям, где градиент маленький",
Вот наверное где я это прочитал,
"Мне ещё интересно, что там происходит в зависимости от соотношения между знаками собственных чисел гессиана. Кажется, что чем больше доля положительных, тем меньше должен быть loss. И, быть может, наши нейросети не сходятся к локальным минимумам, а застревают в сёдлах, где всего одно направление ведёт к уменьшению ошибки",
там был простенький пейпер на несколько страниц где стажер лекуна подробно смотрит на собственные числа гессиана сетки решающей простую задачу (вроде мнист),
"Сейчас по-моему проблемы не выполнить оптимизацию точно, а как бы не переобучиться",
"Есть даже очень популярные статьи, где советуют нормальный шум к градиентам добавлять",
"у кого есть опыт в обучении моделей количество классов в которых постепенно увеличивается? как тут вообще делать? у меня пока мысль такая:
 беру я обученный классификатор (задача распознавания говорящего). когда первый пользователь записывает несколько семплов голоса - мы можем обучить на этих семплах последний слой классифицировать на два класса - (он/не он). потом добавится еще один - добавляем нейрон со случ весом в последний слой и обучаем на новых данных классифицировать на три класса и т.д.
как вообще работает штука типа ok google и пр.? почитал про online/incremental learning  - кажется там все глобальней и сложнее",
"<@U0AD1L5NC>: как там называлась та статья, которую обсуждали на митапе, куда мы ходили?",
т.е. мы например извлекли фичи из спектрограммы - и через knn смотрим возле какого класса они лежат,
"ммм, не совсем. Когда ты используешь - классов уже нет",
"(они могут быть или не быть, когда ты тренируешь embedding)",
Когда используешь - просто вычисляешь feature vector из каждого сэмпла и объединяешь их с каким-то distance threshold,
<@U041P485A> может (или не может) рассказать как оно на самом деле :slightly_smiling_face:,
Но часто бывает полезно потом подучить их работать конкретно как embedding через triplet loss,
Как дальше их сгруппировать по спикерам?,
Сказать сколько их и какой к кому относится,
"ну я не совсем понял, что ты подразумеваешь, но мне кажется, что вполне можно определять класс обьекта как класс ближайшего объекта из трейна в пространстве эмбеддингов, разве нет?",
"а, я думал тут задача в стиле файндфейса, где есть обучающая выборка и нужно сопоставить входящий голос с какими-то  семплами ",
я предполагаю наличие некого режима тренировки - когда говоришь несокльо контрольных фраз - получаешь эмбединги для данного спикера. потом в режиме использования уже просто ищешь k-ближайший,
не знаю как у алексы но предполагаю что примерно так. не?,
она не понимает кто к ней обращается?,
"При таком выборе каналов всё равно непонятно, куда же такой вопрос запостить. Вы вот конференции разные посещаете, туда перелёт, проживание и оргвзнос сами оплачиваете из заработанных денег или же вам работодатель (или научный руководитель) такие расходы оплачивает (командировка для повышения квалификации и т. п.)?",
"Нужно реализовать поиск элемента, который встречается более n/2 раз, где n - длина массива. Решение в лоб - O(n^2), более изящное O(nlogn). (Видел еще решение за O(n), но поскольку в учебных целях пока не особо интересует). Подумал сам, потом нашел псевдокод здесь - <http://www.cs.bc.edu/~alvarez/Algorithms/HW/hw4.sol.pdf> (5 страница). Верно ли то, что  один из ключевых моментов - 5 строчка алгоритма, за счет которой и не идет куча сравнений одного и того же элемента (стоящего на разных позициях) несколько  раз со всем остальным массивом?",
"<@U04725QK7> Ну, ключевой метод - догадаться, как можно свести задачу к более простой, если у тебя есть возможность решать ее на меньшем размере",
"Типа, ключевое наблюдение - что тот, кого больше половины на всем участке - должен быть встречаться больше n/2 раз на одной из половин",
"а там в курсе какой формат? типа отсылаешь решение и оно на сервере гонится, как на контестах?",
"чет легче не стало, я не понимаю как применить и даже трактовать ядро в OPVI, если там действительно то странное ядро",
типа какая стратегия планируется и вообще),
"возможно, самый важный вопрос не задал - позиция то какая была?",
"но ты же в итоге затащил, как мужик, это же главное)",
"там есть термины _test_ и _variational family functions_, второе понятно, но зачем выбирать _test family_?",
"Я потом написал сам для себя, когда узнал, что спрашивают",
"чат, есть идеи, как напарсить датасет по фрилансерам? что-то вроде “задача - откликнувшиеся фрилансеры - выбранный фрилансер""",
"на странице 8 используют нейросеть для тест функции, но как и зачем - хз, почему норма 2, почему tanh?",
"Я вообще хз, как питон на ""низком"" уровне работает ) Надо будет почитать про это",
"int32, раз они ограничены как 10^9",
"Аригато в хату! У меня нубский вопрос про свертки, подскажите плиз. Вот мы берем сеть типа Ленет. Первая свертка - 6 фильтров в виде матриц 5х5, получаем 6 feature maps. А вторая свертка записыватся как 50 фильтров, тоже 5х5.  Сам вопрос: означает ли это, что каждый фильтр ""смотрит"" на все 6 feature maps, и происходит поэлементное умножение и суммирование 5х5*6 значений? И так получаем 50 feature maps.",
"И этот прикол  с использованием только части слоев, как у Лекуна, теперь не актуален и не  особо используется?",
Бармалей зачем ты это позволил?,
"немного вроде даже понятно почему матожидание равно нулю у всех них - ибо tanh симметричная, нечетная функция. Интегрируем получаем ноль",
"<@U0C1BGRB2>  <@U07V1URT9> спасибо, буду посмотреть, когда дорасту до нужного уровня понимания ",
"кто нибудь разобрался в примере с mnist в <https://arxiv.org/abs/1610.09033> ? Они используют нейронки для семейства test functions, уже сутки ломаю голову какой смысл они вкладывали во все это.
&gt; For _f_ , we use a three-layer neural network with the same hidden size as the variational program and hyperbolic tangent activations where unit activations were bounded to have norm two. Bounding the unit norm bounds the divergence.",
Да как бы курсера всем дает возможность учиться бесплатно...,
"&gt; Да как бы курсера всем дает возможность учиться бесплатно...
много курсов содержат платные задания",
"я в принципе могу платить, но для тех кто не может отличная штука",
"мне тоже самое науч рук посоветовал, но было это, когда я уже оплатил курс от яндекса и МФТИ. Он тоже сказал, что курсера ничего не чекает, просто пиши, что студент, но не указывай, что в душе. На тот момент я уже не был студентом.",
"Это раньше так было в курсере что автоматом одобряли финэйд.
С конца 2016 вроде как там реально рассматривают заявки и это может какое-то время занять",
"и вроде как надо, чтобы для всех функций матожидания совпадали, так?",
"выбор нейронки исключительно для того, чтобы сделать этот класс побольше? И это не имеет отношения ни к чему другому, как проверка равенства матожиданий, получается?",
"<@U040M0W0S> не гораздо, в 3 раза медленнее. но в sklearn меньше параметров. vowpal wabbit всех быстрее как всегда.  <https://github.com/RaRe-Technologies/gensim/issues/457> <https://opendatascience.slack.com/archives/nlp/p1483140934000631>",
как будут результаты топиков на фсине будет очень интересно посмотреть,
Я думаю надо тупо FCN с Resnet как бейзлайн пробовать,
"Обычный segmentation я делать вроде как умею. Но вот к спутниковым снимкам у меня он прикручивается пока так себе. Что FCN, что Unet.",
Спасибо. Вот что было на последних конференциях я как раз и не в крусе.,
"Спасибо.

А теперь более специфичный вопрос: ""Какие статьи/блоги/книжки можно почитать про то, как нейронки и прочий CV прикручивается к обработке картографических снимков?""",
"В том-то и дело, что диплёрниг (какой-то) был, но и без него люди как-то жили. Как книга в целом про нейросети она представляет интерес.",
звучит как отличный вариант :slightly_smiling_face:,
"развлекаюсь с universe, гоночки запилил, управление похожее и интуитивно понятное
теперь вот хочется новые окружения потестить — а как собственно играть в саму игру, не знаю
выход — искать сами игры в сети и играться на богомерзком флеше и изучать так управление или есть где описание мб?
забавно, у НИХ таких вопросов должно быть не возникает — большинство же игралось в эти игры в детстве  :ololosh:",
"Есть, но это же флэш ставить надо, а мне прям плохеет, когда я вижу его иконку)
<https://universe.openai.com/envs#flash_games>",
"я тут запелил ноутбук для одной лекции для желающих начать юзать петон не только для говносайтов, я там генерю красивые картинки с плотностями как в учебниках, может кому пригодится
<https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb>",
"<@U06J1LG1M> а ты понял в итоге, почему на 13 степени такой ад выходит?",
а как ты сопоставляешь стадии обучения?,
"Я обычно учу на фиксированном lr и когда лосс начинает расти/калбаситься на месте, то понижаю lr в 10 раз с лучшей эпохи.",
"&gt; Сложно сказать сколько сетка раз выдела весь датасет.
А как ты минибатчи формируешь? Так-то одна эпоха = один проход по датасету",
Поэтому после того как доучу нужно будет прогнать бенчмарки в которых сетка будет использоваться и уже делать выводы.,
"Типа, что за архитектура примерно, какая скорость, как реализовано",
"Это из-за того, что в сети ~300 очень мелких слоев и затраты на запуск куда ядер очень большие)",
"<@U3MH9AU1Z> не, никто не говорит, что неправда, просто реально неожиданно. выкладывайте на гитхаб, будьте как большие ребята )))",
"причины я описал выше, куда хороша когда мало тяжких слоев, а в случае 500 но очень малых скорость самих сверток уже нивелируется накладными расходами связанными с памятью и тп",
"тренирую Keras модель и получаю довольно разные результаты, уже зафиксировал seed где смог, отключил shuffle а всё равно результаты немного разняться… как добиться более стабильной воспроизводимости?",
"красиво будет, когда пыли налетит",
"<https://opendatascience.slack.com/archives/deep_learning/p1483712859001836>
вот для этого seed какой нибудь хотелось бы иметь… чтобы не плавало",
"привет!
вопрос по тензорфлоу в связке с керасом. у меня есть модель которая натрейнена керасом - мне надо ее вызвать с помощью тензор флоу.

для этого я делаю 
import tensorflow as tf
sess = tf.Session()

from keras import backend as K
K.set_learning_phase( 0 )
K.set_session(sess)

далее делаю у 
model = keras.models.load_model(...)

вся проблема когда я делаю sess.run(model.output, {'input_1:0', img})
если сделать вот так, то ругается на не инициализированные переменные
если до этого сделать sess.run(tf.initialize_all_variables()), то все веса перетираются на кашу и выдает неправильные результат",
"самое обидное, что я делал ровно такое месяц назад, но забыл как =/",
"Я читаю диссертацию Minh'a и он там долго рассказывает, как они прикрутили CRF к loss function в задаче сегментации, чтобы усугубить тот факт, что близко расположенные пиксели сильно скоррелироаны.

Его диссертация 2013 года. 

Вопрос: народ все еще занимается такой подкруткой или используются другие методы, или же сети стали насатолько умные, что это уже и не надо?",
"<@U34Q3KU8H> когда я смотрел эту статью, выигрыш был не адский",
"Ну, каждый решает какой input range у них сам",
"Если используешь pretrained, то надо чтобы был такой же, как у тех, кто тренировал",
"С pretrained понятно, что надо все так же как и у авторов.

А вот mean когда вычитают - это же constant factor, который все-равно будет сдвинут во время batchNorm. так уж велика разница в скорости сходимости?",
"Как правильно использовать BatchNormalization в keras, в том смысле, что какие параметры втыкать в mode и axis?

batch normalization втыкается после convolution2d

Я правильно понимаю, что если на вход сети идет (num_batch, num_channels, X, Y)

то надо

`BatchNormalization(mode=0, axis=1)`

?",
"<@U04725QK7> в edit distance на каждом шаге динамики (читай - тупого рекурсивного перебора с запоминанием ответа) выбирается действие, которое надо сделать с последним (можно и первым) символом одной из строк, например, первой
пытаешься сделать insert/delete/replace и вызываешь себя на всех трех вариантах, а оно дальше идет по рекурсии и пробует все подряд, потом берешь наилучший ответ
а вся динамика в том, что перебор долгий из-за огромного пересечения подзадач, ответ на которые можно сохранить и использовать один раз
подзадача тут (что является аргументами твоей рекурсии) - две строки, а так как мы с конца идем, то длины их префиксов (сначала они равны длинам строк)",
"Всем привет

Я хотел уточнить.
Я пробую ковырять керас и просто беру модели из других скриптов as is.
Например, squezenet отсюда:
<https://github.com/yhenon/pysqueezenet/blob/master/squeezenet.py>

Проблема в том, что когда я использую функцию модели она ломается compile time
на добавлении второго слоя ""ValueError: Negative dimension size caused by subtracting 3 from 1”

Учитывая, что поведение идентично для двух разных моделей, я думаю что проблема на моей стороне, но я, честно говоря, теряюсь и не знаю даже куда смотреть. Подскажите куда смотреть

Бекенд — TensorFlow",
"Кто-нибудь может мне ткнуть носом в ресурс, на котором бы доступно объяснили что такое embedding и зачем он нужен, но не с точки зрения математики - это я знаю, а с точки зрения ML/DL ?",
"Благодарствую.

Следующий вопрос: кто мне подскажет есть ли python wrapper'ы для libfm, libffm?",
"<@U34Q3KU8H> почти везде, где есть текст, w2v фичи очень сильные",
"Всем привет. Можете подсказать про правильное использование batch norm. Я правильно понимаю, что после того как я закончил обучение, то мне надо пересчитать для всех слоев для которых которых я использовал batch norm средние и стандартные отклонения по всему тренировочному набору? Просто, наприер, если я в лазанье ставлю `deterministic=False`, то для нормализации будет использоваться накопленная moving average статистика, что кажется не совсем правильно, и я не нашел какого-то удобного способа, чтобы заменять эту статистику на статистику по всему тренировочнумо сету.  Насколько критично пересчитывать нормализацию по всему трейнинг сету?",
"гайз, а те, кто из вк достает данные. какие вы хаки придумываете, что обходить такое:
&gt;&gt;&gt; Обратите внимание — даже при использовании параметра offset для получения информации доступны только первые 1000 результатов.",
"грубо говоря, мне не нужны все паблики. мне нужно их репрезентативное множество. но что такое репрезентативное -- я не знаю. надеюсь, что кто-то решил этот вопрос до меня. и даже уверен в этом. в том числе именно такое ""самоограничение"" я имел в виду под фразой ""хаки"". 
но главным хаком я вижу что-то типа следующего:  
1) выкачать по-максимому 
2) среди названий/описаний найти ключевики
3) использовать их для более конкретных запросов
4) и так далее. 

но как выделить такие ключевики... посмотреть следующие по частоте слова?",
"каким образом – пока не знаю, вбросила просто идею :) ",
"господа, а как так происходит что когда тормозишь цикл обучения, nvidia-smi показывает в перед отключением 75 градусов, а на следующую секунду уже 68, не может же так сразу на 7 градусов холоднее стать?",
"Точнее, скорость теплообмена выше там, где большой градиент температуры :)",
"согласен. но я там в другом канале написал, что некоторых пабликов для Москвы больше 10к находит. тем самым как не сочетаний сортировки -- все не переберешь. может есть еще какие-то хаки!",
"собрать только самые популярные, например, и пренебречь мелкими
потому что по тому, что ты понаписал, не очевидно, зачем парсить все-превсе паблики «подслушано»
и, соответственно, спарсить какое-то большое, но доступное без костылей их количество выглядит как разумное решение",
"Кать, ты зачем меня преследуешь с этим капитанским советом? :pepe_sad:",
"есть у кого положительный опыт использования `mpld3`. или лучше не пытаться, а сразу <http://plot.ly|plot.ly> юзать?",
"Вопрос, есть контент магазинских чеков,как из него аккуратно можно вынуть поля товар+цена,сейчас вынимаю регулярками,но тонны всякого мусора захватывается по пути,есть мысль- вынимать регулярками все что хоть как-то похоже на искомый контент,как-то векторизовать, разметить и научить на этом свм например чтоб он отсеевал все ненужное.Как еще можно сделать и как можно векторизовать такие короткие поля при условии что данных мало?",
то есть для каждого вытащенного поля просто считать сколько каких в нем букв?,
а какие плюсы от оранжа? (а то тож не использую))),
хм. а как в этом superset быть с датой из sqlite? в sqlite все даты -- это строки. че-то не могу сообразить(,
мне интересно на больших данных как будет,
"если знаешь, какую-нибудь sql-бд с кучей данных, и куда можно свободно приконнектиться, то я бы посмотрел",
"А есть какая-то интуиция какую долю должна регуляризационная часть составлять в лоссе при обучении (от ошибки примеров)? 100%, 10%, 1%?",
"Чат привет.
Есть сверточная сеть для классификации картинок с последним softmax.
Чат, посоветуй, как лучше заменить softmax на one-class классификацию для каждого класса?",
"ну да норм, недавно была статья где еще такие картинки после обучения прогоняли через “выправлятор”, кторый из говнокартинок делал нормас картинки",
"Не могу дать точного ответа, но у NLTK есть корпуса и модели, чтобы сделать самому))

Подскажите плиз, когда нужно делать ""stemming"" и фильтровать ""stop words""?

1) Текст -&gt; Sentence Tokenizer = N-gram tokens
2) N-gram tokens -&gt; PoS -&gt; Named Entities
3) N-gram tokens -&gt; Classification",
"Встроенные `from nltk.corpus import stopwords` являются 1-gram словами, и как бы их можно было б фильтровать и делать стэмминг для `from nltk.tokenize import word_tokenize`, но как быть когда ты хочешь получить 3/4-gram токены?",
"да, это смотря какие данные, но если ясно, что их надо выкинуть, то выкинуть их надо сразу, а потом уже векторизовать",
"Спасибо, поидее логично) А делаете ли вы stemming? Он как то неправильно находит корень слова (встроенный в NLTK), можно юзать Lemmatizing, но интересно какой best-practice",
"Хмм.. я сервис пишу, и думаю, как бы это в коде все урегулировать. То есть, если `lang='en-US'` то без *stemming*, а если `lang='ru'` то добавить.. Как обычно в проектах такие вещи контралируются? Для китайского например там будет уже совсем по-другому))",
"Согласен, можно наверное даже разбить на некоторые абстракции типа *кириллица*, *латиница*, *иероглиф*, *арабский*, *иврит* -- думаю, чтоб в будущем когда нужно добавить поддержку нового языка, исходить из этих вещей.",
"вот, например, на кагле сейчас идет соревнование как раз про предсказание кликов",
и смотря на каком моменте хочется вынимать,
"и понятно, что рекламодатель хочет знать еще до того, как он сделал ставку, что ему придет, но очень часто это нереально",
можно попробовать сделать самим датасет вроде стенфордского (использовать проставленные эмоджи как прокси сентимента),
их можно как тест использовать,
"А какие вообще есть русскоязычные датасеты, кроме SentiRuEval?",
"он разжимается в чуть ли не 20, если что (хз как так получилось, где-то надуплицировал видимо сильно)",
"Народ, привет. Кто прокомментирует эту задачку? Какие есть мысли?

Есть юзеры со своими полями. Кроме того, есть действия юзеров с датами и др. параметрами. Нужно построить для выбранного юзера график ""прошло дней с последнего действия "" - ""вероятность что в данный день данный юзер совершит любое действие "". При этом учитывать как значения полей действий, так и полей юзеров для увеличения точности прогноза.

Нужно не однозначный прогноз для юзер давать, а график вероятности по дням.

Например, упрощенно, если
a. 90% юзеров делают действие через месяц, а 10% юзеров делают действие через 7 дней, то на графике для любого нового юзера для месяца должна стоять высокая вероятность сделать действие, для 7 дней небольшая вероятность сделать действие, а для остальных дат еще меньше. Но это упрощенно, конечно, без учета других параметров действия юзера.

Фактически, мы ищем не вероятность действия, а вероятность, что отдельный юзер совершит одно действие.

мне советовали вот это:
1. для временных рядов критично, что замеры делаются через одинаковые периоды времени, поэтому, возможно, нужно будет создать поле ""совершил действие"" и делаем его равным 0, если не совершил действие для данной даты, и 1, если совершил.
2. Возможно, поможет порядковый логит с временной зависимостью, в котором отклик - категориальная переменная.  Включаем в него предиктором время и можем предсказать вероятность действия в зависимости от времени. А там можно и как со временными рядами работать.",
"а не, у меня был корп аккаунт на тб, но как уволился его отжали",
"да, я как раз читал github, вроде обещают как-нибудь обратно впилить)",
"ну как обещают) кто-то сделает - впилят, но это все уже много лет тянется)",
"выложите на <http://academictorrents.com/>, пожалуйста, кто скачает
<https://opendatascience.slack.com/archives/datasets/p1483986757000457>",
конечный результат зависит больше всего от мотивированности того кто этот дашборд запилил,
"не, ну, саентологи из BI отдела в основном на Табло, остальные кому как нравится",
"наверное где-то 60-70% на Табло, 20% на shiny и 10% как придется",
"Народ, привет. Кто прокомментирует эту задачку? Какие есть мысли?

Есть юзеры со своими полями. Кроме того, есть действия юзеров с датами и др. параметрами. Нужно построить для выбранного юзера график ""прошло дней с последнего действия "" - ""вероятность что в данный день данный юзер совершит любое действие "". При этом учитывать как значения полей действий, так и полей юзеров для увеличения точности прогноза.

Нужно не однозначный прогноз для юзер давать, а график вероятности по дням.

Например, упрощенно, если
a. 90% юзеров делают действие через месяц, а 10% юзеров делают действие через 7 дней, то на графике для любого нового юзера для месяца должна стоять высокая вероятность сделать действие, для 7 дней небольшая вероятность сделать действие, а для остальных дат еще меньше. Но это упрощенно, конечно, без учета других параметров действия юзера.

Фактически, мы ищем не вероятность действия, а вероятность, что отдельный юзер совершит одно действие.

мне советовали вот это:
1. для временных рядов критично, что замеры делаются через одинаковые периоды времени, поэтому, возможно, нужно будет создать поле ""совершил действие"" и делаем его равным 0, если не совершил действие для данной даты, и 1, если совершил.
2. Возможно, поможет порядковый логит с временной зависимостью, в котором отклик - категориальная переменная.  Включаем в него предиктором время и можем предсказать вероятность действия в зависимости от времени. А там можно и как со временными рядами работать.",
"GTX 980, производителя не знаю теперь как узнать",
А где второй внизу радиатор? Длинный вертикально установленный вижу,
"Народ, seq2seq для NTM кто нибудь юзал? а то на гитхабе из коробки ни у кого нет.",
пластиковые крышки и металлические радиаторы при нагреве по разному расширялись - как результат протечка,
"Ну и если сильно хочется эмбединги, то почему бы не взять уже готовые и использовать их? ",
"подскажите как я могу в tflearn загружать трейнсет батчами и отдавать на обучение. 
я взял пример с генератором: <https://github.com/pannous/tensorflow-speech-recognition/blob/master/speaker_classifier_tflearn.py>
там так:
`batch=data.wave_batch_generator(batch_size=1000, source=data.Source.DIGIT_WAVES, target=data.Target.speaker)`
`X,Y=next(batch)`
и потом
`model.fit(X, Y, n_epoch=100, show_metric=True, snapshot_step=100)`",
кто ж теану то возьмет под свой контроль,
а почему теану не удобно?,
но закрытые так что всем пох сколько и каких,
ну кароч у них есть свой внутренний чисто для дл и как минимум один,
"второе, по моему опыту, лучше работает, чем всякое моделирование. можно натравить xgboost и анализировать как любые структуры данных, так и любые агрегаты временного ряда (сколько тех или иных действий совершал, когда и проч.)",
"Я имею в виду не зачем запустил, а что конкретно, какой из ганов",
"да, я как раз и спрашиваю, какой профит от спарсности коэффициентов? есть что-то кроме ускорения вычислений?",
"я так понимаю вопрос состоит в том, почему L1 лучше, чем L2",
"тут я имею ввиду тексты, на обычных dense фичах - как повезет",
ну еще добавлю что при L1 на сентиментах еще можно смотреть какие слова положительные а какаие отрицательные,
"хех, почему по сентиментам у всех такие заоблачные точности? я тут попробоал посчитать на твиттере, у меня что-то вроде 0.78 выходит",
"у меня датасет в почти 2 миллиона постов, как скейлить?",
Никто не продает macbook air 2012-2015 годов? Или кто знает способ максимально экономно приобрести такую модель на вторичном рынке?,
"Народ, нубский вопрос. seq2seq для создания переводчика из корпуса параллельных фраз сейчас какой самой крутой считается? тот, что идет из коробки в tensorflow?",
"кавычки меня как Персоны смущают первым делом. 
есть там ручка какая-нибудь, которая меня от этого избавит? или надо своими ручками токенизировать и фильтровать пунктуацию?",
"а кстати есть <#C2LJA6VP0|sberbank_contest>, где орги сидят",
"Коллеги, а кроме <http://gen.lib.rus.ec/> где можно книгу поискать? Знакомый просит найти вот это: <http://www.e-elgar.com/shop/handbook-of-entrepreneurial-cognition>",
"&gt; у нас табло + лукер
если не тайна - в какой примерно пропорции?",
"ё, <https://demo-pnmt.systran.net/production#/translation> ""Make America great again!"" переводит как ""Сделайте Америку великой!"" Лучше чем Google Translate...",
помню когда читал сорсы вроде как раз у addmv видел примеры использования вообще не особо совпадающие с доками,
"<@U04URBM8V> Саша, а какая тема вечерней встречи?",
"<@U09JEC7V0> Будет определяться незадолго до анонса, либо на самом мероприятии. Это аналог завтраков, но для тех кто по тем или иным причинам утром в стреду не может.",
"Ну вроде OpenNMT неплох, но похоже, что корпус они для демо взяли так себе. Актанты не всегда ловит, например ""and we owe her a major debt of gratitude"" перевел как ""и мы в долгу перед ней долгом благодарности""",
"А какая интуиция говорит, что л2 регуляризация не нужна когда есть батчнорм? Мне совсем неочевидно <@U06J1LG1M>",
"где-то еще статья проскакивала, что в дип лернинге про регуляризацию надо по-другому думать, не так как в “классических” моделях: <https://arxiv.org/abs/1611.03530>",
"Я на днях как раз думал, что L2 регуляризация весов бессмысленна, если потом к слою применяется BN",
"Кстати, понятно, почему BN - это хорошо. Кто может поделиться опытом / литературой на тему, когда  BN - это плохо и когда его втыкать не надо?",
"А по опыту, примерно, где граница между совсем маленький vs можно применять BN",
"<@U0AD1L5NC>,  да, статьи быстрее и информативнее, особенно если уже более или менее в теме. но я старомодна: учебники тоже читаю, особенно когда надо с нуля что-то понять.",
"Когда хорошие, а не Курсера там",
А как он по-английски называется?,
"спасибо :slightly_smiling_face: я так, на всякий: мало ли, было что-то, что особенно запомнилось. как леции по линалу от стренга :slightly_smiling_face:",
"Почему вокруг тф больше хайпа, чем вокруг теано?",
почему не для продакшена тогда?,
"под ""продакшеном"" ты имеешь ввиду когда нужно гонять inference на натренированной модели, более-менее в реалтайме?",
<@U3NE8PJ04>: а как же Колмогоров-Фомин? :be-a-man:,
<@U04ELQZAU> знаешь какая слава у русских учебников в мире? ,
"Привет, подскажите какой датасет вы используете для Sentiment Analysis? У меня есть 3 варианта:

1) Использовать AFINN-11, где для каждого слова есть оценка (score). Например, `{ 'like': 1, 'good': 2, 'bad': -1, 'terrible': -2 }` и сам сентимент можно посчитать например *positive &gt; 0, neutral = 0, negative &lt; 0* (<http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010>)
2) Использовать встроенный в NLTK датасет `from nltk.corpus import movie_reviews`, где есть 2 сентимента (позитивный и негативный), и тут получается просто идет поиск слова по словарю из этого датасета.
3) Еще нашел вот такой датасет - <http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/> , но пока не разобрался как его использовать. Тут все 3 сентимента",
"А если я хочу по 5 фолдам проверить, то например выбирать 2 фолда на роль Т1, 2 фолда на роль Т2 и оставшийся фолд на роль Т3 ?
Как потом подготовить этот алгоритм для боевых условий? Когда есть только train и новые поступающие данные? Просто разделить train на 2 таблицы и на них обучиться?",
"<@U3B7T7V8W> выглядит как хрень, там же просто какой-то список кейвордов, притом достаточно ограниченный зашит, судя по сорс коду",
"<@U0QMJJ3SL> Пытался сказать что, все кто были сегодня на завтраке. поставьте :coffee: под этим сообщением.",
"А кто что может рассказать про выбор/подбор количества тем при тематическом моделировании? (например с помощью того же LDA) Притом не ручной — тем же может быть много. Кажется что всякие перплексии позволяют оценить качество модели, но так как зависят от количества тем не позволяют сравнить две модели с разным количеством, или я не прав?",
Есть ещё вариант интерпретировать топики как кластеры (по наибольшей вероятности принадлежности) и тогда использовать метрики качества кластеризации — но меня немного смущает такое допущение,
auc выгодно усреднять как раз таки,
"<https://opendatascience.slack.com/archives/nlp/p1484057153000806>
<@U1BAKQH2M> Привет, а реализацию doc2vec какую использовали? не gensim случайно?",
"<https://opendatascience.slack.com/archives/nlp/p1484133775000856>
да есть кейс где демонстрируется высокая точность с помощью doc2vec но нашёл на stackoverflow парня который Tfidf + LogisticRegression на тех же данных получил больше",
хз как свертки на коротких текстах будут работать,
"Можно mean inverse rank. Поможет в случаях, когда в ансамбле были модели с разными весами для одних объектов и вероятности перекошены. Для ROC AUC отлично работает",
"Ченнел, как принято распознавать капчи?",
А какое железо нужно чтобы такое генерить за адекватное время?,
но у меня есть ощущение что процентов 80 времени сеть тусовалась в аттракторе где то,
"пока то что больше всего опечалило это то что если фиксировать все кроме одной размерности и ее изменять, то картинка особо не меняется, плавный переход от одной формы к другой это какая то нелинейная траектория в скрытом пространстве, которая по идее должна быть disentangled в процессе обучения",
почему должна быть disentangled ?,
"вроде как вся суть representation learning это disentanglement степеней свободы в латентные факторы, ну т.е. ожидается что то типа как конструктор рожи, типа <https://www.youtube.com/watch?v=7OjAVDeBbLs>",
"а пока видно каждая размерность добавляет какой то слой по всему изображения, который похож на эннную компоненту ПЦА, далеко не первую",
кароче человеческому глазу он кажется как шум,
"Я помню статью где описывалось обучение language model, где вектор слова получался через CharCNN, чтобы как раз обойти ошибки в словах. Работало очень хорошо. Но это пожалуй единственное, что я помню из статей.",
"вот есть два стула, модели в смысле, у одной одни нейрон отвечает за открытость рта, а у другой открытость рта это такая траектория когда некоторая нелинейная функция f(x1, x2, …, х10) от значений 10 нейронов удовлетворяет каким то ограничениям

какую ты предпочтешь? наверное первую

это как мне видется понятие развязывания размерностей, как бы во второй модели относительно пространства пикселей очень даже развязана фича открытость рта, но у первой еще более",
"ну это так чисто мое видение, а каких то определений я не видел",
лемпицкий вроде упоминал про всякие адовые ганы где конкретная координата в эмбеддинге отвечает например за поворот головы,
"да я согласен с примером про улыбку, где то есть граница разумного, что например открытость глаза описывается одномерной фичой, а улыбка нет",
"например про эмоции, <http://cbcsl.ece.ohio-state.edu/cvpr16.pdf> тут вот ссылка есть на какую то статью, где физиолог описал весь спектр человеческих эмоций всего 50-100 факторами лица, которые можно обозревать",
"не могу достучаться до судоера на сервере, нет пермиссий писать что-то в директорию с кудой",
поставь куду в хомяка и пропиши в PATH и LD_LIBRARY_PATH,
"я же так себе сейчас еще и тф накатить смогу, с 8 то кудой",
не знаю правда зачем я туда воткнул ембеддинг для символов :more-layers:,
"Обучаю LSTM-сеточку.
Последовательности разной длины, для каждой надо предсказать класс (0 или 1)
Написал все это на керасе, объединил объекты в батчи в зависимости от длины последовательности. Получилось 44 батча с разными длинами.
Далее задаю число эпох (например 1000)
Внутри каждой эпохи обучаюсь на всех батчах в случайном порядке, в конце эпохи валидируюсь.
Первый вопрос - правильно ли я выстроил подход к обучению?
Второй вопрос - почему-то у меня лосс скачет и не устаканивается. Ведет себя практически как случайный шум в окрестности некой величины (валидационное значение - pr_auc тоже скачет)",
"Запедить - тогда в память не влезет. Последовательности есть длины 2, а есть длины 150
loss - binary_crossentropy
optimizer - Adam(lr=0.01)
Какой вообще оптимайзер брать?",
аутпут классов 2 или как у автора 4ре?,
тогда ждём результатов… а какого размера выборка? и какое соотношение классов?,
"<https://github.com/dmlc/mxnet/blob/master/tools/im2rec.py#L132> - единственное место, где используется этот параметр",
"Спасибо, но все равно непонятно, как же png прочитать. В руководстве <https://github.com/dmlc/mxnet/tree/master/example/image-classification> точно заявлена поддержка этого формата.",
"`img = cv2.imread(fullpath, args.color)` на 109 строке как бы намекает, что без pass_through прочитается всё, что умеет читать OpenCV. А умение читать PNG зависит от того, собирался ли OpenCV с его поддержкой.",
"это уже почти так же хорошо, как и кроссвалидированный логрег на словах с подобранной предобработкой",
<@U09S9916X> какой у тебя результат получился в итоге?,
"на третьей эпохе перебил логрег чутка как на валидации, так и на тесте, выглядит приятненько, надо дальше доучивать и посмотреть когда оверфитнется",
понятно что как определюсь какую сетку буду обучать - сделаю новый сплит и все зафиксирую для максиально корректного сравнения,
Не поверю. Чот маргинализовать распределение из совместного или же взять сложить три маргинальных вероятности и поделить на три это не одно и тоже,
"Формально учил классификацию?
На вход слово (в каком виде) на выходе softmax на 4 класса?",
"<https://github.com/petrovich/petrovich-eval> - незнаю только, как это было сконструированно",
"Я просто думал, может быть, тут кто-то знает как по-другому можно на нормализацию зайти?",
"склонение фамилий, кстати, еще от рода зависит как минимум",
а как сетка эту проблему решит?,
"т.е. мне понятно, как этот подход поможет поймать предлог перед ФИО, но если, например, падеж по отчеству определяется однозначно, то использовать его автоматом для фамилии и имени сетку может быть сложно натренировать",
"Я только встаю на путь к нейронкам и всего прочего 
Сейчас пока веселюсь с ner 
Научился выделать персоны
Теперь интересно научиться их нормализовывать
Вот пришел сюда за советом, в какую сторону мне копать, так как то, что придумал я, мне не понравилось",
"а можно для совсем тупых еще раз обьяснить в одном посте обсуждаемый вопрос? а то я перечитал и так и не понял до конца какая в итоге задача, а интересно же",
"в качестве бейслайна можно взять какую-нибудь эвристику, ну там “распарсить все pymorphy2, предпочитать разборы как имена/фамилии/отчества, взять самый частый падеж в сущности, просклонять все слова, как стоящие в этом падеже""",
<@U1BAKQH2M> я спросил знает ли кто-нибудь как нормализовывать ФИО,
"По сути я еще одну сетку обучаю, котораю будет нормализовывать токены, которые веделенны как персоны",
Вопрос следующий. В новой версии кафе теперь как то через одно место инпуты на входы подаются. Вопрос: как? ) В документации найти не могу как получить доступ к блобам уровней.,
"<@U30Q72KLJ> , как для Net получить доступ к layers, ведь layers   const vector&lt; shared_ptr&lt; Layer&lt; Dtype &gt; &gt; &gt; &amp; 	layers () const",
"А расскажите мне немного про современные возможности генеративных моделей, какие рубежы взяты?",
Какого примерно размера выборку нужно иметь чтобы натренировать модель генерировать подобные?,
"гайс, слегка холиварный вопрос, а кто какой питон юзает решая задачи связанные с текстом? Ну или может R. /vote :two: :three: :r:",
а какие в :two: юникодопроблемы?,
"<@U053R9RS6> 
&gt;А что является входом для сгенерированных картинок?
вообще не только шум, модель с гумом она же самая бесполезная, тк не дает возможности вывести фичи из образа; есть VAE и AAE которые позволяют делать вывод фичей и манипулировать представлением; вае и аае это модели как две стороны одной монеты

&gt;А на счет размера выборки?
ага нужно много данных

&gt;Какого примерно размера выборку нужно иметь чтобы натренировать модель генерировать подобные?
в публикациях особо не выбирают датасеты, все предопределенно системой, так что трудно судить о размерах когда все юзают одно и то же; так что только пробовать самим и делиться тут результатами -)

&gt;А там не работает дообучение какое-нибудь?
и дообучение норм и предобучение, например в какой то статье они сначала обучают денойзинг автоенкодер, а потом уже ган; в некоторых статьях энкодер от обученного гана забирают как начальную инициализацию для сети классификации и потом уже супервайзд лернинг на метки

<@U0AD1L5NC> 
&gt;Я слышал, что получается тренировать обратную функцию для GAN’ов
а что за обратная функция для ганов? если ты про то что бы из образа генерить фичо то это ААЕ",
ну это наркомания какая то,
"господа, так скажите свое мнение по этому поводу
```
Instead, for our final models, we provide noise only in the form of dropout, applied on several layers of our generator at both training and test time. Convolution-BatchNormDropout-ReLUlayer with a dropout rate of 50%.: <https://arxiv.org/pdf/1611.07004v1.pdf>
```
как думаете им фортануло или реально дропаут после бн имеет смысл? я не то что бы вижу какие то препятствия, просто не стандартное решение для текущего тренда",
"Коллеги, не подскажите?

Возможно ли (если да, то как) разделить обученную модель на данные не в бинарном виде, содержащие коэффициенты, и исполняемый модуль. (согласно этой статье <http://machinelearningmastery.com/deploy-machine-learning-model-to-production/> можно развернуть сервис, если ли опыт?)

Что-то вроде pickl’a, joblib’a, только чтобы файл был доступен для понимания человеком, например, структурированный какой-нибудь (json, xml и т.д.).

Смотрел в сторону PMML, но, как я понял, можно только экспортировать модель, а предсказывать по экспортированному файлу уже нельзя (так ли это?)

Спасибо, за помощь!",
"Я так делаю, когда тренирую модель в питоне, а потом в джаве использую",
"вряд ли <@U0DA4J82H> претендовал на какую-то абсолютную истину, когда писал это. просто не приходит в голову, как в читаемом и людьми, и машинами виде хранить коэффициенты случайного леса :confused:",
"мне кажется, что в вопросе <@U3QHHC0A0> перемешан технический вопрос про дамп и эпистемологический про трактовку моделей.

дампать можно как угодно, начиная от простого pickle
трактовка - отдельный большой вопрос, который не решен в общем виде для сложных моделей, и трейдофф сложность модели vs трактуемость модели по-прежнему актуален.

насчет трактовки может пригодиться <https://github.com/marcotcr/lime>, но очевидно, что это далеко не silver bullet",
а там написано как он их делал?,
"чот не пойму, деконвы там что ли как то приво сделаны, отжирает памяти больше чем ожидается",
"ужасно интересно, велик ли выигрыш с mxnet в производительности, на каких задачах и в каких ""окружениях""
держите в курсе, пожалуйста : )",
"&gt; держите в курсе, пожалуйста : )
как только руки дойдут сделать честное сравнение отпишусь. Пока выглядит быстрее caffe",
"А еще такой вопрос, кто просматривает тексы в чатиках (лучше игровых), вы как-то аджаститесь к тому, что там часть сообщений с кучей орфографических ошибок? Может словарь какой есть.",
"Просто из интереса, а вообще в каких случаях может быть такое, чтобы лосс вёл себя как шум (при нормальных данных и коде без косяков)? Кроме, очевидно, гигантского lr",
а что значит вести себя как шум?,
"Значит, график выглядит как шум, случайный шум вокруг точки. Как писали выше",
"Мб, когда есть какая-то симметрия в данных что-то такое наблюдается или ещё есть какие-то хитрые случаи",
"Все таки по совету чята пробую перейти от батчей разной длины к зеро-паддингу последовательностей, чтобы была стохастичнойсть
Такая проблема - входные данные у меня имеют следующую форму `(batch_size,max_seq_len,vector_size)` (из названий думаю понятно что это за параметры).
Соответственно это сделано при помощи `pad_sequences`
Но согласно документации(и у меня возникает ошибка) `Embedding` ждет следующее:
`2D tensor with shape: (nb_samples, sequence_length).`
А куда собственно запихивать размерность вектора?",
<@U0XF4GAM8> а почему на входе на embdding третья размерность vector_size - что это такое?,
"а, а тогда зачем embedding? он нужен чтобы сделать как раз векторы из последовательности значений categorical фичи (слов например)",
я не помню как иначе из коробки в керасе делать правильную маску ,
"убедись только что у тебя в реальных данных не может быть ситуации, когда все значения 0",
"У меня вопрос.

Допустим, я хочу в качестве фана выложить демку модели на heroku.
Никаких требования по perfomance нет, как и по красивости кода / интерфейса.
Как бы вы подошли к этому вопросу?

В моем воображении это такая html страница где есть поле аплоад с пост запросом. Это все внутри передается в питоновскую функцию, которая внутри вызывает модель и генерит результат. Потом это все выплевывается с каким-то форматированием обратно.

Где тут можно срезать углы? Где, наоборот, высокий шанс закопаться в ненужную реализацию? Есть ли что-то специфичное для моделек DL / изображений на входе?",
были проблемы когда хостил бота на тф ,
"1. Хостить на хероку кажется будет проблемой (там инстанс гаснет без запросов, верно?)
2. Модельку можно завернуть в прожку, которая в вечном цикле спрашивает новые таски какую-нибудь очередь сообщений.
3. Запросы с вебки падают в очередь, как будет готовы -- показываются.",
"<@U0H7VBQQ1> помню,мы с тобой уже обсуждали это, как проще всего хостить без тф-сервинг, никак не могу найти поиском, не помнишь в каком канале это было?",
"<@U14GG4E69>: а есть ли какие-нибудь инсайды, почему <https://github.com/NVlabs/GA3C> до сих пор не выложили?",
"Как это в веб-демку завернуть, можно посмотреть в стандартных примерах для caffe",
"привет!

А подскажите, если вы знаете, как корпорации переводят значения roc_auc по моделям в экономическую эффективность? 

Вот, например, проводится соревнование на кэггле, метрикой которого является roc_auc. Например, мы пытаемся предсказать churn. Выбирается лучшая модель: roc_auc =0.7, скажем. Компания решает сконцентрировать усилия на 100 клиентах с наибольшей вероятностью оттока. Но, я так понимаю, вполне вероятно, что модель с roc_auc =0.6 покажет те же самые 100 клиентов.

То есть, топ по оттоку может быть не сильно связан с метрикой roc_auc?",
"а, я попутал, там вроде был какой-то таск, где давали скрипт, который сам с твиттера что-то стягивал",
"&gt;будто кого-то волнует что там твиттер запрещает
Ну так как это официальное соревнование, то они должны подчинятся(",
"разметку наверное пробовать по эмоджи делать, как встенфордском датасете",
"ага рассисткий юмор думаю имеет смайлы автора, в итоге рассисткий юмор моделькой будет определяться как позитивный :slightly_smiling_face:",
"test_str = ""its betifl ay""
все еще классифицирует как позитив",
"Вопрос про сетки. Есть сеть, которая на выходе даёт вектор. Я хочу написать целевую функцию, которая наказывала бы за большое расстояние между максимумами в предсказанном и правильном ответе, т.е. синоним к `cost(y_pred, y_true) = abs(argmax(y_pred) - argmax(y_true))`.
Теория подсказывает, что так делать нельзя, потому что недифференцируемый cost — это плохо.
Можно ли это обойти (как-то же при max pooling градиент считают).
Почему это не обходится само, когда я пишу в Keras? ```def custom_cost(y_pred, y_true): return tf.to_float(K.abs(K.argmax(y_true) - K.argmax(y_pred)))```

не уверена, что это вопрос в <#C047H3N8L|deep_learning>, но могу и туда пойти.",
"ну или я не знаю, как его дифференцировать",
"Можно считать argmax поэлемнтным умножением на матрицу из нулей, где 1 стоит в позиции где находится максимум",
"Но ведь DQN так и обучают, когда берут argmax по действиям...",
а как считается градиент tf.select ?,
"Ну т.е. как accuracy аппрксимируют логлоссом, так и эту метрику чем то дифференцируемым надо аппроксимировать",
"и в итоге ненулевое обновление весов уходит только через ту позицию в векторе, где был взят argmax?
тогда должно обучаться, просто медленно.",
Надо смотреть на него как на one-hot вектор,
"в том смысле, что предсказывать значения всех элементов вектора и MSE как таргет?",
вообще выглядит интересно как минимум,
"я пытался делать так:
берем max, затем везде где элемент != max ставим нули
получается one hot",
"вот я и спрашиваю, как select или подобные штуки диффернцируются",
"но вообще да, почему нужен именно такой лосс?",
"ты как предсказываешь, по дням, годам, сколько возможных позиций?",
"<@U1BAKQH2M>: кривое решение, ошибка за 10-80 будет такой же, как и за 79-80",
"52, но порядок ошибки действительно важен, как <@U04ELQZAU> сказал. ошибиться на неделю лучше, чем на полгода :smiley:",
"&gt; скорее всего если у тебя модель боль менее адекватно себя ведет софтмакс будет +- размазывать вероятность вокруг правильного ответа
После того, как модель уже обучится? Возможно, но правильный inductive bias поможет ей сразу учиться нормально",
Где то ещё видел про софтмакс на 256 классов вместо регрессии по значению  цвета в пикселе,
"Это было в PixelRNN/PixelCNN (не помню, как оно называется)",
"<@U04422XJL> А можешь, пожалуйста, подсказать, почему при такой схеме в общем случае не будет переобучения? 
Подгонка гиперпараметров алгоритмов A, B на датасете T2 при достаточном количестве гиперпараметров должна приводить к переобучению, разве нет?",
"какие то у меня подозрения есть, но я туповат лезть в исходники теаны -)",
а расскажи как конкретно ты деплоил,
"ага, спасибо, керас+тф+фласк - как раз мой случай",
"а что можно интересного почитать про предварительное обучение в RL?
вот есть у нас, например, логи того, как это делал человек, хочется на них агента инициализовать
по каким словам гуглить? в принципе итак понятно, как это делать в самом простом случае, но может интересное что есть",
"про dqn надо подумать, как лосс правильно задать. А вот если бы обучался актор-критик, например, то тогда то что <@U04422XJL>  выше описал. Альфа го так и обучали, сначала на играх экспертов, а потом дообучали с помощью RL.",
Мне два дня представители Google на конференции расписывали какой крутой TF и я решил попробовать...,
Вообще tf если использовать tensorflow slim дает почти такой же уровень абстракции как keras,
А может быть вот что еще. Theano выбирает какой метод конволюции работает лучше всего - его и использует. У TF такая фича есть?,
"Откатился обратно на Theano. Пока мой вывод такой - на single GPU, как backend к Keras, TensorFlow - слабоват.",
<@U3HM4KY14> <@U0FEJNBGQ>  спасибо за ссылки! как раз такие стартапы и интересны,
"<@U2R6C4A2H> как насчёт <https://www.climate.com/> , родоначальника xarray?",
"Где можно почитать поизучать про архитектуры рекуррентных сетей ? Например мне не очень понятно когда стоит использовать один слой lstm, когда несколько , что ещё и когда стоит добавлять между ними. Да и во всех примерах что я нахожу модель как правило состоит из одного lstm слоя.",
"А подскажите мне, пожалуйста, батчнорм, когда его есть смысл добавлять, а когда нет? После (между) каких слоев? Какая в этом есть логика?",
"ща все добавляют после сумматора перед нелинейностью, я даже где то читал обоснование, но забыл )",
Вроде не ухудшилось как минимум,
Может keras просто выкидывает слои там где они смысла не имеют?,
Сделай как тут и все,
"всмысле у меня нет ответа на вопрос почему ""Пуллинг и дропаут не добавляют нелинейностей” = “батчнорм не нужен""",
"Тут, вроде, было про то, куда вставлять бн",
"если по статье бн то вроде да, они там на сигмоиде рассматривают и выглядит логично, типа сдвинем туда где сигмоид на линию похож; имхо как то не вяжется это с релу",
"и получить статьи, разложенные по рубрикам. А в идеале — делать так каждый день автоматически и постоянно усложнять условия. Собственно, вопрос: какую библиотеку/утилиту правильнее всего использовать? Язык абсолютно не важен.",
"если рубрики (=классы) заданы, то зачем нести LDA, а не классифицировать в лоб?",
"&gt; зачем нести LDA, а не классифицировать в лоб
Вот хороший вопрос, кмк",
"Я человек простой, вижу сложное слово — лезу в гугл. Scikit - это же то что нужно, да? Это как раз про clusterization и multilabel classification?",
"А как использовать batch normalization в shared сети? У меня есть CNN (inception v3), обучаю через triplet loss. Получается, что каждую итерацию через CNN проходит 3 батча. keras говорит, что в таком случае нельзя посчитать усредненную статистику (то что называется mode=0), а можно использовать только mode=2, в котором статистика в предикте тоже считается по батчу. Как можно это обойти? Хотелось бы, чтоб при предикте уже усредненная статистика использовалась.",
"А как считаете, имеет ли смысл жахать кластеризацией поверх LDA? (если, интересно, собственно, кластеризовать документы) Или просто выбирать по максимально вероятному топику?",
"А зачем кластеры после выделения темы, ну если только не для рубрикатора с большой вложенностью?",
"что там на практике - не очень понятно, т.к. совершенно не ясно как оценивать",
но как бы если просто думать об топик векторах как о каких-то абстрактных данных странно думать что наилучший способ их кластеризовать - просто выбрать наибольшую координату,
"но я вообще не шарю, сейчас кто-нибудь ворвется кто нормально матан понимает и пояснит суть",
"Придумала вчера дифференцируемое приближение к `abs(argmax(y_pred) - argmax(y_true))`.
`argmax(y)` можно представить как `sigmoid( (y - max(y)) * 10 ) * 2 dot [1..len(y)]`.
Но там сразу две неприятности: у float32 не хватает точности (у float64, впрочем, хватает), а магический параметр 10  быстро насыщает сигмоиду (если я правильно понимаю процесс).",
<@U1NMKU9DY> А как же вариант с конвертауией в регрессию?,
"друзья! ни у кого не было такой ситуации, что оптимизатор theano при компиляции вычислительного графа некоторые из операций оставлял на хосте и не выполнял на gpu?",
"не было (вроде) такого, но не удивлюсь если какие то экзотические оставляет",
"ой, а как это посмотреть??",
и где выводиться будет это?,
"ок, понял, спасибо. 
&gt; может, например, загубить градиентный спуск
мне кажется я периодически из-за этого проблемы наблюдаю, но не представляю как при обучении балансированные минибатчи семплировать",
"Коллеги, а может кто-нибудь привести пример хорошей статьи (или просто понравившейся статьи),  где описывается процесс feature extraction применительно к какой-нибудь конкретной бизнес-задаче?",
"Возможно, тем, что лучше убирает в 0 не-максимальные элементы. Но насколько лучше, как это “лучше” измерить и правда ли это вообще — непонятно.",
"А подскажите какую модель взять которая есть pre-trained на imagenet для keras + tensorflow? Ограничения ~300МБ на память и на 100 мб для файла с весами
InceptionV3 не пролез по обоим критериям",
"спасибо, сейчас попробую, как гитхаб поднимется",
<@U0K4S432S>: так а какое отличие в коде вызывает эффект?,
"<@U064DRUF4> А можно как-то встроить в пайплайн собственный механизм кросс-валидации? Просто так фолды нарезать не всегда получается, например, когда временные ряды в данных",
Я не совсем понимаю как влияет батч,
"Не знаю какая там доля модели, но как минимум все что больше 500МБ",
"а как же, разве получится не подгружая в память?",
"Как вариант конвертировать все в tf.float16, и посмотреть на граф, нет ли там лишнего чего-нибудь.",
"Я вот тут посмотрел, что при файн-тюнинге сначала тюнят пару верхних слоев, а оптом уже все остальное
А не пробовал ли кто-то обобщать, например тренировать верхний слой одну эпоху, потом два верхних еще эпоху, ну и так далее
Чисто интуитивно кажется если первое имеет смысл, то и второе. Но наверное это мерять надо, может кто видел статьи?",
"Всем привет. Упрощенный пример: у нас есть сайт на который приходят пользователи и оставляют свои контакты. Есть классификатор, который определяет какова вероятность того, что этот пользователь у нас что-то купит. От классификатора можно получить саму вероятность от 0 до 1 и класс 1-купит, 0-не купит. По результатам классификации мы уделяем намного больше внимание, например звоним, тем, у кого класс 1. На класс 0 практически забиваем. 
Вопрос - как определить качество этого классификатора по логам? Целевая переменная купил/не купил.
Получается что дальнейшее наше поведение зависит от предсказанного класса. И даже если классификатор выдаёт результаты рандомом будет казаться что он корректно работает, так как мы уделяем больше внимания классу 1. ROC_AUC будет &gt; 0.5
Пока пришло в голову измерить ROC_AUC внутри каждго из классов. Но что делать дальше  ?
Я понимаю что можно было для части пользователей ничего не менять в зависимости от класса(контрольная выборка), но что ещё можно придумать?",
"как правило, если поставить такую задачу в терминах обычного эксперимента, многое проясняется :eyes:",
"но это на грани добра и зла, как мне кажется",
"начал делать файн тюн, а картинки препроцессены не так как в оригинальной сети",
"Ну если в лоб и только по логам, то можно понять, например, качество классификатора на промежутке с 0.5 до 1.0 с учетом того, что человеку позвонили. Хотя бы посмотреть, вероятности откалиброваны или нет. Вдруг это что-то покажет - ну, например, что оценка - просто шум (и тогда, возможно, лучше звонить и людям с маленькой оценкой), или что только люди с большой оценкой что-то покупают (может, тогда порог поднять, раз людям с 0.5 хоть звони, хоть не звони - без толку?). По промежутку с 0 до 0.5 можно посмотреть, например, что происходит вблизи 0.5 - оценка ведь все равно “шумная”, а порог точный - можно глянуть, как выглядит окрестность справа и слева, есть ли скачок (если есть - вероятно, звонки дают эффект).",
"&gt; Для тех, кто не сможет присутствовать мы организуем онлайн-трансляцию.",
"Подскажите, пожалуйста, какие есть подходы для бинарной (одинаковые/разные) классификации в задаче сравнивания двух наблюдений?",
"<@U0JHK9001> можно погуглить например behavioural cloning
<https://github.com/parilo/steering-a-car-behavioral-cloning>
как пример",
"всем привет, подскажите терминологию, плз
есть распознаватель (нейросеть, например), хочется понять, фигню он сказал или нет, т.е. можно ли результат прямо в базу заносить, или надо человеку дополнительно показать для подтверждения.
какой термин (по-русски и по-английски) нужно гуглить по такой задаче и как это вообще называется? мы это в узких кругах называем acceptance/rejection/reliability, но что-то не гуглится
или это просто confidence и все?",
"есть еще тема с полуручной классификацией, когда человеку показываются самые сомнительные случаи",
"Еще один вопрос, а на какой задачке поупражняться в DL подходе для текстов?

Хочется, чтобы:
Постановка задачи была понятной обычному человеку
Задача решалась хорошо
Был датасет в открытом доступе подходящий для обучения модели
Не нужно кластера видеокарт, чтобы обучать модели",
"А какой там сейчас уровень качества, навскидку?",
"как идея — взять дамп сообщений реддита, например (они ежемесячно дампы выкладывают)
попредсказывать рейтинг сообщения, или поста, или любую метрику",
"Но лучше дождись комментарии от людей которые этим по настоящему занимаются, а не неделю как",
"вроде толи год, толи 2 назад на физтехе слили базу фоток студентов на пропуска и сделали божественный сервис, где нужно было рандомные фотки девиц от 1 до 5 рейтить, божественный датасет получился бы, но я не знаю где людей искать, у которых он мог бы сохраниться, шифруются",
"Господа нейронщики, всем привет! Сейчас делаю на работе проект по нейронным сетям (я работаю в финансах). Так как не считаю себя профи, а только начинающим, хотел бы задать умным людям вроде вас несколько вопросов:

1. Есть ли какие-нибудь удобные и мощные симуляторы нейронок для людей, которые не очень шарят в программировании? Это относится не ко мне, а к одному коллеге.
2. Если есть временной ряд данных без нижней и верхней границ типа float, то какую функцию активации лучше использовать для предсказания
 на основе нее Y-ка?
3. Так и не разобрался: в нейронках Y обязательно должен быть boolean, или есть варианты, когда можно предсказывать его как float?
4. Какие основные показатели кроме standard error и ginni существуют и реально эффективны?
5. Общий вопрос: кто-нибудь здесь тоже связан со сферой финансов, в частности asset management? Было бы интересно пообщаться.",
"&gt; слили базу фоток студентов на пропуска и сделали божественный сервис, где нужно было рандомные фотки девиц от 1 до 5 рейтить
Я такой фильм видел :not-sure-if:",
"<@U0U2ENJ4U>, спасибо!

2. А какую в данном случае можно сделать предобработку? Пример ряда данных: -1052.2, +2045.8, +155.8.
4. Задача - прогнозирование с высокой вероятностью или получение наиболее близкого к нужному значения. То есть регрессия, а не классификация.",
"Вопрос, а есть какое-нить дешевое добро типа нетбука, чтоб оно могло работать как более-менее удобный ссш-терминал ? 
Может кто что посоветует ? 
А то когда едешь в метро\другом транспорте 13-14 дюймовый ноут, как то таскать с собой не торт, а с телефона много не напечатаешь, может кто-то чего-нить порекомедовать ?",
"<@U3RGHL0UT>: про нейросети в первом приближении можно думать в maximum likelihood фреймворке: мы моделируем распределение `p(y|x, θ) = p(y|f(x; θ))`, где `f(x; θ)` – нейросеть, конвертирующая `x` в параметры распределения `y`, а `θ` – набор весов этой сети. Соответственно, если `p(y|μ) = N(y | μ, I)`, то мы получаем L2 loss и регрессию, а если `p(y|μ) = Categorical(y | Softmax(μ))`, то получается классификация и cross-entropy loss",
<@U04ELQZAU> знает как объяснить по-простому,
Короче линейную регрессию предлагаешь как мин сумму квадратов делать?,
новый макбук как дешманский ссш-терминал?,
а вот на баду - еще как будет,
"<@U2N853RGF> , скажем так: полагаться полностью на алгоритмы смерти подобно, но как доп советник - известно, что результаты улучшает.",
"Хах, персональный - это именно больше разработка приложений и пиар. Так то да, но конкуренция на таком рынке хорошая.

В настоящий момент я хочу сосредоточиться на создании советника для нужд компании, где работаю. Далее - посмотрим)",
"<@U04725QK7>: для лазаньи есть U-Net где то в ее пулреквестах, или в issues, он не предобученный, надо тренить, но рабочий, я проверял ",
"Я хотел вот это <https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf> на лазанье реализовать, но хз как пойдет.",
"Я не знаю куда постить, но я тут короч в берлине буду кофе пить с фаундером <http://www.jplusplus.org/en/>",
А какая доля в твоем общем датасете от этого датасета?,
я планировал использовать размеченные вручную как финальный тест,
"Ребят, есть вопрос, возможно тупой)
Как с макбуком использовать 4k моник?
Есть LG 27UD58-B
При переводе его в режим 4к все становится очень мелким.
1920х1080 норм",
"Кто-нибудь подскажет, как у Keras'овского pretrained resnet50 выходной слой поменять?

Я делаю вот так:
```
model = ResNet50(weights='imagenet')
model.layers.pop()
model.outputs = [model.layers[-1].output]
model.layers[-1].outbound_nodes = []
model.add(Dense(1, activation='sigmoid'))
    
```
Вылетает со словами:

```
model.add(Dense(1, activation='sigmoid'))
AttributeError: 'Model' object has no attribute 'add'

```",
"я такое делал только со своими моделями, где я точно знал что на входе происходит ",
"<@U1ULFPM0U> 
Так вот, при клике на самую правую картинку “больше места”
Я получаю 4К
Но все пиздец какое мелкое
Как сделать так, чтоб макбук относился к этому монитору как к ретине?",
"<@U32J3CRP1> Привет, а есть сравнение как сеть учится машиной управлять с этими data augmentation и без?
&gt;&gt;&gt; randomize image brightness (-0.3, 0.3)
randomly partially occlusion with 30 black 25x25 px squares
very slightely randomly:
rotation 1 degree amplitude
shift 2px amplitude
scale 0.02 amplitude",
"ну дорожные знаки это же картинки из реальной жизни, а картинки для машинки какой-то симулятор наверно сгенерил, я правильно понимаю? Я почему спросил, не помню, чтобы в атари играх, например, добавляли  аугментации с картинкам с экрана.",
вот интересно было  на синтететических данных тоже лучше делать аугментация или лучше оставить как есть,
"Если что, я находил рецепт как уже для готовой модели оторвать верхний слой",
Надо просто подумать какие данные в синтетических данных будут редкими,
"Я еще хотел спросить. Вот сделали артистик фильтры, а насколько сложно сделать что-то управляемое.
Я вот, например, имею базу оригиналов и отфильтрованных картинок. На выходе я хочу сделать свой фильтр так, чтобы он был с одной кнопкой “фильтр”. Насколько это сейчас реально? Если да, то какую базу пар картинок для этого надо примерно по объему? 100? 1000? 10000?",
"Но исследовано не настолько хорошо, как style transfer",
как я понял он хочет image2image из обычных картинок в фильтрованные,
"прикольная идея, кстати, мне тоже подобное когда-то в голову приходило. только я не могу вдуплить, в каких случаях оно понадобится :grimacing:",
"Картинку я взял из блога Сергея Доли, где он описывал шаги по достижению желаемого в Lightroom: <http://sergeydolya.livejournal.com/920762.html> (там три части).",
"Гипотеза: идея в том, как при помощи DL из исходной картинки получить картинку с “эталонными” характеристиками (яркость/контраст/прочее) с сохранением составляющих (трава должна остаться травой, деревья - деревьями и т.п.) так, чтобы они между собой органично сочетались. Т.о. задача сводится не к банальной попиксельной цветокоррекции, а к декомпозиции на отдельные элементы, их правке и дальнейшей композиции. При этом понятиям “эталон” и “органично сочетаться” вполне можно обучить на примерах.",
"Всем привет. Можете подсказать по Keras, могу ли я там как-то learning rate поменять после того как вызвал `model.compile`? видел там есть арумент `decay` когда создается optimizer, но можно ли как-то напрямую задать?",
" Задача распознавания лиц из заданной базы (работники предприятия). Как обезопаситься от того, что левые люди, которых нет в базе, эмбедятся рядом с настоящими работниками и распознаются как работники предприятия? <@U041P485A> ?",
"классификация тупо, дообучать каждый раз как нового петровича на ферму завезли, так себе звучит ",
"В style transfer сетях обычно используют  фичи VGG, а кто нибудь видел сравнение с фичами взятыми у ResNet или других архитектур?",
"еще вопрос - есть ли какая то стратегия куда ставить батч норм?  Я имею в виду не до/после нелинейности, а в целом - после каждого слоя например, или там между основными блоками какими-то?",
"смысл то выровнить covariance shift, так что тут скорее нужно думать где не нужно его юзать, чем где нужно, напрмиер в сетях-генераторах картинок нет смысла юзать в последнем слое, тк нет смысла там что то выравнивать, наоборот нужно дать пикселям найти какой то свой центр",
"Подскажите плс, зачем в U-net копируют свертки с этапа ""сжатия"" в соответствующий этап апсемплинга? Такое вроде во всех архитектурах для сегментации делают. Почему нельзя просто делать последовательный апсемплинг и деконволюцию без копирования более ранних этапов?",
"Вопрос: а как дебажить нейро-сетки ? А то при обучении лосс падал, а при предиктах получаются подозрительно похожие значение(такое ощущение, что сеть какое-то дефолт значение запомнила и входы очень опосредственно влияют) .",
"<@U04725QK7> это позволяет всякие детали высокого разрешения которые есть на картинке передать дальше, без нужды как то их сжимать",
"У меня созрел вопрос:
В документации <https://keras.io/applications/> есть пример ""Fine-tune InceptionV3 on a new set of classes”
Там упущено как именно мы подготавливаем генератор для обучения
А если посмотреть пример ""Classify ImageNet classes with ResNet50"":
Интересующая нас часть:
img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = model.predict(x)
Видно, что тут явно вставлен preprocess_input помимо ресайзинга.

Вопрос: правильно ли я понимаю, что в генератор для файн тюнинга нужно воткнуть подобный препроцессинг?
from keras.applications.inception_v3 import preprocess_input",
<@U14CTBLFJ> книга о том как *не нужно* делать text mining. Сплошное недоразумение.,
а где 1080 в облаке можно взять?,
"друзья, а у кого есть доступ к твиттеру? там вроде какая-то компания продает куда более свободное, чем официальное АПИ. хочется вытащить твиты про последнюю серию Шерлока. 

(пожалуйста, для всех :capitan_obvious:, если хотите убедить меня в том, что мне это не нужны все твиты и или мне достаточно стандартного АПИ, то давайте считать, что вы это сделали -- поставьте в свою TODO галочку на соответствующее место)",
"но я вот подумал, что может у кого такой доступ есть. и он мог бы им поделиться ради вот разовой операции :menorah_with_nine_branches: :panda:",
"У нас она нормально работала, до какого то момента",
а когда новый сезон тренировок откроется?,
"<@U1VSJ4KF0> по сути это confidense. Как я понимаю, ты по имеющимся данным и имеющейся модели определяешь для себя уровень отсечения, и если вероятность выше, то классифицируешь автоматом (причем ты соглашаешься на какую-то вероятность ошибки), если ниже, то отдаешь на разметку и дообучение, пилишь модельку в прод?",
а на какие числа датафест планируется?,
"ну тип зачем тф-контриб если в гугле есть специальный чувак, уже курирующий отличный фронтенд ",
"спасибо,  как раз то что хотел узнать и по поводу чего сомневался!",
кто искал распродажные китайские нетбуки - <http://www.gearbest.com/promotion-jumper-sales-special-1135.html>,
"Как обычно, ODS первыми узнают о Data &amp; Science. Приглашаем 4 февраля поговорить о погоде! Подробности: <https://events.yandex.ru/events/ds/04/#place>",
так явная симметрия это как раз криптота,
а как записаться на тренировку по машинному обучению?,
"А почему всё сразу в цвете генерится? Можно же в grayscale генерить, да и пямяти раза в три меньше будет бессмысленно гоняться.",
"а подскажите, кто в чем красивые картинки с сетками рисует для презентации? да, помню что такое обсуждение было, сам использовал один раз <http://draw.io|draw.io> для этого, в целом сьедобно, но не очень красивенько ",
"А кто-нибудь <http://spark.ml|spark.ml> использует? У меня вопрос, как после гридсёрча через trainValidationSplit или CrossValidator в pyspark извлечь не только лучший эстиматор, но и лучшие параметры? В scala он хотя бы принтовал их, а в pyspark даже не принтует.",
"Полгода назад здесь выкладывали датасет на 150Гб с поисковыми запросами из Метрики <https://opendatascience.slack.com/archives/datasets/p1468503537000274> . Скажите, ни у кого он не сохранился? Просто ни на Диске, ни у <@U1R8X3V54> его больше нет",
"Господа, а какие мультиагентные среды вы знаете? Которые были бы более-менее простые, и при этом не требовалось бы их переписывать с нуля? В атари играх максимальное количество игроков = 4, хочется что-то достаточно простое, но в котором можно было бы увеличивать число агентов до достаточно большого количества (скажем до 50).",
"Круто, спасибо большое! soccer / malmo – очень классные, если вдруг кто еще что вспомнит, напишите пожалуйста",
Кто вообще подобную задачу решал и как?,
"с каких пор CRF - generative? короче, халтура какая-то",
а какой кейс ты решаешь?,
"А какие современные методы сравнения авторства текстов существуют?Просто зная, что ""Поднятая целина"" принадлежит Шолохову, хочется проверить, что там с авторством романа ""Тихий Дон"".",
<@U07V1URT9> расскажи ему как псин и машины в легких искать :more-layers: ,
<@U1X5Q0S1L> спасибо. А не в курсе как он обучается: просто аббревиатуры запоминает или ещё что-то умное делает?,
"Гайз, есть такая проблема: хочется по 68 ключевым точкам человеческого лица определить углы поворота головы. Какая модель машинного обучения сюда лучше подойдет?  Нужно решить задачку быстро, пусть с небольшой точностью. Сори, если вопрос очень тупой, я сам просто тоже тупой.  В некоторых статьях предлагают использовать только три точки вокруг носа, но это как-то не круто. Подскажите кто что может, Христа ради.",
"У фейзбучка для определения людей лица на фотках поворачиваются в профиль(они как раз 3д модель какую-то строят,  можешь попробовать погуглил,как они это делают.",
"<@U0XR20SA1> спасибо, тоже думал таким путем пойти, но выглядит нудновато при наличии других задач. Пожалуй сначала попробую обучить nltk, заодно корпус поднасобираю. А потом, как надоест избыток искусственного интеллекта, примусь за свой. :wink: 
<@U1X5Q0S1L> Ок, буду пробовать, tnx :+1: ",
а сходимость лучше как раз в pymc3,
"Ну так мы же отклоняем H0, когда p-value маленькое. Т.е. вероятность получить такие данные при справедливости нулевой гипотезы мала. Поэтому, чем меньше p, тем больше данные свидетельствуют в пользу H1.",
А может кто-нибудь объяснить как topicselection регуляризатор использовать в bigartm?,
"У них есть ноутбук на гитхабе с демонстрацией регуляризаторов, но он не раскрывает тему до конца. Есть в readthedocs описание регуляризаторов. Находил ещё где-то на просторах интернета презентацию, где было про регуляризаторы, но именно про этот так ничего и не понял.",
Я вот тоже много где искал,
"<@U1Z78RL3X> интуитивно - это когда площадь устремится к трем сигмам, будет высокая статистическая значимость",
Когда вероятность статистики при H1 больше её вероятности при H0?,
"добрый день! скажите, а как вы думаете, какой метод сработает и является более осмысленным: есть задача расширения клиентской базы / сегмента. У вас есть пользователи, которые вас интересуют (базовый сегмент). И есть куча новых пользователей, из которых мы хотим найти похожих на базовый сегмент.

1. Посчитать для каждого из новых пользователей схожесть с каждым пользователем из базового сегмента. А потом усреднить схожесть и выбрать порог, взять тех, у кого схожесть выше порога.

2. Посчитать для каждого из новых пользователей схожесть с каждым пользователем из базового сегмента, выбрать порог, взять только тех, у кого порог выше.

3. Усреднить (то есть, найти центроид для кластера) все признаки для базового сегмента, посчитать схожесть всех новый пользователей с центроидом, выбрать порог отсечения.

или все три хороши / плохи, зависит от того , чего мы хотим достигнуть? и вообще так делают?

мне еще хороший человек рассказывал про LDA по пользовательским запросам , но я сначала хочу разобраться с более простыми вещами..",
"<@U3P82LKDK> интересно, а как пользователи векторизуются? и как схожесть считается?",
"<@U3P82LKDK> расскажи чуть подробнее про то, как определяется сегмент “хороших” пользователей и среди каких пользователей надо искать похожих?",
"Подскажите, используют ли метод сопряженных градиентов для оптимизации. Почему все уперлись в SGD? 
Вот тут <http://ai.stanford.edu/~ang/papers/icml11-OptimizationForDeepLearning.pdf> коллеги утверждают, что сопряженные градиенты вполне себе",
"<@U1G303UTW> это одна из задач, будут и другие, но сегмент ""хороших"" пользователей - это, скажем, подписчики на новостную ленту. Хотим, чтобы когда какой-то новый человек заходит к нам на сайт, то, если он похож на наших ""уже подписчиков"", то ему бы какой-нибудь pop-up высвечивался (подписывайтесь на новости!). 

про векторизацию пока трудно рассказать, потому что данных пока нет, но данные будут приходить из DMP+ CRM. Я так понимаю, как векторизовать пользователей, мне нужно будет самому придумать + из DMP часть пользователей будет уже с размеченными сегментами (высокий доход, хочет купить машину и т.п.). Я тут новичок :slightly_smiling_face:",
"о, это очень понятная схема :slightly_smiling_face: а отрицательные примеры (не подписчики) - тоже из базы взять (их там, наверное, немного, мы же, в основном, только знаем тех, кто является подписчиком)? Или рэндом из новых пользователей?",
"все, кто не подписался, да. 
если их в базе нет, то начать собирать всех, и брать данные только с того момента, когда стали записывать всех",
"в Pinned Items точно имеет смысл заглядывать :wink:
впрочем, как и на завтраки приходить",
"У меня очень тупой вопрос. Есть исходное дерево небинарное, каждой ноде присвоен свой вес - действительное положительное число. Нужно суммируя веса потомков склеивать ноды дерева пока вес в каждой ноде не станет больше заданного числа А. Как называется этот класс алгоритмов/ проблема в теории графов?)) Че гуглить))) спасибо",
"звучит как простой обход дерева.
просто на подъеме склеиваем вершины пока нужно",
"А как ""обход дерева"" в англонете?",
"Вообще, алгоритмы – этот тот странный случай, когда много полезного можно найти на русском",
Поэтому и ищу модуль. Писать распараллеливание ой как неохота,
Хотя...уже есть идея как сделать на R расчет поддерева)),
"То есть, если можно склеивать вершины, то почему бы не представить всё это дело деревом высоты 2, где листья правильного веса прикреплены к корню?",
"кажется что алгоритм такой - обходишь DFS, когда вершина выходит из DFS решаешь как склеить ее детей. Типа обратный topological sort чтоли",
"Глубина не большая совсем, почему нельзя для каждого объекта сделать вектор, на каждой i-й координате вес. Векторно найти кумулятивную сумму координат и потом опять же векторно where &gt; ",
"а может знает кто, где можно найти русские словари для sentiment analysis по твиттеру? или размеченный корпус...
а еще лемматизарот для R тоже не могу найти, snowballC делает немного не то, что хотелось бы",
на какой период надо нагрузку предсказывать?,
"можно ведь предыдущие значения делать фичами. дальше разные из них статистики вытаскивать. а вообще, как писал <@U0FEJNBGQ> , посмотри/почитай решения соревнований и все будет более-менее понятно",
"в xgboost можно занести время, как оно есть (типа секунды с начала трейна), но надо помнить, что он не экстраполирует",
"вон в случаях всякого энтертеймента ( :youknow: )  показывают, что такие сторонние факторы как футбольные матчи / релизы игр влияют очень сильно на нагрузку, у тебя наверняка могут быть аналогичные факторы, которые ты хрен учтешь",
"ну почему, у тебя есть календарь страны, где это происходит",
"<@U0FEJNBGQ> , а где кстати можно популярно про использование этих рядов читать? Всегда было интересно, но как-то руки не доходили.",
<@U0FEJNBGQ> а как по науке строить календарные фичи?,
интересно как на практике себя ведет,
тут как фичи заносятся ряды фурье,
"сап чат, подскажите, плс, где можно отзывов (с оценками) о мобилках напарсить?",
Хотя мне и не ясно до конца как это происходит,
Ну вот я как раз обернул генератор,
"Теперь когда я делаю model.save()
А потом сразу же keras.models.load_model",
"Не знаю почему ошибка выглядит именно так, но появилась после обрезания картинки, кажется больше я ничего не менял",
"2. Квантизация весов. Позволяет кардинально уменьшить размер, но с потерей качества. В runtime'е, всё равно, требует столько же памяти. Как эту операцию правильно делать я не знаю :confused:",
"вопрос про линейные модели, bias и несбалансированный датасет: я учу logistic regression, в трейне положительных примеров 30%, в жизни 0.03%. В итоговой модели я могу поменять bias (он же intercept) на жизненный, но это не совсем то: я бы хотел чтобы жизненный bias стоял уже при обучении и не менялся, чтобы почти все фичи имели положительные веса (это текст, фич много). Вопрос: правильно ли я хочу, и как это сделать в sklearn или чем-то похожем? Пока нашел только intercept_scaling в LogisticRegression но это выглядит как хак.",
"Чувак, который указан как контакт, чтобы инвайт в слак получить, из FAIR",
Глава 21. Ну и на обложке как раз она и нарисована :-)),
"Подскажите, я поставил ALE, и хочу заюзать игру оттуда в другом коде, куда мне надо просто подложить `game-name.bin`
В сорцах ALE лежат cpp файлы, где бы найти скомпилированные (может есть?) или компилить самому?
Гугол что-то ничего мне не подсказал",
"но вообще, имя файла игры указывается в командной строке, так что может находиться где угодно",
ну как обычно для логистической регрессии :slightly_smiling_face:,
"ну там упоминают что-то про neurorobotics и классификацию EMG, так что в целом понятно как так вышло",
"Вопрос по бенчмарку. Никто не пробовал запускать cifar10 из mxnet с resnet? Мне интересно знать, какая скорость является приемлемой. Размер батча 500.",
"А вообще, есть какое нибудь сравнение разных фреймворков на разном железе с каким нибудь набором архитектур сетей? Вроде в этом чате кто что то такое писал...",
думаем вот кто авторам напишет предложение выступить,
как насчёт real-time мониторинга на основе картинки с трансляции? :smile:,
"Теперь представим ситуацию где это приложение нужно, ребенку 3 года, родители увидели этот апп в сторе и такие модет проверим нашего, чот он странный, проверяют и им говорят что сорям но ваш ребенок болен; это же фантастическая ситуация",
"на самом деле, если эту тему развить, и анализировать краткосрочную динамику лица (часто дергается глаз и т.д.), то может быть хорошим дополнением к теме с последнего NIPS, где ребята анализировали речь и классифицировали её в смысле психического нездоровья. deep learning sensor fusion :science:",
но не такие глупые кейзы как в посте,
"^ это, кстати, вроде как IBM. слайд №17 (<http://www.slideshare.net/SessionsEvents/irina-rish-research-staff-ibm-tj-watson-research-center-at-mlconf-nyc>)",
может где у кого есть волный вариант вот этой книги <http://www.statcats.ru/p/blog-page_29.html>,
тип с какими быками спаривали на самом деле мамок самых годных коров,
"<@U1G303UTW> я делал асинхронность через целери, брат жив, зависимость есть, монга как брокер и сторадж сразу",
"Мне кажется, что детектирование течки у коров и момента, когда куре пора отрубить голову, с помощью ML/DL это интересно.",
<@U1CF22N7J> где можно оставить предзаявку? :smile:,
"Ну хорошо, а где тогда их искать по хорошим ценникам?:)",
"Ценник-то смешной, только непонятно, как долго это проработает",
"Есть серия зионов, которые как раз того поколения, они отлично гонятся в таких геймерских мамках и показывают результат не сильно хуже новых без разгона",
"Всем привет! Кто в теме, посоветуйте плиз, что использовать для предсказания овуляции и менструального цикла? Какие признаки генерировать / собирать? Может кто-то для себя или своей второй половинке делал?)",
"Пришёл на собеседование в компанию, у них один из продуктов - приложение женский календарь, но работает, как мне объяснили, на достаточно простых формулах, без какого-либо ML вообще.  Тестовое задание - предложить ML модель для какого-нибудь продукта. Я выбрал себе это приложение. Главное это предложить методологию решения задачи, по желанию могу сам сгенерировать датасет с выбранными фичами, на котором потом показать работу модели.",
"У <@U07V1URT9> где-то был универсальный скрипт, он как бейзлайн к любой задаче подходит -- хоть котиков от собак отличать, хоть в лёгких опухоли искать, хоть овуляцию",
"Я не очень понял:
У тебя есть какие-то данные и на основе их тебе нужно улучшить предсказание даты менструации?
Или тебе нужно придумать какие фичи нужно собирать с пользователя, чтобы на них построить модель и улучшить предсказания?",
"Если приложение на мобильник, то в рамках бредогенератора -- можно отслеживать данные с гироскопа и если ловятся ежемесячные аномалии (типа больше машет руками с телефоном, потому что ругается со всеми, когда ПМС начинается), то уже скоро",
"акцент сделали на том, что нужно предсказывать правильно день овуляции, но как я понял, нужно и цикл правильно прогнозировать. и вот вопрос, какие модели лучше вообще использовать в данном случае..",
"Ответ на твой вопрос будет очень сильно зависить от того, какие данные у тебя есть.",
"Там компания разработчиков, они сразу, можно сказать, предупредили, что аналитиков/датасайнтистов у них нет, поэтому поучиться именно в этом плане не получится, но у них есть несколько приложений, куда они хотят засунуть ML в той или иной степени, и поэтому работы при собственном желании хватит.  Поэтому думаю модель они вряд ли заберут без кандидата) Да и  я сам до этого хотел написать аналогичную штуку для девушки)), т.к. у неё нестабильно всё, а её приложения хреново показывают результат. Так что точно работа будет не зря)",
"С одной стороны согласен, но с другой, можно прокачаться в этих вещах, а дальше расти и набирать людей в команду. Ну и как я понял ожиданий у них завышенных сильно нет.",
"katya: и нужно выбрать дату, когда шансы на беременность минимальные ))",
"А данные они как собирать будут? Спрашивать пользователей, нормальная рекомендация была или залет таки случился? ",
"Зачем спрашивать? Обычно в таких приложениях дамы вводят фактические даты, когда таки пришло. Если красных дней давненько не было, значит прогноз перестал совпадать с реальностью :troll:",
"У меня один приятель разработал какой-то девайс, который с какой-то точностью температуру чего-то на девушках меряет. Так как он умеет танцевать и дело происходило в Дэвисе, то он с легкостью нашел 20 девушек, которые его пару недель носили на теле, передавая данные на смартфон. 

На одной презентации про инновации им даже с приятелем денег каких-то дали. Но он и сам не уверен что это выстрелит и вместо того, чтобы следовать за мечтой согласился на постдока в Стэнфорде.

<https://www.ucdavis.edu/news/biomedical-innovations-take-top-prizes-big-bang-competition/>",
"ребят, я понимаю, что совсем-совсем не в тему, но подскажите, плс, как лучше всего открыть `.parquet` файл? (желательно питоном)",
ternaus: а почему постдок в стэнфорде это круто?,
"Да какие там могут особенности, если ты под убунтой?",
"Бля, помяните моё слово -- как только эпл вхуячит тачскрин в свои макбуки -- всё поменяется и это сразу станет охуенно и модно",
"айпад про -- это как раз один из тех моментов, когда эппл говорил никогда никогда и повторил то, то сделали в MS в точности",
"Интересно, как у него продажи :thinking_face: Как-то не особо слышно про него вообще",
"А я как-то привык заплентиться, сесть как царь за большим столом и сосредоточенно хуячить.",
"у меня ноутбук продуктивность убивает практически полностью, не понимаю, как можно постоянно на нем работать",
Как будто мерялся и проиграл,
"Энивей <@U1BAKQH2M> поздравляю. Если получится скрестить драва intel и nvidia, то напиши. Интересно можно ли так и с какой болью.",
"Тебе отправить может парочку? А то у меня коробочка есть, где их горсть лежит",
Я вообще качаю run c кудой и дровами и ставлю все сразу,
"ну как я и предполагал, драйвероебля",
а куда при инсталляции его не видит,
"Удали ее, и установи только куду, ни на что другое не соглашайся, как бы не предлагали",
А как заставить иксы рендериться на видюхи проца?,
у него в ноуте кпу и оперативка как в телефоне за 15к,
"Что за автор, какие у него статьи, что с профилем на Kaggle, кого на районе знает?",
"как напишет, она появится в Интернете",
"Постдок в Стэнфорде - это печально по деньгам, но вроде как связями можно обрости. Плюс постдок там лучше чем постдок в других местах.",
кто захочет рассказать сам решит)),
"<@U1CF22N7J> ну предложи альтернативу, чтоб дешевле 60к, с видеокартой хоть какой-то, процессор честный(никаких core m), вес не больше 2кг точно, выглядит не как хтонический пиздец из 90х, не бу",
"светиться не будет. впрочем, как на новых прошках.",
"В перспективе хочется на мобилке это крутить, и <https://github.com/garnele007/SwiftOCR> убеждают, что быстрее. Но они только для однострочных мелких фраз, как я понял.",
"прикольная либа, но смотрю в код -- чувак как будто из джавы пришел",
"сильно дешевле не нашел, есть всякие облачные сервисы которые процентов на 30 дешевле, но они какие то не популярные, и не факт что будут жить вечно",
а так фоточки музычку и пдфки нада куда то складировать,
"ну а покупать домой короб с дисками тоже как то не хочется, нормальный стоит 200+ баксов, это больше 2 лет пользования гуглдрайвом",
":thinking_face: меня немного пугает, что в интернете куча историй про то, как Гугл случайно теряет фотоальбомы в photos. про drive такого не слышал, конечно",
А подскажите алгоритм плиз. Есть два набора фраз по 170 тыс. Надо найти соответствие каждый с каждым. По strdist (косинусное расстояние) подойдёт. Но вот что-то идея бежать двойным циклом и сравнивать каждый с каждым матрице не привлекает. Может есть ещё какие алгоритмы предобработки массивов? Ну или простое векторное решение?,
<@U36Q9NJMD> это не отменяет необходимости сравнения *всех пар* - O(n**2). В то время как сложность LSH - O(n),
<@U36Q9NJMD>: посмотрел описание пакета. Не понял зачем строчку-то переделывать? :stuck_out_tongue_closed_eyes:,
<@U3D5HVB51> Там one to many join по максимальному значению расстояния. Можно создать предварительно колонку ID и потом по ней сгруппировать как тут <https://github.com/dgrtwo/fuzzyjoin/issues/18>. Но это слишком быдлокод,
"ой, а для каких случаев openrefine использовать? я пытался пару раз, но он был сыроват и приходилось делать много кликов, а велью -- я не понял. в коде сделать все даже проще было.",
"<@U34Q3KU8H>  вынося из избы <https://opendatascience.slack.com/archives/career/p1485007295011055?thread_ts=1485006557.010981&amp;cid=C0SGCGB52> , для какого типа данных Random Forests может быть лучше XGBoost?",
"Когда данные очень шумные, RF  скорее победит бустинг",
"…а когда много аутлаеров, надо сперва думать о них нежели о том, что лучше поставить учиться",
"Я тоже так думаю, но какой-нибудь интевьюер сунется посмотреть к тебе на github, production qualtiy code ты пишешешь или нет. А так как 9.99 интервьюеров с соревнованиями не сталкивалось и специфики не понимает...",
А может кто-нибудь по лазанье одну штуку подсказать? <https://github.com/FabianIsensee/NeuralNetworks/blob/master/NeuralNetworks/UNet.py> Здесь функция build_unet() возвращает OrderedDict. Как из него получить параметры для передачи в целевую функцию? Тупо из последнего слоя get_all_params?,
"какие архитектуры принято использовать, если нужно на входе из (n_cols, n_rows, n_channels) получить (n_cols, n_rows, 1)? 
это не каггловское распознавание дорог, если что :slightly_smiling_face:",
как космические корабли бороздят просторы финтеха,
"Ох. Как ужасно, когда треды используют чтобы отвечать на сообщение",
А тут нужно чтобы все кто пишут в канале так делали,
<@U04ELQZAU> а в ШАДе какое расписание?,
"Всем привет

Мы вот с <@U1BAKQH2M> только что обсуждали и не поняли зачем DropOut после Embedding слоя в примере для текстовой модели?
<https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py>

Может кто подскажет? <@U0H7VBQQ1> может ты знаешь?",
"а где девочек брать планируется?
(хотя хз, может, это только кажется, что в одс одни парни :thinking_face: )",
почему оно вообще тапк масштабируется?,
"Кто сабмитил задачу Тинькоффа, подскажите, что там за подводный камень в формате сабмита? А то который раз отправляю, все время 0,5. Сегодня на тренировке об этом говорили, но я что-то пропустил",
<@U0L4KM9R9> это в какой версии matplotlib'а?,
вот как на картинке показано,
"друзья! помогите с RMallet, пожалуйста. уже в R спросил -- не получается разобраться(.
как мне сохранить на диск и потом загрузить обученную модель?",
i: а почему mallet пользуешься?,
"а если я впилил вместо не очень большой не ризидуал сети большую и резидуал, и утилизация гпу спустилась с 90-100 до 70-98%, это же проблемы фреймворка поди? типа как то хреново съоптимизировал?",
"кароч одно наблюдение про ганы может кому пригодиться, много пишут кого тренировать дольше генератор или дискриминатор, гудфелоу высказал предполодение что вроде как пох и можно одинаковое количество раз; я тут пробрвал стратегию такую, что считаю EWMA тупо по аккураси с порогом 0,5, и как только достигает некоторого порога, например 0,95, то передавать ход второму игроку; ну и кароче как только я сделал генератор и дискриминатор одинаковыми по сложности (оба глубокие и резидуал), то при такой стратегии они обучаются одинаковое количество итераций (ну почти)",
"full rank advi <https://arxiv.org/pdf/1603.00788v1.pdf>
Вопрос: почему они так смело обращают матрицу L на странице 8? А вдруг она необратима?",
"<@U0AD1L5NC>  эх, поздно заметил твой ответ. А какую метрику использовал для сравнения картинок и выход из какого слоя использовал?",
"но они ее никак не ограничивают, насколько я понял. Где гарантии, что при оптимизации всегда будет full rank?",
мне бы c++ или java так как для продакшена,
за питон спасибо конечно :slightly_smiling_face: а какой размер графа был?,
"<http://approximateinference.org/accepted/RoederEtAl2016.pdf>
интересно насколько сильно это ускоряет вывод? в full rank наверно должно быть заметно, так как часть градиентов просто не считается",
"ну типа того, визуальная оценка -) вроде как лучше метода еще нет",
"Я хочу сделать класстеризацию данных и использовать номер класстера как фичу, в задачи классификации. Какой подходя для этого принято использовать?",
Сейчас хочу MulticoreTSNE посчитать и взять номер класстера как категориальную фичу,
"В numerai так и делают. Пачка tsne на разных perplexity, координаты добавляют как фичи.",
"Где то здесь проскакивала как раз ссылка, что делают разные perplexity. Может быть в канале nlp",
"<https://opendatascience.slack.com/archives/theory_and_practice/p1485102277003316> количество точек для которых алгоритм пытается сохранять расстояния, насколько я помню, наверное в этом контексте можно интерпретировать как рзмер кластера",
<@U0DA4J82H> а как ты используешь класстеризацию обычно в конкурсах?,
"хм, пробовал, но дало плохой результат. А выход какого слоя использовал? Как-то сжимал или преобразовывал выход слоя?",
"Не, ну это просто гауссиана - чтобы ближние кластеры больше влияния оказывали, чем далекие. Как в SVM с RBF ядром",
а какая архитектура сети была? использую vgg-19,
"<@U1FLG6YR1> а можешь в двух словах рассказать что кидается в гбм? ну в смысле как я понял ты пробежал дбсканом, получил кластера, а что дальше?",
"номер кластера ОНЕ и как фичу в гбм, на сколько я понял",
"Если мы не продолбаем пиар феста, 30 это 1.5% от тех кто полчит инвайты на фест :eyes:",
"не я не о том, гбм же юзается как сумервайзд лернинг? чо в качестве целеой переменной?",
"а такое решение для рекомендалки как то обосновано? интересно именн почему такой способ а не то то более классическое, ну там матричку факторизовать",
в общем идет третья неделя как я тут кручу верчу только еще обычный ган,
Я вот сейчас не могу понять зачем им метки,
"У меня в целом цели те же, но мне нравится когда сначала деньги а потом стулья",
"как минимум стоит прочитать ган, дцган, сондишн ган, аае и инфоган, а и еще гудфеловский туториал",
Пока что я попробую просто воткнуть как есть,
<https://github.com/zhangqianhui/AdversarialNetsPapers> вот еще список всего что как то с ганом связано,
<@U06J1LG1M> а ты понимаешь как именно используются метки для гана?,
"Я конкретно про вот эту реализацию:
<https://github.com/yunjey/dtn-tensorflow>

Как я понимаю это используется в “pre-train” фазе",
"как я понял, там речь о том, что стандартный ган это бинарная классификация на фейк и не фейк, а они делают дискриминатор мультиклассовым (точнее три класса), чот в четвертом разделе не упоминается про претрейн ничо",
четвертый раздел в статье как раз об архитектуре модели,
Всмысле когда появляется второй класс,
"У меня есть недопонимания насчет размерности `f(z)`, но интуиция подсказывает, что такая же как у `z`",
там как раз про то как с помощью ААЕ генерить семплы из класса (это позаимствовано из условного гана),
"Поднимаем сервак. Девочки пишут своих ботов, мальчики своих. Они и дейтятся, а те кто подошел друг-другу встречаются реально.",
а зачем транспонировать и умножать тогда? `∇z p(z)ᵀf(z)`,
"Судя по тому, как там написано, я понял, что это функция от `z`",
"Я про то, зачем активации ограничивать",
"А ещё неплохо бы сказать, в какой норме",
"ты ожидал 12 часов работы, как на человеческих ноутах? :troll: :mac:",
"12 часов и китайские поделия сейчас держат, на атомах, где такие же платы стоят как на macbook 12, размером с телефон",
<@U06J1LG1M> а какие интересные эксперименты получились с ганами? тоже хочу погрузиться и думаю с чего начать,
"Ну то есть там где на входе именно лицо, получается еще куда ни шло",
"natekin: я уже начинал и знаю как это сделать, но пока нет времени",
"О, там как раз глава про qa халявная ",
"А подскажите картичные датасеты размера порядка 10^5+ картинок с разметкой классификации с не очень большим разнообразием (например, как разные лица, разные породы котов, etc.)",
"Странно как-то. Все неравенства типа Гаглиардо-Ниренберга-Соболева и т.п. действуют наоборот: норма функции меньше нормы производной. А вот от конкретной нормы, мне кажется, это не зависит, так как там почти всё вкладывается куда нам нужно.",
"Привет всем! Есть кто работает с revenue managment? Предстоит поработать с этой темой, может посоветуете с каких работ/статей  начать знакомиться ?",
"У меня недавно в моем проекте тоже была ситуация, когда тангенсы лучше всего работали, а иначе оптимизиация сдыхала, но я потом разобрался, в чем там дело",
"всё не прямо в 2 раза (точнее, не всегда). 60к тоже немало получается. да и дело не в этом. просто интересно, что есть фастчардж для ноутбука и как он работает.",
"коллеги, <@U1Z7QM16H>, подскажите, пжл, где ссылка на регистрацию на ближайшую зарешку?",
У меня данные в которых пара сотен признаков. Это нормально что когда ч смотрю у бустинга важность признаков он среди наиболее важных указывает такую вещь как номер id или номер дня (хотя номер дня не важен для задачи)? Или значит что-то не так в данных ?,
"при достаточно большой глубине и отсутствии ограничения на минимальный размер листа, могут возникать ситуации, когда в узле остается всего несколько примеров, и любого отличия достаточно чтобы разбить их
ид (или любой высококардианльный признак) в такой ситуации как раз будут получать неоправданно высокий fscore, но качество на тесте от этого конечно же не улучшится",
Softplus и tanh как возникали?,
"Если у кого в бумажном виде есть, то книга-сканер-ODS:pirate:, пожалуйста.",
"Ну требования, как ни странно: метки и размер :slightly_smiling_face:",
"Никто не оспаривает очевидного факта, что заниматься более-менее серьезными вещами как в ML, так и вообще в науке, без английского языка практически невозможно. Однако, если есть возможность книгу на русском, то почему бы и нет.",
"гайз, а какими ключевыми словами искать следующую вещь: когда анализирует один длинный текст, но при этом как бы в динамике. то есть я хочу понять, как увеличивается типа вокабуляр. условно за первый 10% встречались 80% всех  слов, который там вообще есть. и т.п. я где-то видел такое, но че-то забыл, что гуглить)",
Пробую гонять fully connected сетку на данных. Какие есть эвристики для выбора размера dense слоев и их количества ?,
"<https://github.com/alno/kaggle-allstate-claims-severity/blob/master/train.py>
Тут еще можно заценить какую сеть использовал alno",
"Всем привет. Хочу уметь получить список морфем для слова(разобрать по составу как в школе). Пробовал polyglot(morfessor) для русского, но оно какое-то совсем невразумительное: 
грамматический      ['граммат', 'ический']
привет              ['при', 'вет’] 
прискорбно          ['при', 'с', 'кор', 'б', 'но’]
Есть какое-то решение для этой задачи? (распарсить вики-словарь?)",
а еще <@U0JJ69UB1> проинтерпретировал смешивание предсказаний от одной сетки на этапах обучения как какую-то умную баесовщину.,
"если нужно прямо как в школе, то хорошо бы где-то датасет собрать с такой разметкой, ну и там думать уже",
вроде как достаточно часто их так используют,
"если я правильно понял, то когда мы обучаем с минибатчами с помощью SGD и сохраняем веса с определенной периодичностью,  то ""стохастичность"" от минибатчей как бы воспроизводит MCMC семплинг для весов",
<https://opendatascience.slack.com/archives/deep_learning/p1485183859003935> а почему увеличить а не уменьшить?,
"а имеет смысл выправлять решение с тем же самым адам/рмспроп, но более низким lr? просто кажется, что имеет смылс использовать ту статистику которую они накопили, но я не уверен, так как не ставил эксперименты, может кто-то пробовал?",
"рмспроп вроде с адаптивными весами по каждой координате, нет? как раз избавляясь от этой статистики вносишь дополнительную регуляризацию? вроде ты всякими рмспропами можешь загнать лр перед каким-нибудь весом в нулище и он у тебя особо обновляться не будет",
но я не уверен что помню как работает рмспроп,
Ну и список авторов как бы намекает почему там во славу линейной регрессии,
"<@U1CF95PP1> ну как же так? Я по рассказам <@U1UM6S9KN> был уверен, что из ШАДа выходят уберменьши",
"Райгородский в первом семестре это как матан на первых курсах университета. Если осилил, то дальше будет легче.",
"а вобще как специализация на Coursera? Стоит записыватся?
или лучше на <http://lagunita.stanford.edu|lagunita.stanford.edu> пойти?",
"Насколько я помню мы хотим найти такое M так что  || A M - B ||_F минимально
B там уже как результат вылезает",
"мы хотим найти преобразование, которое как можно лучше сохраняет расстояния между объектами",
"ну так смысл примерно такой же, как я понимаю <@U0DA4J82H> хочет что-то вроде dimensionality reduction сделать",
(из разряда теоретических вопросов) если взять нейронку с только линейными активациями - как раз такой оператор и выучится? :more-layers:,
то есть представить M как outer product двух матриц какой-то низкой размерности,
"про то, как это все в SGD запихнуть и все разом учить",
"но, в принципе, хотелось бы с основами разобраться, поэтому интересно, где можно что-то по теме почитать",
"Каким образом можно обученную модель xgboostа перенести в c++ проект? Кажется, можно использовать C api <https://github.com/dmlc/xgboost/blob/master/src/c_api/c_api.cc> для загрузки файла модели (сохраненной с помощью booster.save_model), но C api какое-то стремное на вид, что подскажете?",
"О, я всё ждал когда RL в ШАДе появится",
"Люблю, когда объяснения начинаются с «очевидно что...»",
"Начинаются это ок. Гораздо хуже, когда они так неожиданно заканичваются",
"Тоже много встречал чуваков, рассуждающих о паудере, как круто пройтись первым следом, а так всё и катаются в Степаново на прокатных Wedze :nabros: :trollface: ",
"Зависит от того, для каких нужд и какие требования. Что-то на коленке можно за пару вечеров собрать",
"<@U04422XJL> С датасетом напряг, как всегда, в пару вечеров не уложишься",
Привет. Может у кого в закромах есть статьи книги по методам/моделям применяемых в рекомендательных системах. Делаю обзорный доклад хочу ничего не упустить. Заранее всех благодарю,
"ну  n^3, как и умножение :slightly_smiling_face:, там LU разложение считается, но основной проблемс, что скорее всего матрица плохо обусловлена и будет печаль всякая. Лучше SGD",
"так можно же регуляризацию сделать, как в обычном ридже",
"Тогда все решантся аналитически как <@U04ELQZAU> написал. Если же будут проблемы, тогда уже надо думать",
"если я хочу запилить датасет для обучения NE-классификатора, то в каком формате это стоит делать? где вообще можно почитать развернуто про форматы корпусов, словарей. подскажите, пожалуйста :hugging_face:",
"Насколько я знаю, он будет как обычный ШАДовский курс, открыт только для ШАДовцев",
evgeniimakarov:  А можно чуть подробней? Как это сделать правильно и от чего это спасет?,
Еще дурацкий вопрос: как размечают фотки? Вот ту же рыбу с кэггла? Должен же быть удобный инструмент?,
"Вот и все. Если вы знаете СВД вы победили. Можно выкинуть какое то количество векторов, соответствующих маленким сингулярным числам, и обращенная матрица будет близка к истинной",
"А дальше уже нюансы жизни. Распределение надо анализировать, смотреть, не будет ли ваше SVD еще медленнее считаться, чем какой нибудь Холецкий. Однозначного ответа нет, а пвтору достаточно numpy в его задаче, по моему",
"Ребята, подскажите. какие есть датасеты для  Churn Prediction",
меняешь какую то мелочь и оп работает,
Пример того как (не) получается,
"Самое грустное, что получается довольно “однообразно”, как будто есть несколько классов куда картинки переводятся",
"Да, пока выглядит как gan'ы под бутиратом",
"Мне неясно зачем они претрейн такой долгий делают, у них очень быстро метрика на тесте приходит в некоторый потолок",
<@U06J1LG1M> Расскажешь как правильно должно выглядеть поведение ошибок в примере?,
я уменьшаю комплексити дискриминатора когда виду что тот переобучается,
"&gt;Д = source, Г = target в их именовании?
у них есть f = энкодер, а далее как в гане есть Gенератор и Dискриминатор",
"стадия обучения Д: в Д приходит пачка из реальных картинок и не реальных, его задача отличить какая фейк а какая нет, ну и есть аккураси
стадия обучения Г: в Д приходит куча фейков, но оптимизируется же Г, то и аккураси это сколько из всей пачки фейков было распознанно дискриминатором как реальные",
"Возможно, это как раз отражает факт что одна из сторон “побеждает""",
"ну и да ждать долго нада, вот это меня как то печалит, в супервайзд лернинге часто можно по динамики ошибки остановить обучение если видишь аномалию",
"видно что часто они становятся внезапно хуже чем были, потом становятся еще лучше чем были до того как стали хуже",
"решение в один шаг работает как надо, немного только регуляризации добавил",
"спроси того, кто выдал такое тестовое :wink:",
"я бы по дефолту трактовал как (page[i], page[i + 1]) в рамках сессии, но все, что можно трактовать по-разному, будет трактоваться по-разному",
"Вопрос к тем, кто делал ABSA (Aspect Based Sentiment Analysis). Для текста нужно определить аспекты, освещаемые в нём, и тональность текста по отношению к каждому из аспектов.
Какие метрики использовали, какие алгоритмы можете посоветовать?
Стоит ли разбивать задачу на 2 (поиск аспектов и определение тональности для заданных аспектов) как делают на SemEval? Альтернативой видится такой вариант: каждой паре аспект, тональность ставить в соответствие лейбл и таким образом получается одна задача multilabel классификации. Ещё можно ввести штраф за предсказание двух лейблов, соответствующих одному аспекту.",
"задавай тут, если что, подскажут, куда лучше обратиться",
"как лучше в pandas просчитать наибольшую популряность направлений, например в итоге получится мск-спб",
зачем ты с ним игрался,
"Как их посчитать, если они все один раз встречаются?",
"я вот как раз щас балуюсь с различными, пока лучше всего отрадатывает обычный дискриминатор с пятью слоями сверток 3х3 (это как в статье DCGAN)",
"Ребята такой вопрос: если одна из категорий в каттегориальной фиче тренировочного датасета отсутствует в тестовом датасете, то есть ли резон полностью удалить из train объекты у которых эта каттегориальная фича принимает как раз отсутствующее в test значение, если к примеру таких объектов не много?",
"нет, это как бы проблема - отключил мышь и работать до ребута невозможно",
карточка вообще отображается как девайс от невидии?,
"Кажется, там было что-то странное, когда мне вывалился в лог ошибки вывалился сорс чуть ли не от nvcc",
"Как бы, карточки не видно, оторвалась по ходу дела. nvidia-docker от этого не поможет",
"Если я правильно понял консолидированное мнение, то если lspci не показывает то это не базовая хрень и как минимум проблема не только в дровах",
"так как в биос залезть я не могу, то придется пока статьи почитать :disappointed:",
"да tariff_id я просто когда в dummy закидывал их, думал какие в итоге каттегории можно убрать",
"Там где дисперсия упала, это lr на порядок уменьшил",
"потому мне кажется нада еще и аккураси выводить, что бы хоть какая то интерпретация была",
"Немало, это на какой видеокарте?",
"вот получается что иногда шарахает так что количество батчей Г или Д больше другого в одну итерацию, но в целом получается примерно одинаково, как и говорил Гудфелоу в туториале, что его мнение что нужно одинаково",
а как без паддинга разные картинки пихать?,
"а, MxM. я распарсил как MxN и не думал, что соотношение сохраняется",
"Я пока не гуглил, но какой в этом смысл?",
"<@U053R9RS6> Стой, а какая изначальная задача ?",
"<@U040M0W0S> ""классненько. но на чем его ""обучить"" для русского, чтоб без цензуры?)""

Можно попробовать уже посчитанный словарь частотности слов русского языка, вроде это он: <http://dict.ruslang.ru/freq.php> Но как понимаю он с цензурой.

Второй вариант: библиотека Машкова. Это достаточно много хорошего текста. Плюс после подсчета частот слов нужно отфильтровать только слова, которые есть в русском языке (на всякий случай). В pymorphy есть список всех русских слов. Правда не знаю, как там по поводу мата.",
"как минимум, новомодный генератор юзерпиков для github'а уже есть :slightly_smiling_face:",
просто интересно какого рода там получается изображение. хотя бы для baseline :slightly_smiling_face:,
"Ребят, подскажите плиз как в artm подгрузать модель которую ты до этого сохранил?",
"перед тем как подгрузить её в каую-нибудь переменную sda, по идее сначала надо создать пустую модель.  В туториалах от адептов Воронцова ничего нет по этому поводу, так как судя по всему это очевидно. Но .... я тупой и для меня это не очевидно.",
"<@U053R9RS6> я когда-то пиксель-арт руками делал,  как ресайз + выкрутку контраста в Фотошопе, это какой-нибудь конволюцией наверно легче сделать, чем надеяться на Ган.",
"Сколково совершенно не так далеко как кажется. У меня коллега от выхино добирается за полтора часа в среднем, а рекорд порядка часа. Это от Выхино!!!",
Зачем вы такие ужасы рассказываете?,
"кароче пока вывод такой, что не все сети одинаково полезны, вот резнет Д например легко выучиватеся отличать что угодно, но от него какие то накуренные градиенты в Г приходят и он учится какой то дичи, хотя если на тот же Г повесить Д похожуй на мелкую vgg, то сразу все ок",
"все картинки получаются какими структурированными по сетке, какие то мазки появляются",
"хз, но помню когда резнет в стайл трансфер пихали тоже фигня угловато-гранулированная получалась - это неспроста наверное",
это вроде как сегментация на лес/не лес,
"Аригато в хату! В телеграмовском чатике сформировалась инициативная группа по коллективной покупке (если это можно так назвать) будущей книги <https://www.kickstarter.com/projects/adrianrosebrock/deep-learning-for-computer-vision-with-python-eboo> - вариант за $395. Я эту идею поддержал и взялся запостить сюда, где больше народу. Если есть желающие, то давайте таки соберемся и возьмем, на 20 чел. это всего по 20 баксов будет. Да, есть риски, что написанное окажется фуфлом, но автор на афериста не похож, так что лично я готов рискнуть.",
"Я щас может тупой вопрос задам: а почему CNN на RGB обучают, это ж так себе цветовое представление ? Какой-нить HSV ж полегче будет, грубоговоря смена тона +-  10% в 1 канале, а в ргб это все 3 переменные в разные стороны.",
"&gt;Я щас может тупой вопрос задам: а почему CNN на RGB обучают, 
как мне кажется идея дл (одна из) как раз в том что он сам должен все выучить, так что подсовывать ему то что нам кажется лучше не самая хорошая идея, модели самое виднее что лучше",
"Ну, кстати, когда в сети используются сепарейбл свертки уже не так очевидно",
Версус как всегда подоспел с актуальной ссылкой,
"`cleanliness of the data is more important then the size` тут, скорее всего, имеется ввиду ситуация, когда новые данные не улучшают качество. В общем случае чем больше данных -- тем лучше.",
"ребят, кто учился в ШАДе, подскажите, пожалуйста, хорошие лекции/учебники/сборники задач помимо олимпиады Путнам, чтобы можно было подготовиться к вступительным экзаменам",
"statist: автор не аферист, материал может быть и хороший, но примитивный же. Есть замечательный  <http://cs231n.stanford.edu/> -- куда более разумное вложение времени",
"Всем привет! Я у себя в универе записался на курс Evaluating Technological Change, то есть A/B testing по-нашему. Стоит задача проанализировать любую ситуацию, когда есть Treament и Control Groups, в идеале рандомно распределенные между собой. То есть происходит exogenous shock, policy change, innovation, в любой сфере, и мы проводим наш impact evaluation. 

Вообще курс про policy больше, у меня первые мысли пока что посмотреть, например, на влияние Obamacare на образование, т.к. есть гипотеза у меня такая что раз люди начали сберегать деньги подписавшись на программу, может быть они стали реинвестировать в образование, так как на момент принятия программы как раз был кризис - самое время для повышения квалификации. Почему именно образование интересует - потому что все остальные вроде пытаются изучить эффект на здоровье :grinning: Сейчас буду смотреть есть ли подходящие данные.

Но я супер открыт для анализа других сфер и более интересных вопросов, но и конечно готов отталкиваться в выборе темы и от наличия подходящей panel data по сути. Может есть идеи по поводу подходящих датасетов?",
Сейчас они используют keras как бэкенд,
"<@U053R9RS6> еще я хотел спросить, у меня такой артифакт, что модель в любой итерации дает в основном изображения одной гаммы. Это очень странно учитывая, что гамма постоянно меняется. 
это проблема в том, как говорят в статьях, что ган переобучается вокруг одной моды распределения, и пока общего решения нету, но есть 100500 хаков в статьях, у кого то что то получается",
"Коллеги приветствую!
Подскажите решения для задачи ""Time series Analysis"". Time данные идут по нескольким каналам.
Нужно быстро прогнать csv файл, посмотреть что на выходе.
У нас несколько каналов, и один дискретный.
Когда на дискретном канале 0 становится 1, то нужно анализировать другие каналы (момент перехода) и прогнозировать переход.
Фактически прогнозировать переход 0 в 1
Имеем наборы выборок в виде десятков - сотен csv файлов
Столбцы
- время ежесекундно
- несколько каналов - столбцов, цифры разные
- дискретный столбец",
"Коллеги приветствую!
Подскажите решения для задачи ""Time series Analysis"". Time данные идут по нескольким каналам.
Нужно быстро прогнать csv файл, посмотреть что на выходе.
У нас несколько каналов, и один дискретный.
Когда на дискретном канале 0 становится 1, то нужно анализировать другие каналы (момент перехода) и прогнозировать переход.
Фактически прогнозировать переход 0 в 1
Имеем наборы выборок в виде десятков - сотен csv файлов
Столбцы
- время ежесекундно
- несколько каналов - столбцов, цифры разные
- дискретный столбец",
"Привет! Не расскажет ли кто (или пошлёт по ссылке), каким образом DIGITS распараллеливает обучение сетки в Torch по нескольким GPU? И вдогонку - существует ли библиотека, которая позволяет не заморачиваться на распределении вычислений между GPU внутри кода Torch? Спасибо.",
"А расскажите кто какими клавами пользуется, если сидит на за ноутом и что порекомендует?",
"а предсказывать надо сам момент перехода, или время до него? в реальной жизни как эта задача выглядит",
"```
gen = ImageDataGenerator(
   ...)
g1 = gen.flow_from_directory(train_data_dir, ..., shuffle=True, seed=0)
train_conv_feats = base_model2.predict_generator(g1,  2 * nb_train_samples)
```
вопрос по Kerasу как получить список соответствующих таргетов в данном случае? или список индексов который использовался для выборки примеров и последующей аугментации?
`g1.filenames` или `g1.classes` не подходят (даже по размерности она у них в 2 раза меньше)",
"Тоже не совсем правда. Допустим у нас поверхность в единичном кубе, причём в середине точек много, а на границе значительно меньше. Если в лоб разбить на фолды может получиться не совсем удачно, как мне кажется ",
"что есть нынче хорошего для непараметрического сравнение эмпирических распределений, когда одно распределение задано большим  (относительно, тысячи точек) набором данных известных заранее, а второе тоже набором данных, но постепенно поступающих (сотня и растет), и надо как можно раньше сказать, насколько эти распределения похожи или отличаются?",
"Гайз, вопрос по :tensorflow: :
Как закинуть созданный граф на девайз, а не создавать граф на девайте?

Работает:
```
with graph.as_default():
	with tf.device(device):
```

Не работает:
```
with tf.device(device):
	with graph.as_default():
```

Хочу именно граф на девайз закинуть. freeze не предлагать :slightly_smiling_face::upside_down_face:",
"Кто-нибудь сталкивался в своей практике с расходящимся градиентом (топовый фреймворк, стандартные слои, популярный датасет, адекватные на первый взгляд параметры) (пример обсуждения проблемы <http://stackoverflow.com/questions/38157657/salvaging-diverged-neural-networks>)? Какие могут быть причины, исключая кривые руки, и что делать в этом случае (пример инструмента для решения проблемы <http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization>)?",
"Упрощать задачу до тех пор, пока не заработает, последовательно увеличивать сложность и смотреть, что все как надо.",
"Привет, подскажите насчет *Decision Tree*.. Какие best practices есть для минимазации фич? Например, тут можно сократить *max_leaf_nodes*.
<http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html>

Аналогично для Numeric фич в <http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html>",
"Ну вот и хотелось бы почитать, какие могут быть причины. Сам пока мало что придумал кроме кривого кода, битых пикселей и неадекватной lr",
"Для шарика понятно, почему оно вокруг корки концентрируется",
"кто пользовался datashader? <@U1HHX1QS3> вроде ты использовал? я никак не могу вдуплить как картинку на черном бэкграунде нарисовать, у меня все в чб рендерится, пробую примерно так",
а потом уже как хош так и раскрашивай,
"О, как раз к боулу :slightly_smiling_face:",
"Как всегда, не забуду упомянуть о том, что после облаков серверное хардваре выкидывается и попадает на вторичный рынок. И именно такое железо -- один из самых выгодных вариантов, если не самый выгодный вообще.",
Углы хочу каким нибудь силиконом или типа того заклеить,
"это work in progress, так что там ещё даже есть части на скотче прикрученные, но всё равно лучше, чем когда просто на тумбочке мать стояла со всем вокруг",
Пока выглядит вполне себе как WAAAAGH-teknologi,
"Не знаю в какой канал правильнее написать. Напишу сюда.
Работал ли кто-нибудь с сервера Microsoft Azure для обучения сетей на GPU? Есть ли какие особенности? Как впечатления в целом?",
"Клёво, я тоже смотрел на барахолках корпуса, но у нас по-близости ничего клёвого не нашлось, а тащить откуда-то издалека заленился и решил на уголках (как всякие майнеры делают, у кого по 6-12 видях в риг ставится)",
"подскажите, какие слова гуглить на тему традиционного cv без нейронок? задача очень тупая, есть фотки коробок на +- консистентном фоне, коробки одинаковые, на каждой коробке по одному яркому красному символу, надо посчитать коробки (считай, посчитать яркие красные символы на картинке)",
"я тут понял, что когда сам датасет собираешь желание стакать слои резко отпадает, тем более для задачи решенной небось годах в 80х",
Как я понял это он для себя написал,
а как искать значения цветов для трешхолда? сначала квантизацию каким-нибудь kmeans а потом ручками?,
Как вариант сделать контрол с тремя ползунками и прямо на месте смотреть к чему приводят пороги,
"я почему-то решил, что это популярный пакет. но че-то не смог разобрать в каких кругах",
как думаете что еще за 5 кусков?,
почему форма на зарешку закрыта? Кончились все места?,
"<@U2ZLY7GKG> да, но есть шанс что если лично написать комунибудь из Яндекса кто завтра будет, можно чтонибудь придумать :eyes:",
я юзал folium но не знал как в PNG перевести,
ну удобно настроить overlay или как их,
"i: это как пользователи SE определяют поиск квартиры/дома, кастомные границы",
"Не уверен что это что-то значит, но сами картинки тоже вроде как ничего не значат",
<@U0ZJV6E5Q> а у тебя какая модификация этой платы? asus z9pe d16,
"Память как и процы на ebay. Там есть дешевые 8Gb планки. 2L не было в наличии тогда, а так бы 2L взял.",
Вот здесь есть код как меняют размер входа у модели,
"Небольшой оффтоп - как посчитать, сколько мне надо будет видеопамяти для батча размера Х?",
Разные фреймворки по разному используют память. Я до сих пор не понял какая зависимость даже в mxnet,
"привет, подскажите насчет несбалансированных выборок. пусть у нас 2 класса: 1M и 1K объектов. Кажется, что будет полезно веса первого класса порезать в 1000 раз. Например, если классы пересекаются и первый 10% первого класса лежат на территории 2-го класса, то knn вообще не сможет провести границу. Расскажите, как вы поступаете, если есть сильный диспаланс?",
"спасибо, почитаю. Есть какие-то опытные факты - типа если классы соотносятся как 1 к 100 - то точно нужно с этим что-то делать",
Когда ты можешь половину слотов не забивать планками и неиспользуемые линии используются другими слотами,
"да! если кто-то тут в такое время сидит ))) есть следующий вопрос. Мне нужно закодировать категориальную фичу с большим количеством категорий. Я придумал несколько путей:
1) dummy_encoding с ограничением по популярности (иначе переобучается)
2) hashing trick
3) взять dummy_encoding большим количеством фичей, посчитать на них RF и глянуть, какие из фичей в топе по feature_importance
4) хочется закодировать фичи таргетом, но тут надо смотреть, чтобы не переобучиться на редких. Можно просто заменить редкие (см. выше), но я помню, что была какая-то умная функция. М.б. кстати софт. макс. Чтобы не сильно уводила значение от среднего, если данная категория не часто встречается",
"Нашел какую-то старинную новость про то, что все индивидуально как раз:
'С другой стороны, в зависимости от чипсета может меняться организация модулей. nVidia nForce2 допускает работу с 3 модулями. При этом один канал подключается к первому слоту памяти, а второй - к двум оставшимся. Для реализации 2-ка-нального доступа необходимо либо установить идентичные модули одинакового объема в 1 и 2слоты, либо заполнить все три, но с условием, чтобы суммарный объем второго и третьего модулей был равен объему первого. Чипсет Intel i865PE налагает более жесткие требования. 2-канальный доступ к памяти возможен только лишь при использовании 2 или 4 идентичных модулей одинакового объема, устанавливаемых в соответствующие слоты памяти.'",
"Это ж сервер, я повторюсь -- вообще не видел, чтобы в серверах было как ты рассказываешь :slightly_smiling_face:",
"Кто мне подскажет, где найти код callback  в  keras, который экспеоненциальное усреднение весов делает между эпохами? (Polyak Averaging)",
ternaus: Можно None подставлять вместо N (если tensorflow как backend используется).,
<@U053R9RS6> а какого размера был трейнсет?,
"Просто тут мне кажется есть одна тонкость, с тем, что распределение каждого из столбцов свое, так как является усреднением двух равномерно распределенных векторов шума",
"Было бы что в опенсурс. 
Код? Его навалом, я там ничего не придумал. Например, <@U06J1LG1M> выкладывал свой 
Модель? Это еще далековато от того чтобы использовать как есть
Датасет? Не уверен, что там с правами :pirate:",
так какого размера датасет был?,
"я так понял, что не могли зайти те, кто вчера во второй половине дня регистрировались",
<@U0ZJV6E5Q> а ты какую модификация E5-2670 брал?,
"господа, курящие теану/лазану
есть такой файлик <https://drive.google.com/file/d/0B4bl7YMqDnViMld3V1Jzckx1VFU/view?usp=sharing>
это типа упрощенный conditional GAN, там есть два графа, генератор и дискриминатор, в ячейке 10 объявляется fake_prob, который говорит генератору типа сгенери картинку, и передай ее в качестве одного из входов в дискриминатор (x_img_sym)
но как видите чот в 12ой ячейке вылазит ошибка
(причем если делать не conditional gan, т.е. просто выпилить все упоминания y_condition_sym то будет все ок работать)
может есть соображения на этот счет? (я подозреваю что скорее всего я чтото не так делаю, но хз что, тк если бы был баг в теане то я бы нашел его уже в сети)",
"а если сделать как стактрейс говорит?
&gt;To make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function",
может кто вдруг захочет воспроизвести то вот ноут а не хтмлка <https://drive.google.com/file/d/0B4bl7YMqDnViN0NaT01pWkJJVFE/view?usp=sharing>,
"да как то ничего -) этип то ГАН и бесполезен, что спровоцировало появление ААЕ",
"а так то, если совсем коротко, то статьи три
GAN - базовая идея, но десполезная
AAE - сделали полезную
InfoGAN - показали как можно поизвразаться",
"Привет! Во время профайлинга обнаружил что в tf изменение формы тензора занимает какое то абсурдно долгое время, что прям пользоваться нельзя: 
Пример: 
`import numpy as np`
`import tensorflow as tf`
`x = tf.Variable(np.zeros((100, 1)))`
`with tf.Session() as sess:`
`   %timeit sess.run(tf.squeeze(x), feed_dict={x: a})`
`   %timeit sess.run(tf.reshape(x, (100,)), feed_dict={x: a})`
`   %timeit sess.run(x, feed_dict={x: a})`
Результат: 
`10 loops, best of 3: 14.5 ms per loop`
`10 loops, best of 3: 25.5 ms per loop`
`10000 loops, best of 3: 69 µs per loop`",
"Кто нибудь сталкивался, в чем тут дело? tf 0.12, cpu",
Просто определить эту операцию как доп. переменную. Даже не предполагал что такой эффект будет.,
"Уже час безуспешно пытаюсь заставить работать U-net, поэтому такой вопрос: если U-net предсказывает класс конкретно для каждого пикселя, то каждый пиксель исходного изображения отображается в вектор Kx1 (где K- число классов). Соответственно выход у слоя с софтмаксом будет размера Kx(MxM) - где M - число пикселей по одной стороне. В случае бинарной классификации я должен подать как target не одно изображение с 0 и 1 а склеить в тензор исходную бинарную маску и ""обратную"" бинарную маску (где 1 заменены на 0 и наоборот) ?",
И какой входной размер картинки?,
"<@U07V1URT9> Я пока на спутниках 224x224 гоняю, но там надо разбираться что хорошо и что плохо и как поступать с угасанием receptive field к краям.",
Слабо представляю как связано количество каналов на входе сетки и фрагментарность маски на выходе.,
"А еще, <@U34Q3KU8H> <@U04725QK7> вы как решили поделить локальную валидацию? На каких id тренируетесь, на каких валидируетесь?",
"<@U06J1LG1M> Попробовал твой дискриминатор впилить вместо своего, но чего-то странное получается. А какой там принцип построения когда меняешь разрешение входа?",
"&gt;А какой там принцип построения когда меняешь разрешение входа?
а какой из? там несколько",
"&gt;Conditional GAN еще похоже полезное развитие
хз, не слышал о полезном применении; он же тоже делает из шума образы, а это бесполезно; ну т.е. они к шуму подмешивают какие то контроллирующие параметры, но это не меняет принципиально ничего; главное же уметь из образов доставать фичи",
"как перевести gated units, gated functions и прочие упоминания слова gate, когда речь об LSTM? Ворота? Гугло-переводчик говорит про какое-то стробирование.",
"возьми из старого коммита, ну или проследи по истории зачем удалили <https://github.com/fchollet/keras/blob/ce814302acee2c474a88b2f4bfcdc92ff866f94f/keras/engine/training.py#L241>",
"а как скачать данные этого слака? говорят, они доступны для анализа.",
"кароч запустил Conditional GAN, пока пои ставки что ничо не измениться и рожи будут генериться не особо лучше -) как то все это на шаманство похоже уж слишком",
"<@U1D6QAJPJ> под ваши задачи хорошо ложится Gephi. Там есть как софтина, так и pygephi - jpython API к Gephi (вызываешь из-под python и радостно делаешь все, что вы описали)",
"Привет. Кто нибудь занимался таким упражнением: взять модель кераса (.h5), тренированную с бекендом theano, и сделать аналогичную (те же веса, структура, все) в keras но с backend'ом tensorflow? Сеть сверточная. 
Кажется, что небольшая засада должна быть с порядком осей в сверточных слоях, а все остальное должно быть такое же, но сам еще не успел это проделать.",
"<@U040M0W0S> я из него вызывала алгоритмы кластеризации и укладки, а далее сразу подавала на вход js-рисовалке. это удобнее когда надо системное решение для пула одинаковыхзадач",
"но боли он мне принес немало, это было давно, с порога не вспомню, какие косяки и как их обходить",
а  как ты потом подавала на вход js-рисовалки из питона?,
"вроде как в бэкенде кераса, когда я там последний раз ковырялся, обёрнуты все операции проверками на порядок axis в тензорах, поэтому проблем возникнуть не должно при смене бэкенда",
<@U06J1LG1M> а как картинки с conditional GAN?,
"Ну вот накопишь ты скилл, а завтра придумают как задачу свести к минимизации одной функции ошибки. И то, что ты принимал за скилл, окажется бесполезным опытом",
В виртуалку ты не прокинешь куду,
"я просто помню, как пробовала поставить где-то месяца 1,5 назад, и куча проблем возникало",
"вроде все устанавливалось, но когда вызывала в питоновском ноубуке, все висло",
"<@U0AD1L5NC> а можешь подсказать, где можно посмотреть для совсем нубов?",
"<@U14GG4E69> как мининмум ААЕ полезен примерно так же как любой другой автоэнкодер, но потенциально еще дает всякие штуки по управлению процессом генерации, типа такого <https://www.youtube.com/watch?v=pqkpIfu36Os;> а с точки зрения теории очень забавно то как оно все связанно VAE-GAN-RL",
<@U06J1LG1M> А как GAN связаны с RL?,
"<@U06J1LG1M> полностью на убунту я пока морально не готова переходить. При установке новых программ мне нравится со всем соглашаться в окошке и не думать о тайнах мира и 100500 особенностей терминала. Хотя практически все DS, с кем я говорила, ругали Винду и утверждали, что она не подходит для анализа данных.",
"ты не поверишь но я тоже тайный поклонник винды, си шарп дучший язык, вижуал студия лучшая иде (я потому саблаймом пользуюсь, ибо посе студии все иде кажутся говном), а винда самая няшная ось; но боль установки дс энваермента (еще и не все установишь как нужно) пересиливает мои нежные чувства к мс",
"я-то за. но в прошлый раз ты какую-то неопреденность продемонстрировал))). потому что на тех же рбк и ленте -- много же не новостных заметок. всякой аналитики и т.п. как например с этим быть? ты уверен, что это именно то, что тебе надо? 
составляй список, в общем -- будем разбираться!",
"ааа. так понятнее. а ты RE как делаешь? интересно понять что сейчас для русского работает ""лучше всего""))",
"Из праздного любопытства  -- кто нибудь пробовал запускать tf/keras/etc на Windows пользуясь докером? На убунте просто очень удобно, если есть какой то замороченный пакет (типа RLLab) с мутными зависямостями, и по идее уже есть даже nvidia-docker, т.е. можно использовать ГПУ. А про Windows не знаю, насколько это возможно вообще.",
"Для русского я уже не помню есть ли что-то опенсорсное или хотя бы с апи, где отношения понимаются как founder_of, ceo_of, work_in, а не как какие-то синтаксические ",
"<@U053R9RS6> чот у тебя второй раз странные проблемы с гпу, ты сервак свой юзаешь или амазон или еще что?",
<@U053R9RS6> Какие GPU и сколько их?,
"надо брать html в WARC и самому парсить, там у них парсер плохой, он как раз слова слепляет",
"данные из новостей довольно чистые, как мне показалось, по сравнению с остальным common crawl",
"nefedov_m: когда я советовал common crawl, я же вроде бы уточнил, что есть новости отдельным дампом",
"Насчёт GAN - то как генерировали изображения в google deep dream, подавая в модель шум и итерационно усиливая изображения которые сеть в этом шуме видела, в результате генерируя эти известные изображения замков и др - это GAN? Или нет? Или GAN позволяет делать тоже самое, но круче потому что там forward и backward шаги выделены в отдельные сети?",
"<@U06J1LG1M> У меня последнее разочарование, что генератор зашел в некоторое состояние где все генерируемые картинки стали идентичны. У тебя такое было?",
<@U06J1LG1M> А еще я понял что лучший вариант у меня получался когда у меня батчнорм был в режиме когда он и на предикт тоже “работает”. В результате получалось очень странные артефакты зависимости сгенерированного от размера батча,
"Дропаут выглядит как очень хорошая идея, надо попробовать",
"вообще любой шум с такой системе на пользу, вроде как",
о это прям как в РЛ,
"Это как раз и привело к “статическому” генератору, но зато генератор перестал побеждаться дискриминатором",
Я сейчас попробую оставить BatchNorm как был и внедрить вот это усреднение по последним эпохам. Интересно посмотреть изолировано как это влияет. В моем понимании это должно делать дискриминатор более медленным и более общим,
"<@U06J1LG1M> А как иначе то померять, еще и может оказаться что если то и другое перезапустить пару раз — результаты очень сильно изменятся",
Доля сгенерированных картинок которые были распознаны как настоящие (после обучения генератора),
"""умение развернуть pipeline сбора данных для дальнейшей аналитики""

друзья, а кто из саинтистов занимается ещё и Engineer? Где почитать как надо создавать пайплайны c Pyspark? Т.е. если задача не просто test.csv  проскорить, а модель доработать для применения в продуктиве. Обработка скорее batch, чем потоковая.",
Всмысле на каких классах предобучать?,
"тоже самое как в той ссылке что ты кидал раньше, там сначала энкодер предобучается, а тут будет дискриминатор",
"а так получается что дискриминатор быстро выучивает какую то килер фичу как отличить фейк от реала (например какой нибудь бред типа сркости) и передает это генератору, тот быстро фиксит яркость, на следующем ходу тот изменяет порог яркости и тд, кароче пока дело дойдет до серьезных фичей проходят сутки",
"Это что за саентисты такие, которым надо ""просто проскорить test.csv""? Даже на кагле во многих задачах нужно как минимум фичи из кучи таблиц собрать",
"mephistopheies: а как думаешь не может быть что это наоборот помешает? Ведь с предобучением он начнёт учиться различать 6 меток друг от друга, начнёт этой логикой заполнять параметры. А ведь задача отличить любую из этих меток от другой картинки которой вообще в них нету - это немножко другая задача вообще?",
"<@U06J1LG1M> я как раз сейчас читаю статью которую ты присылал с воркшопа NIPs, как раз  говорят что качество улучшается несравнимо",
"не исключено, я все равно отрежу от головы несколько слоев (как обычно при трансфере), при такой эвристике должны остаться более общие фичи не привязанные к классам (т.е. как раз фичи дескрипторов лица), а специфичные для датасета, т.е. классы, должны отвязаться",
"<@U0DA4J82H> конечно test собран как и train (иногда из нескольких источников). Вопрос в том,  как натренированную модель запускать для обработки десятков млн записей в продуктиве по batch.
т.е. steps = ['extract', 'feature_eng', 'xgb']
Pipeline(steps)",
просто если они в одном графе то как то не понятно как разные алгоритмы применять,
"ага это видел, но я юзаю статистику, чот субъективно лучше",
"а ну да точно, кароче на теане тоже можно разные алгоритмы, чот я туплю",
"ну да, вопрос только как это записать",
"10 млн это не много, самописные костыли вполне подойдут, как мне кажется",
"Кто еще сомневается на счет участия в кэггле со спутниками, самое время начать!
Я запилил кернел с полным пайплайном от картонк до csv файла:
<https://www.kaggle.com/drn01z3/dstl-satellite-imagery-feature-detection/end-to-end-baseline-with-u-net-keras/discussion>
И уже набралась целая банда кэгглеров 2 из которых в топ10 и участвуют в обсуждении в <#C043ZEF6K|kaggle_crackers> 
Один из них даже посвящает в свою личную жизнь (отгадайте с одного раза - кто он?)",
"И вообще, мощный комп -- это кайф, после того, как годы ютился на ноутбуке и удалённых серверах -- прямо значительная разница",
<@U06J1LG1M> попробовал внедрить как можно больше советов из хаков по гану — пока выглядит многообещающе,
Как раз дропаут из генератора выпилил,
"А, еще сделал прям как в оригинале количество слоев в генераторе. Раньше у меня был базовые слой 6 на 6 и соответственно три слоя деконволюшена, теперь 3 на 3 и 4 слоя (как в оригинале)",
"On an unrelated note, а у кого на чем крутится deep learning в продакшене?",
А подскажите релевантный канал где можно спросить про использование вмваре под убунтой ,
"А расскажите мне пожалуйста, а в чем прикол caffe? Просто потому что он старый и те кто давно начинали делать выбирали что было тогда? Или там и сейчас есть за что его выбрать?",
"<@U0AD1L5NC> Я тут просто веду переговоры в стартап где у них своя ветка каффе и вот до конца не понимал, почему именно он. Но если быстро то ясно — им на мобильном надо гонять",
"<@U0AD1L5NC> а почему ты говоришь что быстрый? Я посмотрел рандомные бенчмарки и он вроде где-то быстрее, а где-то медленнее",
"Я подамаю как к этому подступиться, чтобы это не надо было делать руками",
"Задачи в самом деле разные, так как важно не только чтобы дискриминатор был достаточно хорош, но и чтобы он правильно “делился” градиентами на генератор",
"&gt;В GAN это ещё просто не успели встроить, или будет очень медленно работать?
я пробовал большие сети в дискриминатор вставлять, как то не очень работает",
а вот только хотел спросить как оно с графаной,
"Решил таки причесать и выложить свой вариант кода для dcgan под keras, как будет готово — кину ссылку.
Владелец сайта откуда я брал спрайты дал разрешение выложить датасет, так что можно будет сразу запускать!",
"кто-нибудь в курсе, как xgb в распределенном режиме работает? тот, который на спарке запускается",
"Т.е. колонки сортирует по значениям, находит там ""split candidates"", и функцию потерь считает по ним независимо, чтобы найти где оптимальный сплит сделать. В approx режиме это независимо от того, на спарке или без, если я правильно понял.",
Я вот в логах не мог понять ничего дальше момента как rabit стартует таски.,
"Я смог разобраться только до того момента, что на каждом воркере свой xgb запускается, но как они задачи пилят, не знаю.",
"да, там в коде не сразу понятно, что происходит, поэтому спросил тут, может уже кто пытался разобраться",
kiselev1189: а какие места рекомендуешь?,
я сам с таким лицом был :thinking_face: когда у меня начали требовать машин ленинг без моделей и нейронных сетей :but_why:,
Пусть потом главный программист расскажет как он обходится с объёмом данных/количеством энграмм,
"ну которые я знаю, они все на мешок слов, высчитывают вероятность встретить эту комбинации слов из того как часто они встречаются. Можно искусственно создать учет последовательностей подключив нграммы. Но как писал уже, думал, может я каких-то супер крутых языковых моделей не знаю, которые есть и учитывают последовательность вместе с комбинацией слов кроме нграммовых",
"а как пилили заливку в графану? там из коробки все легко, или кодить пришлось?",
"То есть данные одни и те же, но сплиты разные строятся, так? Я пока не понял почему при этом не нужна синхронизация",
"Кстати, а на каких объемах данных xgboost в LinkedIn гоняют? Больше 10^7 рядов в train?",
А давно xgboost вошел в повседневную жизнь как инструмент саентологов в Linkedin?,
"Да какая разница что лить, кидаешь сообщение в формате &lt;metric path&gt; &lt;metric value&gt; &lt;metric timestamp&gt; и всё.",
"ну я рассуждаю как-то так - ну для какого-нибудь ранжирования долго и мучительно подгоняли фичи под какой-нибудь FM, ну если фичи, заточенные под линейную модель пихать в нелинейный хгбуст - профит вроде должен быть условно минимальным, на уровне сглаживания углов, ну и тогда не очень понятно зачем принципиально другую модель вводить",
"Вот я и про то, что после 10^7 там либо надо сильно глубокие деревья или другие гиперпараметры крутить так, как я пока не привык, либо еще что, но навар по точности модели от увеличения размера данных куда-то уходит после этого значения.",
где данные взяли не говорят сволочи,
"Все таки я не до конца понял как оно в распределенном режиме работает. Значит, сначала берем данные и каким то образом распределяем их по воркерам. Далее, каждый воркер строит свое дерево, а потом на этапе синхронизации выбирается лучшее? Или как-то иначе? Я не совсем понял часть про общие сплиты - это как? ",
"а, то есть все-таки синхронизация происходит на этапе выбора лучшего сплита? то есть примерно как в обычном режиме, только тут каждый воркер считает gain на своем кусочке?",
"по умолчанию в обычном (на одной машине) xgb выбирает exact режим (по крайнем мере так было, когда я в последний раз в код залезал)",
"BOW в xgb пихать - так себе занятие, как мне кажется",
"это пример, когда все  будет плохо в распределенном режиме в xgb",
"ololo прав, вроде так и пранируется. Сам еще не полностью вник как и что хотят",
а почему bow (или любая другая разреженная матрица) плохи для бустинга?,
делать полиномиальные фичи по sparse матрицам куда дороже перебора сплитов,
"Ну кормить то можно, почему нет. Просто долго и SVM будет, скорее всего, лучше",
а какие были текстовые данные? объем/язык/стемминг и тп,
я спрашиваю на каких данных у вас не завелось,
"расскажи лучше, на каких у тебя завелось :slightly_smiling_face:",
сколько деревьев и какая глубина была?,
какие параметры - hyperopt подбирал,
"если эксперименты со всей валидацией показывают что лучше, то базара нет
а вообще на малых разреженных данных можно показать как и линейную модель оверфитнуть, если не зарегуляризовывать все мощно",
"<@U0P95857C> го сюда про сложные модельки болтать. Сейчас у меня что-то в стиле символы-&gt;ембединг-&gt;свертки-&gt;макспулинг-&gt;лстм-&gt;софтмакс, работает достаточно устойчиво и точность устраивает для наших приложений, главное хорошо переносится на невиденные ранее данные реальные, но теперь мне нужно эту схему как-то распространять на другие европейские языки, ну по крайней мере продемонстрировать возможность сделать это на испанском. Как думаете, есть смысл пробовать делать трансфер лернинг для таких моделей и пробовать переучивать на другой язык например только лстм + софтмакс, если датасеты для всяких испанских-немецких заведомо мельче чем английский будут?",
"хм, я не помню ничего про работающий transfer learning, но думаю должно быть. датасеты заведомо меньше, но может быть все равно не такие маленькие? понято как точность на английском падает если датасет уменьшать?",
"а как ты хочешь делать transfer learning? что-то сходу не очевидно как его тут делать, интересно",
как на картинках делаем всегда,
"а как эти пожатые нграмы могут переносится между языками, тем более если там большая часть весов? они уже должны понимать слова и их смысл, нет?",
В данный момент мой некий код ни с какой другой нелинейностью кроме нее не работает - так она замечательна,
"<@U13E1AWCX> - Elu - наше все, давно ее втыкаю где надо и не надо.",
"Сопоставимо, но, наверно, чуть лучше, во всяком случае мне аргумент про то, что батч более центрирован, и что веса не вымораживаются, как с relu нравится. Честно, я не замерял.",
"Если кто работает с ELU - попробуйте ещё ARFA (Adaptive Rational Fraction Activation):
это что-то типа HLU <http://ieeexplore.ieee.org/document/7727220/> только обучаемое, и со скейлингом правой части 
формула:
f(x) =  { kx, x&gt;0
             { ax/(b-x) , x&lt;=0 ;  a,b &gt; 0
a - необучаемый параметр (по умолчанию a=1)
b - обучаемый параметр (как в PReLU <https://arxiv.org/abs/1502.01852> или PELU <https://arxiv.org/abs/1605.09332>)
k - необучаемый параметр, в качестве стартового значения берём единицу, а затем инициализируем с помощью LSUV <https://arxiv.org/abs/1511.06422> (как и остальные веса сети). Вроде как даёт эффект, похожий на то, что описано в п. 3.3 в <https://arxiv.org/abs/1602.07261>
параметры b и k - поканальные (channel-wise)
в отличие от ELU (или PELU) при вычислении не используется операция взятия экспоненты, так что работает быстрее, хотя обладает в целом теми же свойствами. В отличие от HLU - обучаемая (и даёт более высокую точность). Вообще в моих экспериментах (пока только на CIFAR-100 и на нашей базе в задаче распознавания лиц) было улучшение по точности классификации и скорости обучения и по сравнению с ELU/PELU и по сравнению с HLU. Особенно помогал скейлинг правой части с помощью параметра k. В общем, у кого есть время и желание - попробуйте, может оно и в ваших задачах будет работать :slightly_smiling_face:",
<@U06K9ELB1> А ты не желаешь показать как все это действо прикручивается к задаче про спутники?,
"Я подумаю.
Да, вот ещё вариант ARFA, скрещенный с RReLU <https://arxiv.org/abs/1505.00853> (тут уже я экспериментов не проводил, так что пока только в качестве идеи). В RReLU в отличие от PReLU для указания угла наклона левой части функции вместо обучаемого параметра используется случайное значение в заранее определённых границах. Добавляемая таким образом случайность (по словам авторов статьи) помогает бороться с переобучением сети.( Авторы этой статьи кстати заняли второе место в первом Data Science Bowl на kaggle). В тестовом режиме используется середина диапазона (для детерминистического результата).

В случае ARFA такую случайность также можно добавить, причём в разные места (можно поэкспериментировать). Можно, например, сделать параметр b не обучаемым, а рандомным, в некотором диапазоне значений (по аналогии с RReLU вместо PReLU)
Но в этом случае мы теряем способность к обучению параметра b, которая нам может пригодиться. Для того, чтобы этого избежать, мы можем сделать следующее:
- на каждой итерации обучения случайным образом определять текущий ""режим работы"" активационной функции - либо она у нас сейчас обучаемая либо рандомизированная (можно с разными вероятностями - например, 0.8 и 0.2). Если режим работы - обучаемая функция, то наша функция - это стандартная ARFA (и мы с ней работаем как указано выше, параметр b - обучается), а если режим работы - рандомизированная функция, то мы используем следующую формулу:

f(x) =  { kx, x&gt;0
             { ax/((b + r*b) -x) , x&lt;=0 ;  a,b &gt; 0
r - случайное число в неком диапазоне, например, от -0.5 до 0.5, а b - это текущее значение обучаемого параметра b. Получается что мы добавляем некоторую случайность относительно (""вокруг"") текущего положения параметра b. В этой фазе параметр b не обучается.

В тестовом режиме используем просто значение параметра b. С точки зрения имплементации режим работы лучше выбирать сразу для всего слоя, а не для отдельных каналов. Ещё можно диапазон значений r постепенно уменьшать к концу обучения (как иногда делают с dropout)

В итоге получается, что функция является рандомизированной (т.е. по идее как и RReLU лучше борется с переобучением сети), при этом оставаясь обучаемой (как PReLU).

Но это неточно (я пока нормальные эксперименты не проводил).",
Тоже кто хочет - попробуйте :slightly_smiling_face:,
" <@U1BAKQH2M> Не очень понял про то, как прохождение по векторному представлению лстм-кой решает проблему порядка слов. Поясни плиз. ",
"Ребят, как обстоят дела со скейлингом изображений? Можно ли юзать это для продакш решений? В публикациях, которые встречались, все очень круто, но хочется узнать в реальном мире как ",
он как то в мейле рассказывал про их решения,
"тут недавно <http://ivi.ru|ivi.ru>  озвучили интересную задачу: как построить модель предсказания успеха фильма ( в кассовых сборах например), которая применима на данных, что есть до релиза даже на закрытом показе (то есть формально о фильме известны лишь сухие факты: состав группы, бюджет, местро съемок, продолжительность, + вся иная информация, которую можно достать в это время). Интересно, как бы вы подошли к построению такой модели?",
"ну желательно, чтобы можно было интерпретировать результат. как говорят, нужна модель с физическим смыслом. Если вдруг что-то пойдет не так, то будут неправильно вложены деньги больших дядь, чего они боятся изначально",
"Ну тогда линейная регрессия + кто-то кто на уши большим дядям горазд вешать, про то, как коэффициенты интерпретировать.",
"посмотреть как сделано в boxofficemojo и повторить то же самое (вроде даже интерактивный калькулятор гдето был)
а потом уже думать как наворачивать доп фичи про маркетинг\конкурентов, впродолжение что сказал <@U0JHK9001>",
"Осталось узнать, что за тулза у нвидиа, и кто ее в мейле показывал :slightly_smiling_face:",
"и есть разные вещи что можно предсказывать:
-суммарное бабло, потраченное в эту неделю\день на каждом рынке
-стартовые выходные\неделя для премьеры фильма, в бабле. 
-стартовые выходные\неделя для премьеры фильма, в процентах от кассы (из серии, почему никто не сомневается что следующий Аватар соберет ярды). 
-относительное падение в первую неделю\две для фильма

и както все это рекомбинировать",
"и опять к комменту <@U0JHK9001> , когда мало данных про историю студий, все равно хз кто сколько соберет. смотри фильмы по вселенной DC с качественным маркетингом, завышенными ожиданиями, и сборами сильно недотягивающими по хайпу\ожиданиям от Marvel",
"а как бы это посчитать, если не из кинопоиска? (не факт, что там норм сигнал)",
"А upscaling это что? Это когда берется картинка небольшого размера на вход а на выходе она же в высоком разрешении? Читал я про это где-то и картинки были красивые, но еще тогда терзали меня сомнения что это будет работать на всех картинках, а не только на тех которые сиииииильно похожи на train set.",
"все-таки это тот случай, когда обычный аналитик с экселем сделает это в разы лучше, руками
не так много релизов, чтобы автоматизировать и терять в качестве",
"Ну, это реальный tradeoff, возникающий во многих практических приложениях:
(надежда на) интерпретируемость модели попроще (как правило линейные модели или одиночные деревья) с точностью\целевыми метриками X1 с одной стороны. и предположительно черный ящик с точностью\целевой метрикой X2. 

Если разница между X1 и X2 невелика (определяется тем кто ставит задачу), стоит выбрать модель попроще. 
При этом надо учитывать, что линейные модели могут быть не абсолютно прозрачными, а даже GBM\:xgboost: при достаточном желании можно трактовать и упрощать.

…и это не единственный незадокументированный tradeoff :povar:",
"Так что идеального трактования и управления модели можно добиться, как верно высказались выше, подвешенным языком, красивыми слайдами и внушением иллюзии контроля",
"<@U0JHK9001> думаю, перебить аналитика со знанием индустрии и excel можно очень хорошей работой с фичами (опять же, как делал нетфликс с их огромными анкетами и толпами нанятых асессоров), но это реально долго и дорого",
"Да, увеличение разрешения
Я почему апскейлинг должен работать только если похоже на трейн сет? Почему та же логика не работает для классификации, к примеру?

Я понимаю логику, что информацию нельзя “вернуть”, но ведь в реальной жизни у нас картинки очень даже предсказуемы в локальности. То есть конечно можно ""сбрить"" какие-то детали, но в среднем нет причин этому происходить",
"еще про инсайты, но это вроде как многие к такой мысле приходят",
"возникла гипотеза, что трейлеры сильно влияют на то, пойдут люди в кино или нет, и даже на впечатление, так как оно есть произведение увиедненного на ожидания",
"так то часто фильм проваливается по рейтингу, но при этом сборы норм
вот ""бетмен против супермена"" в пример
и, по-моему, очень редко блокбастер собирает мало денег
тут куда интереснее предсказать, какой фильм с небольшим бюджетом сможет выстрелить",
"зачем, кстати, решать эту задачу? это могло бы понадобится студии, чтобы понять, какой фильм снимать, чтобы не пролететь, а какой прок иви от этого? у них чтоль большой фикс при закупке прав, и малая комиссия на каждый показ?",
"отправить своего человека на предпоказ, он скажет брать/не брать и какой ценник поставить",
<@U0DA4J82H> куда ж сейчас без мл :yoba_pled: ,
"&gt;А upscaling это что? Это когда берется картинка небольшого размера на вход а на выходе она же в высоком разрешении
це апскейлинг, а если то же самое только хорошего качества, то это super resolution",
"<@U3XE6N4E4> да, ты переобучишься из за использования знания о таргете
Раннюю остановку используют если у тебя кроме тестовой выборки есть еще и валидационная
В этом случае ты останавливаешь обучение исходя из скора на валидационной выборке, а потом для оценки качества работы модели используешь тестовую
Как это делается правильно на кросс валидации я не знаю(",
"<@U0B4L5676>: ну как вариант можно, например, при 5 фолдовой схеме использовать 3 фонда для обучения, 1 для остановки, 1 для получения out of fold предикшенов. Но тогда данных для обучения меньше используется и может получится, что иметь небольшой лик окажется немного выгодней, чем использовать меньше данных ",
"а прокатные конторы не занимаются подобной оценкой? какой-нибудь сони пикчерз же не просто отбалды деньги в маркетинг спендит, а наверняка по какой-нибудь модели(ограничениям). 
я когда в маркетинговом агенстве работал, мы хотели попробовать предсказывать сборы первой недели, и тем самым обосновывать стоимость своих услуг на пре-релизе. в итоге нас стопнули, сказали, что у них свои модели есть, но что там  - :eyes:",
тут есть любители ганов - в статье про SRGAN как раз  очень красивые картинки с суперрезолюшном,
<@U0XF4GAM8>: видел тут обсуждения по твоему вопросу <https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov> . Там тоже вроде как речь про tradeoff между допустимым ликом и количеством данных для обучения моделей,
Имеет кстати смысл добавлять в ансамбль несколько бустингов на том же наборе фичей но с разными параметрами? Какие параметры лучше менять чтобы они поменьше коррелировали? Или это все почти одно и то же будет и лучше других моделек покидать,
"Ребят, есть вопрос
У меня есть корпус научных текстов, публицистики, рекламы и так далее. Я хочу как-то объяснить компуктору разницу в стилистике текстов, но не хочу переобучиться на терминах. Ну то есть не хочу, чтобы текст детектился как ""научный стиль"" только от того, что в нем есть слова ""сингулярность"" или ""аминокислотный"".
1) Есть ли смысл оставлять в тексте *только* стоп-слова, вводные конструкции и прочие, а значащие термины, несущие смысл, обнулять? 
2) Есть ли смысл обращать внимание на пунктуацию и как? (интуитивно кажется, что нужно)
3) Как на основе этого выдать рекомендацию типа ""убери слово 'типа', а то не научно как-то""?",
"но 1 можно попробовать, почему бы и нет",
А текст в данном случае как набор векторов из натренированного  word2vec лучше представить?,
лучше как токены и н-граммы,
"друзья, а посоветуйте где бы можно найти телепрограмму центрального телевидения. чтоб как можно побольше ее там было :slightly_smiling_face:. на яндексе вон вроде за 2 последние недели только. в идеале  лет за 20 б :slightly_smiling_face:",
а есть идея как можно видосики ЧГК  обработать?,
Где вообще можно норм архитектуру для такой сети глянуть?,
"спасибо!
Если кто-то запустит, пришлите комментарии какие были сложности / что дописать в ридми",
"А у кого нибудь есть mit places поделится?
Мне много не нужно, порядка 10^4 хватит, главное чтобы как можно больше видов фонов было, для аугментации",
<@U2TP5JELS> добавил описание к датасету и в двух словах зачем нужен DCGAN,
"собрался испанский твиттер, разметил так же примерно как размечался sentiment140, сбалансированный датасет, примерно 30к трейн, 5к тест",
"Всем привет. У меня есть блокнот юпитера, и в нем я вывожу довольно крупный датафрейм. И мне надо, чтобы при экспорте в html его не было видно сразу весь, а было окошко со скроллингом, как в режиме toggle. Горизонтальный скроллинг там есть по умолчанию, а вертикального нет. Как это можно сделать?
Ну или можно как-нибудь свернуть вывод, чтобы потом можно было развернуть назад? Потому что если сделать там html-кнопочку для сворачивания, она сворачиватся вместе с выводом.",
а как мне исходные фичи проектировать если это последовательности символов?,
"Кто-нибудь пробовал использовать нестандартную функцию ошибки в задаче классификации для оптимизации F1-score? Уже не первый раз сталкиваюсь с ситуацией, когда ошибка (кросс-энтропия) на валидационной выборке растёт, а с ней и целевая метрика (F1-score) до какого момента, после чего болтается около одного значения при продолжающемся росте ошибке, т.е. явном переобучении.",
"гайз, а как мне производные от фамилий превращать в фамилии? например, марксисткий или ленинский -- в Маркса и Ленина соответственно",
разметка по эмоджии?а там везде эмоджи встречаются? с человеческой оценкой это как соотносится?,
"ну я когда на английском обучал - учил на автоматической разметке, а тестил на датасетах размеченных вручную, жить можно",
"ну цифры эти большого значения не имеют, зависят конечно от того как датасеты собирать, но в целом разметка по эмоджи для сентиментов - жизнеспособный боль-менее вариант если нужно много даты на произвольном языке (а у меня прям обязательное требование - масштабироваться под любой язык за какое-то адекватное время)",
"ну да. и этот график кажется не сможет получить локейшн :slightly_smiling_face:. не уверен, но просто помню как  сталкивался с элементами безопасности.",
"Привет. 
Интересно узнать, как вы пользуетесь сервисами Amazon (или чего-то другого) для обучения нейросеток. Желательно какой-нибудь актуальный гайд с самыми дешевыми способами. 
Цель - поучиться, то есть времени на машине тратить буду много и амазоновские предложения кажутся пока дороговатыми. Возможно, я просто не знаю как ими правильно пользоваться.",
"зачем тебе дожидаться, если ты хочешь просто поучиться?",
<@U1G303UTW> Какие ami можешь посоветовать?,
"но сейчас цены вроде как стабильны, то есть умирать не должен",
"<@U2TP5JELS> меньше компрессия, поэтому больше файл. Почему он занимает 5Гб я не понимаю, должен быть 1Гб",
а керас и теано у тебя какой версии ?,
"в 3-ем питоне, как минимум",
"И как?) Ок или не ок? 
Like New
 Very Good
 Good
 Acceptable

с какой категорией был опыт?",
"Опыт такой, что ничем не отличается от покупки новой
У меня Prime поэтому стараюсь выбирать магазы с этой возможностью. Расмматриваю это как доп страховку на случай проблем - если что, разбираться с амазоном",
"С удивлением обнаружил, что материнская плата в домашнем компе не поддерживает больше 32Gb ram, а хочется 128Gb

=&gt; Пора апгрейдится.

И проц хочется пошустрее, с большим чем 8 числом ядер.

Кто тут эксперт? Какую материнку и проц брать? Или просто не париться, и пальцем ткнуть ибо они все на одно лицо?

Покупается исключительно под kaggle.",
<@U34Q3KU8H> как насчёт 2хXeon E5-2670 + 128Gb + мать за 600$?,
Какой у тебя бюджет короче и толерантность к геморою?,
"Разные можно, но я хз как оно будет работать, если на них будет параллельно что-то учиться если оно будет требовать синхронизации параметров -- если одна будет убегать и ждать вторую",
"Мне кажется, тут много не очень актуальной инфы, так как живя рядом с самой Nvidia можно у них и p100 выпросить и новую Volta, сходив на конференцию. Ну и возможностей по покупке больше. Если честно, я бы сам отложил покупку какого-нибудь топового решения, особенно попридержал вложения в графические процессоры. А так как и рекомендует чатик купить что-нибудь б/у, только не очень старое, чтобы можно было потом продать, а не выбросить. ИМХО, сейчас по цене/качеству Intel Xeon E5-2678 v3 последних инженерных версий выглядит очень привлекательно, особенно после того, как его смогли разогнать 3.0 GHz стабильно на ядро с адекватным TDP на мамках ASRock (<https://goo.gl/b48u0j> , на ютюбе на русском можно посмотреть тут <https://youtu.be/kixNutamWiM>). Да, 128 гигов оперативы на ddr4 выйдет дороже, но она будет быстрее, что критично для нейросеток и какого-нибудь xgboost'а. (Хотя уже есть xgboost на GPU, и твой титан будет считать быстрее, чем топовые CPU, если данные влезут в память видеокарты, если не влезут, то там уже может быть спорно и не такое преимущество) Плюс, ddr4 все-таки продержится ещё годик, может два, так что продать можно успеть.",
"А можно больше деталей про кто такие ""p100"", на какую конференцию сходить и у кого надо просить?",
"<http://blog.stratospark.com/multiprocess-image-augmentation-keras.html>
Длиннопост о том, как парень добавил асинхронное батчевание в Керас. 
А что, его там правда не было?",
"Отличная книга, где всё разжёвывается так что дальше некуда- Tensorflow for machine intelligence, читая вместе с документацией",
"ternaus: это я так, это был не основной посыл моего сообщения, я же написал, что смысла в GPU вкладываться нет особого, они там искусственно задерживают/завышают стоимость технологий. И P100 как раз про это. Это самый крутой доступный для enterprise графический ускоритель, который стоит очень дорого, несоразмерно эффективности, к сожалению (хотя тут можно спорить, если кому-то нужно что-то очень быстрое и есть хорошие деньги - пожалуйста): <http://www.nvidia.com/object/tesla-p100.html> . А насчет куда ходить - nvidia на своих анонс конференциях сразу продает карточки, особенно сегмента titan. Так что вполне скоро можно будет таким образом купить новый Titan на архитектуре Volta, но это слухи (опять же, они могут задерживать HBM2-3 память и свои технологии очень долго, сейчас есть явный дефицит памяти всех популярных видов, так что люди готовы покупать заведомо отстающие технологии по хорошей цене. Так что ждать теоретически можно долго, но и прирост опять будет не столь значительным, каким бы он мог быть).",
"По поводу генератора керас с несколькими воркерами, то у меня вылетало с ошибкой, когда я в `model.fit_generator` ставил  `nb_worker&gt;1`, помогло решение отсюда <https://github.com/fchollet/keras/issues/1638>",
"cepera_ang: а кто говорил про то, что подарят? Я немного не так выразил свою мысль, Выпросить в том плане, чтобы тебе её продали одну, они так продаются то только ЦОДам, а от анонса титана до его ритейл продажи обычно нужно месяц ждать(а если ты в России, то вообще полгода можно), а на конференции можно было сразу купить. Так что я говорил про возможность купить. Из России ты P100 даже не купишь домой за приемлемые деньги.",
"Ходить, выпрашивать возможность купить P100, которая стоит как сбитый боинг и по скорости для DL задач будет сравнима с 1080, в чём смысл?",
"Возник вопрос по поводу RL статей, где  тестируют агентов на атари играх. В АЛЕ ввели случайность, что каждое действие может с вероятностью 25% не приняться средой и повториться предыдущее действие. Судя по логу изменений это было сделано еще 22 июня 15 года. Никто не разбирался в статьях очки приведены для ""стохастичной"" версии АЛЕ или для ""deterministic""?
У меня у самого подозрение, что для ""deterministic"", так как скорее всего deepmind использует клон АЛЕ - xitari, в котором этого нет. И почти во всех статьях они еще приводят очки для human starts, а как аргумент пишут, что иначе агент может просто запомнить последовательность действий.",
"<@U0JJ69UB1>,  в коде Deepmind'а скип между фреймами всегда 4. И нет вероятности повторить предыдущее действие. Я заглядывал туда недавно. И openAI, где ссылались на {AtariGame}Deterministic-v3 как на аналог среды используемой deepmind'ом. Но насчет всех статей не знаю)",
"Хотя можно подождать ещё годик, когда они снимутся с продажи и вывалятся из ДЦ  и будут ещё дешевле",
"Привет. Вопрос к знатокам TF. Запускаю distributed код, и наблюдаю такую магию: на компе где устанавлена версия с CUDA, все работает нормально. На версии TF с CPU скрипт виснет на этапе открытия сессии. Везде стоит tf 0.12.1, Mint/Ubuntu x64. Минимальный код для воспроизведения ошибки:
<https://gist.github.com/dd210/2a5b93ef91b7f2e45b448ded38fbfab0>
Кто нибудь сталкивался? У меня кончились все идеи.",
"Пздц бесит, когда проснулся и бежишь смотреть как там сетка обучилась",
"как раз позавчера продал 19"" асер, мой первый моник, который я в 2006 купил",
Прост чот жалко 16 если можно отдать 8. Но benq вроде исправил ситуацию. Спасибо,
ну я наверное и возьму ips от dell как увижу коннект через usb c,
"так да, тот dell что я скинул как раз не ips и 240Гц",
"а есть у кого опыт использования широкоформатных дисплеев, которые 25:10 и прочие curved? я пару отзывов слышал, что удобно и многокода можно размазать, но пока не совсем репрезентативно.",
"Хотя, первый отзыв на амазоне раскрывает как пользоваться:",
И вывести туда показатели как сетки учатся (и курсы криптовалют),
6 огромных мониторов показывают как сетка обучается на одном хиленьком маленьком десктопе?,
"И это только на кухне, как у <@U07V1URT9>",
"Он выглядит охеренно как телевизор, но в плане плотности информации -- это всё фигня",
"я тоже за прогресс, но не вижу смысла покупать девайсы навырост. к тому времени, как технология станет актуальной, девайс у меня уже будет другой",
"А я не спешу :slightly_smiling_face: Пошутил насчёт 4к, у меня телефон, стыдно сказать, уже 4 летней давности, но я не вижу зачем и на что менять (но я не смотрел последний год-полтора )",
"по DPI как 27"" 2560x1440",
а как у вас на них интерфейсы выглядят?,
"<@U041P485A> да, размер точки такой же как на 27"" 2560x1440, делю экран на две половины или четыре fullhd",
"Вопрос к аудитории:
накидайте, пожалуйста, все методы какими можно решить задачу сентимент анализа предложений. (можно на примере задачи <@U1BAKQH2M> описанной немного выше). Нужны все варианты, начиная от логистической регрессии на н-граммах до каких-то конволюционнок. Буду благодарен, спасибо",
"Тут, по сути, обычная классификация текста, так что все методы пойдут. Специфично для анализа тональности - какие фичи брать. Ну, например, кроме н-грамм вроде иногда еще “псевдо-токены” делают - всякие “не” приклеивают к словам, чтоб отрицание лучше обрабатывать. Или словари составляют с +/- словами. Еще специфично - где брать тренировочные данные; часто вручную не размечают, а используют эвристики вроде “есть смайл”, либо тональность и так указана (отзывы с рейтингами - хотя это скорее для документов). Плюс сложности с постановкой задачи - может ведь предложение быть + к одному аспекту, но минус - к другому, субъективная/объективная оценка и т.д. Это задачу немного уводит от просто классификации предложений. У Стенфорда была демка интересная: <http://nlp.stanford.edu/sentiment/> - лежит сейчас, правда. Но <http://nlp.stanford.edu/sentiment/treebank.html> можно посмотреть.",
<@U1BAKQH2M> ты какой туториал/статью юзаешь для применения cnn?,
"ага, похоже ошибка в том что передается что-то не то, как rocknrollnerd писал. я pandas не так хорошо знаю(",
Выглядит как иллюстрации к закону о домашнем насилии ,
<@U2AJE72Q6>: а кто тогда Макар?),
"<@U053R9RS6> девиантарт ок, но что если пихнуть в него немного гитхабовщины и помимо артов добавить возможность еще и код размещать?
то есть маркетплейс исключительно под нейроарт (без конкуренции с людьми, как на девиантарте было бы).

(полагаю, это слишком уж :more-layers: идея, но чо бы нет)",
"python 2 vs 3?

Да не должнго быть. Какой планируешь использовать, под тот и ставь.",
"<@U0989QUVC> Брат. Нас трое. Этот разговор ни по теме доклада, ни по теме канала. Если остались какие либо вопросы, можно написать в лс. ",
"<@U14BPHDK6> Честно говоря, я не понял, что тут предлагается делать, кек. Ну, то есть да, я могу  предсказать вероятности, но как это помогает?",
Потом поправить параметры и посмотреть куда двинулась полнота,
"У большинства (всех?) моделей в sklearn целевая функция фиксирована и изменению не подлежит. Максимум, что можно сделать -- настроить гиперпараметры таким образом, чтобы целевая функция максимально коррелировала с желаемой метрикой, что достигается как раз с помощью GridSearch ",
"<@U04ELQZAU> не у всех, у какого нибудь SGDClassifier можно менять loss",
"Как вообще можно тюнить одну полноту? Модель, константно выдающая целевой класс будет иметь полноту 1, например. Надо уж тогда за AUC-PR следить, а не за полнотой.",
"Уже через неделю в Москве будет проходить четвёртый <#C0E2T8WNM|data_fest>, где ML тренировка <#C1CEM43TJ|mltrainings_live> будет одной из секций первого дня. Будем разбирать следующие соревнования:
- Евгений Патеха и Александр Пономарчук расскажут два разных подхода к решению Kaggle Santander по предсказанию использования банковских услуг, которые привели их к 7 и 8 местам соответственно
- Михаил Горкунов и Дмитрий Загорулькин расскажут, как они решали контест Сбербанка на boosters по оценке коммерческой залоговой недвижимости и заняли 1 место в составе команды skyNET.

Регистрируйтесь на <http://datafest.ru|datafest.ru> и заходите на нашу секцию. Кроме ML тренировки там будет множество других интересных докладов и крутых спикеров!",
а как проверить на чем сейчас рабочий стол рендерится?,
"Какой objective лучше использовать при бинарной классификации с несбалансированными классами? вообще интересует максимизация pr auc на маленьком классе, есть какой-то лосс который может помочь быстрее это выучить?",
Куда откусывает много от десктопа :slightly_smiling_face:,
"Очень трудно пользоваться, если что-то мощно считается, ощущение как будто винду 98 поставили на тачку с 4мб памяти",
"Да у меня прямо сейчас считается, загрузка CPU 8%, загрузка GPU -- 100%, контролера памяти и шины ~60% и всё тормозит как ппц",
"графический интерфейс -- всё рисуется как в старые добрые времена, если драйверы на видяху не поставить",
"хотя имхо больше похоже, что ботлнек таки не в куде :slightly_smiling_face: но надо тестить",
"Еще есть другой лайфхак: устраиваешься в фирму, где сотрудникам дается подписка на safaribooks, а потом увольняешься",
"А еще можно платить за книги, как вам лайфхак?",
"<@U14CTBLFJ> я как раз так и сделал, установил еще расширение print friendly pdf и печатал все страницы в PDF. Но для 900 страниц так затруднительно.",
"Привет! Я только начинаю свой путь в датасаенсе, прошу совета по железу)
Подскажите, пожалуйста, какие требования к железу, чтобы можно было обучаться-заниматься датасаенсом? (преимущественно, ML интересует)
Хватит ли под такие задачки, например, макбука про 15""? И какие должны быть у него параметры?
Или ноутбук вообще не вариант в этом деле?",
"Я пока примерно предполагаю три варианта:
1. мощный ноут (и это самый классный вариант, так как мне крайне важна мобильность)
2. средний ноут (подскажите хар-ки) + где-то подключаться к каким-то облачным сервисам (есть ли бюджетные варианты?)
3. средний ноут + где-то делать свой сервер (подскажите хар-ки и примерный бюджет), к которому постоянно подключаться. (вариант самый не удобный, так как могу куда-то уезжать и переезжать)",
"Что есть сейчас:
Классный любимый 17"" ноут msi gs72 6qe (из штатов). Хар-ки: i7-6700hq 2.6 Ghz 4-core, 16 gb ram (сложно, но можно расширить на 32 gb), nvidia gtx970M 3gb gddr5 (и встроенная intel hd 530), 17"" IPS панель с 4К разрешением (любовь с первого взгляда), 256 gb ssd + 1 Tb hdd

Что не так?
Всё так, очень его люблю, но есть два момента:
1. у меня не получается завести на нём линукс (пробовала убунту и её модификации)
2. вес. Я девочка, и хоть он срезвычайно лёгкий для 17"" ноута, мне всё равно его тяжело таскать (2.66 кг ноут + 0.65 зарядка = 3.31 кг суммарно). Изначально кейс использования предполагал значительно реже ездить с ним куда-то.
3. пока не понимаю, как нужно настраивать его (и хватит ли его) под задачи обучения датасаенса и решения кегглов.

Поэтому подумываю о том, чтобы его продать (и громко плакать), и купить что-то более компактное.",
"3 вариант, когда уже понятно во что упирается ноут и насколько дорого будет обходиться соразмерное облако",
а прошка какого года - норм? я в макбуках пока почти не разбираюсь,
"<@U3USG6R3P> любое где есть 16гб памяти, больше не дают, а видеокарты в макакбуках всё равно куду не тянут.",
куду поставил на свой эйр уже :kekeke: ?,
а еще лучше когда на работе его выдают,
cepera_ang: 1080 + 128гб уже стоят как эйр,
"<@U1BAKQH2M> как я понимаю, в моём ноуте три сложных момента:
1. uefi
2. вторая карта в виде nvidia gtx970
3. экран с разрешением 4К
+ может какие-то ещё проблемы совместимости и драйверов по хитрому железу, которые msi впихнул в этот ноут (типа звуковой системы, или клавы с подсветкой, или ещё чего)

удалось запустить загрузочную флешку (нужно было дописать acpi=off) и даже установить убунту на второй диск (hdd). Граб появляется, дальше при выборе загрузки убунты появляется загрузочное окно с надписью убунту и точками внизу, но система так и не загружается",
печаль. а какой вариант может подойти для начинающего?,
"ну если всё таки на буке можно, то да, чтоб на буке гонять. а если не бук, то как ещё можно?",
"просто, как правило, датасеты делятся на два типа, нормально влазиющие в память ( до 30 гб) и оч много (100гб+). вот первые еще имеет смысл гонять на буках, вторые все равно надо вытаскивать на серваки",
"если на буке гонять, то это бук как у тебя сейчас. большой, массивный, часто с припиской игровой",
"мой не то, чтобы массивный (2,7 см в ширину), но из-за диагонали 17"" все-таки не самый легкий. А nvidia gtx970 с 3гб как-то может делу помочь? где про это почитать можно (для чайников)",
"<@U14BPHDK6> вот у нас есть какая-то тема (запрос в гугл). Мы хотим найти слова, которые лучше всего использовать для хорошего СЕО текста, чтобы он имел рейтинг для этой темы.

Сначала парсим гугл, вытаскиваем все возможные слова из выдачи, ранжируем их каким-нибудь простым методом типа tf-idf. Затем находим народ, и просим разметить часть слов - сказать, какие имеют отношение к теме, а какие нет.

Потом строим модель, в качестве фич можно использовать статистику для каждого из  слов: частота использования в тексте, в title, в h1-h6, в ссылках и т.п, так же добавляем всякие idf и word2vec фичи и много чего еще. Затем все это суется в лог. регрессию (можно xgb).

Это дает слова, которые лучше использовать в тексте. Так же можно посчитать, сколько раз каждое слово должно использоваться.

Для оценки качества текста можно посмотреть на вывод модели и посчитать, сколько предложенных слов используется. Если используются все - текст хороший.",
"не совсем, отчасти только. Скорее понять, какие слова важно использовать",
"У меня наверное глупый вопрос, но я не могу понять что за softmax function? Для чего она нужна? Вроде бы уже везде прочитал, понимаю как работает, а практический смысл ускользает",
а где _обычные человеческие тексты_ берете?,
"Всем привет! 

Мы тут один интересный вебинар готовим - <https://flyelephant.net/events/webinar-introduction-to-singularity>. Поговорим о Singularity (<http://singularity.lbl.gov/>). Рассказывать будет лидер проекта - Gregory (HPC architect and developer at BerkeleyLab)  Если в двух словах, то это докер для научных расчетов и как мне видится, очень перспективная штука для дата сайнс. Присоединяйтесь, должно быть очень интересно :wink:

П.С. Также мы будем делать похожие вебинары с лидерами интересных проектов каждый месяц, так что можно заказывать, кого было бы интересно вам послушать (пишите в личку). Предварительно, следующий будет с ребятами из Julia :wink:",
"<@U14BPHDK6> не совсем понятно, можешь более специфицировать, какие например функции ты там видишь? Возможность продавать код/генераторы арта? Вместо просто картинок? Или совместить с печатанием их? А то гитхабовщина - она уже есть - на гитхабе, люди публикуют картинки, снизу ссылки на их гитхаб, и так уже всё работает.",
А так идея интересная. Лично мне кажется что нейроарт - очень большая но нераскрытая пока область. И не очень понятно как её раскрывать.,
"Всем привет. Может кто знает, есть ли для русского языка датасеты с проставленными метками семантической близости пар текстов? Аналог английского Semantic Textual Similarity с SemEval. Помимо <http://Paraphraser.ru|Paraphraser.ru>",
"Разница не в производительности, а в том, что иксы не отжирают оперативу видюхи. И рендеринг рабочего стола не глючит, когда считаешь что-то на вдюхе.",
Но какой за этим математический смысл?,
"<@U0AD1L5NC> если взять категориальное распределение и записать его, как пример из экспоненциального семейства, то софтмакс там вылезает сам собой. Лично я это проделывал только для Бернули, но википедия, говорит, что для N классов это тоже работает <https://en.wikipedia.org/wiki/Generalized_linear_model>",
"А можно чуть подробнее? Какое изначальное допущение на модель, что является результатом?",
"<@U0AD1L5NC> можно еще не через GLM, а через вывод линейного и квадратичного дискриминанта, тут <https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb> в самом конце есть раздел “Logistic regression”, там как раз вывод верез предположение о нормальности фичей при заданном классе p(x | c = i) ~ N(m_i, s_i); если проделать тоже самое при количестве классов больеш двух, то получится несколько log odds, и при том жепредположении о фичах будет софтмакс",
"вообще куда более интересная связь софтмакса с распределением Гиббса, никакой математики, просто если трактовать классы как состояния динамической системы, это у Хинтона в статьях про машины Больцмана описано",
"так сходу не вспомню в какой, вроде как это было и в обычно машине больцмана и в ограниченной

точно помню что про эту связь упоминается в 11 главе третьего издания Саймона Хайкина neurl networks and learning machines",
"<@U1Z78RL3X> так а все-таки зачем avi-файл тогда, если у него одни минусы?",
"Кто в чем презентации пишет?

В универе я использовал Latex + beamer. Симпатично, но геморно.

По работе Google Slides -  страшно, но бысто.

Что есть такое, что быстро и красиво?",
"я взаимосвязь курсов стенфорда нарисовать решил фо фан. 
но не могу придумать как это подобного стиля нарисовать на js, чтобы интерактивные тултипы были",
"слайды все-таки текстом писать напряжно, хочется постоянно видеть как оно получается, и там что-то передвинуть/местами поменять",
"ребят, волнует следующий вопрос: вот берем мы логистическую регрессию, допустим без регуляризаций, чтобы все было по-честному. На выходе получаем вероятности классов. Какие ограничения накладываются на распределения внутри классов, чтобы полученные вероятности были честными?
в <http://www.machinelearning.ru/wiki/index.php?title=Логистическая_регрессия#eq:1> есть такие слова:
""Теорема. Пусть:
функции правдоподобия (плотности распределения) классов p_y(x) принадлежат экспонентному семейству плотностей"" и пошли дальше формулы...
я так понял из формул, что от нас ожидают, что два класса имеют нормальные (или какие-то похожие на нормальные?) с одинаковой дисперсей, но разными мат. ожиданиями?",
"&gt; А в обычной линейной регрессии одинаковая дисперсия кажется странным условием?
о какой дисперсии идет речь? о дисперсии ошибки по y? или о дисперсии фичи x?",
"Что-то я не понял, а почему это у tf нечитаемые исходники?",
"Мой совет: изучение логистической регрессии начни с GLM. Любая книжка, где этот фреймворк строится последовательно из базовых принципов.",
Там как раз первые пару глав очень хорошее введение даётся.,
"Распределения внутри классов фичей это  x|y.  Результат конечно от него зависит, вопрос же был когда получатся вероятности. Пусть x_i значение фичей для i-го объекта, y_i значение метки класса, b(x_i) любая функция из X в [0;1], наш алгоритм.Тогда минимизациям м.о  лог-лосс потерь  обеспечивает что b*(x_i) = p(y=1|x_i).",
"если b(x_i) - то я смогу построить такую, что м.о. потерь будет 0. При этом она будет всегда отдавать ровно 0 или 1. И p(y=1|x_i) будет давать всегда 1 или 0. Но для всех x_i вне тестовой выборки она будет отдавать рандом. Так что мы накладываем ограничение на то, какие у нас бывают x при заданном y - про это и есть тот пункт в <http://www.machinelearning.ru/wiki/index.php?title=Логистическая_регрессия> : “плотности распределения классов принадлежат экспонентному семейству плотностей “. И тогда можно взять сигмоиду на место произвольной функции b(x_i), и она будет отдавать нерандомные вероятности для элементов тестовой выборки",
"про |y-z| не понял (  Но у меня есть еще 1 аргумент: когда оптимизировали лог-лосс, мы уже сделали предположение о типе распределения плотности класса. Потому что оптимизация лог-лосса - на самом деле подбор параметра распределения в экспоненциальном распределении",
"Мне жаль, что там R. Но тут есть неразрешимая проблема - мат статистика это сугубо прикладная наука, поэтому чистые теоремы довольно бесполезны, нужны примеры. А для примеров лучше всего подходит или R или Python. И даже R  в данном конкретном случае наверное лучше питона, потому что по сути `lm` это чуть ли не центральная функция вокруг которой он строился.

По большому счёту, в первых двух главах R как такового очень мало, там крошечные сниппеты, буквально на три-четыре строчки. При желании они легко переписываются на питоне (главное найти правильные аналоги, типа qr разложения).",
"Лог. регрессию можно ввести, задав обобщённую линейную модель `logit(p) = w^T x + b`, но тогда нужно верить в её линейность
К тому же самому можно прийти, рассмотрев, как написано на <http://machinelearning.ru|machinelearning.ru>, модель, где есть 2 класса, распределения которых отличаются лишь средним. Тогда оптимальное решающее правило имеет линейный вид `w^T x + b &gt; 0` для `w` и `b`, зависящих от распределений классов (правда, я не уверен, можно ли здесь где-либо минимизировать rmse)",
"есть штука для распознавания лиц, потенциально она должа обслуживать много камер и комнат (прям десяток), есть ли смысл ставить гпу-сервер под предикт, и если есть, то какое железо?",
"Зависит от того как и что у тебя робит. 
Если там покадрово, посчитай сколько стоит предикт. Вероятно нет необходимости распознавать каждый кадр, только ключевые, а между ними трекать на оптическом потоке или что-то подобное.

С точки зрения логики лучше то, что проще всего писать и отлаживать.
Кажется разумным такие вещи делать как сервис, в бекенде которого крутится пул воркеров. Как они внутри устроены -- твое дело. Можно агрегировать запросы в батчи и кормить в GPU-воркер. Можно запросы по-одиночке кидать на разные CPU-воркеры.",
"Если говорить про экспоненциальный класс в общем, то у распределения из него плотность имеет вид `p(x|θ) ∝ h(x) exp( η(θ)ᵀ T(x) - A(θ) )`, где `A(θ)` – логарифм нормировочной константы, а `T(x)` задаёт достаточные статистики. Тогда отношение двух плотностей из этого класса, различающихся только параметрами имеет вид `exp( (η(θ₁) - η(θ₂))ᵀ T(x) - (A(θ₁) - A(θ₂)))`, т.е. в log-домене оптимальное решающее правило будет определяться аффинным преобразованием `T(x)`",
"Ну и какую модель тут использовать, как размечать данные, сложно как-то
Я сразу подумал про твиттер, взять частоту и время отправления, прикрепление картинок, среднюю длину, да.",
"можно и над символами, почему нет",
ололо в статье как раз нейронки,
"Привет! Есть здесь кто нибудь кто работал с distributed tensorflow или пробовал разобраться? Очень хотелось бы обсудить некоторые вопросы с тем, кто уже на эти грабли понаступал. С самыми простыми случаями я вроде разобрался, но все равно есть моменты, где это все выглядит как натуральные танцы с бубном. И работает 50/50 :thinking_face:",
"тоже закину удочку, но больше про теорию. в голове какое-то время крутится дурацкая мысль про то, как бы скрестить adversarial loss и anything2vec; может, кто-нибудь читал про такое и подкинет статью? (ну или сразу скажет, что это невозможно и не подкинет)

суть примерно такова: когда мы делаем негативный сэмплинг (в w2v и в восстановлениях всяких дырявых матриц, например), у нас обычно есть какое-то статическое распределение, откуда мы сэмплим - либо униформное, либо по частотам встречаемости слов в корпусе. печалит как раз тот факт, что оно статическое - мы не можем сэмплить с контекстом, мы вынуждены надеяться, что из этого распределения можно достать более-менее все репрезентативные примеры (а если оно сильно скошено, то это далеко не факт), плюс были всякие пруфы про то, что градиентный спуск в таких ситуациях быстро останавливается (<http://webia.lip6.fr/~gallinar/gallinari/uploads/Teaching/WSDM2014-rendle.pdf>), и т.д.

кажется, было бы круто, если бы негативный сэмплер мог на ходу учиться искать новые коварные негативные примеры, неожиданно подсовывая их основной модели. получается такой adversarial-сэмплер - он обучается понижать метрику основной модели, может принимать на вход контекст (например, профиль пользователя) и апдейтится на ходу.

проблема в том, что если я правильно понимаю, такой сэмплер нельзя обучать градиентным спуском. где-то на выходе у него должен быть условный софтмакс и argmax по нему, чтобы выбрать наиболее подходящий негативный сэмпл, а эта операция недиффиренцируемая. или еще какой-то полуоформленный альтернативный способ подразумевает искать минимум в пространстве оценок, выставляемых основной моделью, но для этого нужно будет  поставить оценки всем словам из словаря (заново в каждой эпохе), чего делать бы не хотелось.

может, кто-нибудь встречал что-то подобное?",
"Всем привет.
Посоветуйте где можно с наименьшими затратами(времени) нарыть датасет фактов, которые можно будет представить в виде: вопрос - ответ(число), желательно из разных областей.
Например:
    1)Год рождения Кутузова - 1745.
    2)Экваториальный радиус земли - 6378 км
    3)Количество зубов у домашней кошки - 30
    и т.д.
    
Интересует на русском или английском языке. 
Самому в голову приходит только надергать через api фактов из Wikidata, но как-то заморочено и может получиться узкоспециально или однообразно.",
"пока без контекста, верно. контекст можно ввести в виде дополнительного входа сэмплеру (правда, в случае word2vec я навскидку слабо представляю, какой в нем смысл. в условных рекомендациях, как в статье по ссылке, это может быть искомый юзер, а модель будет учиться pairwise-ранкингу)",
"(это я пытаюсь вслух рассуждать). пока для меня немедленным образом не очевидно, как такую метрику сделать",
"Можно этот ""параметр масштаба"" трактовать как штрафы за ошибку \lambda в оптимальном байесовском классификаторе",
Но понятное дело подразумевается что веса примеров соответствуют распределению того как “в приложении”,
"Мне сложно просто обсуждать, так как я плохо понимаю на самом деле как устроен w2v",
"я тоже с ним близко не работал, мне проще оперировать всякими bayesian personalized ranking)
энивей, спасибо, что прошелся пошагово - по крайней мере, не стало очевидно, что я где-то сильно затупил)

напрашиваются, конечно, всякие неградиентные оценки типа REINFORCE (сделать из негативного сэмплера, условно говоря, reinforcement-агента, у которого количество возможных действий - это как раз количество слов в словаре и есть). но как-то это выглядит совсем грустно.",
как вариант просто начать с того чтобы запилить эвристику которая была бы лучше случайной,
"Если как-то научиться оценивать как хорошо “добавило” каждое слово, то можно попробовать обучить что-то supervised",
"То есть подытоживая (извиняюсь, наверное частично повторю уже написанное)
1. Есть логистическая регрессия - это вполне стандартное и однозначеное понятие, частный случай GLM для биномиального семейства с logit link.
2. С другой стороны есть LDA, смысл которой в том, что строится поверхность, которая делит классы и при дополнительном условии равенства вариаций (homoscedocity) эта поверхность является линейной.
3. Общего между LR и LDA только две вещи: а) и тот и другой подход используется для классификации, б) вероятность принадлежности к тому или иному классу в случае LDA задаётся logit функцией ([Efron, B. The efficiency of logistic regression compared to normal discriminant analysis, J Am Stat Assoc, 70, 892-898 (1975).] )
4. Отвечая на вопрос топикстартера: ограничений на распределение классов никаких нет. Почему соответствующая теорема приведена в вики лично мне не очень ясно. Имхо, при всём уважении к составителям, тут они какую-то ересь написали, мне кажется, потому что смешали разные вещи в одну кучу.
5. И ещё имхо: понимать логистическую регрессию канонически (то есть как частный случай GLM) более  корректно, потому что тогда проще понять такие вещи как probit регрессия и cloglog регрессия, найти их аналог через LDA скорее всего затруднительно (если не вообще невозможно).",
"наверное тут лучше будет спросить, чем где-либо еще: кто-нибудь слушает какие полезные подкасты ? если да, заделитесь, что полезного есть  на регулярной основе",
"Talking Machines, наверное, тут все знают)
еще мне Linear Digressions примерно нравились - там обычно довольно лайтово, без математики, но когда какая-то незнакомая тема, легкий обзор оттуда можно получить

и в отличие от Talking Machines, он постоянно выходит",
"не, потом пихнуть в какой-нибудь LDA, рассматривая юзеров как темы, и вырубить характерные распределения фич для каждого",
"<@U1G303UTW> а расскажи, как он на кластеры смотрел. чем? :slightly_smiling_face: есть код?",
"<@U4051KF2R>: а на какой loss обучать семплер-то планируется? И на каких данных? Что помешает семплеру выучить те же вектора внутри, и всегда заваливать эмбеддинги?",
"а почему он должен подавать семантически близкие слова?
давай возьмем ситуацию с контекстом; кажется, она в какой-то степени репрезетативная. обучаем мы w2v, а в качестве контекста передаем жанр текста - техническая проза, публицистика или сказки для детей. твой умный сэмплер должен понять, что в физическом учебнике слова ""body"" и ""political"" не должны стоять вместе (и одно может идти негативным примером к другому), а в статье про про политоту это возможно, а вот ""body"" + ""solid"" - уже вряд ли.",
"<@U2GTUS0CB>:
&gt; вероятность принадлежности к тому или иному классу в случае LDA задаётся logit функцией
я бы сказал, что решающее правило (в случае LR это `logit(p(y|x)) &gt; 0`) является линейной функцией в обоих моделях, в LDA вероятности p(y|x), кажется, вообще не моделируются (хотя, это как посмотреть)",
"Смотрел там, но мне нужно чтобы можно было ответ представить в виде числа, в ЧГК  вопросы другого вида, скорее всего придется как советовали ниже брать с википедии свойства объектов.",
Даже деревья? С таким условием один решающий пенек всё как надо разделит,
"И мне не ясно, как контекст ""medicine"" (что это вообще?) скажет семплеру о том, что ""body"" и ""torso"" близки, если они не встречаются в совпадающих контекстах",
"это не должно случаться часто, потому что сэмплер все еще ограничен своими окнами из скип-грамм - он может выбирать только из тех мест, где искомое слово не стоит рядом. у него все еще есть шанс наткнуться на семантически соседское слово, но основная масса соседских слов все-таки будет стоять рядом со словом X",
"С точки зрения того, как ты хочешь делать adversarial sampling, семплеру *выгодно* найти синоним и подать его",
"Я бы все-таки вместо попытки научить семплер давать сложные, простые, какие-то еще примеры, пробовал обучать его давать примеры которые улучшают модель. Вопрос только как это мерять",
обратный от 1/2 как раз 0,
"с применением мелков и школьной доски картинку можно представить примерно так - у нас есть большое пространство предметов, которое нельзя обойти все, и нам надо его как-то поменять так, чтобы близкие предметы лежали рядом, а далекие - на расстоянии

модель ""улучшается"", когда все становится чуть более правильно разложено. соответственно если у нас _уже_ много предметов правильно рассортированы, улучшить мы ее можем только найдя следующую ошибку, неправильно расположенную пару предметов. вопрос в том, как ее быстро найти) 

<@U04ELQZAU> кстати, возможно, это запоздалый контрпойнт про синоним, который выгодно найти - если сэмплер действительно найдет его и w2v растащит два семантических слова далеко друг от друга в пространстве эмбеддингов, то теперь рядом со словом X будут лежать сколько-то семантически _не_ близких примеров (которых должно оказаться больше, чем случайных синонимов), и сэмплер скоро подберет их и затолкает слово X обратно

вопрос о стабильности такого обучения, конечно",
"&gt; сэмплер скоро подберет их и затолкает слово X обратно
Почему обратно?",
"ну, куда-нибудь, где минимальное количество случайных синонимов. если нет такого места, где их ноль, то это будет место, где их один",
"triplet loss основан на разнице между исследуемым объектом с негативным и позитивным примером
цель состоит в том, чтобы embedding был как можно ближе к позитивным и как можно дальше от негативных
основная проблема - долго учится и нелегко находить тройки
<https://arxiv.org/abs/1503.03832>",
"<@U0KQ5M6KX>: нет, триплет лосс тут ни при чём, тут речь про то, как адаптивно находить негативные примеры",
Так же как в GAN'ах дискриминатор учится по мере обучения генератора,
"про скитания, пожалуй, неочевидно - если мы берем несколько, скажем, негативных примеров, и при условии общей похожести они взяты равномерно-случайно ""вокруг"" позитивного, общее направление апдейта все равно должно быть в сторону, где их меньше

правда, я не очень уверенно себя чувствую в таких высокоуровневых рассуждениях. спасибо за инсайт, подумаю)",
"почему нет? когда слово отошло от слова-товарища, оно оказалось в пространстве несоседских слов. любой шаг в сторону будет означать то, что сэмплер подберет негативный пример, соответствующий этому направлению, и запретит туда ходить. если мы соберем несколько таких примеров в батче, они в среднем будут вести обратно к слову-товарищу - потому что в том месте все еще жили часто встречающиеся с искомым слова, а случайный-синоним-товарищ на данной стадии не видим сэмплером и не дает никакого пенальти. мне кажется, это больше похоже на колебания туда-сюда. которые все еще не очень сходятся, конечно",
Хотя походу он уже там был... Но мне почему то только сейчас инвайт скинули.,
asobolev: Мне просто на почту пришел. Я вообще не в курсе как это работает :slightly_smiling_face:,
"скажите, а как работает бэкпроп в условиях, когда у нас данные с одного слоя нейронной сети используются дальше в двух местах (простейший пример - 2 выхода)? Ошибка суммируется или берется среднее?",
"но я как-то им не пользовался. точнее. чего-то он мне показался не таким классыным, как сигма или д3.",
"Стоит как мак, начинка лучше. Кажется неплохой альтернативой.",
"<@U04422XJL> для повседневных, на замену эйру, когда он совсем превратится в тыкву.
если в ближайшие 1.5-2 года это произойдет, то брать новый макбук не вариант (ибо лажа), а этот выглядит крайне няшненько и как будто даже внутри ничо такой ¯\_(ツ)_/¯ 

но я спросила в основном из праздного любопытства :smiley_cat:",
"Много кто из тех, кто юзал новый макбук, сказал тебе, что он лажа? Или это в основном были чуваки прочитавшие обзоры? все, кто пробовал юзать новый мак, печатать на нем и так далее, при этом обладающие прошлой версией, сказали что новый мак - космос. Выборка 4 человека :)",
"Не выглядит как просто огромная коробка весом 3 кг, куда запихали топового железа и забили на то, как это выглядит",
"Ну и как всем известно, Black is color of sex",
В незалежной локализацие наверняка звучит как Taras,
как у вас тут тепло :yoba_pled: ,
"Пожалуйста, подскажите, какие русскоязычные выборки для задачи анализа тональности есть? (Кроме Рубцовой и парсинга кинопоиска)",
"(скорее всего это вопрос в datasets)
Подскажите какой Part-Of-Speech Tagger (POS) вы используете для русского языка? в NLTK нет модели кроме английского

Нашел вот такой TreeTagger с поддержкой многими языками 
<http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/>",
"Ну типа ""нескучные"" ландшафты выбирать - где хочется погулять.",
"Всем привет. А не подскажите как правильно в керасе  последовательность кадров сначала подавать в сверточную сеть, а потом  в rnn сеть. На входе данные с `shape = (batch_size, seq_len, n_channels, height, width)`.  Я думал сделать  reshape  слой так чтобы преобразовать входные данные в  `shape = (batch_size*seq_len, n_channels, height, width)` затем пропустить через сверточные слои, flatten слой и потом обратно сделать reshape, чтобы получить `shape = (batch_size, seq_len, conv_out )`. Но че-то не понял как это сделать в керасе, так как в документации для reshape слоя написано, что он не затрагивает batch_dimension:
&gt;&gt;&gt;target_shape: target shape. Tuple of integers, does not include the samples dimension (batch size).",
там как раз насколько я помню  шейп входа как у тебя,
"очень хотел это как глаза агента потестить, но времени нет",
"тогда у тебя последовательность кадров в последовательность и будет преобразовываться , а потом расплющивай как угодно (через conv3d или return_sequences=False и Flatten)",
<@U0AD1L5NC>  а зачем тогда деконволюшн слои если страйд 1?,
"Кто не стеснен в деньгах может себе клевую карту прикупить - Квадру на GP100 в десктоп. Даже две, они NVLINK-ом соединяются. <http://www.anandtech.com/show/11102/nvidia-announces-quadro-gp100>",
"Я ж говорю, для тех, кто не стеснен в деньгах",
для тех кто не стеснен - новый титан. а это для каких-то МЛ олигархов,
я просто не понимаю где профит,
"&gt; я просто не понимаю где профит
в Nvidia профит :kekeke:",
"И не могу найти ничего, потому что вся выдача гугла завалена наоборот жалобами на то, что ""всё жрёт, как ограничить""",
"```
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))```
Если у тебя на той видео точно ничего не висит, то должно отработать. И  CUDA_VISIBLE_DEVICES=”0” (если твоя нейрокарта первая) лучше задай, чтобы тф даже не пытался куда-то ещё лезть. По крайней мере с такой командой оно честно попытается отъесть 95% памяти видеокарты. Вот если она недоступна будет, надо копать кто жрот.",
"Всем привет! Я недавно занимаюсь RL и у меня есть вопросов. Я понимаю каким образом можно обучать агента с помощью обычного Q-learning, например, в Атари игре, когда мы начинаем с ""нуля"".  Но как быть, если в реальной жизни начинать учиться с какой-либо случайной реализации очень долго/дорого и никто не будет рисковать. Если есть набор исторических данных о том, какие действия, реворды и переходы были у агентов, их, несомненно, нужно использовать. Подскажите, пожалуйста, где можно почитать/посмотреть про batch решения, которые помогут подготовиться к экспериментам в реальной жизни.",
<@U3HM4KY14> за то как это называется большое спасибо :slightly_smiling_face: ( anaphora resolution ),
"```
gen1 = image.ImageDataGenerator()
trainGen = gen1.flow_from_directory(‘data/train/’, …, seed=0)
model.fit_generator(trainGen, ...)
```
а кто нить пробовал такую связку в Keras? ImageDataGenerator при этом не делает никакую augmentation
как с загрузкой GPU? у меня почему то очень низкая и время обучения относительно большое, если предобработать данные а потом просто `fit ` то скорость возрастает на порядок
похоже на баг",
"какой самый кошерный способ написать скриптину, которая бы из CLI генерировала бы такой хедер? В идеале - чтобы использовала какой-то макет/шаблон, который можно было бы редактировать",
"да, вот и думаю что было бы проще. те программа-минимум прямо как ты сказал - вхреначить лого и текст, но было бы круто сделать general решение для любого макета в люстре/svg",
"есть электроды на еденичной сфере, на каждом какое-то значение какой-то величины, мне нужно из этого 2д-картинку получить, расшаривать как нормальные ээг-люди это делают немного лень",
Может кто подсказать статью по регрессии через нейронки. Какие требования предъявляются к входным данным ,
"Ну так выходит же просто, что ты делаешь нелинейное преобразование над данными. Я в нейронках не спец, но мне кажется, они это переварят. Только зачем картинки и интерполяция тогда.",
"*apprenticeship learning* – это про то, когда у нас в истории нет rewards и есть только исторические записи поведения экспертов, и мы также предполагаем, что это поведение оптимально. 
*off-policy*  – если есть записи того, как действовал агент и какие награды были получены, то это простой off-policy learning,  и ярчайшим представителем этого семейства является как раз Q-learning.",
"У меня с оптимальностью в истории проблема, так что, как я понимаю, мой выбор off-policy. А как можно Q learning обучать на исторических данных? И еще, не встречалась ли кому-нибудь диаграмма всего RL, которая структурирует все возможные подходы? Типо chit shit'а своеобразного.",
Ага. А для нее есть готовые импортеры из твиттера? (которые понятно как запускать :slightly_smiling_face: ),
"Ага, если оно позволит это как распареллелить, то это будет круто - в прошлый раз мы делали свой велосипед, а тут уже все готово должно быть",
"Ок, я понял, что softmax и формула логистической регрессии естественно вылезают, если предположить, что каждому классу соответствует нормальное распределение фич, где у каждого класса свой mean и variance",
"Какое предположение нам нужно принять, чтобы финальный слой был softmax?",
"sim0nsays: тут я склоняюсь к тому, что говорил <@U04ELQZAU>: softmax -- это гладкая (полезность гладкости упоминал <@U13E1AWCX> ) аппроксимация максимума. По поводу предположений: то, что фичи с последнего слоя распределены нормально -- подходит, но это нам немного дает, поскольку мы их не наблюдаем. Вообщем, я не знаю, почему берут именно эту функцию, надеюсь, другие участники дадут более строгую аргументацию.",
Но почему мы применяем его к deep networks?,
"Собственно, точно тот же вопрос, почему для бинарной классификации нужно сигмоиду применять",
"Сигмоида тоже не единственный вариант для двухклассовой классификации.
Софтмакс обобщение сигмоиды, и как следствие тоже не единственный.",
как кинуть картинку в тред?,
"<@U0ZHQNKQS> я не понял, если честно, куда нужно кликнуть, чтобы зарегистрироваться",
"Котаны, такой вопрос, есть у меня тайм серис, сначала события происходят редко, потом часто, потом опять редко. Как мне сделать так, чтобы plot() по оси Х откладывал равные промежутки времени?",
"В продолжении темы о том, что тормозит убунта.
Кто под какой оболочкой убунты сидит? Перекатился под xfce, но пока что-то не проникся, хотя не тормозит сильно меньше",
"Да собьёт режим, с кем я буду рано утром болтать? ))",
"дамы и господа, а какой сейчас самый модный и молодёжный ami со свежими tf теанами и прочим каффе?",
"Я только не понимаю, что дадут эти промежуточные значения? Почему просто хитмап не сделать без интерполяции и натянуть его на квадрат?",
"&gt; В этом докладе мы расскажем о процессе обучения моделей ранжирования. Для этой задачи у нас есть система, позволяющая в несколько кликов запускать сложные графы вычислений распределённо. Она называется «Нирвана». Мы объясним, как она устроена, и покажем примеры её использования.",
"Какие можно использовать архитектуры для классификации последовательностей тектов? На ум приходит что-то вроде LSTM, которая на первом урочне получает последовательность текстов, а на втором смотрит на текст на последовательность слов",
"чат, общий вопрос прилетел в ВК и я ничего умнее чем поиск линейных комбинаций и модели все против всех не придумал
```Имеется несколько дискретных(непрерывных) случайных величин. Их количество больше чем 2.
Хочется понять независимы ли они в совокупности? Характер возможной зависимости не важен, может быть линейная, монотонная, функциональная или даже не функциональная.
Законы распределения не известны, но точно ясно что они не нормально распределены. То есть интересуют не параметрические тесты.
Есть куча тестов для проверки независимости двух величин. Но попарная независимость не влечет независимость в совокупности.```
пример: матрица 1000* 50, интересует как комбинации разных столбцов взаимосвязаны с другими столбцами",
"<@U14GG4E69> 
&gt;ничего умнее чем поиск линейных комбинаций и модели все против всех не придумал
ага. наркоманские варианты - натренировать xgboost-ов и разложить все сплиты и листья в фичи, и посмотреть куда падают точки. извлекать dark knowledge из всех сплитов, что найдутся моделями для таких типа-корреляций с поправкой на взаимодействия всех фич",
чот вы куда-то не туда,
"Смотри, когда мы учим классификатор просто на временных рядах - мы не используем вообще никак информацию о положении электродов. В двумерную картинку эта информация вложена, тогда при обучении получаем модель, потенциально учитывающую всякие смещения, таким образом более устойчивую.",
<@U0XF4GAM8> а где ты такой датасет взял?,
"чат, а кто как собирает глобальные статистики для batch normalization в lasagne?",
"&gt; ага. наркоманские варианты - натренировать xgboost-ов и разложить все сплиты и листья в фичи, и посмотреть куда падают точки. извлекать dark knowledge из всех сплитов, что найдутся моделями для таких типа-корреляций с поправкой на взаимодействия всех фич
это как?",
"почему же, больше нездорового сетевого общения",
<@U14BPHDK6> Это типа как hot keys vs mouse,
"но выглядит как штука со своим трехэтажным набором абстракций, в дополнение к TFовским",
"хз в каком канале спросить, перенаправьте, если что.
А есть какие-то готовые нейронки, которые на лицах обучаются, чтоб ее можно было обучить (или взять готовую) отрезать последний слой и попытаться что-то свое дальше доучить, т.е. типа тех же реснетов и т.п. только наученное на датасетах из лиц ?",
"maxim.milakov: ради любопытства, а как это внешне выглядит. Я видел P100 просто с обычным PCIe расширением, то есть если иметь две такие карточки, то NVlink'ом их уже не соединить? Или там вместо SLI порта будет что-то для NVlink? Если нет, то получается, что нужно брать специальные P100+специальную материнскую плату?) ",
"Пару Quadro GP100 можно соединить NVLINK-ом посредством специальных мостиков, вроде как физически этот NVLINK выведен вместо SLI",
"Но даже на этих материнских платах специальных процессор - x86, который не поддерживает NVLINK, то что этот NVLINK связывает только GPU между собой, что на самом деле как раз и важно для DL",
"Я правильно понимаю, что для того, чтобы сделать FCN из pre-trained resnet надо оторвать последние слои и прикрутить на их место пару итераций UpSampling + convolution?

Где-нибудь кому-нибудь попадался пример того, как это на keras делается, чтобы я не изобретал?",
Вот как это в tf slim для vgg - я думаю это прям один в один в керас переводится,
обрати внимание как pool примешивается,
"Все-равно не понятно.

Эксперты по Keras, как мне pre trained которые приглагаются к керасу перебить под FCN?",
"Я у модели слои оторвать оторвал, а новые как добавить пока непонятно.",
А она у тебя как sequential?,
"попробуй какой-то слой оттуда подать как входные данные для, положим, для теста, conv слою",
"Кто помнит, какие mean values по каналам на ImageNet?",
<@U34Q3KU8H> так и как же?,
"Я даже не знаю кто больше наркоман: <@U34Q3KU8H>  что такие вопросы задает, или я что это помню",
Ну точнее лого как раз нарисовал знакомый дизайнер в порыве креатива,
"<@U0AD1L5NC>: не суть как важно, какой именно там полином, всё равно он немонотонен",
"Здесь кто нибудь знаком с работами Юрия Н Перова (Yura N Perov) ?  Он оригинально из  Institute of Mathematics and Computer Science, Siberian Federal University",
"что глянуть про эластик? а то я что-то запустил, пихнул в него пару датасетов, чутка совсем пошатал картинки в кибане, вроде немного работает, а что собственно происходит и как какую-то логистику с разнородными  датасетами там устраивать - совершенно непонятно",
"если, конечно, предполагается использовать ES по назначению, то есть для индексации документов, а не просто как key value storage  (как некоторые почему-то сейчас делают)",
"почему эластик плохо как кей-валью сторадж? пока выглядит приятнее чем просто монга, какие подводные камни?",
"ну как бы это для индексации документов, там люсина внутри, зачем ее для чего-то еще использовать",
"меня всякие sql пугают, там надо думать когда запросы пишешь, и вроде не так тривиально разной разнородной не совсем табличной дичи напихать",
Как использовать гармоники Фурье для предсказания большого временного ряда?,
"Что-то не могу найти примеров того, как это делается",
"идея того, что хочется - не база для какого-то прода (будто у меня есть прод :but_why: ), а какой-то датасторадж для ковыряния в ноутбучиках, сейчас все данные в совершенно хаотичном виде в файлах разных форматов, в монге, хрен знает где еще, а хочется какую-то хреновину общную которой можно было бы сказать что-то в стиле `дай все данные по этому мухосранску в которых есть текстовое поле`",
"хз, это холиваром попахивает, но имхо для около DS задач нужно иметь более или менее стабильные датасорсы, а не условную монгу, куда сыпется вообще все",
"кстати, релевантный вопрос: кто как работает с фичами с точки зрения инфраструктуры?
вот у меня намечается N задач со своими моделями и частично пересекающимися фичами, хочу какой-то сторадж для фичей с разными клиентами намутить. Смущает, что слегка пахнет велосипедостроением - может есть какие-то общепринятые решения?",
какая разница где будет хаотичная хреновина: в монге или в хоум дире?,
"на каком круге ада я нахожусь если куда не работает с неизвестной ошибкой, при том что нвидиа-сми спокойно видит гпу?",
"<@U0DA4J82H> а почему ты говоришь что эластик идеологически только для индексации документов? вот у них на лендинге видос красивый <https://www.elastic.co/products/elasticsearch> , так позиционируется вроде как многофункциональная такая база, особого фокуса именно на поиске в тексте и нет",
"<@U1UM6S9KN> а как у неё бумага? Плотная, белая?",
"Надо еще несколько раз прочитать, но пока понимаю так:

- инпут — любой encoder, можно любой rnn, они используют “learned multiplicative mask followed by a summation”, поэлементное умножение embedded input на набор векторов, потом потом суммация. видимо это работает как pooling поверх нескольких attention
- внешняя память представлена несколькими (5-20) независимыми однослойными рекуррентными модулями, каждый по 20-100 юнитов. все модули имеют используют один и тот же набор параметры для формирования значений памяти, но разные параметры для update гейта. таким образом они следят за ищут разные паттерны но запоминают одинаково
- для декодирования в память подается query vector и делается стандартный soft attention по содержанию модулей
все это тренируется через BPTT в любой конфигурации, декодер может быть тоже рекуррентным, генерировать query vectors с нескольких шагов",
"А есть кто-то из чата, кто когда-нибудь заказывал машины на <https://www.hostkey.ru/dedicated/ru-gpu/> ? 
Можете поделиться отзывами?",
"Добрый день коллеги! Есть ли у кого опыт работы с TFlearn? Наткнулся на странную ошибку, хотел бы обсудить...",
"всем привет!
подскажите, плс, как в sql посчитать квартили по группам?
грубо говоря, сейчас у меня есть `select id, count(*), avg(price) from data group by id`, а хотелось бы еще добавить четыре столбика со значениями соответствующих квартилей. 
(про `ntile` читала, но он чото сложный какой-то, не могу представить, как его в уже имеющийся запрос впихнуть о__о)",
"да и они вроде как не данные, а растры выложили",
"А какой сейчас топовый способ обучения joint embedding между изображениями и текстом, если есть пары текст-изображение?",
<@U428C5XRN>  а какой шейп у тензора после tf.expand_dims,
"возьми keras тогда, там тоже примеры по классификации есть
сложно тебе чем-то помочь, как я понял ты не понимаешь что в коде происходит :slightly_smiling_face:",
как написал - код не мой... взял его без изменений... до автора не достучался... тут скорее проблема в сборке TFlearn...,
"ну может api давно изменилось, проблема в том, что надо пониматься как эти слои и  само api tflearn работают, чтобы их поправить :slightly_smiling_face:",
"подскажите какие есть питон библиотеки для проверки правильного написания слов  с учетом грамматики, контекста? язык английский",
"tflearn это какое-то адское поделие, скопированное с keras :slightly_smiling_face:
не знаю кто его использует, пришёл чувак и скопировал многие моменты из keras 
уж если собрался изучать все плотно, то надо голый tensorflow брать :be-a-man:
тем более автор keras говорил, что его библиотека будет официальной высокоуровневой абстракцией для tf",
"Зависит от твоего средства виртуализации, вангую, что просто карточку не пробросишь, so не надо возиться с кудой и гонять CPU only. Ничего страшного в этом нет.",
"У меня сейчас lenovo helix от <@U07V1URT9> с убунтой.
Как планшет использовал один раз за полгода. Тач трогаю скорее никогда.",
"гайз а какие есть методы сжатия сильно разреженных векторов с сохранением информации?
вектор размерности несколько тысяч, по сути напоминает Bag-of-words, но там не слова , а индикаторы событий",
а как приянто подбирать оптимальное k для truncatedSVD в частности (и для разложений в целом)?,
"а вообще - вставлять в модель и смотреть, какой k показывает результат лучше",
"ну вот я примерно так же, только цифры другие)
кажется, что можно смотреть на какие нибудь метрики, типа преобразовал - восстановил -&gt; расхождение",
по моим экспериментам k больше 150 редко когда особенно необходимо,
идет кто в Яндекс сегодня? :slightly_smiling_face: <@U3TF60E65> Андрей?,
"Звучит как обычная классификация, берешь pymorphy, а потом svm/логрег",
"Существуют гайды, как деплоить :more-layers:  модель в продакшн?
Особенно интересно — обязательно ли для предикта использовать тачку с ГПУ?",
"<@U09JEC7V0>, ну и что толку от канала? Как ни спрошу - все молчат",
"Ну в общем, можно) Благодарю!
А про гайды — может не про deep, а про обычные модельки — есть какие best practices? Просто ссылочки было бы достаточно)",
"<@U0BE0B24R> на самом деле тут уже искали данные ВК, просто ни у кого их нет.",
Крутяк. Может быть наконец-то узнаю почему некоторые юзают Jupyter,
"а чего так дорого все, 1070 за 33+к? столько как раз самая понтовая асусовская стоит вроде",
"<@U14GG4E69> поясни за цену карточек правда, непонятно с чем связано, там какой-то разгон с завода чтоль? а то реально дорого, референс вон 26 стоит, за какие фишки 7к наценки?",
"потому что это же как консоль, только еще и с графиками и с маркдауном
можно делать удобные книжули с кодом и результатами его работы
а в цеппелине графики вообще :rainbow_mouth: ",
Где и какой референс стоит 26?,
"а официальные представители средне по больнице не дороже ли магазинов в условиях СНГ? им как бы нужно консультантов держать вежливых, марку держать, гарантийные обязательства - вот это вот всё, маечки там брендированные.",
"напомните, пожалуйста. где логин в аккаунт на <https://corp.mail.ru/ru/press/events/315/>",
а у кого есть опыт распознавания номерных знаков именно российских?,
"&gt; на наши буквы 
это какие например?)",
"Где секси картинки, я вас спрашиваю",
"<@U049HDR2Z> У меня поугас интерес к ганам. Я статью, открыл, подумал и закрыл. В общем не стоит ожидать от меня ревью. Как опять интерес проснется, наверное с нее и начну",
"что обычно делают, когда для рядов нужно построить модель для прогноза на несколько точек вперед, например, на неделю? тренируется 7 разных моделей?",
"в россмане на кагле вроде на много точек вперед предсказания были? что-то пытаюсь с кодом разобраться, не очень понимаю, как там делали",
разве не так как на сбере? куча линейных регрессий и сверху xgboost?,
"а я не знаю, как на сбере было",
на сбере там как раз на много дней вперед надо было предсказать?,
<@U0M39M6LS> а ты как предсказывал? делал 30 разных моделей?,
"Приветствую, у кого есть Imagenet? Пробовал с academic torrents скачать скорость просто ужастная.",
и как xgb справлялся с числом дней от начала? или там ряды разной длины были?,
"Если у нас все ряды одинаковой длины, например 100, то число дней от начала всегда 100, а в тесте будет 101, 102 и т.п. - такого xgb во время тренировки не видел. Вопрос в том, как он справлялся с таким. Но если ряды разной длины, то число дней от начала всегда разное, то есть xgb какие-то значения мог уже видеть раньше и на них сделать нормальные предсказания, так?",
"а ни у кого нет датасета со спарщенными новостями? причем хотелось бы за длительное время)
там, дата + текст + что-нибудь еще

Желательно на русском, но не обязательно)",
"Во-первых, непонятно, почему байесовское RL строго model-based",
"Во-вторых, почему conventional model-based это динамическое программирование",
"щас я попытаюсь его сюда позвать, чтоб он сказал,  какая логика была использована",
"Всем привет! По поводу таблицы. Во-первых, она не исчерпывающая, я использовал её в таком виде, чтобы был понятен дальнейший ход рассказа. Во-вторых, к model-based подходам я относил те, в которых явным образом строится модель среды, прежде всего, матрица переходов. Существует не один способ скрестить RL и байесовские методы. Самый популярный - воспринимать элементы матрицы переходов (вероятности) как байесовские неизвестные со своим prior. Поэтому я и причислил этот набор методов к model based. Что касается conventional model-based, то динамическое программирование, в том виде как оно описывается в книге Саттона, - классический пример model-based подхода, выстраивается модель среды, которая решается, к примеру, либо policy iteration , либо value iteration методами",
yurapekov: а кто такой бобук?,
"а он как-то давно или успешно цифровой медициной занимается? или почему он, кстати, в жюри?",
"Приятно слышать)

Мы ввели плату, чтобы пришли те, кто точно хочет. А то бывало, что записывается две сотни, а приходит меньше половины",
"платный хакатон, не надо так. соответственно пачка тех кто не хочет платить за свое же время - не придет опять же из симполических побуждений",
А как же еще одно мероприятие чуть раньше: <http://medit-2017.ru/>,
"те, кто не хочет причину все равно найдет",
"плюсую, пипец как странно звучит про платный хакатон
тем более что участники хакатона и есть обычно сами главный ""товар""",
"<https://xnor.ai/index.html>
кто знает что у них под капотом? свои модели? или они удачно портируют?
выглядит очень круто",
А не встречал ли кто исчерпывающего tips&amp;tricks по классификации для time series?,
"тип всякие штуки вида нормализовывать/нет, какой смысл у сжатия-растяжения по времени, вот это вот все",
"если попросить после феста, можно и данных пошарить, мб с кодом (хотя какой там код блин, сабсемпл, пара преобразований, glm и ggplot)",
"Есть поток транзакций по оплате услуг. За разные услуги мы берём разные проценты, например, за эротический массаж много, а за перекопку огорода мало. Дальше мы предполагаем, что другой стороне выгодно платить нам меньше процентов, и она проводит некоторую долю эротических массажей как вскопку огорода. Соответственно мы строим гипотезу, что распределение транзакций по времени у двух этих услуг будет близким. Графики это подтверждают. Но дальше мы хотим автоматом обнаруживать такие схожие распределения или сравнивать большие периоды опять же автоматически. Подскажите, в какую сторону копать? Особенно интересует критерий схожести",
"Вау результат, это когда может генерировать картинки способные “обмануть” человека. Ну хотя бы одну из 100",
"Всем привет, никто не подскажет, где может взять switchboard датасет телефонных переговоров (сами звуки)? Может у кого-то есть и может поделиться? :) Нужно воспроизвести статью, а там этот корпус использовался",
наверное зависит от карт. И еще какие изображения планируется выводить на монитор? Т.к. в принципе его можно и к материнке по идее.,
"дамы и господа, у меня просьба - стукнитесь в личку кто-нибудь, кто может помочь с поиском площадки для проведения конференции",
"Кстати, а какое определение у deep learning'a с точки зрения математики?",
А зачем ты даешь код неспециалисту для запуска?,
Есть группа энтзиастов клипмейкеров. Я хотел им попробовать сделать сетку для интерполяции промежуточных кадров. Вот думаю как им потом дать на запуск,
"zfturbo: Если что, готов потестить как это всё таскать. Всё равно хочу разобраться как нейросети под виндой мутить",
"Отлично. Можно даже не ждать той сетки, а попробовать на чём-нибудь другом ) Я напишу как будет что-то готово.",
"<@U43FTJQ2V> я для себя в ближайшие дни буду делать live USB ubuntu для десктопа, но я буду конкретно для 960 карты делать, не знаю как оно будет работать на других ",
"_(хотя надо подумать, кто победит в сравнении с младшими i3, конечно)_",
Ну вот собственно первый раз вопрос поднятый выше у меня и возник когда я хотел дать этот код дать другому человеку. ),
<@U1CF22N7J> есть гайды как это делается. Расскажи на каком этап застрял подскажу.,
Такой вопрос кстати. Когда в качестве фичи добавляется какой либо Group By - нужно считать средние путем двух отдельных group by на тесте и на трейне или можно на объединенном датасете одним проходом посчитать?,
"По идее если группы очень маленькие, то и аффектить будут мало, даже при переобучении - так как мало объектов",
"если попадётся число, где все примеры примеры положительные — то большая модель на деревьях легко это запомнит",
как раз на последнем самом толстом датасете с конкурса bosch,
"Там же бинарный не сжатый формат. И то, что сам датасет весит в csv меньше 8 гигов ничего не означает. Было бы интересно посмотреть, сколько он занял памяти видео и какие-нибудь логи, чтобы понять, где узкое место.",
"Очень странно, если честно. В любом случае такой датасет не в какую из имеющихся у меня GPU не влезет) А сильно дольше работает? Какой CPU?",
"В алгоритмах, где операции с матрицами(перемножение, декомпозиция) занимают самое большое время в работе алгоритма сопоставимый GPU должен работать быстрее, чем CPU, если матрицы влезают в память. В алгоритмах на деревьях это не всегда так:) Но приятно бывает сразу считать и на CPU и на GPU)",
"Не, пока не разобрался.

[1] Они действительно лучше чем UpSampling + Conv?
[2] Как перебить Unet под Deconv?
[3] И вообще все детали мне очень интересны.",
"После Deconvolution надо Concolution наворачивать, для сглаживания, как после Upsampling или он сам это делает и в этом его сила?",
"<@U053R9RS6> А LeakyReLU из каких соображений? Она лучше отработала чем ELU, PRelu ?",
"Мне кажется оно все еще пока не особо работает, как ни крути",
"Задача про спутники на кагле, которую мы массово пытаемся решить. в <#C3Z5S49GV|proj_kaggle_dstl2017> Половина из топ 10 с Public LB там.

Есть спутниковые снимки, вроде как с Вьетнама, на 25 из них попиксельна промаркирована классовая принадлежность. Надо создать модель, которая будет для других снимков (test = 425 images) тоже ее определять. Классы не взаимоисключающие.

В общем надо чтобы визуализация предсказаний выглядела как-то вот так:

<https://www.kaggle.io/svf/650368/89902d556d0fa19464b7d6c3f306bff5/6120_2_2.png>",
"По поводу TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS <https://openreview.net/pdf?id=rJTKKKqeg>, есть реализация на tensorflow: <https://github.com/jimfleming/recurrent-entity-networks>. Хоть автор кода и общался с авторами статьи, но ему не все результаты удалось повторить. Так что не совсем понятно, как удалось пройти все babi tasks.",
<@U34Q3KU8H> я с ходу как-то не понимаю а где здесь upsampling?,
"На приведенном выше происходит предсказание части исходной картинки, это да. Тут я согласен. Но в целом, если не кропать центр при осуществлении shortcuts, а брать целиком, как мы сейчас и делаем - ну очень похоже на autoencoder.",
"Да, в задаче надо предсказать класс для каждого пикселя.

Выход не такой же как и вход. На выход подается маска. В этой задаче у нас 10 классов =&gt; (10, N, M) на выход. То есть это не autoencoder.

Но если бы мы поменяли выходной слой так, чтобы он output выдавал в том же формате, что и было на входе. =&gt; можно было бы использовать эту архитектуру как Autoencoder.

То есть Unet без crop (на картинке выше мы crop используем, я на такую вариацию только что наткнулся, когда картинку искал.)- это не AutoEncoder, но архитектурно очень похож, с точностью до выходного слоя.

Даже есть идея преиницилизировать Unet, используя его как AutoEncoder на всех 450 картинках, а уже потом fine tune под предсказание масок.",
"В общем есть мнение, что пытаться предсказать пиксели, используя sigmiod - это мало, хочется добавить некой глобальной структуры в loss function. Но так как ответ у нас бинаризированный, то и каждый пиксель бы хотелось предсказывать более-менее точно. На задаче про сосуды, чтобы была летом, я не заморачивался  и просуммировал обе loss function. Ту что за пятна (глобальная структура) и ту, что за пиксели (локальная структура).

В общем функция потерь выглядит это вот так:

```
def jaccard_coef_loss(y_true, y_pred):
    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)
```",
Я не удивлюсь если кто-нибудь похожую идею на целый текст на arxiv растянул с аргументами почему это может сработать.,
то есть -K.log(jaccard_coeff) у тебя как пенальти работает за неправильное предсказание?,
"Я помню работу, где в сегментации помогал crf ",
"Где-то тоже попадалась. Но я не придуумал как CRF сделать частью loss function, да и во многих работах он используется уже именно как post processing, чтобы улучшить качество предсказания. Хотя если кто-то поможет мне прикрутить его к функции потерь в Keras, буду премного благодарен.",
"Подскажите такую штуку. Когда у нас в сети convolution layer и подаётся 3 канала на вход, то это значит что одинаковые фильтры применяются ко всем трём каналам?",
Что у нас должно быть на выходе первого такого слоя? Куда денутся три входных канала? :confused:,
"Я чувствую, что если бы получал по доллару каждый раз, когда рассказываю как работает CNN, можно было бы давно не работать ",
"Да, я тоже подумал, что не стоит мучать Семёна свёртками в очередной раз и пойти глянуть Карпатого, вчера смотрел лекцию как раз перед свётками, надо продолжить",
"У живого человека спросить - очень важная штука, какие бы хорошие иллюстрации не были ",
"Вот в твоём объяснении ""Пусть в следующем слое 10 нейронов"" -- следующий слой -- это какой и после какого?",
"Ок, т.е. если input (x, ,y, *20*) --&gt; conv2(10) --&gt; output (x, y, 10), то у каждого нейрона будет пропорционально больше параметров и всё, дальше по сети пойдёт информация такого же размера как в первом случае?",
"Да я всё про спутники, там вопрос в том, как лучше делать: натренировать пару (тройку) отдельных сетей на обычные rgb изображения + всякие инфракрасные и потом объединить предсказания или достаточно будет объединить все каналы в одну толстую картинку и дальше одна сеть разберётся сама. Ну вот исходя из нашего разговора выше, в тонкостях взаимоотношения каналов будет по сути разбираться только первый слой, а остальные будут видеть информацию уже через призму его понимания. И, имхо, этого слоя будет недостаточно, чтобы найти нужные взаимоотношения, т.к. у нас разные каналы реально очень разные (одни и те же объекты могут совсем по-разному выглядеть и масштаб разный и даже сдвиг может быть нехеровый)",
IMHO Если бы у нас было данных как на ImageNet - можно было бы смело в 20 слойный бутерброд. А так как с данными у нас вилы - лучше разные входы и по середине или ближе к концу склеивать.,
"Весь вопрос где объединять, да",
"Есть работы, где объединяют перед последним слоем, который предсказание делает и сравнивают с тем, что добавляют сеть, которая смотрит на предсказания каждой половины и активации последнего слоя и учится их балансировать",
"Идея в том, что каждая половинка по идее и сама должна дать +- правильный прогноз, но может ошибаться, где-то, где другая права и вот этот последний кусочек должен выучить кто-где ошибается и выбирать правого",
"кроме случая, когда у тебя oracle на консьюмерских дисках :slightly_smiling_face:",
<@U35GH0DDH> ну это типично что нормализация слов для русского не работает. По факту ты удаляеш информацию про синтаксис которая как раз содержиться в окончаниях,
"Вот прям как тут отлично сработать может. tfidf + bayes. и крутить alpha  у классификатора.
<http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html>",
"Если убирание не будет менять скор, то зачем делать лишнюю работу.",
"а теперь вопрос как убрать все названия компания, если одно и то же название можно записать несколькими способами, да еще и с опечатками :but_why: неужто руками прописывать все варианты? :filosoraptor:",
Ну как список для левенштейна пойдёт,
"sovcharenko: Не понял, как можно получить дескрипторы, используя <http://www.robots.ox.ac.uk/~vgg/software/vgg_face/> Авторы утверждают, что ""This page contains the download links for the source code for computing the VGG-Face CNN descriptor"", хотя у модели на Torch на выходе 2622   чисел, то есть сеть тренировалась на классификацию, а не на embedding.",
"Надо брать выходы с fc8. А модель, где embedding триплетами обучали, они не выложили.",
"привет, вопрос возможно не по адресу. Существует проблема, использую docker для работы с amazon, возникла проблема на wwindows 10, вероятнее всего в сертификате, при таге пакета ругается : time=""2017-02-13T17:33:29+05:00"" level=info msg=""Unable to use system certificate pool: crypto/x509: system root pool is not available on Windows"" . Есть ли кто знакомый с работой docker 1.13.1 на винде , кто решал подобную проблему?",
"о спасибо, я как раз его книги хотела почитать и на этом возник вопрос",
"Кто-нибудь пробовал обучать что-то вроде word2vec, но вместо слов использовать индикаторы событий? Грубо говоря есть разреженный вектор one-hot большой размерности, между данными векторами можно установить локальный контекст. Можно ли по этим данным обучить свой word2vec и как наиболее просто это сделать?",
"Всем привет!
В эту субботу 18 февраля будет зарешка, будем решать два соревнования: Tinkoff Data Science Challenge и Two Sigma Financial Modeling Challenge.
Собираемся как обычно в ШАД, с 12 до 16-ти. Начнём в Принстон, потом переедем в Оксфорд.
Регистрация доступна до 14:00 пятницы (чтобы успеть организовать проход), учтите это пожалуйста:
<https://events.yandex.ru/surveys/4453/>
Приходите!",
"ld86: не понял сходу, как эмбеддинг выглядит? Вроде же они вектор natural parameters получают на точку при контексте, а он длины словаря.",
Я если честно сам до конца не понял как оно работает,
"В шаде ща проводится курс по RL. Я ссылочку потерял, у кого нить она есть?",
"офтоп, а почему чат называется reinforcement_learnin, а не reinforcement_learning?",
"всем привет! кто что помнит свежего про генерацию музыки? давайте попробуем собрать актуальное (6-12 мес) в тредик. статьи, гитхабы, ключевые слова из памяти, что угодно. многим было бы либо полезно, либо интересно",
Тогда почему тебя волнует производительность!,
"о, я кажется понял, почему во враппере жестко прописан ЦПУ для эмбеддинга",
"Для тех кто считает, что :kaggle: — это уже не серьезно, пробуем организовать регулярный <#C45CUFESK|true_story> кейс-клуб. Будем обсуждать: как ds/ml приносит пользу в компаниях, в чем на самом деле заключается ремесло дата саентиста, как правильно поставить задачу, какие проблемы ждут при выкатке в прод. :true-story: 

Первая встреча в экспериментальном режиме — уже на следующей неделе в Вышке. Возможно не сможем всех уместить в выделенной аудитории. Кому интересно посетить — регистрируйтесь и ждите отбивки по e-mail. Кому интересен подобный формат — приглашаю его обсудить в <#C45CUFESK|true_story>.",
"Когда считаешь w2v, эмбеддинги на GPU дают 30% прирост скорости. Наверное для LSTM уже не так важно, там другие затычные места.
В примере принудительно положили на CPU, да",
ololo: А как такое в Sublime устроить?,
"ololo: хочется велосипед, а не трактор, который диктует когда писать код, а когда ждать, пока он нагреется.",
"Привет! Кто-нибудь знает где можно найти побольше различных статистических данных по регионам россии ? Желательно в табличке, вот что-то вроде такого <https://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D1%81%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D1%83%D0%B1%D1%8A%D0%B5%D0%BA%D1%82%D0%BE%D0%B2_%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D0%B9%D1%81%D0%BA%D0%BE%D0%B9_%D0%A4%D0%B5%D0%B4%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8>",
"Там есть бесплатные данные (voxforge, librispeech, aspire etc) они с какого то лингвистического консолиума берутся (LDC). Другие корпуса платные

На русском языке есть тоже ресурсы",
"Если кто то хотел заглянуть на Data Science завтрак, у вас есть отличная возможность сделать это завтра.",
"<@U049NHC4X> автодополнение и навигацию по коду когда под conda env. чтобы при ""go to defenition"" по пакетам из окружения ходить, а не по системным. для этого через лес костылей продираться придется, или пары велосипедов хватит?",
"господа, а вувпал вабит могет онлайн лернинг? не в смысле сгд по одному примеру, а запуститься в виде демона и принимать как тестовые примеры так и терйновые, что бы моментально обновлять модель",
а можно как то дообучать так же?,
А когда захочешь обновить модель послать ‘save',
Молодец какой - бэг оф вордс нагуглил.,
"Всем привет! 
Есть какие-то бест практис как организовать хранение сериализованых моделей?",
"Они херово работают, когда картинку вращают и ультрапросасывают методом на основе кей поинт, когда делают ебанный коллаж из фотографий",
"ребят, не подскажите какой самый быстрый лемматизатор для английского языка?",
"В моем случае это когда чувак взял фотку, обрезал небольшой кусок/повернул на небольшой градус/слегка поменял контраст/все вместе",
"Поищи статьи, где дескрипторы ключевых точек обучают нейросетями, а не хендкрафтят.",
"Ну, как решили: последний не я делаю, поэтому выбор стэка на нынешнем авторе :slightly_smiling_face:",
"Кто посоветует ноутбук ""для дата-сайнс""? Желательно с комментариями, чем хорош. Отдельно напишите, если пользуетесь таким сами. (правильно я понимаю, что заниматься диплёрнингом на ноуте невозможно?)",
"а что такое ""устойчивые тематические модели""? то есть как веб-интерфейс помогает достичь их?",
"Веб-интерфейс помогает оценивать модели, можно проводить эксперименты с разными комбинациями регуляризаторов и смотреть как они влияют на модели",
"звучит как философский камень топик моделинга! а про результаты где-нибудь можно посмотреть, почитать?",
"из того что я читал в отзывах на подобные - куда ставится, единственный “минус” - нужно полностью выключать мак, подключать док, потом включать. хот-плаг не работает, ну и выдернуть док и пойти на диван не получится.
и немного напрягает на вот этом вот например сайте ""windows only support”.",
"Вообще да, разумеется, непонятно как это делать в общем случае",
"Да и вообще как сделать это лучше, чем в каком-то приближении",
"есть инструкции как самому спаять, там вся цена в красивости + микросхеме.",
для тех кто тож озадачился: <https://en.wikipedia.org/wiki/Moscow_mule>,
<https://opendatascience.slack.com/archives/hardware/p1487101380002194> почему здесь <@U043814R6> :wat:,
ну что ты-то имел ввиду — понятно как раз),
"Одна из проблема на задаче про спутники, с которой я мучаюсь - это нахождение Вьетнамских тарантасов с высоты пртичьего полета на спутниковых снимках. Image Segmentation не справляется. Есть идея использовать оконный детектор. Кто мне подскажет грамотную литературу, а лучше сразу сетку, которая с этой задачей может успешно сработать?",
"посоны а чо есть для визуализации дашбардов? что то типа драг энд дроп, где тупенькие BI аналитики могли бы отчеты себе создавать?",
есть еще redash кстати но я им пользовался только как гуи клиентом к престо,
этих bi как грязи :disappointed:,
"Ребята, помогите плиз решить такую проблему.
Мой сервис должен выдавать Sentiment Analysis, Text Classification от распознанного текста с Speech Recognition.

Я натренировал 2 линейные модели (generalized) с одинаковым алгоритмом, но training данные у меня разные были. Например, для Text Classification - 20newsgroups, а для Sentiment - movie reviews..  Для обеих моделей тренировал еще Count, Tfidf vectorizer (+ chi2) с этими же датасетами. --У меня теперь получается 2 разных словаря. Но у меня также есть словарь с тренировачных данных в Speech Recognition.

Проблема: У меня получается 3 разных тренировачиных данных (1) speech text + acoustic model; 2) text + category; 3) text + sentiment). 
Вопрос: Как обычно решают такую проблему когда у тебя разные данные, с разными лэйблами, но есть необходимость связать их? Я думаю, если оставить тренировачные данные как есть, но 3 словаря объединить (+ выбрать через chi2 самые лучшие) и использовать в классификации (в CountVectorizer есть параметр *vocabulary*)",
Т.е. почему они в итоге лучше обычных работают,
"А если просто все оставить как есть, какие-то проблемы возникают?",
(если использовать autoencoder как generative model),
"<@U0AD1L5NC>: вопрос как раз в том, как пространство кодов регуляризовывать ",
" А как сделать так, чтобы код был интерпретируем? Вроде срепени улыбки на лице?",
<@U1LNBRZ29>: без внешнего источника supervision 'а -- не знаю как ,
"Ну вот да, мне видится основное приемущество VAE как мощную регуляризацию на пространство",
"Тебе главное построить правильно валидацию, что бы не оказалось, что аккураси ты поднял, а когда дойдёт дело до боевого тестирования, а ты жёстко оверфитнулся ",
"А ни у кого нет датасета с прошлогоднего контеста DiDi? Ну или, может, знает кто того, у кого он может быть?
<http://research.xiaojukeji.com/competition/main.action?competitionId=DiTech2016>

Спасибо.",
"Мне кажется, и тот, и другой совсем не такие, как в VAE",
"всем привет :wave:
как правило любое «простое» решение BI из коробки ограничивает а отжирает кучу времени чтобы в этом интерфейсе разобраться а в итоге толку мало, если данные статичны то гугл чартс или эксель заруливают, на мой взгляд. ну или какой-нибудь <http://charted.co|charted.co>

если задача нестандартная, то проще найти что-то что оборачивает D3 состояние что вы просто суёте им данные в джейсоне и конфигурируете отображение а оно само рисует
что-то типа
<http://c3js.org/>
<https://vega.github.io/vega/>
<https://dc-js.github.io/dc.js/>

короче, проще брать что-то что реально простое и допиливать под задачу)",
Привет. А есть ли какие-то хорошие практики как подбирать шаг и оптимизатор для градиентного спуска? Использую tensorflow.,
"<@U06J1LG1M> Если платное решение -- норм, то Sisense как раз подходит для ""тупеньких аналитиков""",
"<@U042UQC96> лол, а почему с Ромой вообще есть эмоджи? ",
а инфографику кто нибудь красивую генерил онлайн?,
"<@U04URBM8V> Интел рассказал о каком-то своем FPGA, зачем мне что то думать об этом? :slightly_smiling_face:",
"как работник студии инфографики с 4-летним стажем, ответственно заявляю - почти все по ссылке - гавно на палочке, а не инфографика. 

Но по сути результат -  ""или шаблон или самому"" это не меняет.

Если хочешь, взгляни на  вот эту
<http://rawgraphs.io/>",
"<@U1BAKQH2M> лол, а почему с Ромой вообще есть эмоджи?",
"На практике после того как распарсил их чудную аннотация у меня выходит, что у каждой шотки может быть 5+ лейблов из одной категории",
"если есть идеи, как переименовать - милости просим :slightly_smiling_face:",
Но я не знаю как в реальности,
чот уже третий из заявленных спикеров,
"этот чел говорит как будто “здравствуйте, я тоже алкоголик""",
и так как на углях программируешь,
"там китаец вышел, он выглядит как будто он написал половину тф",
"<@U2TP5JELS> по видео оно вроде как отвечает на вопрос ""_какого цвета_ водолазка?""",
libfun: а mean как у людей нету?,
"Если это live, то почему субтитры показывают текст который еще не произнесен =/ причем явно не машинный перевод, только что видел в субтитрах слово ""operaors""",
"Keras с TF 1.0 - входит в как пакет tf.keras, к слову",
"Подскажите какие методы вы использовали для текстовой классификации с *20_newsgroups* датасетом

1) CountVectorizer: использовали ли вы свой словарь? Если нет, то какой был у вас ngram_range (униграммы, биграммы и тд)
2) Что использовали для Feature Selection? У меня 2 варианта: LinearSVC(penalty=""l1"", dual=False, tol=1e-3)   и chi-squared --  SelectKBest(chi2, k=n_features)       [n_features=10000] (делал feature selection)
3) У меня 2 лидера в моделях:  MultinomialNB и LinearSVM",
"И фичи ещё не отбирать а просто юзать как есть, только с сильным регулязатором. :slightly_smiling_face:",
"LR учится быстрее в разы, а качество по сути почти такое же как у LinearSVM",
"В следующую субботу 25 февраля будет проходить ML тренировка <#C1CEM43TJ|mltrainings_live>. Разбирём недавний хакатон DeepHack RL и контест Avito BI. Программа будет насыщенной как никогда! Приходите, будет интересно :slightly_smiling_face:

Не забывайте про регистрацию:
<https://events.yandex.ru/events/mltr/25-feb-2017/>",
"а вообще, зачем теперь :theano:  объясните? я правда ее не юзал ниразу",
"mtrofimov: Как я понимаю, RF с элементами ET.",
"воо, когда мы делали артисто, там модель тоже с кучей связей (стайл лосс), как и в юнет, там тоже межслойные есть; у нас тогда проблемы были что тф слишком много памяти аллоцировал, и то что делалось в теане с батчем 30 в тф 1-2",
mephistopheies: На спутниках не желаешь показать высокий класс или просто порассуждать как бы ты к этой задаче подошел?,
"Да че вы спорите, когда есть mxnet",
"а чо мхнет разве могет? куда ему до монстров типа теаны, тф и торча?",
"А с каким объемом ты играешься, сколько минут аудио?",
"У меня пока руки не дошли это правильно оформить + для этого нужен GrandMaster, а до него мне пока как до луны.",
<@U041SH27M> А как ты на тему спутников? Не желаешь?,
И еще память так не отжирает как тф,
"но ведь всё равно за тф будущее, на нем будут писать все, как на самом популярном. Все новички будут тф учить сперва",
"На TF будут писать те, кто очень сильно в теме.",
Это как писать ассемблерные вставки ),
"Кто нибудь в курсе, как в керасе задать количество потоков?",
Для начала по пальцам постучать тем кто новый дизайн выкатил,
"гайз, я тут недавно, выбирая между библиотеками, почувствовал легкое головокружение. и для углубления своего понимания решил их как-то сравнить. хотя бы по функционалу из документации.
в гуглодоках состряпал табличку. до конца еще заполнить не успел. но вот решил опубликовать в хтмл/жс. 
получилось вот так: <https://theotheo.github.io/nlp-tools/>

может подскажите какую тулзу я упустил. или по какому параметру их еще сравнить можно. 
так же интересно, как можно подобные сравнительные таблицы верстать максимально удобно! я даже подумываю об svg, чтобы можно было б зумить туда-сюда. 
в общем любому фидбеку буду рад.",
"Вопрос есть классификатор который либо xgboost или random forest на классификацию он работает долго около 10 предсказаний в секунду. Размерность пространства около 400. Хотелось бы ускорить без особой потери качества, какие есть к этому подходы кроме как отсортировать фичи по значимости и малозначимые выкинуть?",
"<@U0UJ10A9J> сколько деревьев, какая глубина и как хранятся фичи (массив или мап)",
"да, и на какой машине запускаешь",
"Из кераса, как я понял, хотят сделать высокоуровневую спеку, чтобы можно было готовыми кубиками сети клепать, а реализацию спеки -- на откуп желающим.",
"<@U14BPHDK6> с телефона :100: плохо. я даже смотреть боюсь :joy: я экспериментирую c <https://datatables.net> жс-библиотекой. она дает возможность зафризить колонки и хедер, но за это приходиться платить. 
ссылку можно: <https://goo.gl/nFwkVG>
но одной из моих целей это понять, как такие таблицы в вебе верстать так, чтобы они были как можно более удобнее.",
"какой сервер по характеристикам, какая нагрузка еще на сервере?",
Кто тут работал со слоем Cropping2D в Keras? Есть пара вопросов?,
"А как у тебя crop1 соединяется с моделью, не вижу что-то",
"Кто что может сказать про DART: Dropouts meet Multiple Additive Regression Trees? В lightgbm это второй тип бустинга, как он себя ведет?",
Как в :tensorflow: делать аналог `theano.clone`? Что-то я так и не понял есть там это или нет,
"Друзья, подскажите, на чём можно тестить своего RL-агента, при этом не такую простую среду, как cartpole, но и не такую долгую, как атари?",
"<@U0DA4J82H> :joy:  скинул скрин, как и должно выглядеть сейчас. а что не так? черный цвет? ну это просто одна из дефолтных тем гитхаб пейджех, в которых нет сайдбара. да и вообще я больше люблю на черный попятлится :slightly_smiling_face:. 
думаешь лучше белый или что-то другое совсем плохо?",
А где лучше в этом слаке искать контрибьюторов в model based testing фреймверк? ,
"пс, а в какой бюджет сейчас девбокс может обойтись? ну тип 2-4 гпушки+ гб 32+ оперативы и тд ? такой чтоб 1.7 тб гугла влезли )",
"Я не был уверен на какой комп его поставлю, а разбирать корпус было лень",
<@U04725QK7> это ты чего такое собираешь и под какую задачу?,
"По поводу харда - все маски со спутников хранить не выходит, да и хард с виндой уже помирает. По поводу оперативки - просто интересно. Когда собирал свой комп заморочился этим, но в итоге забил - решил, что 64 она точно потянет, а мне пока хватит 32 (не хватит).",
Ну <@U041P485A> же рассказывал как водянка протекла и унесла жизни пары титанов,
а насколько хорошо ща работают такие девбоксы со множеством видюх? как это вообще в коде оформляется? Одну модель тренируете на 4х или 4х модели паралельно?,
блин. а крутая тема. понять бы теперь есть ли у нее апи и как ее шерить,
Какой framework наименее прожорливый по памти? Хочется больших сетей с большими батчами. :mxnet:  ?,
"Дык эта, зачем большие батчи?",
"В iTerm это прописывается в профиль, и тогда открытие профиля приводит к открытию окна с сессией прямо на remote-хосте. Я прямо из меню эти профили и вызываю, когда нужно попасть на какой-нибудь далёкий хост через пару-тройку туннелей.",
"как это может быть ""вообще ничего не понятно""?",
"если ты хочешь сказать,  что так, например, парсеры сравнивать практически бессмысленно, то мне самому интересно, как туда добавить инфу о бенчмарках. она ведь где-то есть. или можно просто как-то особо отметить, что это типа state-of-the-art",
"<@U0E4S5LU9> а в каком формате такое сравнение можно сделать? просто списки? или что-то наоборот более сложное. типа разбить на несколько страниц... 
пока я думаю, что чего не делай -- все равно придется туда-сюда покрутить. в тоже время ""очень неудобный"" -- это все равно удивляет. с большим интересном познакомился бы с примерами удобных форматов.",
"<https://opendatascience.slack.com/archives/nlp/p1487239992002519>
<@U1CF95PP1> , а быть может надо отбирать и сокращать dimension? Хотя это не совсем подходит для текста, когда у меня метод ""bag-of-words"". Думаю попробовать на gridsearch - PCA, LinearSVC или chi2.",
"По надежности WD Red меня радуют, но медленные, для надежной мусорки 3 - 4 гига самое оно. Они бывают и быстрые, но уже дороже стоят и это интерпрайз, вряд ли у тебя нагрузка 24/7, то есть переплачивать за супер надежность две цены за быстрый хард как то не очень. Для быстрого бери SSD",
"я не совсем понял, почему там градиент такой",
и как это в MSE воткнуть,
"Runner/Queue выглядит как преждевременная оптимизация, если человек еще совсем не разобрался. Обычный предрасчет и векторизация вычислений дадут значительный буст.",
"<@U0ZJV6E5Q> сэмплер как раз и работает на видеокарте, основан на динамике Ланжевена. Я на ней считаю градиенты для перехода в mcmc",
"<@U0H7VBQQ1> я как раз np.array и использовал всегда. Просто увидел эти строки в доке, и засомневался",
"То есть с помощью очередей можно запихивать новый батч в память видеокарты еще перед тем, как досчитался прошлый? Мне кажется, у меня узким местом является именно работа с памятью gpu",
"Ну там надо смотреть где что разместится. Но да, не ждать вычислений очередного батча и пересылки на GPU.
Попробуй профайлер <https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659>",
Привет. можно ли у кого имаджнет взять поиграться (на официальном сайте молчат чего-то)?,
"а кто-нибудь добился с этими очередями одновременной пересылки следующего батча host-gpu одновременно с расчетом текущего?
НЯП оно только помогает следующий батч приготовить в host ram, а пересылка стартует только когда session.run() вызываем, т.е. уже после обработки предыдущего.",
Человек на реальных примерах из жизни рассказывает какие тесты для чего.,
"Как померять, что количество рождаемых птенцов значимо изменилось от внедрения нового корма, например :slightly_smiling_face:",
"Прикольная книга по статистике (тесты и мотивирующие примеры в основном): Резник ""Книга для тех, кто не любит статистику, но вынужден ею пользоваться""",
"А есть “Книга для тех, кто не любит работу, но вынужден ходить на неё”?",
"Про Градиент можно думать как о коэффициенте касательной к функции в заданной точке. Для гладких функций она и единственна. А для модуля в нуле, например, таких касательных существует бесконечно много с углом наклона от -45 до 45 градусов. Одна такая касательная называется субградиентом (и совпадает с градиентом в гладком случае), а весь набор -- субдифференциалом ",
"я что-то подзабыл, как обычно производная модуля берется? мы просто рассматриваем два случая, когда подмодульное выражение меньше нуля, и когда больше?",
"а ну и когда равно нулю еще, получается",
"<@U0JKYTE4B> а трансляция будет, не знаете? Или материалы. Как то поздно увидел это мероприятие, жаль. Должно быть полезно",
"их будет два, так что не поможет. В нуле не ноль, а множество векторов, как написал Артем.",
А как правильно работать xgboost и RandomForest с категориальными переменными? Какова самая правильная практика?,
"Я видел как ее не берут, а минимизируют аналитически. Т.е. если у тебя модуль от LASSO, то саму целевую функцию приближают квадратичной (с градиентом и гессианом - метод Ньютона), а сумма квадратичной и модуля минимизируется аналитически раскрывая модуль.",
"Ты по сути облегчаешь работу для деревьев, когда переводишь в числа.",
"Да, но для таких фич в принципе не может быть порядка как для чисел",
"ld86: как правильно сглаживать, кстати?",
"кстати действительно вопрос, а как бы модифицировать mean value encoding для категориальных если у меня вдруг при таком кодировании многие категории склеиваются?",
:wat: как у тебя получается 63 категории после кодирования средним?,
тогда я не понимаю почему это плохо,
там схема примерно как в стекинге получается,
"вернее это и есть стекинг, где в качестве модели - предсказывать среднее для группы",
ololo:  статью прост как вектор как-нить статистик представлять?,
"Вроде еще конкурс на бустерс был, где цену на недвижимость предсказывали по ценам ближайших соседей ",
"В некоторых видео курсах типа Udacity это называют нормализацией, так что я подумал, что это название будет более ближе.

Также как функцию логистической регресии называют сигмоидом, а не логит-функцией.",
"Спасибо за ответы!

Я просто подумал, почему софтмакс не нормализуют как `(x - xmin) / (xmax - xmin)`, более стабильные ведь результаты получатся.",
"То есть, как 2 отдельные карты их юзать нельзя? Для dl",
"Если это к моему посту, то смысла нет в sli, а не в двух картах. У меня как раз две 1070",
"А, то есть такой вариант, что ты покупаешь sli, играешь, используя sli, а работаешь как с двумя раздельными видюхами, возможен?",
"Просто забавно как сумма и так сильно завышенных значений получилась 490Вт, значит надо БП на 700-750",
"Может есть какие то общепризнанные оптимальные конфигурации? По частоте памяти, количеству плашек, hdd/ssd, синергии от комплектующих одного производителя, каких то особенностей материнки и видеокарты. С ценой и бенчмарками)",
"<@U1UBZLMKK> <http://deeplearning.net/software/theano/library/#theano.clone>
В краце это утилита, чтобы подменять части вычислительного графа. Полезно, когда есть вычислительный граф от инпута, а мы хотим поменять источник инпута. Использую, когда надо схлопнуть все стохастические переменные в один длинный вектор и получать этот вектор из апостериорного распределения. Тем временем апостериорное эффективно считается с помощью векторных операций",
"а калькуляторы какой БП рекомендуют проверяли?
<https://ru.msi.com/power-supply-calculator>
<http://www.coolermaster.com/power-supply-calculator/>",
"Второй круто, забил свой комп -- показал прямо как есть :slightly_smiling_face:",
Хотя и такое потребление достижимо только когда одновременно работает какой-нибудь furmark и prime95,
"Есть такой момент, как избыточная мощность бп, и это не есть хорошо.",
"с <http://overclockers.ru|overclockers.ru>: 

1. Снижение КПД на низких нагрузках (режим рабочего стола Windows, офисной нагрузки, интернет-серфинга и т.д.).
Вот у меня специально нарисовано сравнение типовой зависимости КПД для БП с мощностями 750W и 1000W и сертификатом 80+ Gold: График!
Для информации: система с i7-3770K с разгоном до 4.6 ГГц и Radeon HD6950 без разгона + 4x4 Гб памяти + 4 дисковых устройства, запитанная от Seasonic SS-760XP2, потребляет под LinX + Furmark (1920x1080 burn-in) 1.6A из розетки, т.е. около 350 Вт (собственное измерение с помощью токовых клещей). С учетом КПД (это ""платиновый"" БП) нагрузка на него самого при этом около 320 Вт. Поскольку LinX + Furmark - это практически максимально возможная программным способом нагрузка, такая система с этим БП будет в режиме низких нагрузок, в общем-то, всегда.

2. В более мощные БП часто приходится ставить более высокооборотные вентиляторы, чтобы хватало охлаждения на полной нагрузке (все-таки качественные БП рассчитывают на использование и при 100% паспортной нагрузки). Это сказывается на уровне шума и в простое.

3. На более мощных БП - более высокий порог срабатывания защиты от КЗ. Соответственно если какое-то комплектующее ушло в КЗ, на менее мощном БП выше вероятность, что он успеет его спасти, вовремя отключившись.

4. При включении БП с APFC происходит зарядка входного конденсатора. Это сопровождается кратковременным резким всплеском потребления тока из розетки, порядка десятков ампер (inrush current). У кого электросеть на это не рассчитана, может легко выбить пробки/автоматы. Причем этим можно подложить свинью не только себе, но и соседям. Чем мощнее БП, тем больше емкость входного конденсатора (одного или нескольких параллельно соединенных) и тем больше inrush current.",
Ни у кого нет датасета по ставкам в букмекерских конторах во время матчей?,
"какой ты наивный ;)) такие датасеты денег стоят и на дороге не валяются
начни собирать да продай через годик",
"А что и где почитать по линейной регрессии, в смысле математики, ассампшенов, гарантий и проч., а не как зафитить на R?",
"Здравствуйте, ситуация такая: препод по орг управлению сказал подготовить кейс там по изменению орг структуры ну по орг управлению в общем. Хочу прикрутить к этому DataScience. Мб у кого-нибудь есть идеи как это сделать и соответственно какой-нибудь датасет, типо можем показать на этой структуре получили профит а на этой нет, чтобы минимизировать какие-то пространные рассуждения",
<@U3P82LKDK> А какую именно из его?,
"gsoul: хоть бы написал, на какой нагрузке от максимальной достигается такое КПД:)",
"гайз, еще такой вопрос -- а как текст препроцессить для исправления ошибок? например, когда ""забылипробел"" или ""на оборот лишний""",
"ты можешь на предобученых векторах считать максимальное правдоподобие или negative sampling и сравнивать результаты. явно, что ml “ наоборот лишний” выше чем “оборот лишний” или “на оборот""
но это когда ты лишние пробелы ищешь
а когда ты хочешь разбить длинное предложение без пробелов на слова, то я бы шел по словарю и искал слова слева направо. если хочешь заморочиться, то и туда можно ворд2век прикрутить, что бы выбирать нужные слова, если есть несколько вариантов составить слово",
"но если хочется :science: то можно натренировать seq2seq выдавать уверенность для слов: кормишь по буквам, там где слово настоящее - говоришь, что это единичка, а там, где мусор - ноль",
"господа, напомните как называется отношение максимального собственного значения матрицы к минимальному",
"народ, поскажите как вы выбираете лаг для критерия Дики-Фуллера при анализе временных рядов?",
как этот критерий по английски называется?,
"любопытно, а с какой практической конечной целью этот тест используете?",
"отвергает нулевую гипотезу даже тогда, когда на глаз заметен тренд",
"есть такая проблема <http://xyproblem.info/> , поэтому таки интересно, а зачем вам тест на стационарность в итоге?",
вот для этой аримы как раз нужен стационарный ряд,
"были случаи, когда модель на оригинальных данных (с честными трендами) была хуже модели на дифференцированных?",
далеко ли надо предсказывать? (на какой период в будущее),
<@U0FEJNBGQ> недельная сезонность как раз учтется гармониками,
"какое разрешение данных, какая длина прогноза? есть ли дырки в данных?",
"а у меня проблема в том, что ACF не затухает, и я не очень понимаю как это полечить",
"зачем тесты, если и так известно, что ряд нестационарный?",
"тут же физическая природа состоит из сезонного поведения (каждый день недели в одно и то же время одни и те же люди едут в одно и то же место) и странных возмущений типа концертов и прочих событий, когда внезапно толпе надо разъехаться, после которых всё возвращается к сезонному уровню",
ну и цель хоть как-то понять как это все работает),
"как-то вы странно сезонность выпиливаете, и как следствие, она не выпиливается. Либо надо взять один сезон - неделя и никаких синусоид, либо все сезоны зафитить синусоидами и линейной моделью, а авторегрессию потом на остатках",
"всё-таки я не понимаю, почему в вырезке из учебника авторы рекомендуют одну сезонность есть аримой, а остальные регрессорами, что мешает всё в регрессоры пустить?",
идея как мне кажется показать что модели с аримой плохо масштабируются,
"у меня например был вопрос - можно ли параметра K для синусоид брать &gt;=7 в связи с тем, что при K = 7 регрессоры начинают учитывать суточную сезонность. Я как раз взять его = 7, так как иначе линейная модель показывала совсем плохое качество. Сейчас понимаю что видимо зря - так как с сезонностью стало сложнее бороться",
а ты как боролся с незатухающей ACF?,
"Сейчас, согласно заданию, тебе нужно построить sarimax, где в качестве регрессионной компоненты используется тригонометрический ряд. Его в модель sarima мы добавляем чтоб им смоделировать недельную сезонность, т.к. арима жрёт только самую маленькую сезонность (суточную)",
Недельная сезонность в остатках у тебя останется даже когда ты добавишь в модель 35 синусов и 35 косинусов,
"почему дается прямое задание сделать странное, тогда как можно было дать задание на перебрать разные варианты и выбрать лучшее? в процессе перебора вариантов будет полно шансов сделать хрень и увидеть это",
"Можешь добавить в регрессионную компоненту что-нибудь ещё, задание это не запрещает, можешь взять ещё больше синусов и косинусов. Но! Питоновская реализация sarimax будет пыхтеть часами и ты будешь много материться, когда будет падать ipython-kernel. Лучше оставь все эти дополнительные признаки не следующие недели, а сейчас построй какую-нибудь простую sarimax (p,d,q,P,D,Q не больше 3 и K где-нибудь 14 для тригонометрического ряда). На следующей неделе тебе таких моделей нужно будет построить 102 штуки.",
"“гайз, еще такой вопрос -- а как текст препроцессить для исправления ошибок? например, когда ""забылипробел"" или ""на оборот лишний""""

В грэммэрли прекрасно работает без машинного обучения, но нужно писать код. Была когда-то давно статья в интернете, как это делать. Сейчас не помню уже точно. Но примерно алгоритм: сплитит во всех местах и проверка после сплита, что оба слова получились из словоря, потом еще проверка по нграммам, что вписывается в контекст. Это если два слова в одно слиплись, чтобы обычно и происходит при письме. Если слов больше, то возможно можно примерно также, но вариантов перебора сильно больше становиться",
"<@U040M0W0S> Анатолий, привет. я тут увиделтвой коммент про добавления русского в спейси:

Не продвинулся больше. Мне ничего не ответили. Скорее всего за деньги могут сделать, как у них написано.",
"Пытаюсь для классификации собак и котэ обучить сетку при помощи mxnet. Взял архитектуру <https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol_resnet-28-small.R>, картинки сделал квадратными (аспект не менял, добавил пустого пространства по краям) и уменьшил до 50х50. Обучается быстро с батчами по 100 картинок, качество растет с самого начала процесса. Пытаюсь обучать на картинках 224х224 с батчами по 20 картинок (пробовал и 10) - эпик фейл! Топчется на 50% точности. С чем может быть связано, куда копать? Неужели :more-layers:?",
в общем не очень понимаю зачем переплачивать 150$,
"Попробуй визуализировать сетку как здесь:
<http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/>",
На ибее продавцы как-то очень чутко реагируют на колебания спроса и предложения и когда компоненты начали иссякать -- цены прилично поднялись,
"n01z3: спасибо, я сразу не осознал, какой размера слой получается после `mx.symbol.Flatten`. Поэкспериментирую с более подходящими архитектурами. Визуализация в R, кстати, похуже выглядит: <https://statist-bhfz.github.io/mxnet_intro.html> (и даже такое не сразу получилось нарисовать)",
"Так и я говорю, что экзотика :slightly_smiling_face: Что уже работает ""из коробки"" на питоне - в R требует плясок с бубном. Зато когда что-то получается, можно полчаса гордиться собой.",
"Всем привет. А у меня вот такой вопрос. Есть статья: <http://ieeexplore.ieee.org.sci-hub.cc/document/7738816/?reload=true> про идентификацию диктора по голосу с помощью сверточных сетей на TIMIT. На второй странице авторы описывают архитектуру небольшой сверточной сети, которая должна работать. Но при попытке воспроизвести она у меня не работает и учитывая то, что сам датасет маленький, а 2 последних полносвязных слоя размером в 6300 и 3150 - я даже в теории не представляю как она может работать и не оверфитится. Собственно мой вопрос - кто-то сталкивался, как можно заставить такие (с такой “шапкой"") сетки работать?

По моим расчетам размерность перед полносвязными слоями: 32 * 25 * 64 (далее полносвязный слой на 6300).

Мое видение реализации сетки из статьи: <http://pastebin.com/KnDxsdaZ>",
"Экономить на блоке питания точно не стоит. Как и на электрике при ремонте квартиры. Но и самое дорогое не всегда хорошее. Нужно смотреть на компоненты, кто их делает, схемотехнику. Хорошие производители пишут на каком уровне нагрузки достигается максимальная эффективность, но это больше для европейцев, хотя, если там сильные перекосы, то об этом следует подумать....
 Вообще можно неиспользуемые ресурсы отдавать на майнинг, так чтобы достигать максимального КПД по потреблению энергии и износу оборудования, но это в идеальном мире где-то в моей голове.",
"gsoul: второй конв слой  разве не model.add(Convolution2D(64, 3, 3, activation='relu')) должен быть вместо 32?
И что значит не работает, оверфит? Если оверфит, возможно делаешь слишком много итераций обучения. Выдели validation set, смотри как там ведет себя лосс в зависимости от итерации и останавливай обучение когда там начинается рост. <https://en.wikipedia.org/wiki/Early_stopping#Validation-based_early_stopping>",
"а train accuracy? если train не меняется, скорее всего что-то с параметрами оптимизатора (learning rate слишком большой или слишком маленький итд). Попробуй для простоты выкинуть пока dropout и добиться оверфита тупо на трейне. Увидишь какие параметры оптимизатора работают, сможешь плясать от них потом уже с дропаутом.",
"1 конечно уже много кажется, я бы попробовал и уменьшить тоже. Но чисто для отладки можно попробовать задрать абсурдно высоко и убедиться, что train accuracy хоть как-то меняется, необязательно в нужную нам сторону :) Если не получится, мб что-то с самой процедурой обучения, параметры не обновляются. Или с инициализацией проблемы (хотя в керасе conv слои вроде инициализируются по умолчанию как-то разумно).

Еще чтобы проблемы с сетапом исключить, можно просто сделать 1 полносвязный слой картинка-&gt;софтмакс и его обучить. Когда там будет разумный результат, вернуться к нужной архитектуре.",
"Ну и кстати Адам кмк не любит большие дропауты (т.к. шум градиента от дропаута увеличивает знаменатель в размере шага и шаги затухают быстрее чем надо). мб просто Нестеров, как в статье, будет надежнее работать.",
"Навскидку - задача прямолинейная и можно работать в лоб. 

29 классов и  1.9млн фото - это очень хорошо, причем даже если данные средней паршивости. (Бывает хуже. 10 классов и 25 картинок, как в задаче про спутники). Плюс, как я понял, тебе не надо решать задачу локализации ( где именно какие коробки), не надо считать сколько коробок каждого класса. А просто сказать - есть данный brand или нет.

=&gt;

Дергаешь pre-trained resnet, перебиваешь последний слой на 29 классов вместо 1000, вешаешь на него sigmoid и понеслось. К вечеру будет работающий прототип. Ну а дальше визуальная оценка предсказаний, подкрутка, эвристики и прочая магия, чтобы это отправить в production и оно там таки заработало как требуется.",
"я прост как раз сейчас понемногу считаю количество коробок конкретного вида на фотках, вроде классический цв норм подходит для поиска лого",
"Как написали в соседнем чатике про проблемы производительности тензорфлоу: ""Ну, да, бывает есть небольшая просадка, когда серверов параметров становится за тридцать"" :troll:",
"Да и сейчас поди есть, я им просто в почту написал и попросил счёт на пейпал выставить с доставкой (около ста баксов вышло). Как вариант -- можно на адрес форвардера отправить и консолидировать с другими заказами, так за ту же сотку баксов можно куда больше груза привезти :slightly_smiling_face:",
а на 2х гпу ты как считаешь?,
"Я пока на двух GPU не считаю. Но думал прикупить.

Если TF на 40 процентов медленнее TH на одной GPU, то на двух у них как раз и будет паритет.",
"Мб дело в том, как именно на 2 карты раскидывается работа?",
"<https://arxiv.org/abs/1701.08734>, для тех кто хочет саму статью",
А по какому принципу перемапить?,
"<@U1X5Q0S1L> У нас занимались. В целом это было построено на NER обученном на основе модели из OpenNLP (переобученной, но не помню уже на каких корпусах). NER помечал определенные слова принадлежащие к категориям: имя, органихация, геграфическое название. Они заменялись на тэг. Плюс конечно замена всех чисел, email, url на тэги.",
Смотря как потом предполагается текст использовать,
"есть статьи, есть категории для них, статей около 500k, категорий около 100 (наибольшая имеет около 40000 статей наименьшая около 40) количество ненулевых фичей для каждой от нескольких до 100. данные немного отфильтрованы min_tf=2, min_df 100, но в целом планировалось использовать logreg как baseline, поэтому особо не тюнили, и пока не с чем сравнить. но все же даже для мультикласса 0.5 кажется мало",
"Интересно, когда они следующее поколение выкатят",
"Все, кто хотел -- вполне могли себе купить в Штатах :slightly_smiling_face:",
"Господа, у меня довольно простецкий вопрос: как происходит выбор значения параметра, по которому происходит разделение в решающем дереве? Возьмем самое начало, потому что дальше все происходит по аналогии. У меня есть пространство объектов и как, кроме полного перебора, определить значение параметра x_j, которое оптимизирует целевую функцию?",
"Вроде бы не придумали. В ШАДе есть задачка, где нужно написать сложность построения дерева.",
"В этом году в Санкт-Петербурге совместно с НИУ ВШЭ и ИТМО проводится летняя школа (1-8 июля) для тех, кто заинтересован в Web Science, data mining
<http://wwsss17.com/>
<https://vk.com/wwsss17>",
"ребят, а кто занимался анализом данных в фарме за пределами клинических исследований? есть несколько вопросов по тому, какие бизнес-процессы , как и насколько сильно можно автоматизировать / оптимизировать с помощью ML в фармкомпаниях и CRO. Буду благодарен, если напишете в личку. Спасибо.",
Пацаны тем временем у нас в канале <#C3E3KUWPK>  первый вопрос. :achievement-unlocked: кто готов дать первый ответ? :all_the_things: ,
Как боженько вопрос дополнил просто,
"jgc128: Надо еще вспомнить про ошибки первого и второго рода, - что хуже: смеяться, как дурачок, там, где не следует, или пропускать шутейки? :thinking_face:",
"Нужен большой датасет, где была бы вся разметка. Плюс, часто шутки строятся на принципе искажении названия чего то очень известного (Harry Potter and order of Big Mac) - те модель должна иметь доступ к огромной базе данных сведений из внешнего мира",
"Тонкий юмор с аллюзиями, предполагающий знание оригиналов произведений как определять?",
"У нас была идея если не использовать какую-то специализированную бд, то хотя бы сделать языковую модель, и, так как в том корпусе много шуток типо той про гарри поттера, то сомотреть на отклонение предплогаемого слова, и того, которе должно быть (или просто суммировать word embeddings) - так модель сможет на каком-то уровне понять, что там происходит что-то не то",
"```
def get_traces(df):
    with pm.Model() as model:
        lambda_ = pm.Uniform('avg_price', lower=8, upper=100)
        price1 = pm.Exponential('price1', lam=lambda_, observed= df['price1'].values)
        noise_sd = pm.Uniform('noise_sd', lower=0, upper=10)
        noise = pm.Normal('noise', mu=0, sd=noise_sd)
        bias_sd = pm.Uniform('bias_sd', lower=0, upper=10)
        bias_mean = pm.Uniform('bias_mean', lower=-5, upper=5)
        bias = pm.Normal('bias', mu=bias_mean, sd=bias_sd, observed = (df['price1'] - df['price2']).values)
        price2 = pm.Deterministic('price2', price1 + noise + bias)

        trace = pm.sample(100)

    return trace

if __name__ == '__main__':
    trace = get_traces(df)
    ax = pm.plot_posterior(trace)
    ax[0].get_figure().savefig('pymc.png')
```
в итоге на графике видно, что для каждого семпла строится отдельное распределение, как будто семплы в observed трактуются как dimensions (например, <http://take.ms/iH1se>) 
спецы по pymc3, подскажите, что я делаю не так?",
"хочу, чтобы мои observed price1 и price2 фитились как переменная shape = (1, ). соответственно, на графике ожидаю не кучу графиков отдельных переменных на каждый семпл, а одно распределение",
"Скорее да, чем нет. Я постоянно использую. Правда не в режиме lr = f(n_epoch), а как вышли на плато так lr /= 10 Хотя и первый вариант можно будет попробовать.",
"на каком этапе флаттенить?
я только что попробовал заменить determenistic на простое выражение `price2 = price1 + noise + bias`, оно исчезло из трейсов, модель стала фититься значительно быстрее, но выглядит как некая магия",
Как минимум хуже стать не должно.,
"Я там прислал ссылку, где обсуждается вариант делать anneal когда validation перестаёт уменьшаться ",
"именно так же, как и с observed?",
"<@U1PDQ6VAS> Очевидным образом, разработчики библиотек нагло нас всех обманывают, и вместо qr разложения подсовывают что-то другое :slightly_smiling_face:

Если серьёзно, то я пропустил пару шагов. Во-первых, надо помнить, что исходная матрица $X$ не квадратная, а прямоугольная, поэтому надо использовать не обычное `QR` разложение, а его обобщение, которое выглядит следующим образом:

Любую вещественнозначную матрицу A размера nxm можно представить в виде произведения: Q R', где Q - это ортогональная матрица размера nxn, а R' это матрица nxm, где верхняя часть (первые m строк) представляют из себя верхнетреугольную матрицу R, а оставшиеся (n-m) строк равны нулю.

В слаке сложно набирать матрицы :disappointed: Но если погуглить википедию ""QR разложение"", то там будут эти матрицы выписаны.

Если теперь внимательно посмотреть на получившееся разложение, то несложно заметить, что последние (n-m) столбцов матрицы Q умножаются только на 0, поэтому это разложение можно переписать в эквивалентной форме: A = Q' R, где Q - это первые m столбцов матрицы Q, а R - это верхнетреугольная  матрица размера mxm (из предыдущего разложения). Именно эта конструкция и используется для решения задачи регрессии.

Далее, если ещё немного подумать, то можно понять, что должно выполняться условие (Q')^T Q' = I, где I - единичная матрица размера nxn, потому что матрица Q' составлена из ортонормированных столбцов. Обратное условие при этом вообще говоря выполняться не будет.

Всё, что остаётся сделать, это применить это разложение (и свойства матрицы Q') для получения конечного результата. Это можно сделать напрямую, можно через формулы любезно предоставленные Мефистофелем (они плохи для практических расчётов, но с теоретической точки зрения никаких проблем в них нет).

Например, для вычисления y^hat можно использовать так называемую матрицу проекции, или A^hat матрицу, которая имеет вид: 
```
A^hat = X (X^T X)^{-1} X^T, 
y^hat = A^hat y
```

Подставляя Q R разложение  получаем
```
A^hat = Q' R (R^T Q'^T Q' R)^{-1} R^T Q'^T = Q' R(R^T R)^{-1} R^T Q'^T = Q' R R^{-1} (R^T)^{-1} R^T Q'^T = Q' Q'^T
```
ровно та формула, которая использовалась для вычисления вектора y^hat. Вывод формулы для коэффициентов оставляю на собственную проработку (это полезно и несложно, ну и формулы известны, легко гуглятся).

При этом можно обратить внимание, что
1. Нигде не оборачивалас ь матрица Q, использовалось только транспонирование.
2. Матрица R оборачивалась законно, в том смысле, что R - матрица mxm, и если у неё отсутствуют нулевые собственные значения (что разумеется в случае треугольных матриц просто означает отсутствие нулевых элементов на диагонали), то обратная матрица определена и существует.",
А как насчёт такой машинки <https://pcpartpicker.com/list/FbsB7h> на одном cpu для тех же целей (kaggle+dl)? Буду рад замечаниям/комментариям.,
"а зачем материнка за 300, когда есть за 150?",
"чота альтернатив народному комплекту (tm) не остается, на ebay ощутимо подняли цены на аналоги",
а разве у них много? меня интересует список доменов с CMS для web-коммерции (веб-магазины и какая CMS),
"<@U1BAKQH2M> Спасибо за инфу про reduce lr on plateau ) Еще вопрос - как вообще узнавать про новые фичи кераса при выходе новых релизов? Релиз-ноутсы fchollet не пишет, описания коммитов тоже неинформативны, гугл-груп на эту тему пустой и вообще... Я не умею искать? Или единственный путь - заново прочитывать доку и искать отличия?",
"Шутка, наоборот они хотят сделать качественное описание, как я понял",
"То есть, условно, надо интересоваться тем, как люди learning rate менеджат, а не фичами кераса",
"(смущенно ковыряет пальчиком) ну да, есть такое... Не всегда можно точно сформулировать одну проблему, которую надо решить с фреймворком. Например только прочитав новую доку я нашел, что в 1.2 ввели (наконец-то) precision/recall как метрику качества, сейчас вот про адаптив lr, ну и т.д. То есть вводят новые фичи, которые зело полезны, но тебе не приходит в голову их искать, потому что уже привык обходиться своими костылями. Релиз-ноутсы решили бы эту проблему полностью, не понимаю, почему fchollet забивает на это.",
Привет! Есть у кого более-менее значимые результаты в классификации Музыки? Интересует определение жанра/инструментов/настроения и темпа композиций.,
Увидел вопрос на netflix собесе - что такое L0.5 (L1/2) регуляризация и почему она не используется. Озадачен,
"Товарищи, мб немного не в тот топик пишу, но все же. Никто случаем не знает, где в сети можно найти данные по соотношению биотек компаний с точки зрения конкуренции. Идея заключается в том, чтобы на основании этих данный и уже известных данных о миграционных тенденциях из одной компании в другую, предсказывать валидность перехода человека из компании А в компанию Б. Спасибо ",
Привет! Интересует классификация музыки/определение инструментов/темп/настроение композиции. Только-только начинаю разбираться как это все готовить. Буду благодарен за любую полезную инфу/вектор в данном вопросе :slightly_smiling_face:,
"Но ещё удивительнее, когда та же самая модель с бэкендом теано стала загружать шину на... 2%",
"<@U1R2F70HH> я точно не знаю, но подозреваю, что это что-то типа p-нормы (только при p=0.5 это уже нормой не будет), т.е. 
```
x_hat = min_x { (||Ax - y||_2)^2 + lambda (||x||_0.5)^0.5 }
```
Опять-таки, не знаю, как показать, но если взять производную от этой штуки:
```
2A’Ax - 2A’y + 0.5 * lambda * sqrt(x)
```
есть интуиция, что такая задача будет невыпуклой.
На всякий случай уточню, что в невыпуклой задаче может быть несколько локальных минимумов, следовательно, нет гарантии, что найденное решение будет оптимальным.",
а какая при этом загрузки видокарты? и как загрузку шины меряешь?,
"Мне рассказывали ужасы про то, как под виндой диплёнинг не делается, но оказалось, что вполне себе делается, если ты прирос к ней корнями :slightly_smiling_face:",
почему бы им не быть?,
"&gt;переходник vga-hdmi
это как вообще работает-то, лол)",
"Ткну пальцем в небо. Думаю, что тем кто не в теме, но что-то мог увидеть, помогло бы название нескольких компаний из начала и конца списка и несколько критериев (фич/признаков) конкуренции.",
"Привет всем, опять я с вопросом. Куда лучше копать, если нужно выделить наиболее популярные приветствия, поздравления и т.п. в зависимости от времени, места? Есть база писем, есть свои модули, выделяющие поздравления и приветствия. Получается мы имеем распределение поздравлений по времени и локации. Вроде как задача классификации с фичами времени и локации. Но из-за привязки к времени, походу нужно что-то типа ARIMA подключать. В то же время хотелось бы, чтобы модуль не просто научился правильно указывать, какое приветствие использовать из тех, которые мы ему сами указали при обучении. Но и сам выделял новые трендовые приветствия, поздравления, которые мы упустили, а он такой ""вот смотри, тут что-то часто с этим поздравляют сейчас, давай ты тоже это используй""",
"Ребят. Пытаюсь разбираться с tensorflow.contrib.learn и вот такой вопрос возник. Я так понимаю они намекают что бы я старался пользоваться input_fn для fit'a. Все бы круто а как организовывать минибатчи? Есть у кого-то пример как это вообще должно выглядеть? 

То есть если я не использую input_fn я передаю x, y, batch_size и все работает. Но вот если используется функция то тут у меня голова сломалась )",
"как тут арима поможет я сходу не вижу, но думаю можно сделать что-то типа group by по месяцу и городу (или какому-то другому географическому id) и посмотреть на самые популярные",
"Мне кажеться, если позволяет, то можно  попробовать время и место выделиться дискретно(типа часы в сутках и временные пояса, или страны), и считать MLE приветствия, имея эти данные,   С трендами не знаю как быть",
"<@U28J6CP29>  идея хорошая, но у нас и так большая часть данных из линкедина и если идти по пути того, чтобы брать HR сотрудников компании Х и смотреть работники каких компаний преобладают в их сетке будет работать не всегда и опять же скорее только у достаточно больших компаний. Изначальная проблема по большому счету актуальная для мелких компаний-стартапов численостью до 10 человек, вот у них на основании активности в соц. сетях не удастся в полной мере определить какие-то явные фичи",
"<@U0DA4J82H> <@U132PPTQU> эти идеи вначале тоже возникли, но хочется чтобы отслеживались изменения во времени. Вот как раз тренды, новые приветствия, что на первых данных не заметили. Единственное решение, которое вижу сейчас также как ololo предложил, но просто проигрывать скрипт по расписанию",
а почему в линкедин уперлись? есть же и другие соцсети. может в одних и тех же группах на фейсбуке люди состоят ;)) на одни и теже митапы ходят,
"Подумал, что это можно представить, как обнаружение разладки. Начал гуглить и первым делом про Фоменко наткнулся. :thinking_face: ",
"хочу попробовать прикрутить аналог word2vec к музыке. эдакие music2vec.  нашел статейку по этому поводу, и пытаюсь разобраться, как можжно уже имеющиейся либы переложить. может подсказать кто, с чего вообще начать ?",
"в общем буду рад любым советам :slightly_smiling_face:  датасет выглядит как song|genre users|song|count. в принципе могу достать еще плейлисты и прочую лабуду, типо скип песни, сколько слушал, альбомы и метаданные о песне",
"Было бы круто, если бы ты делал Speech Recognition человека, который напевает песню или скажем в чате переписываешься с кем-то, и из ходя из word2vec твоего месседжа он подбирает музыку :coolstorybob: .... хотя не, баян какой то :slightly_smiling_face:",
"как обычно решается проблема, когда предсказания всегда должны быть положительными, а регрессия иногда предсказывает отрицательные значения?",
"mabrek: просто из регрессии взять коэффициент и использовать его, как фичу?",
"я как раз оттуда это и взял :slightly_smiling_face: в смысле, использование тренда из линейной регрессии, как фичи в xgb",
А какой в этом смысл перед народным получается?),
"Да хз, какие у кого задачи",
cepera_ang: это ты где нашел?,
"Отличный буст был, когда оказалось, что опенцв из коробки все 32 потока может захавать :slightly_smiling_face:",
"<@U28J6CP29> по биотек профилю не так много инфы в фб том же. В целом в ближайшем будущем подразумевается выкачка данных и из фб как раз для анализа личностных показателей сотрудников, но это несколько иные данные и вряд ли на них удастся определять конкурентов",
"&gt;&gt;фб как раз для анализа личностных показателей сотрудников
недавно тут пробегал парень с которым имел очень интересную дискуссию насчет личностных показателей",
Можно в кратце к каким итогам пришли?,
"alex.ozerin: Не понимаю, почему это происходит. Ошибка возникает  всего-навсего при вызове метода predict, и вне функции обработки сообщений все отлично. ",
"Ребята. не подскажите где можно найти 500 датасетов с ImageURL? в формате csv, tsv",
"первый результат в гугле (<https://groups.google.com/forum/#!topic/keras-users/3NSKYn4FnVU>) говорит что это действительно может быть доступ из нескольких потоков, бороться с этим можно тем что обращаться с моделью в одном потоке - как конкретно уже зависит от фреймфорка где все крутится",
"Всем привет! Меня интересует задача по предсказанию количества событий в конкретной геолокации (области жестко не заданы, можно разметить произвольно, если это будет давать лучшие результаты) в конкретный промежуток времени на основе выборки уже произошедших событий.  

На данный момент есть идея предварительно кластеризовать выборку, а потом использовать на входе модели не координату, а кластер. Чтобы учитывать относительное положение кластеров есть идея делать их не просто категориями, а вектором состоящим из расстояния от центра масс данного кластера до центра масс других кластеров. Время предполагаю кодировать в набор признаков: выходной, день недели, год и т.д., так как события генерируются людьми и предположительно зависят от этих признаков.  Буду рад любым советам или ссылкам.",
cortwave: каких именно событий или секрет?,
"точное время, точные координаты уже произошедших событий, при желании можно дополнительных признаков достать, но пока в голову не приходит, каких именно",
"У меня съедает 840 Мб, потому что два моника. Но это не страшно. Печалит, что все тормозит, когда видюха нагружена",
"где предполагаемое количество событий может быть, впринципе, дробным числом",
"регрессионый анализ где промежуток времени, область независимые переменные а предполагаемое количество событий     зависимые переменные, естественно сначала нужно посчитать корреляцию чтобы убедиться что зависимость есть. для начала конечно надо структурировать данные, сделать более дискретными... вот что пришло на ум из того что понял, если можете продемонстрировать как то нагляднее, то есть еще способы",
"Всем спасибо. Вроде получилось, когда сделал свою функцию для получения сообщений безо всяких там многопоточностей",
а какой процент в датасете попал на главную?,
И эти люди рассказывают мне какой линукс прекрасный :slightly_smiling_face:,
"<@U040M0W0S>, в смысле в каких пропорциях была обучающая выборка? <@U48A2PYAJ> у меня там кода как такого не было - была блок схема со стрелочками которую я сделал тутору с ютуба (еще был код сбора самих цитат). попробую сегодня вечером модель найти",
"Мне больше интересно как это мерить, но не будем о метрологии",
"Вот мне удобно, когда картинку 500 мбайт винда открывает влёт, и неудобно, когда линукс зависает наглухо при таком действии",
"это объективный тезис: венды неюзабельные в принципе, макось как система не так плоха, даже в чём-то лучше всяких убунт и проч., но как продукт (покупайте наши макбуки и маккоробки без ГПУ и возможности выбрать железо) - так себе",
"""венды неюзабельные в принципе"" -- как же так? Неюзабельные, а 90% людей юзают",
"как ось для домохозяек, макось - лучший выбор",
"а если серьёзно, то он вроде как platform-independent IIUC",
"Общался со всякими чуваками из айбиэма -- так они вообще живут в мире, где кроме мейнфреймов и пауера ничего нет",
Всякие линуксоиды-сишники -- в мире где линукс самая удобная ось и кроме си особо ничего нет,
"&gt; Всякие линуксоиды-сишники -- в мире где линукс самая удобная ось и кроме си особо ничего нет
просто потому что все девелоперы на линуксах",
"Имхо, достаточно много людей с маками, не только те, кто под макось пишет. Тут дело личных предпочтений, в выборе между маком и линуксом",
А какая версия драйвера у тебя?,
"противопоставление мака и линукса как-то неверно в корне. эппл железо и не эппл железо, или разные оси. как бы ОС поставить можно с разным накалом БДСМ любую и на любой лэптоп.",
"С виндой немного другое, там очень много тех кто пишет под дотнет и намного меньше остальных (опять же, имхо)",
"нене, у меня нет претензий к тем, кто под платформу пишет: у них нет выбора, да и под дотнет ничего кроме студии лучше нет",
"наброшу всем:
- тем кто на юниксах: запустите мне дебаггер для ARM проца
- тем кто на винде: putty пфпффпфпфпфп",
<@U1CGKK865> а как же макбуки и осХ?,
"имхо, если есть бюджет, то нужно разделять то, что тренирует модель и то, на чем ты кодишь. И если для первого - да, до пары месяцев назад выбора не было, только никсы, то сейчас венда вполне себе гоняет теаны и прочие ТФ. А уж какую IDE - открывать, и так всем понятно, лучше всего Emacs",
"Да, это в некотором смысле изучение интернета. Вообще, Web Science - это новая междисциплинарная область науки, целью которой является понимание того, как всемирная паутина меняет общество. В ней используются передовые методы социальных и компьютерных наук. Если интересно, то, возможно, предполагаемые темы докладов могут дать лучшее понимание контекста проводимой летней школы:
- Introduction to Web Science 
- Multimedia analysis 
- Digital health and online interventions 
- Risky content detection online 
- Online gaming 
- Cities online 
- Online experiments for psychology and wider social science",
"теперь, как я понимаю, автор работает в facebook и активно пилит с коллегами",
Жесть какая этот vs code,
Как будто вокруг плиточного приложения блокнот обернули,
"Ааа, и это ещё вдобавок как принято обёртка вокруг хрома",
С раздражающими надписями капслоком и как это сейчас принято конфигурацией в джсоне,
"в theano '0.8.2' были замечены странные баги:eww:, исправляется апгрейдом. Баг в том, что на минибатчах не было сходимости, хз где они именно",
дай тот диплом где чувак выбирал модель которой забустить на следующем шаге,
"Ребят, а как вы обычно токенизируйте текст на 3-граммы? У меня есть тексты, я их классифицировал и сентиментизировал и добавил NER, теперь мне надо чтобы серч, скажем на Elastic, смог найти мои тексты... Я думаю выцепить ключевые 3-граммные слова... Есть в общем 2 варианта. 

Первый вариант: я пробую вот так с NLTK

1) токенизируем текст на слова и фильтруем `tokens = [t for t in nltk.word_tokenize(text) if len(t) &gt; 1 and t not in stopwords]`
3) создаем 3-грамы `trigrams = list(nltk.trigrams(tokens))`
4) тэггируем каждый 3-грам с POS `... nltk.pos_tag()`
5) по след. комбинации POS фильтруем еще раз 

существительное число существительное -  число существительное существительное -существительное герундий существительное - прилагательное существительное существительное - прилагательное существительное число .... ну и так далее

Просто либо надо перебрать все возможные комбинации, либо просто фильтровать нерелевантые POS тэги для поиска (типа глаголы и тд), ну последний вариант не совсем гуд, мне кажется.

Второй вариант: просто скормить весь текст серчу, и доверится Люсьен алгоритму.",
"господа, если кто не в курсе, то у нас теперь есть <#C486WV5TR|ods_habr> и адепт <@U041P485A> пишет пост про :theano:, может среди нас найдется несколько человек которые после этого поста напишут аналогичные посты про :tf:, :mxnet:, :torch:, :caffe: ? если есть такие добровольцы то го в <#C486WV5TR|ods_habr>",
"я просто аналоговую хочу заказать, но цена смущает — доставка стоит почти как сама книга",
"Привет! хочу сделать интерактивную визуализацию tsne (чтобы зумить можно было бы) для картинок (лиц), но не могу найти как в bokeh вставлять картинки вместо точек на scatter plot. Ну или может какая-нибудь альтернатива bokeh есть? Может кто-то видел уже реализацию?",
Схожий вопрос - а кто что юзает для 3D визуализации embeddings?,
"Всем привет, у кого есть опыт получения картинок товаров из Амазона? Или вдруг уже есть готовые датасеты?",
"Привет! Как всегда, для ODS самый первый анонс. Приглашаем на следующую Data &amp; Science, посвященную квантовым компьютерам и архитектуре алгоритмов будущего: <https://events.yandex.ru/events/ds/18-mar-2017/>",
"<@U1Z7QM16H> почему галочка у ""Я уже посещал мероприятия Data &amp; Science"" обязательна?",
"Может кто-то дать совет? Хочу найти карту Москвы, по которой можно было бы самому автомобильные и/или пешеходные маршруты строить. Т. е. чтобы можно было легко понятять - по таким-то улицам пешеходной дороги нет (например, ТТК), а на таких-то дорогах одностороннее движение (или вообще движения не может быть). Мне хотя бы понять, в каком формате это должно быть. Никогда раньше с навигацией не приходилось дело иметь.",
Elastic Net - это когда у вас сумма L1 и L2 штрафа <https://en.wikipedia.org/wiki/Elastic_net_regularization>,
"Да, хочу ради эксперимента попробовать построить автомобильный маршрут, который проходит через все улицы Москвы, но с минимальным количеством повторений. Задался бесполезным вопросом, какая получится длина и сколько это времени займет.",
"Ага. Тем более, я даже конечную цель и задачу сформулировал. Вдруг кто уже знает, какого ежа и с какими ужами надо скрестить. Для 10М вершин я вообще уже сомневаюсь, что ее можно будет решить самым оптимальным способом. Только каким-то субоптимальным алгоритмом.",
<@U0M39M6LS> а как ты собрался отдельно тренировать эмбеддинги? с каким objective?,
"у меня пошла ассоциация на ворд2век и я не мог понять, как оно все это тренерует одновременно.",
"Я уже придумал, как мою задачу перевести из графа в набор точек на плоскости",
Привет! Какие кто использовал подходы для поиска случаев неверного разделения слов - два слова склеились или одно слово разделилось посередине (пробел стоит не в том месте)?,
"sklearn все равно лучше, как мне кажется",
"А кто-нибудь работал с автоэнкодерами sequence to sequence или VAE, которые должны генерить последовательности? Я вот сейчас в блоге кераса натолкнулся вот на такой кусок кода:
```
inputs = Input(shape=(timesteps, input_dim))
encoded = LSTM(latent_dim)(inputs)

decoded = RepeatVector(timesteps)(encoded)
decoded = LSTM(input_dim, return_sequences=True)(decoded)

sequence_autoencoder = Model(inputs, decoded)
encoder = Model(inputs, encoded)```
также видел примеры кода, когда примерно такую же архитектуру использовали для декодера в variational autoencoder.
Мне показалось немного странным, что для декодера на каждом таймстепе используют один и тот же вход.   Это стандартная практика? Кажется, что на вход лучше бы подавать выход с предыдущего таймстепа или использовать teacher forcing (подавать на вход исходную последовательность). Кто нибудь может поделиться своим опытом?",
"я когда авито парсил, эту штуку юзал <http://rest-app.net/api>",
"Серьезно? А я как раз сделал аналог <http://builtwith.com|builtwith.com>, правда чуток похуже, но все же....
Хотел сравнить результаты",
Да просто тупо стенограмму доклада на непрофильной конфе выкладывать как статью,
"sim0nsays: Предъява не по сути, но у нее русский, как у меня английский - короткие рубленные предложения.",
"<@U34Q3KU8H>  у меня нет аккаунта на хабре, поэтому отвечу на твой комментарий здесь. В MegaFace ты можешь поучаствовать - он бессрочно продлен. В dsb3 я, лично, планирую участвовать как и в предыдущих двух. На спутники, увы не успел вписаться.",
"Спасибо. Это я комментарий написал скорее как провокацию, чтобы серьезный народ влился и в процессе рубки и после окончания, по обсуждениям, можно было бы много идей получить.",
"Для основ ML мне больше Бишоп понравился — более структурировано, более строго, есть задачи/решения. Для DL я бы советовал читать статьи, так как там сейчас быстро все меняется и информация со временем теряет актуальность. 
Имхо, в многих местах DL притянуто за уши, скорее всего, в целях маркетинга. Но в целом книга понравилась.",
"И еще вопрос про Keras. Пусть я хочу склеить несколько слоев в один, так чтобы было удобно сеть строить.

Например, у меня в Unet много блоков вида:

```
conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(inputs)
conv1 = BatchNormalization(mode=0, axis=1)(conv1)
conv1 = keras.layers.advanced_activations.ELU()(conv1)
conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv1)
conv1 = BatchNormalization(mode=0, axis=1)(conv1)
conv1 = keras.layers.advanced_activations.ELU()(conv1)
```

Как определить функцию или класс, чтобы работало как:
```
x = sandwich_layer(parameters) (input)
```
?",
"Следующий вопрос, как в Keras в цикле вот такую штуку сделать с произвольным числом слоев? <https://i.imgur.com/MMqosoj.png>

Если бы не было shortcuts, я бы просто в цикле использовал functional api, но вот как это с ними сделать?",
"скоро, наверное, уже GUI сделают, где просто накидал слои, мышкой соединил, параметры подкрутил и пошло",
sovcharenko: есть даже с видео <https://youtu.be/K-ZFPGanTW8> где все подробности и нюансы на русском,
vadim: А со мной-то как это связано?,
"зачем DL, если есть :mac:",
"Я уже говорил -- это вопрос вкуса и привычки. Мне нравится то, что всё работает так как я привык. То же самое в линуксе/маке для других. (только некоторые другие почему-то настаивают на превосходстве их привычного уклада, а я вовсе нет)",
"да, этого как раз хватает. я скорее про то, что качественные саммари на вес золота, и их не всегда легко найти",
"Зачем читать старое, когда есть отличное новое, которое лучше хотя бы тем, что с течением времени стало понятно какие подходы работают, какие нет",
вот я и спрашиваю где ее найти :slightly_smiling_face:,
"вообще если пофиг на glove/word2vec (заранее не скажешь кто лучше будет, если будет разница), то можно просто сделать pip install gensim и тренировать word2vec",
"удивился, почему там нет glove",
"А потом отпиздила тех, кто и правда начал делать, что хотел (причём сначала ещё отпиздила за то, что неповерили, что и правда нужно делать, что хочешь, только после этого начали)",
Где можно найти корпус русского нейтрального текста?,
"<@U2M0WG6F2>: получилось с зумом? Если нет, то давай придумаем как этого добиться. Датасет -- это просто картинки с координатами? ",
"Если у кого будут идеи - поделитесь плз. Пытаюсь в комп с 2-мя 1070 картами и материнкой gigabyte x99p-sli вставить третью карту - 970 и система не грузится, даже в биос не заходит. Раньше пикал 8 раз при загрузке, но после того как я попытался в опциях связанных с загрузкой - включить легаси опции - пикать перестал, но все равно не включается.",
"&gt;&gt;<@U2M0WG6F2>: получилось с зумом? 
Ну типа есть сет фейсов -&gt; с них facenet’ом вытягиваю эмбеддинги -&gt; по эмбедингам делаю tsne. Уже сделал так как у карпатого (чтобы они не перекрывались и были под сетку) и дамплю супер огромную картинку - это как я сейчас сделал. Но что-то более прекрасное я не придумал",
<@U0M39M6LS> А зачем тебе именно glove на keras? Религиозно неправильно тренировать его оригинальным кодом glove на C? :slightly_smiling_face:,
"Привет! Обращаюсь к коллективному разуму: вот есть колл-центр и есть у него там какой-то набор сценариев в зависимости от намерения пользователя. Предположим мы хорошо определяем намерение пользователя и все нужные параметры из текста. Дальше мы переходим к сценарию решения запроса. Вопрос в следующем: как ведут пользователя по сценарию? Отслеживают ли как-то его под-намерения в процессе или всегда предпологают что пользователь отвечает на текущий запрос системы и не рыпается? Хотя бы как такое называется в диалоговых системах: state tracking, user action prediction или как-то по другому?",
"Почему здесь гауссовское ядро нормализуется на 16, а не на 16 ** 2,  чтобы сумма была 1? <http://docs.opencv.org/2.4/doc/tutorials/imgproc/pyramids/pyramids.html>",
"Блин, меня недавно Лассо пипец расстроил :disappointed:
Были две фичи, я знал, что таргет  как-то просто с ними связан. Не вдаваясь в подробности, рейтинг своей команды и рейтинг команды соперника - это признаки, а предсказать надо максимальный получаемый рейтинг (выводится игрой) Я сделал фичи типа целочисленно делённых признаков на 2, 3, 5, 7 и т.д. Ещё там был бинарный признак есть бонус или нет, но тут я просто забил и домножил фичи на 2 там где он был и на 1 там где не было. В общем, лассо давал какую-то муть с большой МАЕ и левыми коэффициентами.
Потом я беру тупо лин. регу обычную из статсмоделс и строю её на рейтинге соперника.
И по сути сразу получаю идеальную прямую близкую к истинной формуле 150 + ER/4
Ну не совсем, коэффициенты немного другие конечно, а деление в настоящей формуле целочисленное. Но у меня все промахи прямой были меньше единицы (характерное значение таргета - в районе 400), а Лассо давал МАЕ в 12 и даже близко не подобрался.

Я вот сейчас ради фана даже убираю признаки в Лассо, кручу параметр реугляризации, по-разному добавляю/убираю константный, но он не раскусывает нифига :disappointed: То есть как бы мало я ему ни оставлял - всё равно не выходит на те же коэффициенты.
Ну и я ему ставил только положительные коэффициенты в какой-то момент, но без них он ещё хуже справлялся. Короч чё-т очень странное. Не, я мог накосячить, но вроде же негде. Normalize в парамерах Лассо тоже ни на что не повлиял.",
"что за реализация и как параметры подбирались? бывает, он в константу иногда предсказание выводит, хотя можно было и фичи использовать",
"Sklearn-овская. Параметры не подбирал, крутил руками :slightly_smiling_face: Ну то не, понятно, что там мультиколлинеарность будет, но он её ж должен убить за счёт регулярзиации раз и непонятно как он вообще сошёлся куда-то не в минимум MSE.",
"про параметры - я попробовал просто парочку экстремальных значений альфа (типа 0.0001 и 200), потом несколько между ними, понял, что разницы принципиальной нет и успокоился. Вообще я всегда считал, что в таких случаях (много признаков, от каких-то целевая переменная линейно зависит, но от каких - неясно) Лассо как раз и может помочь, но оказалось, что чёт ниоч :disappointed:",
"Вот такой вопрос, к тому, кто сегментацией изображений занимался: “Правда ли что train / val loss ведут себя более шумно, чем при задачах классификации?""",
"Тот момент, когда бот собирает больше лайков, чем ты",
"Сопутствующий вопрос, а как бы это сгладить? 

Может под сегментацию какие-то специальные оптимизаторы, которые не стандартно используемый Adam. 

Или может Momentum увеличить / уменьшить? 

Или слабую L2 регуляризацию влепить везде?

Или разлениться и прикрутить Polyak Avregaing.

В общем что у кого получилось для сглаживания кривой обучения. А то скачет как не родная и непонятно, плато, или нет или вообще что происходит.",
matrix_cat: В каком плане проверенные?,
"Компьютер в сборе или 2 плашки памяти, хм, какой сложный выбор :joy:",
"Хотя не удивлюсь, если через три года терабайт памяти домой будет так же доступен, как сегодня 128gb :slightly_smiling_face:",
"Скажите как принято в вашем этом диплернинге: начал тренировать модель, сразу и валидируюсь (разделил 0.8 и 0.2). Так вот вопрос: мне нужно выбрать лучшую эпоху и перетренировать на всем датасете? (как и в классике) или тут свой подход?",
"Почему такой вопрос возник, потому что некоторые модели могут тренироваться днями-неделями, нужно и валидироваться сразу. Но вряд ли будут ждать еще неделю, что бы заново натренировать модель",
"Особенно потому что есть например техники, где гиперпараметры меняются в зависимости от значения лосса на валидации",
"мое ```[0.72019052723325172,
  0.76180440504036739,
  0.7832143553318005,
  0.80174934216937921,
  0.81397402787833595,
  0.82630835201247443,
  0.8369554624363712,
  0.85023998635029674,
  0.85928515739787592,
  0.86805014130595415,
  0.87723540589423943,
  0.88564101938220352,
  0.89396744955470131,
  0.90086858006042292,
  0.90731897476460432,
  0.91011475490880123,
  0.91721688918046884]``` и ```'val_acc': [0.73189747585630849,
  0.77207387197135013,
  0.78744761722615997,
  0.7970714355674523,
  0.79980021443890725,
  0.80082350642276123,
  0.80196861904297823,
  0.79643796897401542,
  0.80377156225494817,
  0.79914238375421276,
  0.80430757233232397,
  0.80308936754870208,
  0.80155442942769983,
  0.80386901859093474,
  0.80133515249601217,
  0.80062859367527306,
  0.79972712214189878]``` я бы эпох 7 гонял, если бы это был xgboost. То есть лучшей эпохой я считаю 6, но так как к-во данных выростет, то лучше увеличить время тренировки",
"sim0nsays: это ты про уменьшение lr , когда достигается плато?",
"Еще нубский вопрос: тренирую бинарную классификацию, метрика accuracy. Ориентиироваться лучше на val_loss или val_acc? у меня такая штука получилась, что когда модель выходит на плато, то acc – прыгает туда-сюда, а loss начинает рости. Значит лучше выбирать эпоху с меньшим loss?",
Тогда у меня вопрос как ты его слушал),
"или переставать учить в момент, когда начинает расти :idk:",
"Всем привет. Может кто подскажет, есть ли обученная модель\ размеченный датасет для категоризации коротких informal предложений на английском по типу активности о которой в них идет речь (например, I'm down for a light snack - [eating]).",
"Как обычно подходят к задаче классификации во времени? есть действия пользователей во времени и надо предсказать вероятность того, что он отвалится  в следующий период. на пользователя есть порядка 3-5 точек",
"mabrek:угу он самый. в геймдеве пробуют фитить бета распределение, но это как-то не по мл. а как именно временные фичи использовать для какой-нибудь SVM ?",
"у меня на работе 128 Гб, я пока не понимаю зачем так много :slightly_smiling_face:",
"хочу под ubuntu 16.04 desktop заставить работать встроенную intel карточку для рендеринга gym. Сейчас никакой display manager не запущен, хожу только по ssh. `lightdb start` или `status` выдает `update-alternatives: error: no alternatives for x86_64-linux-gnu_gfxcore_conf`, `prime-select intel` ничего не меняет. драйвера переустанавливал меньше месяца назад из nvidia ppa. Подсажите в какую сторону копать? Как сказать lightdm забить на nvidia?",
Мне один раз удалось сделать как чувак советовал. Но потом обновил убунту и все по пизде пошло.,
"ага! спасибо, для gym как раз то же советовали, но я все хотел обойтись без переустановки драйвера. тогда как спутники закончатся снова попробую",
"В Украине за весну планируется как минимум 5 конференций по дата сайнс, в прошлом году наверное всего 5 было :slightly_smiling_face:",
"Это небольшой троллинг или серьезное предложение?) Если это будут новые титаны на hbm2 как планируется, то это либо будет не подъемно по деньгам, либо на всех не хватит. Острый дефицит сейчас на эти чипы памяти. Так еще придется думать над пропускной способностью PCI, так как это станет узким местом скорее всего. А если оставят текущий стек технологий, то смысл переплачивать. Говорят там будет 16 нм FinFET. Будем ждать второй или третий квартал 2017 года",
"При уменьшении ЛР на плато, кто сколько эпох обычно выставляет patience и потом на какой коэффициент уменьшает?",
это при каком размере датасета и сколько времени на 1 эпоху уходило?,
"О, это интересно! Как раз собиралась в конце марта-начале апреля в Голландию, теперь буду знать, на какие даты точно нужно ехать ;)",
"Народ, помогите разобраться с пониманием кодирования изображений. Упражняюсь на предмете стандартного дата-сета с цифрами.
Как я понял, для анализа изображений - достаточно представить их пиксели как векторы признаков. Но, например, человек распознает цифры, по соотношению взаимного расположения пикселей. 
Не понимаю как это взаимное расположение  выражается в многомерном пространстве, когда мы переходим к признаковому описанию и каждый признак просто описывает отдельный пиксель. 
Или я что-то не понял про то как картинка превращается в набор признаков? 

Если где-то есть готовое объяснение для этого - ткните носом, пожалуйста. Сталкивался с описанием алгоритма на курсе coursera, в паре книг, но везде он даётся мимолётом, понимание ко мне так и не пришло.",
"Чот погуглил, показало цену 780 19к, 1060 -- 14к :slightly_smiling_face:",
"ну, нулевую 780 наверное резона брать нет, потому как те немногие, которые еще продают, цену держат имхо не совсем адекватную",
"Чот за пять штук они находятся только ""фо партс о рипейр""",
"Да и памяти что-то всего 3гига, а не 4, как сначала показалось",
"да у меня как раз 780. задумался об апгрейде (все побежали - и я побежал), а брать всего в полтора-два раза шустрее как-то не хочется",
<@U1CF22N7J> хочу как следует разобраться,
"Когда читаешь статью, а там написано -- для того, чтобы повторить наши результаты вам нужна будет машина с 4 карточками, минимум 11гб в каждой",
"Вообще, есть два ~стула~ подхода: традиционный, когда мы из пикселей всякими ручными хитрыми преобразованиями делаем фичи и учим на них какой-нибудь классификатор или чего нам там надо и нормальный, когда мы скармливаем пиксели свёрточной сети и получаем на выходе ответ",
как в матплотлибе менять contourplot из колбэка? хочу посмотреть как меняется граница принятия решения в процессе обучения,
"спасибо, но 780я у меня в наличии - каких-то явных нареканий по вычислительной части не имею. Вопрос, скорее, в том, когда уже пора апгрейдиться и на что именно.",
"Всем привет! Наверное, многие читали, что на sberbank data science day командой MCC2VEC было предложено решение по детектированию подозрительных транзакций с помощью библиотеки word2vec. Кто знаком с методами распознавания аномалий или с библиотекой word2vec, можете оценить жизнеспособность их идеи? Ссылка на материалы с той конференции: <https://habrahabr.ru/article/318160/>
Какие вообще есть подходы к решению задач поиска аномалий?",
"Продолжая разговор из <#C0SGCGB52|career> про инвертирование гномиков на собеседованиях, но в разрезе академической среды.


В UC Davis на физическом факультете в аспирантуре есть вот такая штука, думаю, что и в других вузах практикуется. 

Одно из требований на долгом пути получения PhD - это сдать письменный closed book экзамен по физике. Материал - бакалавриат.
Проходит 2 дня, по 6 часов.
День 1: Теормех, Классическая электродинамика, математика
День 2: Квантовая механика, Статистическая физика, Мутные задачки, которые в другие темы не попадают.

Каждый день 5 задач, за каждую как максимум 100 баллов. Чтобы пройти экзамен надо 200+ за каждый день и 500+ в сумме.

Ссылка на экзамены прошлых лет:
<https://sites.google.com/a/ucdavis.edu/physicsprelim/prelims>

Сдаются в начале сентября до начала учебного года.
- Иностранцы имеют 2 попытки. Одну по приезду, одну в начале второго года.
- Американцы имеют три попытки по приезду, в начале второго года, в начале третьего года. Хотя я знаю, что некоторые девушки по 4 попытки заклыдывали, но они идут по женской квоте на двойных гендерных стандартах в США.

Можно сдать день 1 в один год (250+), а метариал второго дня в другой год.

Это все я к тому, что сдача этого экзамена не гарантирует, что ты физик нормальный, или то, что ты сможешь вообще науку двигать.

Экзамен проверяет смог ты на этот экзамен натаскаться или нет. Есть гипотеза, что инвертирование гномиков на интервью падает в ту же категорию.",
<@U1D4RRA7K> Ты какой прооцент задачек сходу одолеешь?,
"Ага, ну вот математика впринципе как раз бакалавриат российского матфака. Анализ посложнее алгебры",
"На тему азиатов и их подготовик к экзаменам. 

В 2011 году.

У меня 611 - третье место.
У девушки одной - 615 - второе место.
Китаец один 780 - первое место.

Остальные, кто прошел, 500-550

Но очень сильно летишь на том, что не помнишь какое-нибудь разложение или какую-нибудь очередную формулу. Девушка эта их месяц заучивала. Я забил и выводил прямо на экзамене, что мог. Но смог далеко не все.

Но 780 - это, реально, больше, чем много.",
"Но хочется отметить, что времени на все это с избытком, такого временного напряжения, как на интервью нет.",
А не как у нас,
"буууу, это как раз то, за что убер в т.ч. пинают",
"Это когда для выравния баланса между количеством мужчин и женщин, ибо показное равноправие, для женщин снижают критерии.",
"Это вполне себе медвежья услуга. Они после окончания работу ищут в режиме, когда двойные стандарты отменили - и сильно прищуриваются и вопят, что двойные стандарты ввели.",
"Мои самые любимые экзамены были - take home. Это когда тебе дают задачки и у тебя 24 часа их решить. В теории предполагается, что ты не будешь гуглить, читать книжки и общаться между собой.",
"И, кстати, когда мы, таки, общались на первом таком экзамене, одна студентка настучала преподу.",
"Да… Все таки в Америке как плюсов много, так и минусов. Все эти вещи их чересчур чур как-то",
"какую метрику лучше выбрать для бинарной классификации, если нулей как обычно больше чем единиц, причем цена ошибки ложного срабатывания (0 классифицирован как 1) значительно выше, чем 1 как 0? а что если поровну 0 и 1, но цена ложного срабатывания все так же выше?",
"А может кто-нибудь подсказать : у меня есть object detector с инпутом 300*300, проблема в том, что входное изображение может быть совершенно произвольным и, часто, сильно больше, например 3000*1900. Однако, если я просто делаю ресайз, то качество распознавания сильно падает, так как есть много объектов, которые становятся слишком маленькими. Вопрос - есть ли какой-нибудь открытый фрэймворк, который бы оптимизировано бегал по всему фото и где-то делал ресайз, а где-то нет в зависимости от уровня достоверности распознанных объектов. То-есть и объекты по максимому находил и ресурсов много не тратил.",
"в лекции Рябченко, той что выше, как раз упоминалось в конце. вот их презентация 2015 года о regularized linear regressions for time-series <https://www.slideshare.net/seanjtaylor/automatic-forecasting-at-scale>",
"<@U47EXAQ8H> спасибо за ссылку! Цены билетов заставили задуматься о посещении более осознанно) как думаете, насколько оно того стоит?",
"Кстати, насчет гусеницы, где про неё можно почитать? ",
У меня профессор во времена СССР учился на севере где преподаватели из союза приезжали.,
А как решился вообще в китай ехать? Сосвем другая культура же. Язык опять же,
Ребят подскажите какими python библиотеками можно выгрузить данные с этого сайта? <https://bmis2.buildingmgt.gov.hk/bd_hadbiex/content/searchbuilding/building_search.jsf?renderedValue=true>,
"Подскажите по keras: если я добавляю `EarlyStopping` с `patience=3` и хочу сохранить лучшую модель, то мне нужно делать `ModelCheckpoint` и забрать последние сохраненные веса или модель по умолчанию “откатится” до лучшего loss, как в xgboost?",
"vradchenko: а зачем тебе, без самого корпуса даже?",
"в каком смысле зачем? у меня данных мало, свои нормально не натренишь. поэтому хочу заюзать уже готовые. 
почему маленькие – хочу увеличить скорость тренировки и сравнить как меняется точность",
"Есть ли здесть люди из 2гис, яндекс карт, хабидатума? Хотелось бы узнать, можно ли каким либо способом получить срезы карт россии по годам?",
а почему нельзя просто научить 50-мерные вектора?,
какая будет примерно закономерность на тесте?  как выбирать критерий остановки при тренировке?,
"для временных рядов какие фичи обычно помогают? уже добавил средние по окошкам разной длины, среднее по выходным и разным дням недели,  тренды по окошкам и лаги размером с интервал предсказания",
"во всяких аримах еще используют ошибки (MA часть), но как их сюда приделать пока не понял",
"дамми типа day_of_week и is_weekend навешал, какие еще можно?",
"но на 2к рядов я хз как такое сделать, это больше ручная работа",
"панельные ряды - это когда у нас кучка рядов, например, для разных магазинов, как у меня?",
"Опишу тогда сразу проблему: не получается сделать так, чтобы Z значения в `contourf(X,Y,Z)` зависели от порядка X и Y. Условно, есть матрица с входными данными (переменными Xn). Допустим X1 идёт по оси X, X2 идёт по оси Y. Я делаю `meshgrid` и с учётом тренировочных значений в Z рисую мой contour plot. Но когда мне нужно, чтобы X2 шёл по оси X, а X1 шёл по оси Y, у меня выходят те же самые значения Z. В итоге, получается тот же самый contour plot (что неверно). Опытным путём я выяснил, что нужная мне матрица Z во 2 случае - транспонированная первая матрица Z.

И тут встает главный вопрос: стоит ли делать какие-то костыли и пытаться учесть этот факт, или есть более лёгкий и удобный способ все сделать?",
"Друзья, кто-то работал с задачей определения авторства? Какие там фичи будут самые важные? Реально ли методами unsupervised learning выделять авторов из массива текстов, не зная точного количества авторов, или тут только supervised learning поможет?",
<@U48MV3WKS> а тексты какого рода? Это статьи или что?,
"Я раньше думал, что Entity Linking - это обобщающее название для вещей типа нахождение дубликатов, определение одинаковых пользователей по кукам или людей в соц сетях. Но статья в википедии <https://en.wikipedia.org/wiki/Entity_linking> немного про другое. Какое имя правильнее использовать для дубликатов? Record Linkage?",
С какими оптимизаторами нужно юзать в keras `ReduceLROnPlateau` и `LearningRateScheduler` для улучшения перформанса? Расскажите про свои хаки,
"гайз, че-то берут сомнения.
вот я рисую два плота на одной фигуре: `fig, axes = plt.subplots(1, 2, sharey=True)`
как мне потом нарисовать еще один плот отдельно? какой-то `plt.close()` надо искать? 
или как лучше?",
какой метод из keras лучше использовать для уменьшения lr?,
"Когда нужно было обучить 100500 моделей, то я написал свой велосипед. Типа учил N эпох с запасов при фиксированом lr для SGD или фиксированном batch size для Adam. Потом выбирал лучшую эпоху, уменьшал LR или увеличивал батч сайз и стартовал обучение с нее.",
Вот как раз для сохранения лучших весов юзал каллбэк специальный.,
Когда учил lstm и cnn для текста. Сейчас не учу :mk_kapitan:,
а хочется точность такую же как на цветных?,
"&gt;а хочется точность такую же как на цветных?
Хочется гуглнет со сверточными слоями построенными на нормальном датасете, что потом свой датасет прокачивать.",
Да. Так как в моём датасете есть ошибки т.к. лейблы не всегда правильно проставлены,
"А нельзя взять pte-trained веса googlenet и поменять веса только первых сверток, чтобы как раз конвертнуть для greyscale?",
lr = 0.000983722 есть еще куда стремится :slightly_smiling_face:,
"А как лучше пихать спарс матрицы в сеть? keras, вроде как не умеет на вход принимать такие.",
"Keras, сегментация. Выход сети (C, N, M), где C - число классов.

Как прикрутить softmax?",
"Просто посмотри какой тензор прилетит в жаккард_коеф и поправь, чтобы по другим осям считалось всё, что там считается",
"Или я совсем не понимаю как это работает, или откуда сети это знать, если у тебя лосс считается усреднением всех классов, да ещё и по целому мини-батчу?",
"Для коротких сообщений всё сложно. LDA скорее всего не сработает. Мне кажется, тут нужна помощь экспертов, которые вручную этим умеют заниматься, потому что так просто признаки не выделишь. Чтобы специфический для человека словарь выделить нужно очень много его сообщений. Какой-то стиль в пунктуации, грамматике, построении предложений непонятно как в модель пихать.",
"Я думаю, биграммы и триграммы сработают. Очень интересная задача. Рассказывай, как будут результаты какие-нибудь.",
"я бы ещё на кафедру к лингвистам вломился:
1) Они должны знать о чём речь и как это делать эффективнее
2) Много клёвых студенток",
"да, что значит пользоваться? если ты собираешься на этих данных модель обучать то почему нет?",
"Спасибо - надо как раз вот такого плана, но только на английском",
"Где он как раз обосновывает, что комбинациями отображений можно расширить множество получаемых функций",
"Вопрос обывателя. Как к такой архитектуре можно прийти? принять что-то? Сходу не сказать что есть какая-то логика.
<https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAoQAAAAJDMyYzgyYzRhLTg1NjUtNDg1Yy1hMjY0LTJjZWFiMzJkOTk1NQ.png>
<https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur>",
"Можно прийти к такой архитектуре побившись головой об стол, когда нихера больше не работает :slightly_smiling_face:",
"почему же нет — разные эмбеддинги пар вопросов, потом всё в кучу и полносвязная сетка с BN, DO и PReLU",
"вот когда на inception смотришь, возникают вопросы о том, сколько машиночасов сожгли на перебор параметров сетки",
"Привет! Вопрос такой: у меня есть набор картинок, из которых мне надо для обучения вырезать отдельные квадраты размером, скажем, 32х32, и сохранять как отдельные картинки. Есть что-нибудь из простых утилит, или писать скрипт самому?",
А будет форум какой нибудь для сбора команды?,
Чуть менее тривиальная вещь -- поглядеть каких размеров получился замердженный вектор и сделать на нем большой дропаут,
"Нет, нельзя. свертки только там, где локальный порядок есть. С чего вдруг ему быть при мердже какой-то фигни?",
"сверток по фичам - это как? там же везде эмбеддинги, как сворачивать ",
"Можете подсказать как мне в керасе проинициализировать сеть так, чтобы веса были одинаковые для tf и theano. Я задаю `np.random.seed(0)` до импорта кераса и получаю одинаковые веса, если несколько раз запускаю теано бекенд, и одинаковые веса если несколько раз запускаю tensorflow бекенд, но вот веса теано и тензорфоу отличаются. Пока проверяю на обычной dense сети",
но как статистика оптимизатора влияет на предсказание?,
"у меня кстати в керасе были какие-то проблемы со статистиками оптимизатора как раз, криво подгружалась модель из дампа, приходилось выбивать вручную из дампа его поля",
"когда я читал документацию по керасу там было написано, что вся статистика оптимизатора сохраняется",
"А как в дополнение к части речи морфологические признаки (род, число, аспект и т.д) можно предсказывать? ",
"Вообщем какая разница, какие у нас метки, POS или полное морфологическое описание... главное, чтобы корпус размеченный был. syntagrus, например, размечен",
"есть кстати мысль что имеет смысл делать файнтюн другим оптимизатором, помню у <@U053R9RS6> что-то давал  файнтюнинг сгд поверх рмспропа, сгд как раз статистики не нужны :good-enough:",
Получается каждое сочетание тэгов как отдельный класс просто рассматривается?,
"блин, как картинку в тред прикрепить...",
"можно... но тут вопрос тогда в каком порядке выстроить признаки...и как порядок повлияет на результат... вообщем, мы остановились на варианте, который на картинке",
"Помимо описанного подхода, можно нагенерировать фичей. Например, длина пути, средняя скорость, число смен направления и т.д. И далее уже решать как классическую задачу классификации. ",
"Всем привет!
В эту субботу 4 марта будет зарешка, начнём обсуждать Data Science Bowl 2017.
Кроме того, можно присоединиться к группе ребят, решающих MLBootCamp.
Собираемся как обычно в ШАД, с 12 до 16-ти. Начнём в Принстон, потом переедем в Оксфорд.
Регистрация доступна до 14:00 пятницы 3 марта (чтобы успеть организовать проход), учтите это пожалуйста:
<https://events.yandex.ru/surveys/4527/>
Приходите!",
"На картинке немного странный вариант тем, что связи вроде “падеж предыдущего слова - такой же, как падеж текущего слова” через вероятности переходов “ловиться” не будут. Это можно в какой-то степени в фичи зашить, но немного не то будет.",
"реализовали так, как написано в статье, которую кидал выше",
"<@U13QJJR6F>, да, я даже не спорю - HMM, как правило, лучше сработает. Но опять же зависит от задачи. И попросили ЛЮБЫЕ подходы. :slightly_smiling_face:",
"<@U06J1LG1M> набросаешь оглавление, как самый стар^Wопытный?",
"вообще говоря, статья окончательно ставит точку в вопросе кто был первым",
"нужно просто плясать от того, каким образом данные получаются (частота сэмплирования, шум, etc), и какие инварианты есть",
dremovd: а твой вариант чем хуже? забыл уже как он назывался,
"привет! Вопрос наверное немного больше в тему edu_courses но он больше про NLP чем про ""ML"" вообще, поэтому поспрашаю тут. Есть мысль построить ""тулзу"" которая бы ""читала"" новости, анализировала их сантимент. Пока понимание того, как это должно быть реализовано весьма абстрактно, так как с обработкой текстов не сталкивался. Если кто может кинуть ссылками на ""почитать полезное"" на эту темуили даже поменторствовать в процессе был бы очень был бы очень признателен",
"пилю анализ реплик на форумах, прогонял леммы всех слов через CountVectorizer и потом скармливал их Байесу. Сейчас решил добавить к леммам слов частеречную разметку. Собрал ее, теперь вопрос - как обучить CountVectorizer отдельно на леммах, отдельно на частях речи, а потом все это скормить одному классификатору?",
"тональность и тему. Это для анализа фин новостей. Т.е. например есть новость что компания Х получила повестку в суд так как производимый корм для хомячков вызывает панический страх у потребителей. Соответственно нужно превратить это в короткую фразу типа ""X повестка в суд"", оценить что это повестка а не решение суда и на основе аналогичных событий предположить реакцию цены типа ""с вероятностью 90% цена упадет на 10%""",
"подскажите плз статьи по детекции end-to-end, где нет последним шагом никакого локального декодинга вроде супрессии. RL тоже подойдет.",
"не, я не про это. А про то, как теоретические наработки выше перечисленных ребят повлияли на DL, хоть это и не отражается в “Deep Learning” и обзорах подобных “On the origin of DL""",
"ololo: а как это скормить векторизатору? он будет эту конструкцию воспринимать как 1 слово или поймет, что это метка части речи?",
Какая то большая линейка конкурсов по Reinforcement learning: <https://www.general-ai-challenge.org/>,
"можно тогда через Feature Union, как ты написал выше, или через scipy.sparse.hstack",
"А можете подсказать какой нибудь грамотный вариант как можно визуализировать n-мерное пространство? У меня есть задача кластеризации и хотелось бы посмотреть как у меня расположены элементы, найти выбросы и т.д.",
"Подвох в том, что все сливки с титана сдоили, и как раз процесс отладили (может ещё чипов накопилось, которые на полный титан не  тянут)",
"Всем привет попрос по theano на cpu. Я сейчас запускаю разные архитектуры сетей на cpu в 2 вариантах:
1) ограничиваю количество потоков через `OMP_NUM_THREADS=1`
2) никак не ограничиваю количество потоков

Получаю странные результаты, если я ограничеваю количество потоков до 1, то результаты сильно лучше, чем если совсем не ограничиваю.
Когда запускаю htop или top с 1 потоком, то вижу утилизацию в 100% 1 потока. Когда запускаю без ограничений, то top показывает загрузку нескольких потоков, но суммарная загрузка процессора 25%.
А htop показывает, что запускается 8 потоков, но много красных полосок.
Я подозреваю, что как-то не так настроил теано, но не могу понять как правильно. В чем может быть дело?

Ниже на всякий случай прикрепляю графики (дополнительно прикрепляю графики для tensorflow и для него все ок).
Все делал в керасе меняя backend (dim ordering в явном виде задавал керасу перед построением сетей).
В основном архитектуры которые в rl можно использовать.
CPU - Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz - может работать в 8 потоков.
версии: Theano==0.9.0.dev5; tensorflow-gpu==0.12.1",
"<@U49L4JLTA> про сентименты тут в закрепленных есть комментарий Миши, где он говорит, что это ведь классификация: <https://opendatascience.slack.com/archives/nlp/p1486065127001733?thread_ts=1486064235.001730&amp;cid=C04N3UMSL>
Если смотреть так, то, например, для твоего случая можно намайнить не какой-нибудь твиттер, а свой датасет. Например, выкачав новости про какую-то компанию и соответствующее ей тикеры. Или можно новости скачать и индексы. И в итоге должна получиться классифиукация не какой-то там спорной тональности, а того насколько текст может изменить стоимость ценных бумаг. Хотя я тут же вспоминаю про гипотеза эффективного рынка и не понятно как тогда оценивать. Ну так интереснее.
В общем, гугли про классификацию. И, кстати, уверен, что есть немало статей прям по твоему кейсу.",
"Как переключиться на интеловскую встроенную видюху, чтоб монитор через нее работал, а Nvidia только для куды? В биосе все поменял, терминал и экран входа отображаются, но в систему не заходит. Это nvidia driver мешает? Если я его удалю, то cuda не будет ведь работать? Кто с таким сталкивался, как быть?",
"а че-то слышал, что тсне плохо, потому что находит то, чего нет. или лучше все раано пока не придумали? почему не просто svd порекомендовать?",
"<@U24QAAZBL>  ясно, спасибо, я так понимаю не нашел способа как справиться с этим?",
"Привет! Сейчас, кажется, уже не так сложно решать большинство задач gym. Возьмем к примеру блуждание по лабиринту. Здесь очевидны состояния, действия и награда. Знает ли кто-нибудь случаи, когда умный человек заменил состояния и действия на что-то более прикладное и применил на практике? Например состояния лабиринта заменил на состояния диалога между ботом и человеком, а действия на генерацию определенных ответов. Т.е. получился чат-бот. Ну или что-то поумнее...
Буду признателен если кто-то поделится похожими примерами. Сам гуглил, но почти ничего интересного не нашел. В основном всё про игры рассказывают :disappointed:",
"Кстати, я бы узнал у опытных парней как заказать только что анонсированную карточку, если ты в России. <@U1CF22N7J> ты как свою 1080 брал?",
"Ну, данных то вроде как раз достаточно.",
"Я знаю про задачку перемещения по википедии, когда тебе нужно из стартовой дойти до нужной статьи, только кликая по ссылкам.",
Ну а как же бандиты и самоходные повозки?,
"kmike: звучит прикольно, я правильно понимаю, что количество actions не фиксировано, так как после перехода ссылке могут прибавиться новые ссылки и соответствено количество возможных действий может увеличиться?",
Я как раз сейчас думаю на что обновить свой старенький титан блэк,
"Ага, именно в этом была идея. Хуже-лучше - от задачи сильно зависит, видимо; вцелом - часто не сильно хуже, но бывают случаи, когда второй и далее порядок помогают, а обычный классификатор ссылок работает не лучше BFS обхода.",
"Тебе вообще могу просто пожертвовать, как перспективному дип лернеру и за заслуги перед одс",
"На слайдах Хуана указано, что они разогнали до 2 GHz на ядро в рабочем режиме (66С) из-за каких то улучшений в питании. Неплохо так. ",
"Не я про то, что тема - небольшой набор слов сравнимый по размеру с твитом, т.е. твиты наверно можно полностью генерить, а вот посты - выглядит, как более сложная задача.",
"Я как то задавался вопросом, можно ли генерить SEO тексты. Выглядит непростой задачей, так как требования гугла меняются",
"Привет, 
Появился шанс поехать на стажировку в универ Германии. Надо быстро разобратся с `liquid state machines` и `echo state network`(`RNN`)

Я не давно начал разбиратся с ML, пока на уровне классических методов. План для себя накидал, как вьехать в эту тематику:

- выборочные лекции с cs231n
- курить deep learning book, чтобы прояснить узкие моменты
- покурить echo state network tutorial

Но пока запара с практикой.

Посоветуйте куда копать на Python какие фреймворки учить?(TF?) И есть ли набор практических задач, чтобы освоить эти типы сетей? Или пытаться прикрутить к любой задаче классификации?",
"Кстати, а вот сейчас же многие апгрейдиться будут, продавать старое. Где разумнее всего секонд-хенд карточки брать? И второй вопрос, если у меня, скажем, две разных 1070 EVGA и там ASUS они вместе нормально работать будут или тут как с оперативкой, нужно брать одинаковые?",
"а где ESN/LSM используются - они разрабатываются как инструмент моделирования чего-то более похожего на мозг, или как методы для решения практических задач?",
"В описании к стажировке, нужно будет моделировать молекулярные структуры как динамические системы. В часности пептиды и протеины.",
"У меня никогда не было двух карточек, но я бы не парился из-за этого. Сэкондхенд -- как обычно на ибее",
"Ну и там нет всякого вот этого ""защита покупателя"" и т.п., на кого попадёшь, так и пойдёт. И никаких тебе рейтингов и т.п., только на глаз",
И знаю кто еще одну продает,
"вот только там анализ сложный, и вообще ее заставить работать как надо проблематично, если данные сложнее игрушечных",
lotek93: скриншот с этой ссылки на самом деле. и там у меня как на скрине),
"Да, буду детально копать RNN. Я так понял хорошей идеей, будет все пытаться сделать на numpy/scipy, чтобы понять как они работают
Спасибо, за papers. 

Кстати, есть мысли по поводу `reservoir computing`?
Нашел сайт их <http://reservoir-computing.org> и как я понял, это такая deep RNN в которой мы обучаем только верхние слои?
Буду благодарен за любую инфу.",
"ну надо просто попробовать, просто думал вдруг у кого более up-to-date опыт есть",
"Ни у кого случаем нет ""Statistical Rethinking"" Richard McElreath или чего-то ещё годного по байесианской статистике?",
"а кто какими библиотеками пользуется для линейной регрессии? Я тут смотрю, что есть statsmodels, который поддерживает категориальные величины, в отличие от sklearn.LinearRegression. Кто-нибудь его использовал?",
"ага, спасибо! Для проверки значимости факторов как раз нужны статистики",
Там еще формулы как в R можно использовать,
"&gt; судя по тому что сеть вообще не учится, я голосую за баги.
если бы баги в препроцессинге были, то почему моя кастомная учится. Если баг в структуре - то тоже не совсем понятно, т.к. код предельно прост.

Размер свертки - попробую поподбирать, спасибо. Дейтсвительно явным образом он там не указан, а (4 * 4) - это макс.пулинг",
А таргеты в каком формате подаются сети?,
"клевая статья с интерактивными картинками про то, как (не надо) интерпретировать тсне <http://distill.pub/2016/misread-tsne/>",
"День добрый. Я только начинаю реализовывать на питоне нейросети (направление - рекуррентные, автоэнкодеры). Мне нужно определиться на чем работать. Из понятного мне: (1) keras, (2)lasagne, (3)Theano и (4)TensorFlow. Подскажите, исходя из практики, какую связку фронт-бэкэнд лучше всего использовать? Почитал, что пишут, но однозначный вывод сделать не смог.
Спасибо",
"но все же не 90круб, как за титан х",
"Чот дорого на нвидии.ру, 30% сверху",
"Как я понял, керас дает выбор бэкэнда, лзанья, как и говорили - только теано. На простых задачах я пробовал - переключается легко и это прельщает, один код. А вот что на сложных - вопрос. С другой стороны для теано есть предобученные модели (zoo).  Опять же, какой бэкенд быстрее  на GPU...",
"Как думаете, через сколько они станут доступны для простых смертных вне США?",
"И, кстати, кто как их везет через границу?",
"Вот еще одна прикладная статья о том как использовать RL на практике, от  David Silver. Насколько я понимаю это еще до того как он в Deepmind стал работать.

Эксперименты они проводили на симуляции email-marketing, но судя по составу соавторов, все это делалось для работы с клиентами. <http://www.jmlr.org/proceedings/papers/v28/silver13.pdf>",
"нормальная книжка вполне, для начинающих как раз. 350 страниц, в основном про ARIMA-модели",
"Насчет TH vs TF  <https://github.com/wjaskowski/tensorflow-vs-theano-benchmark> 
TLDR:   TF (cpu) быстрее TH(cpu)  в ~ 2-4  раза.  А TF(gpu)  медленнее  TH(gpu) в ~ 1.5-2.5 раза.  
Интересено почему TF(gpu) так много медленнее TH(gpu)  ?",
"ipaulo: wtf, они в theano backprop меряют как forward + backward",
"Как жаль, что чейнером никто не пользуется почти",
и как то это крайне подозрительно,
"Какие инструменты ETL есть, которыми именно в контексте DS пользуются?
Я знаю энтерпрайзные аналоги (Таленд, Пентахо), но интересно, есть ли в DS что-то специфичное.",
"а что кто то вообще пользуется платными инструментами типа пентахи? (ну я понимаю есть деды привыкшие платить SAS и потому они юзают весь спектр платных продуктов, но мы то про нормальных говорим)",
Кмк чаще все руками пишется на спарке или каком нибудь luigi,
"если кому интересно, то вот тут  <https://github.com/fgvbrt/dl_rl_betchmarks> я правда в results.md вынес только gpu и cpu с однимо потоком, так как это типа для целей reinforcement learning больше подходит. Но в файлике results.txt лежит и для cpu на всех возможных потоках.",
"Всем привет! У меня есть задача кластеризовать услуги по поведению абонентов которые их подключают. У абонента может быть несколько услуг. Поведение абонента можно описать тысячью фитч: траффик, поведение, данные абонента и тд. Услуг порядка 200-300.  Понятно что визуально оценить сложно. Есть ли какие то стандартные подходы в данном направление. Может быть PCA и кластер. Или еще какие то подходы есть.",
"Чуваки, читаю статью <http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf> . Автор проводил кучу эксперементов с lstm и gru. Интересный момент с неиспользованием каких-то гейтов или созданием новой связи между ними. Если правильно расшарил документацию keras, то на нем нет никакой гибкости. В какой фреймворк нужно сунуться, что бы такое провернуть?",
"vradchenko: почему в keras нет гибкости?
можно ж собственные слои писать :kekeke:",
"это похоже на кастомный слой, вот тут есть пример, как строят сеть с помощью кераса, потом из этой сети достают входной и выходной тензоры,  и пишут свой кастомный loss для rl агента <https://github.com/coreylynch/async-rl>",
Там где sas стек неплох Enterprise Guide.,
"Ребята, хочу ноутбук обновить на более-менее подходящий для DS, кто-то озадачивался таким вопросом в недавнем прошлом? Может место где взять кто посоветует.  Не мак.",
"Здравствуйте, сейчас решаю задачу рекомендаций с помощью nmf -  у меня сейчас есть готовое разложение пользователи-товары, и вопрос таков - как в уже имеющееся разложение ввести нового пользователя, без пересчета всего разложения?",
"Ну по крайней мере гипотезы какие то тестировать, не подключая AWS и прочее",
"<@U0FEJNBGQ> Мы рассматриваем Flume как механизм заливки из Hadoop в Kafka, какие тогда альтернативы или может другие best practice для этого?",
"друзья, а кто на семинар в Вышку пойдет сегодня?",
а кто на data science meetup?,
Как на сходимость GloVe влияет выбор метода оптимизации? Авторы использовали AdaGrad. Я использую Adagrad. Кто-нибудь пробовал RMSProp/Adam/Adamax?,
"Накодил небольшую библиотеку на theano, забавы ради, можно сказать. Если я запускаю код в jupyter notebook единым блоком, всё работает отлично. Если запускаю тот же код кусок за куском, кернел падает без какого-либо трейсбэка. Если запускать через командную строку файл ноутбука, переконверченный в .py, всё работает нормально, но когда-то умирало с ошибкой floating point exception: 8, тоже без трейсбэка. Как эту жуть дебажить?:sweat_smile: до этого библиотека была на tf и работала исправно, в коде никаких очевидных ошибок нет",
npetrenko97: у меня как то теана падала и валила за собой ядро из за версии scipy,
кто на Data Science Weekend? :nyancat:,
не могу что-то загуглить. не подскажешь как это реализовать?,
"Вместо второй части про json можно просто использовать код создания модели, такой же как при обучении",
"хм. то есть когда я сделаю лоад, он загрузит в оперативку и по дефолту будет использовать CPU при операциях с моделью?
я до этого использовал save_model / load_model и он загружал сразу на видео-карту модель",
"А, думал вопрос в том, как загрузить, а не в том, как выбрать цпу или гпу :slightly_smiling_face:",
"гайз, а посоветуйте видео, где no free lunch объясняется подробно, но чтоб сначала и на пальцах)))",
"Для тех кто сегодня в Мэйл, вдруг релевантно
<https://www.instagram.com/p/BRKxeHGgMmH/>",
"anna.mashera:  Правильно понимаю, что можно удаленно участвовать в конкурсной программе и выиграть приз? Есть ли канал где обсуждают хакатон, где объединяются в команды для удаленного участия или поездки в СПБ?",
"Может кто-то писал скрипт, который обрабатывает логи кераса и переводит в табличный вид? или скажите как залезть внутрь, что бы вытащить несколько чисел",
"может на таких объявления ставить :heavy_plus_sign:  тем, кто пойдет?",
"Я вчера как раз скачал, но качество ужасное",
Кажется я знаю какую я книжку закажу следующую с амазона :slightly_smiling_face:,
"я имел ввиду, что можно делать вот так: как только появляется какая-то проблема, идти в гугл и искать решение",
"Я тут ковыряю задачку автодополнения при наборе текста до слова, но вот бенчмарка пока не нашел. На какие цифры надо ориентироваться и как это принято мерять?",
"Алсо <http://www.labri.fr/perso/nrougier/teaching/matplotlib/matplotlib.html>

(я хз зачем по матплотлибу _целая книга_)",
просто когда начинается набор букв нового слова уменьшается число кандидатов на следующее слово,
а в ем-алгоритме на практике как часто гауссиана используется?  :thinking_face:,
"&gt; На E-шаге (expectation) вычисляется ожидаемое значение функции правдоподобия, при этом скрытые переменные рассматриваются как наблюдаемые. 
Я правильно понимаю, что вот эти скрытые переменные -- это параметры распределения? Среднее и дисперсия собственно в случае нормального? ",
"Коллеги, приветствую!
Решил начать знакомство с нейросетями с конкретной задачи: определения того, есть на фоторграфии номерной знак автомобиля.
Нашел статью на хабре про keras <https://habrahabr.ru/post/321834/> где рассказыватся, как сделать transfer learning.
Собрал выборки, дообучил модель, как показано в статье. Теперь я беру, загружаю модели из snapshot'ов, которые были созданы во время обучения и вызываю метод predict на двух классах обучающей выборки. И там, что в первом классе, что во втором подавляющее число предсказаний -- нулевые. При этом точность при обучении была 0.7. Классы сбаланстированы: половина выборки изх первого, половина из второго классов.
Обучается сетка вот таким кодом: <https://github.com/MaxTitkov/Keras_InceptionV3_Binary_classification/blob/master/complete_model.py>

Вопрос: если взглянуть на репозиторий из статьи: есть ли там особенности того, как следует загружать обученную модель из snapshot`ов? Ведь автор там над keras inception дообуча два полносвяных слоя прежде чем подать на логистическую регрессию.",
"<@U0DA4J82H> я вставлю своих 5 копеек. Будь аккуратнее с прогнозированиями по панельным моделям. Они в основном сделаны для того, чтобы экономисты могли статьи писать. А так градиентные бустинги деревьев будут давать лучше результат с хорошими фичами. Особенно осторожно нужно, если в тесте есть новые магазины, которых не было в трейне. А то линейная модель с дамми по магазину съест много информации на эту дамми:) А вот SARIMA, GARCH, ARCH посмотреть стоит. Причём можно ими что-то предсказывать по каким-то кускам и пихать это как фичу в xgboost/lightgbm, ровно как и разбивать на кластеры каким-нибудь CART/C5.0 и внутри классов делать эти модельки. Не переобучись, удачи:)",
"Так эта. До того как сохранил в snapshot, результаты нормальные?",
"Привет товарищи, можете скинуть датасет первой задачи тинькова, у кого остался? ",
"подскажите по выбору признаков в xgbfir
как интерпретировать когда комбинации признаков состоят из повторяющихся признаков?
то есть в наличии признаки A,B,C
а в Interactions выдаётся A|A|B или вовсе A|A|A",
А какие элементы в множествах? ,
"понадобилось немного с картинками поработать, как в ImageDataGenerator в керасе пихнуть препроцессинг с вычитанием константы из каждого канала?",
пока кажется что немного никак и надо свой генератор писать (ужас то какой :scream: ),
"perplexity может и по буквам считаться, почему нет?",
"сорри, выглядит как будто от твоего имени))",
"anyone? репощу из <#C1UEK73H6|mltrainings_beginners>, не уверен где спросить

а в tf в rmsprop если указать моментум значение, оно будет делать нестеров моментум или что вообще происходить будет?
<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/rmsprop.py#L25>
mean_square = decay * mean_square{t-1} + (1-decay) * gradient ** 2
mom = momentum * mom{t-1} + learning_rate * g_t / sqrt(mean_square + epsilon)
delta = - mom
и что такое g_t?

Правильно ли я понимаю моментум нестерова
сперва прыгаем на mom потом считаем градиент в новой точке и уже его добавляем к моментуму, и если подключать рмспроп то нужно делить этот добавляемый градиент на sqrt(m_s)?
а то я как то раньше не задумывался и предполагал что если я при создании рмспроп оптимайзера напишу momentum=0.9 у меня магическим образом будет моментум нестерова одновременно с рмспроп",
"и еще такой вопрос.
в LSTM оригинальной статье как я понял(читал два года назад) нет ограничения по количеству айтемов в последовательности и там используется truncated grdaient а в tf есть ограничение в айтемах
это так? почему если так?",
Судя по истории правок некто Vokov. Кто бы это мог быть?,
cortwave: для линейных моделей с константой актуально. Так как все дамми дадут линейную зависимость в регрессорах,
" Никто не подскажет, как можно посчитать доверительный интервал для корреляции? ",
"Люди, не подскажите кроме aws, какие еще серваки есть, идеальный вариант:
- сервер с gpu
- linux OS
- доступ через ssh
- c установленным cuda
- оплата часовая (обучим сеть, нам этот сервак больше не нужен)",
а с перевод каких терминов у тебя были сложности?,
А знает кто-нибудь как на hetzer смотреть  gpu на аукционе ?,
"к слову о переводе, как лучше всего перевести odds?",
"<@U06TZHSSJ> Ну как же, для: ""делать не только  предсказания (классификация / регрессия), но и  probabilistic reasoning about uncertainty""",
"<https://opendatascience.slack.com/archives/nlp/p1488573936003704>
Привет, я такую фичу как автодополнение не делал, но я бы расммотрел сперва саму модель, а она очевидно последовательная, то есть Hidden Markov Model. Исходя из этого, тут можно голимую HMM натренировать, используя Viterbi search algorithm. Или пойти в сторону нейронок типа RNN сетей.  Насчет датасетов, думаю, есть 2 варианта

1) Попроще. Я бы нашел бы какой-нить готовый на твой язык N-граммный (лучше наверное 3-грамм) словарь, и скормил модели. Скорее всего фича автодоплнение используется в поиске чего либо, поэтому много граммов ставить нет смысла.
2) Поинтереснее. С первым вариантом есть очевидная вероятность, что ты оверфитнешься, если твой контекст, который ищешь не особо подходит под  готовый скачанный словарь.  Тогда, тут скорее всего можно поразному (может быть даже тренировать ничего не надо, а просто тупо использовать search алгоритмы в тексте. На ум приходят BFGS, KMP)
- я бы достал весь твой текст, разделил на train-test или использовал бы K-fold CV  (если на Питоне, то это можно через sklearn). 
- Потом через NLTK (Python) пометил их POS тэгами, отфильтровал stopwords
- разделил на N-gram, обратно отфильтровал через POS комбинации и с этими фичами построил HMM модель.",
"входящие данные я могу как угодно обработать, а как обратать выходы в условиях моей задачи не знаю",
"Мне тоже так показалось, но я на всякий случай решил уточнить у тех, кто понимает.",
хз как на 4 шагах себя лстм может вести,
"generall: я правильно понял, что ты заюзал эластик на том же датасете викилингкс, чтобы для каждой именнованной сущности намайнить еще где они там встречались?",
"у кого нибудь тут есть больше одного монитора/ультра вайд? стоит ли увеличивать количество мониторов если есть только 24"" 1080п?  если да, то что стоит прикупить к моему 24?",
"у меня 24"" 1920х1200 как основной и слева 19"" 1280х1024. Вполне себе нормалек",
"На втором монике можно фильмец крутить параллельно, можно документацию какую открыть. Тупо расширение одновременно обозреваемого пространства. На ютубе можно найти видосики, как народ стыкует 6 моников в два ряда, повернув в портретное положение (практически это бесполезно).",
"1) битых пискелей бояться стоит, хотя некоторым людям на них накласть
2) без понятия, я в магазе всё проверял, и пёр сам до дома.
3) как уже сказали, сетки быстрей не учатся, а программерские привычки разные бывают, кому-то и трех моников мало будет",
"<@U07V1URT9> :dolan: я снова в смятении, что делать. второй монитор хочется, но уже месяц не могу выбрать какой именно
&gt;Одинаковые моники -- глаза меньше устают от переключения.
нет этой проблемы?",
maxim.milakov: на скрине код сети на куде без какого либо фреймворка со своей имплементацией bp и графа вычислений?,
"Какая есть книжка или ресурс, которые SQL best practices + примеры того, как делать не надо?",
"<@U07V1URT9> 
&gt; VA
а как же IPS все дела? плюс он ток 1080п..",
"Я когда только начинал, 2011 год, не было фреймворков удобных. Ну а теперь интересно его поддерживать, понимаешь, как все работает",
"<@U1SJ2U4V9> Можешь обриcовать как GAN вели себя в разрезе спутников, то есть для сегментации спутниковых карт по сравнению с сегментацией в лоб используя Unet?

P.S. Меня на той неделе в Decartes Labs на интервью спрашивали как и что я могу рассказать за GAN и как они могут это использовать. Ничего на этот вопрос ответить не смог :disappointed:",
У меня там ссылка и инвайт на Кружочки. Если придумаем как совместить - я готов!,
"Тут желательно все таки уточнить, какой именно диалект SQL нужен. Потому что помимо стандарта, у каждой базы есть свои нюансы. Например, limit в стандарт не входит и каждый извращается как может. Или MySQL не поддерживает оконных функций и cte. И так далее.",
"Я не знаю... В ADAM вроде как не надо уменьшать по ходу обучения learning rate. Но меньшей ошибки на тестовом множестве в ImageNet это не дает, я проверял.",
"Может, мы избалованы собственными лонгридами, но статья короткой показалась. И многовато англицизмов влезает туда, где можно русские фразы задействовать. Даешь импортозамещение!",
Какие англицизмы больше всего режут глаз? Может поправлю,
Какие есть практики работы с пропущенными значениями в нейросетках? У меня они ща как 999999 закодированы и это наверно ппц),
"Думал попробовать нечто вроде замены их на свободные переменные и зафитить их. Как лучшее решение для nan в хгбуст, только лучший дефолтный инпут в сетке ",
"На cpu обучал? У меня теано на  cpu быстрее работал если я ограничивал количество потоков. Но там в htop  было видно, что загрузка не полная была, когда считал на всех ядрах ",
"Кто к курсе, можете прояснить ситуацию с Керас? Как там у них дела обстоят?
То, что удалось выцепить: <https://github.com/fchollet/keras/issues/5299> обозначил, что Keras теперь - это не либа, а набор спецификаций API. Этот набор будет имплементироваться в Keras-2 (новая инкарнация либы) и отдельно в tf.contrib.keras (-&gt;tf.keras). keras-contrib, вроде как, будет содержать какие-то куски типа лосс функций, метрик, слоёв,  и т.д. которые недостаточно хороши для Keras-2 (которая, как бы, будет LTS), но и не полный мусор (потому что какие-то типы уже назвались мэинтейнерами и довольно критично всё ревьюят).
Далее, вроде бы совместимость с TF 1.0 есть (<https://github.com/fchollet/keras/pull/5317>, <https://github.com/fchollet/keras/issues/5623> и т.д.), но модели в <https://github.com/fchollet/keras/tree/keras-2/keras/applications> используют старые ссылки на веса. Кто-н их тестил, они вообще работают теперь?
В продолжение предыдущего вопроса, pre-trained модели теперь можно где-н найти рабочие? <https://github.com/tensorflow/models>, судя по issues&amp;PRs сильно отстают от движухи, и с TF 1.0 работают всего несколько штук. С <https://github.com/fchollet/deep-learning-models> ситуация вроде бы как с <https://github.com/fchollet/keras/tree/keras-2/keras/applications>.
Последний вопрос: какой бранч Кераса теперь хостится в PyPi? Keras-1? Keras-2?",
"&gt; Keras-2 (которая, как бы, будет LTS)
:yeah-sure:",
<@U1UNFRQ1K> а это вы какими лонгридами избалованы? дайте почитать!,
"<@U0AD1L5NC> сейчас неохота уже все перечитывать, но как примеры: ”чтобы человеку не нужно было придумывать loss function”, “если использовать как loss просто… “,  “вход дается input image”, “В пейпере люди” (жаргон)",
"<http://hackathon.mts.ru/images/picTeam/t3.png> вот эта вот картинка, не понимаю, как она к dream team относится",
"Я вот пишу раз в год, вот где проблема :(",
"Это какие то левые люди из hr мтс мутят хакатон, мы им почти не помогаем",
"Это выглядит пока как нездоровое бурление. Думаю, стоит еще немного подождать. Сам пока юзаю Keras 1.2.2 с Theano бэкэндом, потому что быстрее.",
Правда данные им какие то анонимизированные вроде выгрузили,
"о,anvar,  спасибо. я посмотрел на результаты и как раз начал подозревать что-то такое :slightly_smiling_face:",
"в докере не пробовал, у меня тоже как бы все потоки загружены, но htop показывает много красных полосок (жалко нельзя принскрин прикрепить)",
Как раз кэггл временно отпустил и добби свободен,
"<@U041P485A> Все будет, всем рады как обычно.",
Ребят у меня такой вопрос. Может кто подскажет как лучше сделать. Есть матрица 15 столбцов 10к+ строк. Есть уверенность что часть из строк коррелируют между собой или же вообще отличаются на какой-то коэффициент. Как лучше всего искать такоие группы строк?,
да положим могут быть 3 строки. Вторая отличается на коэффициент 2 а третья на -5 (пример из головы. не знаю какие реально могут быть коэффициенты),
и так для каждой строки получается? пока не понял как это поможет. но явно где-то близко,
нет. строки это клиенты. столбцы это скажем результат их действий. Мне надо найти группы тех у кого действия коррелированы на столько что выполнялись одинаково с разными коэффициентами. Сложно объяснить но точнее к сожалению не могу (,
"Привет! Вопрос: есть два excel-файла. В одном в произвольной последовательности идут в первой колонке имена, во второй - id. В другом файле только имена в произвольной последовательности. Как во второй колонке заполнить уже имеющиеся id? Возможно ли сделать так, что одна колонка - ключи, а вторая - значения данных ключей словаря в pandas?",
"а если у меня есть сеть с такими слоями
[6,3,2] где 6 - input, 2-output
3 и 2 сигмойдные слои.
Я решаю добавить софтмакс.
Как правильно это сделать?
заменить 2 на софтмакс - убрать сигмойды и фидить xW+b прямо в софтмакс юниты или же
добавить еще один слой 2 с софтмаксом с соединениями 1к1
то есть
[6(inp),3(sig),2(sig),2(softmax)]
где все слои fully connected кроме последнего, там 1to1",
"1. у тебя своя нотация. Она непонятная.
2. почему сигмоиды, а не tanh (или что-то более современное, типа relu)
3. софтмакс на два выхода -- это бинарная классификация?",
"прихожане :church:, на путь праведный встать помогите; есть в общем k бандитов, одноруких, нужно как водится найти самый кошерный, правильно ли я понимаю то, как все это работает и поправьте плз там где я не догоняю:

пусть p = [p_1, p_2, …, p_k] это тру распределение, его мы не знаем и мы ищем q которое поближе к этому

модель такая:
q ~ Dir(a)
x ~ Mult(q)

1 - так вот, есть трафик, нужно его направлять на автоматы, для этого мы для каждого визита семплируем из DirMult(a) (то что получается после выинтегрирования вектора q) индекс автомата, и направляем юзера туда, он дергает, записываем результат

2 - делаем эту процедуру какое то количество раз (размер батча), и получаем вектор исходов y (вектор интов)

3 - обновляем параметры приора как a_new = a + y

4 - начинаем новый батч или останавливаем

затем ищется MAP оценка DirMult(a) и говорим что тест показал что этот автомат кошерный",
не знал как еще написать.),
"<@U04ELQZAU> спасибо.
а еще тако вопрос. когда есть два или более класса но они skewed - одного класса 0,1*N другого 0,9*N или 0,01*N и 0.99*N
что делать в таких случаях?",
"а то ты такой решаешь заняться мл, делаешь мнист где все цифры в равномерных количествах, а потом бац и в реальнйо задаче у тебя 1000 примеров больных раком и 99000 здоровых",
а когда совсем уже плохо и 1 на 1000 тогда это уже детектирование аномалий?,
"упд: ошибку я даже виже где, но хз как по другому - в пункте 2 есть вектор исходов, но для Mult сумма веторов исходов вроде как должна быть равна количеству испытаний, но не понятно как обрабатывать ситуацию когда ручку дернул, а выйгрыша нет (автомат бинарный, например показ банера и был клик или нет)",
а если бы руки были зависимыми то тогда бы мультиномиал как раз был бы или как?,
"Подскажите кто знает, хотя я понимаю что вопрос может быть странным) Нужен пример *красивого* кода при реализации какого-либо ML. Интересуют именно какие-то практики, как код должен быть оформлен перед выпуском модели в продакшн. (например реализация через классы, или через функции или просто plain код в айпайтон ноутбуке)",
"Могу посоветовать <https://arxiv.org/pdf/1510.00757.pdf>, отсюда дальше уже проще будет выбирать что-то конкретное исходя из твоих требований. 

Мультиномиальное распределение соответствует независимым испытаниям с количеством возможных исходов больше, чем два. То есть грубо говоря reward=1 наблюдается только у одной руки, а у всех остальных = 0. И такая схема повторяется много раз.  Зависимость тут  как раз в том, что 1 может прийти только в одну из рук (если ты всегда дергаешь все). Не могу припомнить где-либо упоминания использования мультиномиального в бандитах. Если вдруг кто-нибудь знает, поделитесь, пожалуйста. Если уж хочется эксклюзивности reward по рукам, то лучше, наверное, категориальное распределение брать. 

В целом ограничений на вид распределений наверное быть не должно. Просто при сложные распределения будут приводить к approximate bayes inference  при подсчете апостериорного (MCMC, variational methods)",
юнит тесты это какой то бич,
"Господа, а кто может помочь разобраться в STL? <http://www.wessa.net/download/stl.pdf>

Мне в принципе все ясно кроме одного момента - откуда берутся изначальные сезональные и тренд компоненты ряда для первой итерации в inner loop (пункт 2.3)?",
"гайз, у меня опять эзотерический вопрос :filosoraptor: 
можно ли сказать, что статистика началась, когда посчитали не только среднее, но и среднее по тому как каждое значение отличается от среднего, т.е. дисперсию/стандартное отклонение?
или может дадите ссылку на соответствующую ретроспективу ?",
"i: Википедия: ""Статистика — измеримая числовая функция от выборки, не зависящая от неизвестных параметров распределения элементов выборки."" Так что среднее - это уже статистика. Если говорить о науке, то четкого определения нет, как хочешь так и говори (началась статистика или нет при подсчете среднего) :smiley:",
"статистика как наука, я имею в виду. понятно, что среднее и дисперсия -- тож статистика. 

я имею в виду началась в историческом смысле :slightly_smiling_face:
когда стали считать не только среднее, а типа стандартное отклонение,то там быстро оформилась типа новая веточка математики.",
"вот это стремный момент, когда реализуют через классы ради классов",
"Рискну предположить, что статистика началась с Фишера, по крайней мере в том виде, как мы её знаем.",
У Хинтона есть отличная статья про то как строить и тренить RBM. В 17 разделе про пропущенные значения как раз. Ссылка на статью: <http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf>,
"Привет всем. Пишу QA систему. Вопросы и ответы превращаю в вектора и сравниваю через скалярное произведение. Столкнулся с такой проблемой -- на тестовой выборке, когда правильный ответ нужно выбрать среди 10 случайных, recall@5 = 97%. Но когда правильный ответ нужно выбрать среди 100 и более неверных, качество зверски падает(ну и понятно почему). Сталкивался ли кто-нибудь с подобными проблемами и как можно это победить?",
"Привет. А есть тут кто из аспирантуры сколтеха (ИВТ), кто CV занимается?",
"<@U2GTUS0CB>  UCB это не более, чем эвристика, которая достаточно устойчива к  разным распределениям reward, но эвристика неоптимальная в асимптотике. 
Thompson sampling асимптотически оптимален, поэтому если удается подобрать разумные и хорошо описывающие распределения наград (в бинарном случае клик/неклик это просто), и есть возможность подержать бандита на данных подольше, то TS is the best one can do. 
Если вдруг у кого будет желание разобраться, рекомендую 
<https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/thompson.pdf> – just empirical evaluations from microsoft research
<http://jmlr.org/proceedings/papers/v37/komiyama15.pdf> – theoretical investigation 
<http://drum.lib.umd.edu/bitstream/handle/1903/12540/Gupta_umd_0117E_12914.pdf;jsessionid=A94F2A1588E3BF21C406B23D9C44299C?sequence=1>  – thorough dive in 
<http://engineering.richrelevance.com/recommendations-thompson-sampling/>  – practice view",
"Есть выборка заголовков научных статей от разных универов, не подскажете как можно оттуда часто встречающиеся конструкции достать?",
"""вели всякую статистику."" это в другом смысле статистика, статистика как учет (само слово образовано от слова ""state"" и в таком смысле вообще еще в Египте было).",
"лично у меня древний seiki, брал для интереса, когда они только появились занедорого",
ну если коротенький официальный мануал по pandas занимает 1800 страниц - почему тут должно быть меньше?,
А зачем затаскивать в диапазон и затем пропускать через сигмоиду?,
"Ну просто если моделька считается для конкретного месяца, другая для другого месяца и т.д. это имеет смысл. А так - нет. Сигмоида может применяться как некоторая нормализация",
"i: функционал — это термин из функционального анализа. Это отображение из какого-то пространства в число. В то время как функция — это отображение из числа в число. По-английски так и будет: functional. Например, матожидание случайной величины — функционал, так как перевод пространство случайных величин в число.",
"Ребят, поясните, пожалуйста, насколько machine learner'у нужна статистика уровня выше базового. Я видел в чате ссылки на курсы и учебники статистики, но не уверен, будет ли от них практическая польза. Понятное дело, что без понятия о среднем, дисперсии, статистической модели и оценке правдоподобия далеко не уедешь, но реально ли пригождаются хотя бы критерии симметричности распределения, критерии однородности или что покруче? Может, этот вопрос уже где обсуждался? (интернет рассказывает больше про сходства и различия классической статистики и ML)",
"С проверкой гипотез могут быть хитрые случаи. Например, когда одна гипотеза объясняет лучше другой только какой-то строго очерченный набор данных - хорошо бы уметь этот набор данных найти.",
"""Интерливинг"", если я правильно помню, это способ A/B-тестирования ранжировщиков, когда мы ""переплетаем"" их результаты, затем показываем общий список пользователю, и потом смотрим, что он выбрал (а не показываем два списка по отдельности)",
"Да, идея интерливинга — не разбивать выборку на две (и ломать голову над соблюдением всех необходимых условий репрезентативности), а использовать одну и ту же выборку (людей), но показывать ей смесь ответов двух алгоритмов (актуально для рекомендаций/ранжирования/рекламы), запоминая, что выбирал пользователь, куда кликал. Погуглить тут стоит team draft interleaving",
"У меня самого создаются приоритеты, такие, что нужно развиваться как в сторону хорошей мат.статистики и тервера, так и алгоритмов и программирования",
"Если отрезать знак вопроса из обычного корпуса, то предложения всё равно будут очевидно вопросительными, типа ""Какой сегодня год"". Смысл датасета в том, чтобы классификатор смогу найти как раз неявные признаки.",
"всем привет, у меня базовый вопрос по тому как строятся чатботы на базе рекуррентных сетей. На выходе сети должно быть нечто, что можно интерпретировать как индекс конкретного слова, обычно предлагают ввести n выходных нейронов для n слов словаря (где 1, такое и слово). Но это же очень много связей на выходе!! Как с этим жить? может быть ""уплотнить"" выходной вектор до log n нейронов хорошая идея?",
"<@U1UNFRQ1K> по мотивам хабростатьи про аренду машинки на AWS для Jupyter: когда аукцион отнимает у тебя спотовую машинку, окружение, которое ты на ней настроил, также пропадает безвозвратно?",
"Кажется, такие случаи ничьей считаются, то есть да, на метрику влияют толькоте позиции, где ответы отличаются",
<@U49LRC1C6> почему на выходе должен быть индекс слова? Чатбот одним словом что ли отвечает только? И где такое советуют?,
"Проверить на гетероскедастичность результат линейной регрессии явно не мешает, как и на другие предпосылки МНК, если он используется. А так, думаю что многие вещи можно не применять, а просто менять спецификацию модели.",
"Или использовать ansible. Как пример (кривой, но все же) мой playbook, который покупает spot и ставит туда всякие полезности (zsh, tmux, miniconda, ...)

<https://github.com/kernelmode/ansible-ml>",
"А кто-нибудь здесь разрабатывает модели для предсказания спортивных событий? Очень хочу узнать, где можно достать базы данных для этого, хочу попробовать покрутить модельки и ""сломать казино""))",
"Как определять размер групп для A/B тестов если распределение success достаточно мутное, да и sucess - это не 0/1, а неотрицательные числа?

Хочется ответить на вопрос: каков необходим размер групп, чтобы проверить что относительная средняя разность улучшение B vs A - это 10% +- 2% где CI - 95%

Ну или опровергнуть эту гипотезу.

Когда мне нужно дать ответ на вопрос - вот у нас две экспериментальные группы, какова mean(B - A), то я бутстраплю, но тут так нельзя.",
"Это как? Я понимаю что такое MCMC, но я не понимаю, как оно сюда прикручивается",
"ternaus: ночью уже тяжело думается, но кажется, что либо надо модель распределения написать, либо все таки бутстрап. Не очень понятно, почему нельзя? Просто размер выборки по сетке перебираешь пока заданной точности не получишь. 

Если что, то завтра можно попробовать додумать мысль. ",
кто-нибудь знает какие сейчас существуют интересные SOTA (но необязательно) методы для content labeling? То есть присваивать человечески читаемые категории текстовым документам.,
"Гайс, а есть у кого в продакшене H2O-стек? Как оно? В чём вообще преимушество(если есть) H2O в ML?",
"Я пробовал H2O использовать как либу в джава проекте, повозился пару дней и забросил это дело. Как-то не очень дружелюбно для пользователя там всё. У них основной упор на использование этого всего добра из R и питона, а не из джавы, хотя сама либа на джаве написана",
Чем он лучше предыдущих? Быстрее тренируется? Достигает лучшего результата? Как это в цифрах показать?,
"Никто не видел статей про байесовскую оптимизацию, когда evaluation больше не осталось? То есть нужно уже выдать argmax функции и больше вычислений делать нелзья?",
то как потом по вектору найти индекс слова == его написание,
"Генери по символам, их ~100 штук всего :wink:
Искать ближайшее по вектору -- нормально, но это по меньшей мере так же сложно, как нормальный софтмакс (с которым нужно возиться, если у тебя скажем 1М слов в словаре)",
"Делаю бинарную классификацию lstm’кой. Входящие данные могут быть разной длинны, поэтому я либо обрезаю текст, либо пихаю 0. Одно из решений – забить на весь текст и обрезание текста в конце. Там находится самая важная инфа. Люди очень часто в конце делают вывод. Если текст меньше указанной максимальной длинны, то я юзал `masking` или забивал и пихал 0 эмбеддинги. Кто как пробовал решать такую проблему и что сработало лучше всего в вашем случае?",
"Раз уж зашёл разговор про классификацию текстов и lstm, подаётся на вход сразу вектор с текстом или по очереди каждое слово? Просто в примерах, которые встречал подаётся весь текст (ну один отзыв например), и тогда непонятно какой профит от того, что модель может запоминать предыдущий контекст",
"Очередная встреча Data Science кейс-клуба <#C45CUFESK|true_story> будет посвящена компании *Avito*, пройдет в офисе компании уже в следующий вторник.

Специалисты из BI департамента Avito Андрей Рыбинцев <@U21G3ACDN>, Василий Лексин <@U1GG0N16H>  и Андрей Остапец <@U1A5S3J95> расскажут о том, какие бизнес-задачи решает машинное обучение в крупнейшем сервисе бесплатных объявлений в Европе. В частности, поговорим о модерации объявлений, рекомендательном сервисе и монетизации.

_Начало_: 14 марта в 19:00.
_Место проведения_: офис Avito, метро Белорусская, ул. Лесная 7, 15 этаж.
Форма регистрации: <https://goo.gl/forms/83hiODGnTzjwxJkY2>",
"а что если выкинуть lstm, создать матрицу входного текста (строка == слово), скормить всё это cnn слою (слоям) и потом сделать max pooling по выходу каждого фильтра в последнем слое. В итоге получим что текст всегда превращается в n-мерный ""вектор фич"". Дальше можно пару полносвязанных слоёв и вроде как всё должно работать.",
"<@U0M39M6LS> если будешь пробовать идею, то отпиши плиз как оно)",
"На небольших датасетах - Cifar10/Cifar100/SHVN показывает одни из лучших результатов. Тут есть сравнительная таблица если кому интересно - <https://arxiv.org/pdf/1608.06993.pdf#5>
Для ImageNet - достигает такой же точности как и ResNet но с меньшим количеством параметров - <https://arxiv.org/pdf/1608.06993.pdf#6> - figure 3.",
"я как дойду до этого, то отпишу тебе :slightly_smiling_face:",
где про бинарные деревья почитать доступно?,
"в следующий понедельник в блоге ОДС  <https://habrahabr.ru/company/ods/blog/> выйдет третий урок по МЛ, и он будет как раз на эту тему",
"Я скорее про логику merge-слоя. Когда собираешь очень большой вектор фич, нередко `keep_probability ~ 0.1`",
"<https://habrahabr.ru/post/65617/>  и <https://habrahabr.ru/post/66926/> у нас в ОДС про деревья решений будет. Они хоть и бинарные, но имелись в виду деревья как структура данных",
"В keras в дропауте задается вероятность дропа, в tf -- вероятность оставить элемент. So, под большим дропаутом, я имел в виду случай, когда из всех фич остается всего ~10%.",
понял :slightly_smiling_face: а на веса W и U ты какие обычно ставишь dropout’ы и добавляешь ли W_regularizer и U_regularizer ?,
"Сейчас попробую 0.11 поставить, как на машине, на которой обучалось.",
"Поставил версию 0.11 (такую же как на машине, на которой обучался). Результат не изменился. Только я ставил версию без GPU (на ноуте старый GPU еще и с optimus).",
"да, я имел ввиду unsupervised, когда нету контента откатегоризированного. Типа LDA только со словами которые можно было бы использовать как лейбелы. Но ща понимаю, что, наверное, ерунду спросил - действительно проще дату собрать и делать текстовую классификацию.",
"Всем спасибо! Можно расходится. Виновником оказался keras, были разные версии. Поставил версию как при обучении и все работает! :tada:",
"Будет, но только для тех, у кого уважительная причина не приходить на митап)",
"Как обычно, просто подать на вход ",
"чорт, я тут посмотрел в гугл школяре сколько ссылок на статью алекснет (ImageNet Classification with Deep Convolutional Neural Networks), и как бы был слегка удивлен, попробуйте угадать перед тем как идти смотреть",
"<@U06J1LG1M> ее первым делом все цитируют, когда начинают в интро ""recently, deep learning models demonstrated state of the art performance on many tasks""",
"Кажется, что все равно это можно понимать как augmentation улучшил variance",
"то есть мы выбрали что будем эту задачу решать с помощью xgboost с деревьями глубины 5, потом нааугментировали данных, и теперь хотит понять как это повлияет на модель",
"Всем привет!
У меня возник такой вопрос:
Если мы пременяем L2 регуляризацию к обычной модели логистической регрессии это тоже самое, с точки зрения лосс функции (максимального правдоподобия), что мы требуем чтобы веса модели, как случайные величины были распределены по нормальному закону.
Если мы применяем L1, то тоже самое, но только мы говорим, что веса должны быть распределены по Лапласу. 
Эту логику я дальше переношу на всю сеть и не могу понять, почему в статьях на тему прунинга очень сильно восхищаются тем, что веса всяких VGG распределены по нормальному закону. Может быть я что-то не так понимаю или что-то упускаю?",
"Мой пост без злости, если что) Я сам deep learning'ом серьезно начал заниматься вчера. Вот тут во второй лекции есть много полезного, что ты хотел сделать: <https://github.com/oxford-cs-deepnlp-2017/lectures>
Тема очень перспективная и можно улучшаться. Кстати, может мне ответит кто-то за одно, что понимают под embedding нормальные люди? В этой лекции эмбеддингом называют как и изначальную матрицу мешка слов на окне (и обозначают E, ещё бывает добавляют прилагательное sparse- это понятно), так и то, что получается в итоге тоже называют эмбеддингом и обозначают E_hat (прилагательное dense- это тоже логично). Ещё я видел, как некоторые люди, если хотят вставить в середину сетки свои фичи тоже называют это embedding. Где соль?",
"obednikov.alex: это от куда такой вывод? То, что ты добавляешь в loss слагаемое, которое возрастает со значением весов линейно или квадратически ещё ничего не значит. Где тут предположения о распределении? Это больше про подход Баеса)",
"Вот примерно так я и думаю, но как четко определить, что такое bias, а что variance для не MSE ошибки я толком и сам не знаю. Тут у меня сплошная субъективщина и интуиция.",
"<https://github.com/pymc-devs/pymc3/wiki/GSoC-2017-projects>

Способы улучшить PyMC3, может у кого ни будь будет время этим заняться.",
"Привет, наверное сюда надо написать, а может и в <#C040HKJF1|_random_flood> :slightly_smiling_face:   Но в общем, объясните насколько и почему популярен коммерческий Perforce, когда есть git?  Недавно узнал, что Nvidia юзает  Perforce, наверное поэтому Линус послал их :slightly_smiling_face: &gt; Nvidia, fuck you!",
А где можно будет запись посмотреть?,
"такое лучше в <#C0SGCGB52|career>. Вообще странно, в каком отделе у них Perforce? Могу уточнить, у знакомых вроде git таки",
Привет! а какие инструменты TSNE промышленно используются кроме как в пакете `sklearn.manifold`? Стоит задача сделать кластеризацию для более миллиона записей с размерностью признаков порядка 300 (ну или после понижения размерности от 80 до 100)?,
"Интересены данные о поисковых запросах +- крупных поисковиков.
Пока интересуюсь не самими данными, но теоритеческой возможностью их достать(в том числе за деньги). Подскажите, пожалуйста, куда копать, кому писать.",
"Да, тоже хотел это написать. Вообще за априорное распределение часто берут наиболее общее распределение, если нет особых догадок. При нормальном количестве данных апостериорное будет таким, каким должно быть:)",
"Хм.. А такой фокус нормально работает?
А с гуглами, рамблерами, мэйлами, и-кто-там-еще-жив-как-поисковик-в-снг?

Просто это такой случай, когда проект не мой, а соседнего отдела(и это только стадия подготовки к пилоту), и проще найти партнёра/аутсорсера, чем пилить это самим. 
Наверняка есть умельцы, которые уже провернули этот фокус.
А, быть может, у кого-то из поисковиков есть вполне адекватные механизмы партнёрства на этой почве.",
"<@U3R32B38W> Вообще лабелинг кластеров - тема известная, но довольно не простая. Т.е. чтобы действительно получить нечто человекочитаемое, придется попотеть. Например, после того, как ты накопаешь потенциальных кандидатов на название кластера, поверх придётся прикрутить языковые правила (с учетом частей речи и т.п.), которые скажут, что этот потенциальный лейбл соответствует человеческому языку.",
"а зачем снижать с помощью тсне размерность до другой, собственно говоря? делать мтерическую кластеризацию по результатам тсне довольно таки бессмысленно, как кажется",
"Гайз, подскажите, пжл, как называется игра из линий, где надо посадить корабль на Луну(Марс?) для обучения агента. Моим Гуглом не гуглится.
Спасибо.",
"Напоминаю, что завтра будет проходить ML тренировка <#C1CEM43TJ|mltrainings_live>, где будем разбирать Tinkoff Data Science Challenge и Kaggle Melbourne Seizure Prediction. Регистрация пока не закрылась (<https://events.yandex.ru/events/mltr/11-mar-2017/>). Погода уже улучшилась, так что самое время не оставаться дома и смотреть трансляцию, а приезжать и пообщаться со всеми лично. К сожалению, у нас есть некоторые технические проблемы, поэтому просим кофеманов и не только позаботиться о напитках по пути. Начинаем в 12 часов.

Кстати, недавно был праздник. Будет мило, если вы решите поздравить наших девушек-организаторов приятным словом или цветочком :slightly_smiling_face:",
"Чат, киньте в меня ссылкой или мыслями на тему плиз. Кто-нибудь пытался совместить SGD с динамическим обновлением весов примеров по типу adaboost?
Мотивация такая: в задачах классификации, если датасет сильно несбалансированный, то классы, у которых гораздо больше примеров, задавливают общий лосс и на редких классах модель может сильно ошибаться. Это можно лечить колхозом с увеличением важности редких классов ручками (гемор, добавляет гиперпараметры). Либо делать что-то (не знаю точно как) в ключе GAN. С другой стороны, Adaboost должен автоматически отрегулировать веса примеров оптимальным образом. Тренить много сеток как weak learners при этом особо не надо, т.к. обычно capacity одной сети уже избыточна. Получается, что SGD на одной сетке с Adaboost весами должно работать лучше на классификации (игнорируя оверфит для простоты). Логично или что-то упускаю?",
"Есть такая задачка: датасет твиттов человека, указать, насколько отличается новый твит от всех остальных этого же человека. Я как-то свожу это к novelty detection, но всё что я видел было супервайзд. Пытаюсь сделать через тф-идф, но не могу понять, как мне непосредственно получить эту вероятность, пока что могу получить наиболее похожие твиты этого человека через similarity. Нашел вот такую статью <https://arxiv.org/abs/1401.1456> но она как-то тяжело читается. Еще нашел вот такой репо <https://github.com/Lab41/pythia> но что-то тут вообще не могу разобраться. Они тут похоже сводят задачу к супервайзд 
Может кто что посоветовать?",
"чтобы получить вероятность unsupervised, нужно все равно какую-то модель принять, как соотносится похожесть твитов и то, какова вероятность того что этот тивт чужой. Но тут же вроде нет проблем с данными для supervised обучения?",
"Ну я собирался сравнить новый твит с корпусом, составленном из старых, так и получить вероятность. Мне почему-то кажется если просто как супервайзд сделать со случайными твитами вместо второго класса, то будет получаться как-то не очень",
а какой движок нейронных сетей?,
"Есть временной ряд кол-ва авиапассажиров помесячно за один полный год N и первый месяц года N+1. Подскажите пожалуйста, какую бы модель на него натянуть, чтобы спрогнозировать ряд до конца года N+1 ?",
"мне кажется надо от исходной задачи плясать, если это ""указать, насколько отличается новый твит от всех остальных"" то можно просто similarity взять, если нужна вероятность, то уточнить какая - если вероятность того что твит этого человека а не любого рандомного, то собрать твиты этого человека и рандомные, и это будет два класса.",
"Ну а как через похожесть решить это? 
&gt;могу получить наиболее похожие твиты этого человека через similarity",
"Вернее, на завтрашнюю трениировку охране дано строгое указание - пропускать только тех, у кого есть хотя бы один цветок.
Те у кого три и более - проходят вне очереди.",
что у вас случилось и какие напитки можно с собой приносить?,
"Т.е. для применения аримы нужно как минимум два года, чтобы определить годовую сезонность? Или ты имеешь в виду ещё более долгие циклы, соответственно, нужно больше, чем даже 2 лет?",
Да хотелось cuDNN и в TF пописать. Так как deeplab курс на TF будет:(,
"это когда у тебя есть корпуса  слов по разным темам , например:семья(мама, папа, брат) и т.д.
считаешь кол-во слов  в тексте и их сумму. В итоге у текста получается сводка
содержание темы семья:10%
содержание темы спорт:30%
содержание темы наука:30%
...",
"Скажем так, это тот случай, когда можно :good-enough:",
"Всем привет! Вопрос: есть данные анкеты dataframe, в нем логин (повторяется по 21 разу), № вопроса (опять же от 1 до 21 кучу раз), Ответ ( к каждому вопросу)
Как делать чтобы Логин был в id (не повторялся), № вопроса были в названии столбцов, а оценки под ними?",
"Кластеризовать *с помощью* tsne. А вы по другому прочитали, как будто tsne можно кластеризовать (это же не данные).",
"так что такое кластеризовать с помощью тсне то? тсне сопоставляет вектору вектор, как из этого берется сопоставление вектору номера кластера?",
"вопрос в другом, зачем тебе кластеризация?",
и почему если она тебе нужна не прогнать ее сразу,
"Может тогда поясните, зачем раскидывать вектора признакового пространства по облакам странной формы как не для последующего анализа: а что же за этим лежит?",
"ну да конечно, именно за этим не знаю почему <@U1BAKQH2M> против",
"я просто может туповат, но до сих пор не понял, в каком порядке <@U383XH7QE> применяет операции",
"гляди я что делал, у меня была задача где куча признаков, куча наблюдений, примерно такой же порядок как у тебя, я выбросил почти все признаки, сделал несколько подвыборок данных",
"<@U3SKM61F0> &gt;признаки выкидывал просто из соображний предметной области
а если ошибся и слил полезные? Ты конечно молодец, что руками понизил размерность, но когда признаков 150 и непонятно что выкидывать, то ничего не остается как  -чтобы пусть само ""кластеризуется""",
а какая итоговая цель селекции признаков?,
просто у меня какие-то кластеризации пользователей по покупкам даже интерпретируемые получались когда я как номер кластера брал вектор базисный в свд-разложении по которому максимальная координата была у юзера/товара,
"<@U1BAKQH2M> как я тебя понял / (не понял):
1. выкинь описанный выше подход: для каждого клиента 150 исходных признаков (статистики по периодам+свойства товаров
2. составь матрицу клиент / товар (например по сумме покупок)
3. разложи с помощью SVD, на три матрицы: 1. клиент/его предпочтения * 2. диагональная матрица (веса) * 3.товар/его свойства
4. дальше я не понял, что с эти делать",
"Вот как-то так работает (писал на R, сорри)

Идея в том, что пусть исходное распределение даётся допустим вероятностью покупки умноженной на размера чека (последний в данной модельке задаётся как логнорм, но это неважно на самом деле).

Ты берёшь исходные данные и говоришь, как и в бутстрапе, что они являются хорошей презентацией исходного распределения. Пусть тогда воздействие заключается в том, что параметры распределения изменятся мультипликативно, тогда новое распределение запишется как 1.1 * старое распределение.

После этого ты бутстрапишь из обоих распределений и подбираешь размер бутстрапа таким, чтобы выполнились заданные условия по CI",
"никто не знает как можно в lasagne сделать ExpressionLayer, у которого нелинейная функция - это другая нейросетка?
или как-то можно в середину одной сетки встроить другую?",
"Моделирую предсказание доли, которая лежит от [0, 1] естественно, доля сильно скошена к нулю обычно (0.00317 среднее), какое бы правдоподобие выбрать?",
тип предсказываю логитом среднее и существует какое то sd,
"интуитивно кажется, что выводы сильно зависят от того, в какое sd я верю",
"ну, я имею ввиду, как скормить нейросети некоторый набор текста. Этот текст должен быть в каком-то специальном формате?",
С какой целью и какой алгоритм будешь использовать? есть много методов препроцессинга текстов,
Так а что это за схема такая хитрая? <@U04ELQZAU> кинь ссылку где такое вычитал или расскажи,
а как это связано с вопросом кодирования символов на входе?,
"Как правильно оптимизировать гиперпараметры модели через кроссвалидацию, если решается задача бинарной классификации и в исходном датасете присутствует много повторяющихся записей с разными исходами? Просто оценки качество на кроссвалидации получаются смещенными в этом случае.",
"Распределение с сапортом [0;1]. Дисперсия ограничена сверху дисперсией юниформа 1/12, как выяснилось. Ща проблема в том, что MCMC не эмцэ. NUTS застревает в одной точке и не двигается. ",
"Likelyhood сделал beta(mu, sd) где приор на sd U[0;1/sqrt(12)]",
а вдруг кто=нибудь помнит пример: как бы мне легко и просто напечатать в Python направленную сеть с привязкой нодов по координатам?,
"Нубский вопрос  - какой docker c набором python 3, pandas, theano, keras, cuda используетет?",
"Если хочется, чтоб ошибка для всех повторяющихся учитывалась как одна, можно, наверное, удалить повторяющиеся записи к чертовой матери, а метку для них заменить на среднее значение - будет одна запись с y=0.8. Только классификатор тогда нужно такой, чтоб что-то вроде кросс-энтропии минимизировал. Но это от задачи зависит, как ее поставить.",
"И вот ноутбук с примером, как обработать текст, чтобы подать в сетку с lstm <http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/notebook21.html>",
А какого размера корпус будет?,
"я понимаю, как оставить, например, пунктуацию с помощью него, но там же на вход регулярное выражение подается, нет разве?",
"Вопрос очень серьезный: есть ли тут люди, которые парсили сайты и обращали внимание на `Terms of use`? Очень часто в этой вкладке есть такая строка ```access, monitor or copy any content or information of this Website using any robot, spider, scraper or other automated means or any manual process for any purpose without our express written permission;```
Если я правильно понимаю, то в таком случае парсить сайти запрещено. Если кто-то сталкивался или знает истории, в каких случаях все-таки можно парсить или как написать руководству сайта, что бы разрешили использовать некоторые их данные для некомерческого использования?",
"Привет, кто-то подскажет какие требования в аспу сколтеха?",
"всем привет! друзья, нужна база пользователей OK (:wat:) деанонимизированная до уровня: фамилия (ио не нужны), пол, возраст, город/страна. Если база не новая, это еще лучше, предпочтительнее до 2015 года (не спрашивайте зачем это все)",
А какие есть датасеты с русской речью? Пригодные для тренировки speech-to-text.,
"&gt;Кино с сабтитрами?
да кстати реальная тема, на одном митапе в мейле выступал проф из инрии (или ирния, ну кароче известный вуз во франции), он же в конторе visionlabs работает, и говорит что для мультимодального обучения текст+речь+видео именно кино с сабами, показывал демки где по текстовым запросам запросам ищутся фрагменты видео",
"Сделаю-ка сюда вброс. 
Тут есть представители крупнях контор вроде Яндекса, Мейла, etc.
Какие институциональные барьеры есть против открытия внутренних датасетов? Понятно, разбазаривать конкурентное преимущество нельзя, юзерский privacy всякий. Но гугл же делает иногда. Как сделать чтобы этого было больше?",
"Привет. Через неделю олимпиада Вышки для студентов. Есть те, кто писал её в _регионах_? Можете рассказать, как это проходило?",
а какое полное название книжки?,
"Если серьезно, то давно хочу для рыбок сделать детектор. Ты для какой задачи этим занимаешься?",
"Омайгад. Я то думал. Десять страниц это вообще ни о чем, они не прибавят знаний ни в лернинге, ни в английском. 

Возьми рандомную статью с первой страницы выдачи гугла, сдай преподу свой реферат или как там это сейчас называется, выпей пивка и наслаждайся студенческой жизнью вечером субботы",
Кто нибудь в курсе почему в YOLO не пользуют ResNet?,
"<@U0GV3LMUY> архитектуры действительно хороши, но ResNet много где решает",
"Народ, может кто посоветовать где можно взять предобученный парсер для построения синтаксических деревьев для русского языка?",
"да автор постоянно что-то меняет, как ему взбредёт. В серьёзных проектах лучше не использовать, только для тестов на коленке, иначе в следующем апдейте всё может работать по другому и в тихую выдавать не тот результат",
Так вот почему Keras + TF работает медленнее чем чистый TF или Keras + Th...,
"Может, когда в TF таки полноценно возьмут, доведут",
"<@U1HHX1QS3> а какая там была штука, где миллион точек на карте визуализировали?",
"Существуют ли ML разработки, которые умеют сами генерировать сценарии? То есть, обучили какую то модель для прогнозирования, например, оттока клиентов с рядом экзогенных параметров. Далее, смотрим на динамику экзогенных параметров и формируем для них три сценария: позитивный, инерционный и негативный. И в соответствии с каждым из них строим прогноз оттока.",
"<@U0FEJNBGQ> нет, это совсем другое. Confidence intervals могут быть у всех прогнозов целевой переменной - по каждому сценарию. Они показывают в каких границах будет с наибольшей вероятностью находиться целевая переменная в рамках каких-то конкретных предположений о динамике экзогенных переменных.",
"я в таких задачах моделировал сами сценарии (вплоть до спросить эксперта что будет с каким индикатором) и подставлял их в обученную формулу. после чего смотрел что произойдет, рисовал всякие красивые графики и делал далеко идущие выводы",
"понятно что это не панацея и в реальном мире при новых сценариях сама модель происходящего может поменяться (например вчера уровень безработицы не был значим, но в новом сценарии безработица большая и в реальном мире бы уже сильно повлияла на все, но мы про это не знаем и у нее как был близкий к 0 вклад, так по модели и остался)",
"ipaulo: tf.contrib.layers - отличная вещь, как керас, только уже в тф",
а почему вы таки спrашиваете?,
"Значит ты вы этом разбираешься.... :slightly_smiling_face:

У какой архитектуры есть потенциал на задаче для рака легких?",
Подскажите как построить синтаксическое дерево (или как это правильно называется). Хочу сделать примерно такую же штуку на основе своей модели как вот эта <http://nlp.stanford.edu:8080/sentiment/rntnDemo.html>,
В смысле как визуализировать синтаксическое дерево?,
"А как мне сгенерить эти Np, vp и т.д.?",
"Вопрос задачался с месяц назад, когда приперло по RAM на задаче про спутники. Но там я выкрутился через то, что хранил данные в  float16 и моих 32Gb хватило.

На задаче про рак этот фокус не прошел =&gt; нужен апгрейд.

Хочется motherboard + CPU + 128Gb RAM 

чисто под Kaggle, и чтобы можно было если меня опять что-то припрет 3 Титана туда добросить к одному существующему когда сунусь на ImageNet

Мы что-то обсуждали месяц назад, но может что изменилось в лучшую сторону?",
"пиши in:hardware from:cepera_ang и jump to сообщения, где  интересное пишут",
А на питоне в виде какой-то либы есть такое? или хотя бы скажи как это правильно называется,
"Это вы как до такого lr догадались? Кстати `*` лучше, наверно, заменить на `\times` 
```
AdaM with learning rate of 27 ∗ 10−6
```",
"И почему у вас бинарная классификация, а на выходе Softmax?",
"&gt; не опечатка ли
опечатка, спасибо

&gt; Это вы как до такого lr догадались?
эту часть <@U0B4374S1> писал, он гонял тесты",
Кто-нибудь пробовал Batch Renormalization <https://arxiv.org/pdf/1702.03275.pdf> или проблема маленьких батчей пока ни у кого не стоит?,
"друзья, подскажите, плес. набор символов в начале при семлипнге char-rnn -- это ж прайм? а почему он такой бессмысленный? из какого распределения он семплится?
<https://opendatascience.slack.com/archives/deephack/p1489379692043633?thread_ts=1489182137.906624&amp;cid=C074F6E1K>",
"Чат, а какие есть архитектуры сверточных сеток с приличным качеством на imagenet (==сравнимо с AlexNet хотя бы), но компактные и чтобы feedforward был быстрый?",
"Просто вычислительная сложность у squeezenet вообще чуть ли не ровно такая же, как у AlexNet.",
"Вообще, есть мало ресеча, когда люди стараются применить все современные методы к старым архитектурам, чтобы их оптимизировать",
"А есть где-нибудь почитать про то, как архитектуры оптимизируют?",
sim0nsays: разве батчнорм не замедлит forward pass? Он же вроде как сходимость только ускоряет?,
"Привет, подскажите, какой подход используется в <https://api.ai/> для entity, intent extraction?  У них есть API, но не могу найти информацию касательно их технологий. Является ли это word2seq, seq2seq?",
"Может тогда вообще уйти от batch_norm <https://arxiv.org/abs/1603.01431> ? 
Интересно было бы узнать, кто на практике пробовал сравнивать normalization propagation vs batch norm.",
"занятно, что их CTO как будто бы не занимается NLP: <https://github.com/artemgoncharuk?tab=stars>",
"На русском языке не порекомендуете какие-нибудь хорошие книжки (или курсы) по Neural Networks, Deep Learning и Reinforcement Learning? Желательно российских авторов, так как они часто довольно глубоко рассматривают материал. Поверхностные книжки неинтересны.",
"&gt; Особое внимание уделено методам исследования динамики нейронных сетей как важнейшего класса обучаемых распознающих систем, а также достижениям Петербургской школы математической кибернетики В.А.Якубовича, основанным на проксимационном подходе. 
Проксимационный подход -- зэ бэст",
"<@U06J1LG1M> Паш, че-то я не врубился. а как в пуассон-бернуливской рбм  бег ов фордс кодируется? вот на первом слое у нас пуассон. а как тогда слова и нейроны соотносятся? не 1 к 1?",
"i: да вот тупо как есть и подается, потому никто и не юзает ",
"Мне кажется, что человек троллит. Самое лучшее в таком случае, как мне кажется, просто не реагировать на такие вбросы.",
"Безотносительно статьи: есть где-нибудь простое объяснение почему энтропия задаётся как `p * log p`?  Я имею в виду, какой физический (информационный?) смысл стоит за каждым из сомножителей. Ну, или с другой стороны, чем такой выбор энтропии лучше, чем `p^2 log p` или `p log(1 + p)` или любая другая функция от `p`?

Про принцип Больцмана я знаю, но от него всё-таки очень далеко до этой формулы, ну и какого-то очевидного мостика между идельным газом и информационными системами вроде нет.",
"если да, то где можно его взять?",
"в какой-то мере. я же не знаю, какие данные нужны :slightly_smiling_face:",
"как правильно задать правдоподобие на  Beta имея среднее, но не имея стандатное отклонение: вешаем prior на него? Сложность в приоре для стандартного отклонения",
а какое число линий оптимальное ща? 40?,
"Друзья, какими методами сейчас можно посчитать similarity между документами? И что можно почитать на эту тему?",
"вроде не было
<https://arxiv.org/pdf/1703.01988.pdf> 
кто понял как сделать dnd объясните, кажется это похоже на память в dnc, только попроще, потому что пишет всё, но как сделать всё равно не понимаю :confused:",
"Вообще хотелось бы реализовать что то типо аггрегатора новостей. Я хз как это правильно делается, мне кажется как то через similarity документов.",
По темам. Как у яндекса на главной. Вот произошло событие какое-то и он все новости с интернета аггрегирует в одну кучу.,
"А есть что нибудь, где слои постакать можно? LDA как то не то.",
"<https://opendatascience.slack.com/archives/nlp/p1489430614468064> там вроде топик моделинг как раз лежит в cluster-document матрице, не?",
"там же целая интернет-драма была локального масштаба про этот doc2vec, все пытались результаты повторить, ни у кого не вышло (втч у Миколова, второго автора), ну и он в итоге в другой статье написал, что результаты неправильные и их можно повторить только на специфичном трейн-тест сплите",
"короче я сам пробовал док2век как экстрактор фичей для классификации, тип text-&gt;doc2vec-&gt;xgboost",
"Не понимаю как в моем случае использовать кластеризацию. Я хочу аггрегировать новости не по типу ""политика"", ""спорт"" и тд. А вот как на главной у Яндекса, там более специфичные события, например, сейчас там есть что то такое ""Чубайс подаст заявление в МВД о возбуждении дела за шантаж""",
"Взять слова, покластеризовать, затем представить  каждое слово как id кластера, к которому оно принадлежит",
"<@U1G303UTW> кластеризовать векторы слов корпуса, а потом закодировать доки как bag of clusters (количество слов из каждого кластера или tf-idf, тут можно импровизировать)",
"Я содержание глянул, там, как мне показалось, совсем основы рассматриваются, разве нет?",
"Ну да, но эти основы - это часто очень сильный baseline. Тут пока не как с картинками, где кроме нейросетей особо ничего не осталось в state-of-the-arts.",
"+ для всех этих штук куча готовых заоптимизированных реализаций, так что попробовать несложно должно быть. Если потом будешь RNN пилить, будет удобно смотреть, какой бэйзлайн-то.",
"а кроме wiki, какие существуют хорошие датасеты для multilabel text classification?",
"любые наивные штуки которые  не учитывают последовательность встречающихся слов не проходят очень тупой тест вроде ""london is not a nice place to live"" vs ""no, london is a nice place to live"" где на перестановке одного слова меняется сентимент, так что это как-то заранее унылый  подход",
"были прикольные картинки, где rnn с аттеншеном выделяла вот такие вот конструкции",
"у меня на похожем датасете просто bag of words по заголовкам давал что-то в районе 97% auc, казалось бы, куда тут нейронки пихать :slightly_smiling_face:",
"Спасибо за ссылку, хотя этот подход мне не очень понравился. По факту, он решение подгонял под ответ, то есть зная как устроена энтропия он подобрал список свойств, которые фиксируют её однозначно, за что честь ему и хвала, но абсолютно не объясняет происхождения отдельных сомножителей.

Но в принципе, если читать эти две ссылки последовательно, то получается более-менее ок - сначала вывод энтропии, как наилучшей компрессии алфавита, потом вывод основных свойств и доказательство, что она единственная, кто этим свойствам удовлетворяет. Получается красивая, замкнутая теория.",
"Stan что-то вешает на параметры, если ты их не задаешь по дефолту. Точно сказать не могу, надо там искать, но кажется, что там U[-oo,+oo] что то такое, где бесконечность - очень большое число.",
"Могу сказать, что это пальцем в небо было, без какого либо обоснования",
"Ну а посмотреть, как в Stan сделано?  Ты что использовал?",
"Он, небось, batchnorm прикручивал, как Артур на спутниках пытался поначалу.",
"Прежде чем советовать Intel Xeon E5-2683 V3 2.0GHz 14-Core Processor нужно уточнить, что
1) Нужно брать инженерный образец, причем довольно тщательно его искать на ebay и подобных барахолках
2) У этого инженерного образца должен быть в CPU-Z stepping 2, остальные степпинги не нужны. Дополнительно себя обезопасить можно посмотрев видео с канала <https://www.youtube.com/channel/UCB2ryEmk8bQ6XYfdagG6FtA>
3) Под линуксы еще турбобуста из кнопочной коробки не завезли, так как тема довольно новая, но не должно возникнуть трудностей, впрочем в апреле узнаем:)
4) Нужно покупать особые материнские платы, где легко прошить свой кастомный биос.
5) Нужно покупать память ECC, которая неконфликтна. Обычно это прописывается на сайте материнской платы. Не нужно гоняться за скоростями, большинство 2600+ конфликтны.",
"как вариант - из питона генерировать JSON, и подкладывать его в JS фреймворк для визуализации - например вот этот <http://visjs.org/network_examples.html>",
"По поводу серверное vs консьюмерское жезело еще закину тему на обсуждение. 
Частота отдельный ядер почти всегда у серверного решения ниже, чем даже у какого-нибудь i5 6600. Да, их намного больше. Но так ли часто каждый из нас пишет по дефолту многопоточно? Особенно для дл и особенно для кэггл сорев, когда хочется по-быстрому реализовать идею и посмотреть ее работоспособность. 
Кроме того, у двухпроцессорных решений нужно контролировать на каких ядрах выполняется код, потому что если она начинает гулять между ядрами разных процессоров, то перфоманс падает.
Это все к тому, что мне не очевидно, что дешевое серверное железно с 40 потоков по 2 ГГц будет лучше, чем 4 потока по 4 ГГц.",
"Привет, а что вы используете для визуализации времени в Seaborn/matplotlib? У меня есть данные где время записано как `07/Mar/2017:06:32:44 +0100` , мне нужно конвертиовать это в `datetime.datetime()`  чтобы было удобно работать?",
"В эти выходные в физтехе ML хакатон, кто что нибудь про него знает? <https://vk.com/wall-142135418_5>",
"Делай хорошо, живи правильно, питайся регулярно, спи как надо и у тебя все будет нормально.",
"Вопрос в том, каких усилий стоит ""переписать"". Имхо, это примерно 3-5 минут. (при условии, что до этого функции были написаны прилично)",
"То нибудь знает пример кода на JS который позволяет визуализировать параллельные процессы на оси времени которые можно ""зумить по этой оси"" а также выкидывать текст как Легенду по клику на определённый процесс в момент времени",
"Только обидно, когда это 8-летняя китайская девочка",
Там же для студентов. Такие старые как ты никому не нужны.,
"Про двухпроцессорные решения - полностью согласен. Про однопроцессорные - выбор серверного решения или геймерского - я бы предпочел серверное, так как в турбобусте там сравнимые 3 ГГц, но есть плюшки, которые из игровых систем вырезают - много PCI линий, ECC память, большая стабильность 24/7, много кеш памяти, возможность маштабироваться, более медленное устаревание проца и потери его стоимости. И та же стоимость, если говорить об инженерных версиях. Рассуждение диванного аналитика",
гонять эксперименты и смотреть с каким параметром заходит лучше,
"<@U09BY2N3X> Да просто у меня в области олимпиада проходит почему-то не в Новосибирске, а в криминальном городе-спутнике в чёрт пойми какой школе. Поэтому интересно, как вообще обычно связаны с Вышкой люди, проводящие олимпиаду в регионах, и насколько они компетентны.",
"<@U041P485A> где брал, если не секрет?",
"Есть какой-нибудь из недавних чемпионатов о котором можно почитать, что и как делали в NLP???",
"<@U47EXAQ8H> это понятно, я знаю какие есть модули, я больше хотел бы готовый пример , может что то на pyd3 видел?",
"<@U32H8EW2F> поищи task description статьи в semeval 2016 - <https://aclweb.org/anthology/S/S16/S16-1000.pdf> , в task description обычно кратко описываются подходы участников к решению задач - там есть как и классическое NLP, так и DL",
"NER модель обычно обучается под определенные сущности в тексте и готовые модели тебе не подойдут, так как сущности могут не совпадать с твоими. Есть подозрение, что тебе нужен не NER, ищи по словам intention and slots detection, если тебе для чат-бота.",
<@U0ZHQNKQS> а почему платная конференция? не хотите стать первой бесплатной конфой в Украине?,
"Я бы еще вносил в серо-черный список тех, кто не пришел, хотя регистрировался и с них в следующий раз брал деньги",
"Галеры хотят и пропиариться как спонсоры, и в плюс выйти по деньгам - ужоснах!",
"Цена данной конференции не высокая по сравнению с другими, просто интересно как не организатору, зачем брать плату, если есть спонсоры и с них можно взять больше",
"Каким образом зарабатывают со-создатели языка и прочие СЕО - более-менее понятно. Они сидят на грантах. А какой выхлоп будет простым смертным, которые готовы уверовать и изучить?",
Динамических графов вычислений. Хотя хз как в ты с этим,
<@U041P485A> ты как обычно или все-таки придешь?,
а есть где почитать можно как это сделать? я сейчас запустил простую lstm на keras c tf,
А как ты их кормишь сети?,
"<@U0H7VBQQ1> я правильно понял, что без разницы как пихать в lstm вектора?",
"а можно ссылочку на то, как без embeddings слоя пихать вектора? мне кажется, что где-то это есть уже написанное",
"Спонсоры - это отдельная история)
Я начинал с бесплатных, но если хочеться делать ивенты на хорошем уровне, привозить серьезных спикеров, то тут невозможно бесплатную делать, даже если спонсоры будут. 
Пока у нас денег хватает сугубо закрыто косты по местным спикерам, но я уже смотрю, как бы вытаскивать зарубежных. Пока с ними общаемся, начали делать вебинары, а дальше видно будет, кого когда привезти можно будет;) 
Еще относительно бесплатных, мы на базе вузов сейчас семинары начали устраивать, там особо расходов нету.",
"можно LSTM как первый слой сети поставить, насколько я помню",
"Ну так никто не говорит про организацию без спонсоров, спонсоры должны быть обязательно и чем больше, тем лучше, что бы покрыть все расходы. 
&gt; Есть люди, у которых основная профессия такая - event manager, они за это зарплату получают, кушать то им надо. 
это нормально, если он за это получает зп от компании, которая датасайнс двигает, а не когда ивент-компания организовывает датасайнс конфу за 5к гривен с целью словить хайп и срубить побольше бабла",
вдруг кто еще не видел,
"Да, сейчас столько развелось конференций, что я просто офигеваю))) Я еще понимаю, когда компании делают свои ивенты, чтобы легче было новых людей нанимать, но когда ребята начинают организовывать ИТ ивенты с опытом до этого свадьбы/корпоративы, то это весело)))",
"&gt; как без embeddings слоя пихать вектора?
заэнкодить еще в батч-генераторе. 
вместо [batch_size, seq_len] int32, передавать [batch_size, seq_len, dim_size] float32, а из сетки убрать embedding-слой.",
"Благородные доны, а как выглядит data science завтрак? ",
"какой сейчас state of the art для сравнения картинок на похожесть? чтобы учесть ресайзы, повороты, вот все такое",
"подходов несколько, но все сводится к выучиванию какого то эмбеддинга нейросетями",
"не в курсе, обычно сам делал на основе какой либо обученной сети",
"спасибо, и еще один дилетантский вопрос: если использовать питон, то эмбеддинги потом сравнивать как лучше? что-то типа cosine similarity numpy векторов?",
Возможно я не увидел - а где именно будут доклады? А то башни то большие),
"Как решить такую задачку: есть множество из n элементов, на каждом шаге выбирается случайным образом его подмножество из k элементов, нужно найти мат ожидание числа шагов, за которые  все n элементов хотя бы раз будут выбраны.",
"<https://opendatascience.slack.com/archives/nlp/p1489385830673198>. Фоллоу ап. Написал на ФБ Илье, не ответил (у меня пустой профайл в ФБ скорее всего из-за этого). Кто знает что-нибудь о <http://api.ai|api.ai>?",
"интересно! а как именно attention прикручен, какая архитектура используется?",
"И всем, кто побежал обновлять: там в паре мест сломалась обратная совместимость.
Скажем теперь `y = np_utils.to_categorical(y, num_classes=666)`
Раньше было `y = np_utils.to_categorical(y, nb_classes=666)`",
"Надо разбираться как attentionLayer там сделан. Меня смущает, что его к результату BiderectionalRNN применяют.",
Как и TH попросить преаллоцировать память и несколько ускорить вычисления,
asobolev: и сидеть с лицом лягушки и подбирать бинарным поиском какую долю памяти под твою модель и батчайз можно аллоцировать :pepe_sad: ,
"<@U41311QKU> Да, это нормально. Attention должен вычисляться по отношению к чему-то, как в машинном переводе по отношению к переведенному слову на выходе. Без привязки он обычно скатывается к последнему слову, в общем не работает.",
"ramfs ещё как вариант, если в ram влезает",
"Это задача  называется ""мат. ожидание числа шагов в марковской цепи"". Здесь у вас состояния это число выбраных элементов: оно равно в точности k на первом шагу и потом переходит в состояния k...2*k (за 1 шаг) k..3*k (за два шага) и в итоге в состояния от k до n. Соотвестствующие вероятности перехода считаются одинаково (см. ниже), (возможно, не считая случаев, когда число шагов m таково, что n-k*m&lt;mod(n,k), а может и там формула той же будет, подумайте плз пока без меня)",
"Ребят, какие существуют best practices по заполнению удаленных элементов на фото? К примеру, есть такая афиша, откуда я хочу вырезать эти страшные буковки и заполнить образовавшуюся пустоту <https://media.ticketland.ru/images/250x250/d3/5f/d35f081672869a6509d8da68b8d6359a_1869957.jpg>",
"если просто с выводом плохо, то обычный clear помогает вроде как",
"сами ворочайте этот ваш керас, зачем мне об него пачкаться :kekeke:",
почему люди вообще этим пользуются?,
"каким местом нужно писать фреймворк, чтобы так получилось?",
"мама, как у меня пригорает от этого поделия",
"У mxnet'a кстати, все почти всегда хуево с доками. Мне приходится чужой код часто смотреть, чтобы понять как какую-нибудь штуку сделать",
"Как посчитаете, примерно такой код в R позволит проверить ответ: 
n=50
means=c()
for (k in (1:n)) {
  counters=c()
  for (j in (1:500)) {
    z=c()
    flag=T
    counter=0
    while (flag){
    counter=counter+1
    r=sample(n)[1:k]
    z=union(r,z)
    if (length(z)==n) flag=F
    }
    counters=rbind(counters,counter)
  }
  means=rbind(means,mean(counters))
}
s=data.frame(n=1:n,means)
plot(s$n,s$means)",
ну как минимум отвечает на багрепорты,
"Ну драйвера переставь. Как впервый раз под линуксом оказался, чслово)",
"просто меня бесит эта боль каждый раз, почему никто не собрал тулзу для траблшутинга вида ```fuck-you nvidia```, в которой были бы собраны решения для основных источников боли?",
"ага но интегрируется всё в Keras легко, смотришь как у них то или иное реализовано и быстро своё допиливаешь удобно",
"pytorch ещё бета, как то не хочется тестировщиком выступать",
на ворочанье dim ordering как минимум,
а какой самый человечный и безопасный способ дать левому юзеру что-то погонять на своей системе? чтоб давать всяким другим студентам нейроночки шатать (т.е. должно быть секьюрненько все),
Докер отказать. Юзера с анакондой и путями для куды в bashrc,
"какие подводные камни в установке двух разных видюх в одну систему? хочу 1080 и 960 в одну коробку запихать, как я понимаю никаких проблем возникнуть не должно?",
"Только моральные страдания от того, какое 960 говно древнее",
"коллеги, кто в курсе, оффлайновые (если везти на себе) цены на железки за бугром сильно отличаются от онлайновых (если заказывать из России)? интерес в разрезе возврата ндс.",
<@U1CF22N7J> а онлайн он каким образом вычитается?,
"Как я говорил, вряд ли он ответит на потенциально фейкорвый аккаунт ФБ",
"Что-то жизнь меня не готовила к тому, что в дл фреймворке мне нужно ручками определить даже функцию, которая будет считать точность.
Я точно должен все писать сам как тут или есть коробочные решения?
<https://github.com/rdcolema/pytorch-image-classification/blob/331819df6732b0e1c3b321d6eebce6b97287c585/pytorch_model.ipynb>",
"в TF мне очень не нравится, не стабильность результатов, и кроме как пропатчить его самому я вариантов не знаю, Theano в этом плане надёжнее. Да даже если идти на низкий уровень и что то дописывать, это всё равно не значит всё с нуля и на низком уровне.",
Не обнаружил в Бойде проксимальных операторов :disappointed: У кого ок написано?,
"ни у кого нет догадки, почему такое может быть?)",
":are_you_fucking_kidding_me: 
Ты вчера где доклады слушал?",
Почему не Gigabyte GA-X99-GAMING 5P или MSI X99A GAMING 9 ACK ?,
"Ну ок, почему не ASRock X99 WS-E? т.е. почему обязательно супер топовую?",
"<@U4CLTL65C> оборачивать код всё же лучше в тройные обратные кавычки (на клавиатуре там же, где буква ё).",
"..есть кто с позитивньім опьітом работьі с ‘bsts’ в R (bayesian structural time series)?
тут вот поверхностно описано..
<http://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/>",
"dselivanov: я увидел когда tSNE плоты, сразу видно было что перплексити тюнили. Но статья хорошая в целом.",
"Отличный гайд, как заюзать Spot'ы на амазоне <https://medium.com/slavv/learning-machine-learning-on-the-cheap-persistent-aws-spot-instances-668e7294b6d8#.72zlbqnwq>",
"Где есть такая возможность, что можно заказать что-то? Обычно дают, что есть:((",
"<@U3HM4KY14> а как на Xeon”е бусты стакаются ? гдет слышал мнение, что для этих целей лучше i7",
"<@U1NLYUY5N> Воу, а где ты теперь работаешь?)",
"в сочетании с дешевым терабайтным storage, доступным там же, и отсутствием платы за трафик очень выгодно для тех, кто постоянно что-то крутит",
"сетап упал, был 150 евро
может и на аукцион когда выстаят",
"зачем две карточки, если обычный ml
через полгода опять что-нибудь новое выйдет, проще пока одну взять",
"Что-то 300 долларов за 512 ссд овердофига. Я накануне взял М.2 на 512 за 180, тем более что если с него стакать хбусты, то лучше взять подешевле, чтобы когда посыплется не жалко было :slightly_smiling_face:",
"<@U3HM4KY14> какой? Я много где накидывал для условий РФ, можно купить все существенно дешевле, если мониторить ebay/amazon. Если новый брать, то там гарантия у многих 5+лет за эти деньги. И скорость максимально возможная",
"блин, ну почему фейсбук заложил сезонность через фурье, а праздники отдельно? чтобы фурье зафитил реальную сезонность нужно не меньше параметров, чем просто коэффициенты по месяцам и тем же праздникам. но если делать напрямую через коэффициенты, то можно накладывать на них какие-то взаимные зависимости, чтобы еще минимизировать число параметров",
"нубский вопрос. а когда finetuning делаем для нового датасета, то препроцессинг картинок остается тот же? нужно ли к примеру mean, std пересчитывать для нового датасета ? (картинки отличаются от pre-trained)",
"<@U0G29N5U4> мне конечно бьіло интересней именно про особенности bsts-пакета поговорить, ибо, когда начал закапьіваться, то понял, что много вещей там плохо описано/документировано… посмотрю на днях пайтон… если говорить “в общем”, насколько реально там отследить causation? ну то есть, например, есть 10 рядов, которьіе предположительно могут влиять на ряд Y.. ARD єтот в пайтоне поможет?",
"да, но весьма продвинутьій, насколько я сльішал, могет как ad hoc exploration, так и создание промежуточньіх вьюшек на стороне БД, что типа делает его особенно интересньім вкупе с Редшифтом, например…",
"модель тут стандартная как для опенсорсовского продукта. У них есть поддержка, у них есть комменрвечкие версии… 
А простые смертные могут брать язык и на нем писать свои решения. Сам язык сейчас очень стремительно набирает популярность и соответсвенно вакансии начинают появляться, куда можно идти работать;) Это как минимум…",
"кстати, если кто-нибудь с этим будет сталкиваться (парсить nginx логи), то используйте `GNU gawk`, (в нативном `awk` есть лимит на кол-во строк) -- работает молненосно
<https://opendatascience.slack.com/archives/lang_python/p1488884934005811>  типа вот так `gawk '{ print $1"",""$4 $5"",""$9"",""$10/1048576"",""$11"",""$7 }' $log | sed -e -e 's/\.com.*""/.com/' -e 's/""//' -e 's/\[//' -e 's/\]//'` ,  а то я сначала использоал `grep -o` и как в час по чайной ложке с ним :but_why_equity:",
"<@U1CF22N7J> так все таки, к тебе как к эксперту железа, сильные различия между xeon и i7 ?",
"<@U4J4RPAR0> Мне кажется `awk` лучше подойдет. Потому что во-первых: ему не надо передавать построчно как `grep`, и у него уже есть встроенные регескпы по которым можно взять что-то по позициям `$1 $2 ..`",
какие у меня перспективы сейчас есть в области конвертации не совсем простой модели на каффе на что-то юзабельное?,
"Гайз, заказывал кто комплектующие с <http://computeruniverse.net|computeruniverse.net> ?
Неплохой дисконт получается (вычет немецких налогов) на карту gtx 1080 ti : 53k у нас vs 43k у них + доставка. 
<https://www.computeruniverse.ru/products/90688354/evga-geforce-gtx-1080-ti-founders-edition.asp>
Пс. говорят к концу марта она у них появится.
Ппс. Думаю: связываться с ними или нет.",
Очень часто выбор определяется исключительно личными предпочтениями. По описанию задачи речь идет как раз о построчной обработке. Но многие предпочтут awk более простым инструментам.,
Я нашел реализацию attention которая вроде такая же как и в оригинальной статье  - <https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2>,
"можем по ОЧЕНЬ большому блату, исключительно как гостя из Минска, пустить. Ожидаем full house, предупреждаю сразу.",
"Ну вот и вопрос, как свое замутить)",
<@U1BAKQH2M> а ты в итоге разобрался как это делается? <https://opendatascience.slack.com/archives/nlp/p1485342831001436>,
"Один хрен, я попробовал несколько вариантов, в том числе как в статье. Качество на моем датасете не меняется от этого",
"Ну у меня проблема в том, что я только начал разбираться со всем этим и не могу понять как буквы засунуть",
И вот не могу понять как это сделать,
"Выкинуть керас, взять чистый фреймворк и сделать как хочешь.
Три входа: для слов, для букв, для меток.
Правда в этот момент становится понятно, что можно и в керасе это сделать: несколько входов.",
"тогда сложнее чуть, да, а как выглядит код просто для слов?",
"ну да я как раз сделал такой, но что с ним делать пока не додумался",
не знаю сходу как в коде выразить эмбеддинг для символов в таком случае ,
"раз уж начал спрашивать - а вот если у меня каждому слову надо приписать не один лейбл, а несколько, то тут можно как-то это обернуть или вариант, где заранее лейблы конкатенируются и сочетания как отдельные классы рассматриваются самый оптимальный?",
"<@U07V1URT9> слушай, а почему именно 40 линий так хочешь? тут встречались разные мнения по поводу того насколько это влияет. Ты сталкивался с проблемой какой-то может конкретной? Или чтобы расшириться проще было?",
Когда там уже подвезут 1080Ti?,
"а почему так критично такой быстрый винт? тормозило при загрузке и процессинге картинок? Может больше RAM взять и туда пихать все? Я так доебался потому что сам пытаюсь решить что брать, а нихера не понимаю в железе",
"1) Если нужны 40 pci-e линии, почему не посмотреть в сторону 6850k? Он подешевле будет. 
2)M2 накопители имеют свойство быстро перегреваться и значительно теряют в скорости буквально за первые 2 минуты. Так что если планируется какие-то нагрузки больше, чем использовать под систему, лучше задуматься над данным выбором или получше изучить данный вопрос для выбранного SSD. 
3) Intel 600p удивительно плох в тестах, я бы взял все-таки что-нибудь другое.",
"<@U3P08AARK> 
1. Я хочу брать инженерную версию, она где-то 32к стоит
2. Планируются нагрузки. А где можно почитать про перегрев и фиксится ли это радиаторами?
3. Например?",
"в зависимости от региона цены отличаются, но, как пример, plextor m8pe имеет неплохое соотношение цена-качество. А насчет перегрева, почитай <https://3dnews.ru/941477/page-2.html>. Тут и тест данного накопителя, и показан на примере троттлинг от перегрева. Каких-то конкретных решений данной проблемы я не встречал, даже у топовых накопителей такого форм-фактора те же проблемы. Немного помогает хороший обдув корпуса и тест накопителя на разных слотах m2 в мат. плате. А вот “модные” нашлепки-радиаторы от производителей мат. плат наоборот, только ухудшают ситуацию.",
"Всем привет! Говорят, в чате когда-то ссылочка была на статью, где было расписано, какого рода задачи решают современные нейронные сети. Типа `Изображения: сегментация, классификация, style transfer; Текст: генерация, etc.`.  Подскажите что-нибудь такое, пожалуйста.",
"Вдогонку вопрос: где можно было бы почитать про антипаттерны применения сетей? Например, мы попытались решить ими такую задачу, вышло плохо, объясняется тем-то, тем-то. А то куда ни сунешься везде расписывают, как классно сети решают разные проблемы, а про ограничения сложно найти что-то вразумительное.",
"<@U47L3B4JE> кто их напишет, это ж neural network мафия :slightly_smiling_face:",
"Может, такое есть смысл завернуть в какой-нибудь awesome-list, как  <https://github.com/ChristosChristofidis/awesome-deep-learning> вот эти ребята, как думаете? Чтобы сформировать общую картинку мира для тех, кто не может перечислить этот класс задач по памяти? :slightly_smiling_face:",
"Ну, как ни послушаешь разборы задач (kaggle etc), мало где их используют, только говорят, что пробовали, но не вышло ничего хорошего, а почему не уточняется. Разве что слышал один раз было предположение про неоднородность признаков. Пытался за это зацепиться, но ничего не нагуглил.",
"наверняка же можно их обобщить и предположить, почему для них сети плохо работают",
"А сети работают там, где много ""неформальной"" информации и нужно её творчески преобразовать -- картинки, аудио, видео, тексты обычным языком, всё это вместе",
"можно это обобщить, как задачи, где надо выделить метапризнаки?",
"Слушай тогда теоретические курсы, там тебе объяснят, что первый слой fc слой с сигмоидой в качестве активации разбивает линию на отрезки, второй - отвечает за объединение и пересечение, третий - дает любую логическую функцию. Обычно в любом хорошем курсе по конкретной теме говорят, что взлетает, а что нет и дают интуицию почему так.",
Где есть баш там есть и питон,
"<@U47L3B4JE> вот будет workshop почти на эту тему на CVPR <http://negative.vision/> - The First Workshop on Negative Results in Computer Vision. (Там, правда, про компьютерное зрение, а не про нейронные сети, но, думаю, без них не обойдётся). Только нужно дождаться когда засабмитят, отрецензируют и выложат статьи.",
"<https://opendatascience.slack.com/archives/C047H3N8L/p1489750760033503>
а есть вообще причины почему он API поменял? с какой целью, некоторые решения выглядят спорно (раньше было лучше)",
<@U36Q9NJMD> какую мамку посоветуешь кроме Taichi? ,
"<@U14L5TKNJ> да проскакивала такая мысль, только зачем одно апи? важнее чтобы было удобно и последовательно",
"<@U07V1URT9> ты же хочешь инженерный проц взять(и почему не xeon v3, раз уж QS + разгон?), там же и мамку возьми. Вообще гугли мамку + проц, который хочешь взять(и еще память тоже можно включать) и ищи, что люди говорят про совместимость. Это очень важно для инженерных образцов, так как заводятся не на ASRock""ах они плохо, много танцев с бубном( на эзроках тоже бывают танцы, но говорят, что меньше). И это больше связано с совместимостью с оперативкой, поэтому я и советую брать ту, которая протестирована на совместимость либо производителем проца+производителем мамки, они обычно пишут, либо оттестирована кем-то на форумах. Надеюсь не отбил желание брать Xeon",
"Да, я как раз погулил и безпроблемные только Asrock, похоже ",
"Есть ли у кого тезаурус синонимов русского языка? В интернете упоминается, как самый большой: В. Н. Тришин. Большой русский словарь-справочник синонимов и квазисинонимов системы на полмиллиона слов с синонимами. Вот этот или схожего размера.",
"А проц нашел на авито уже? Будь осторожнее, в Москве там ещё те барыги. Я её и посоветовал вместо обычной Extreme4. Просто не понимаю, почему taichi не достать, а  extreme4 достать за вменяемые бабки. У ритейлеров они примерно одинаково стоят.(И дофига дороже, чем за $ в амазоне)",
"Недостатки НС:
 - никто до конца не понимает как они работают (глубокие статьи на эту тему исследуют что-то типа двухслойных НС с линейной функцией активации), поэтому все усовершенствования по сути эвристики, которые внезапно могут перестать работать по той или иной причине. Но хуже всего то, то если НС работает недостаточно качественно, нельзя до конца понять почему, можно только применять методы из общих соображений (типа увеличить выборку, уменьшить шум, регуляризацию, снижать переобучение и т.п.);
- interference learning, нельзя так просто взять и обучить сеть обученную на задаче 1 для решения задачи 2. Но тут есть прогресс - <https://nplus1.ru/news/2017/03/16/deep-learning>
- adversarial examples, легко можно запутать сеть внося небольшие целенаправленные шумовые изменения в образ, причем человек этот шум даже не замечает. Это доказывает, что способность к обобщению у сетей несколько преувеличена.
- нельзя запросто увеличить размерность вектора признаков, придется переобучать или надстраивать еще один слой\классификатор поверх.
- НС вообще очень чувствительны к задаче, т.е. мало обобщают на похожие задачи. Мне приходится много заниматься распознаванием речи, так вот распознавание речи с ТВ или с записей телефонии (а еще хуже - диктофон) это две большие разницы. Вы можете например взять гугловскую (или чью бы то ни было еще) распознавалку и попробовать на диктофоне или телефонной речи в сравнении с речевыми запросам. Да, запросы замечательно, а остальное - шлак. И так везде.
- даже адаптация к подвыборке это некоторая головная боль, которая проходит почти всегда жестко. Да и сама процедура файнтюнинга и аналогов выглядит подозрительно.
- какие-то запредельные потребности в объеме выборки для достижения приемлемого качества. Например end-to-end распознавание речи хочет тыщи часов выборки (если не десятки тысяч). Мои коллеги со мной не согласны, но я так считаю, что человек учится распознавать речь максимум на сотнях часов (и то, не всегда ""размеченных""). Но уж точно, те сети, на которых language model строят, строятся на таких объемах текстов, которые человеку не прочитать за тысячи лет. Скажете, неявное знание, семантический контекст и т.п.? Ну м.б., но я чего-то сомневаюсь.  Вот и в кагле, видимо, НС не катят именно из-за малых выборок. Слабовата способность к обобщению.
- так они еще и тормозные. С другой стороны, это стимулирует микроэлектронную индустрию.

Если кому-то не захотелось читать стену текста выше, краткий вывод - НС говно.",
А какой алгоритм ML сеть обученный на задаче 1 можно  так просто взять и обучить для решения задачи 2 ? :thinking_face:,
"В том, что барыги на московских радиорынках дофига умные. И стоящие вещи за дешево не продают, в основном хлам или после ремонта. Я в Xeon v4 разбираюсь куда меньше, чем в Xeon v3, но есть общие рекомендации по тому, как определить хлам от не хлама: брать инженерные версии последних образцов, т.е. в CPU-Z это stepping 2, stepping 3 (но не 0 или 1, в среднем первые инженерники бывают действительно бракованные, но лучше конкретно гуглить по модели, серийному номеру, что у проца на крышке). Это еще если продавец совсем не зашкварился, а так могут вообще продать одну коробочку(а внутри там какой-то целерон похожего размера. Так что желательно просить теста, если цена 30к+. Приходить со своей мамкой, наверное, перебор, но я бы и так сделал)",
Как же все непросто :pepe_sad:,
"Если у меня есть два значение и я хочу показать, что одно в N раз больше другого, то какую визуализацию мне стоит выбрать. Логарифмическая шкала не подходит, потому что business people ее не понимают",
"Если в данных нет координатной или временной корреляции между признаками, что 95% задач, сетям на фоне других алгоритмов, как правило ловить нечего.",
Про расширение пространства признаков: почему бы не добавить нейронов на каждом слое и обучать только их веса?,
А cnmen какой у theano в конфиге?,
"Ребят, у Xeon-ов нет поддержки инструкций SSE или интел просто не пишет это, как само собой разумеющееся?",
"эх. а может подскажешь еще. если уже есть репозитарии у нескольких аккаунтах, то мне как их проще всего в организацию добавить? 
просто пусть каждый юзер добавит remote и запушит?
или лучше как-то из интерфейса гитхаба?",
"а как насчет кружочков-пузьірьков? там вроде легче воспринимается как раз когда  “грубая разница в разьі""",
"<@U43STE20Z> идея прикольная.. а сам инсталлянт лукера у вас есть? ))) я тут на днях спрашивал, где достать )))",
"шок в том, что я пропустил, так как думал это позже будет. Материалы/презы у кого-нить есть? кто участвовал - давайте в личке поболтаем?",
"А вот такой вопрос: вейвлетные разложения пытались совать вместо RGB картинок на вход сетям? Очевидная идея, но ни кто этого не делает, почему?",
"Там где раньше делали линейное PCA, сейчас есть сети, которые делают нелинейное PCA и т.п. :slightly_smiling_face:",
"А зачем городить лишние этапы процессинга-препроцессинга, если и так работает отлично?",
а где про это можно почитать?,
"Эм, даже трудно что-то посоветовать, любой курс/туториал по нейронкам описывает как кормить модель данными по частям",
"а если с фурье работать, то можно же как бы свертки на исходном изображении считать за линию в фурье спектре?.. О_о",
"Тогда каждую свёртку-фильтр надо делать размером как картинка, наверное по памяти плохо выходит. ",
зато перемножать то как быстро!11,
"ну и я тогда не понял, почему так все не делают)",
"Ну т.е. когда оперативки в карточках станет ""чуть"" больше так видимо делать начнут?",
А как они там оптимизируют? По ядру и каждый раз новый образ находят?,
"Как можно при бинарной классификации оценить вклад (оно же вес, значимость) каждого признака?",
а как ты tf устанавливал?,
И как это связяно с TF vs TH?,
"Кто его знает, может тут загружает один гиг, а теано 10.",
т.е. отслеживание watch nvidia-smi  + htop мне кажется должны показать где же оно реально крутится,
"может что-то не то, как оно патчи семплирует?",
"как тут уже писали, keras 2 сносит tensorflow-gpu по умолчанию, так что могло и на cpu соскочить. но если пишет о подгрузке куды, то вряд ли это твой случай",
"А расскажите какие вехи произошли в object detection за 2016 год?
Условно, после Faster RCNN, YOLO и SSD
Помню YOLO2
Или все, задача считается решенной?",
"Пользуется ли кто при анализе данных взаимной информацией? Если да, то зачем? Были ли случаи на практике, когда взаимная информация давала что-то, что не давал обычный коэффициент корреляции?",
"Пример, когда в нелинейном случае что-то сломалось?",
главное что для personal и academic целей делай что хочешь. зато как только начнешь за деньги чтото свое аналитическое на основе их данных предлагать - к тебе могут прийти,
"Вспомнил еще, что в EEG соревнованиях mutual information between channels заходил как фича.",
"я имел в виду крупняк типа imdb и прочих, на основе кого публикуются

если найдем тех кто прям ненавидит, будем насмехаться над ними и гнать их :feelsgood:",
"друзья, а в каких случаях в xgb стоить бустит линейными модели, а не деревья?",
"1. когда не научился готовить линейные модели другими средствами :good-enough: 
2. когда пишешь статью про бустинг в общем случае :kekeke: 
3. когда у тебя дофига времени и хочется понять чем будут отличаться разные линейные модели на тех же данных",
"в одном из контестов был случай, когда по умолчанию линейные методы в бустинге показали результат получше, чем деревья (типа 0.96 auc против 0.94). чтобы деревья были лучше (а они всё-таки били линейные), параметры надо было тщательно тюнить. но я с этим не справился)",
"Какой лучший способ делить на предложения русский текст? Есть ли какие-то открытые решения, которыми все пользуются?
У меня не соцсети, текст относительно литературный.",
"Копаясь сегодня в том, как бы использовать функан и теорию представлений в ML наткнулся на очень интересный диссер — <http://people.cs.uchicago.edu/~risi/papers/KondorThesis.pdf> 
Кто любит группы и теорию представлений — интересно будет почитать. Научруком у чувака был вроде бы глава ML нетфликса",
"Да, но бустинг линейных тоже линейная, не понятно почему такой процедурой должна выучиться лучше, чем просто учить линейную",
"результаты линейного бустинга и концептуально и практически похожи на elasticnet, но не везде с ними согласуются и от него отличаются (во многом изза сабсемплингов и процедуры отбора по какой отдельной фиче добавить кусочек линейки). если время на эксперименты есть, пробовать точно можно как альтернатива elasticnet линейнкам",
"на первый взгляд выглядит очень хорошо, даже как тьюториал",
"По каким ключевым словам копнуть на тему feature selection for clusterization? Гипотеза, что в датасете есть фичи, по которым он хорошо кластеризуется, и пачка совершенно ненужных фич, которые будут шатать евклидово расстояние и портить кластера. Хочется отделить хорошо кластеризуемые фичи от остальных.",
"Оно не будет работать, так как Jupyter - это non-GUI backend. Нужно искать другие пути, например через ipywidgets.",
а на какие сейчас я могу успеть? Посоветуй плиз (штаты),
"при ответе в тред лучше снимать галочку ""also send to""
Несколько отрезков с конца имеет смысл делать для стабильности оценки прогноза, т.к. в одном отрезке может странное попасться (типа праздников, или еще какой аномалии)",
"<@U0XR20SA1> Очень интересно! А думали как использовать в практике? Делали под заказ или собственный эксперимент? 
Возможно мой пример покажется не корректным, но всё же. В психологии есть методика ""не законченные предложения"". Когда человеку даётся начало предложения и просят дописать окончание. Есть примеры стандартных ответов, особенно для детей. Методика нередко применяется при диагностике стрессовых состояний, фобий и т.д. Последние проявляются через опреденные слова и фразы. Ваше решение может проводить сравнение со стандартом и выявлять слова и фразы по определённым категориям?",
"всем привет. А можете порекомендовать сервис для проверки/ремонта железа в Москве. У меня что-то не работает (матплата, бп, оперативная память,  процессор). И есть сильное подозрение, что это материнская плата, но сам я проверить не могу, так как нет компонент.",
"Я тоже хотел посоветовать punkt.english; на литературных русскоязычных текстах работал неплохо. Однако какой ""лучший способ"" -- не скажу, специально не исследовал вопрос.

Давным-давно делали с товарищами-студентами для домашки-конкурса свой сплиттер на регексповых правилах (и он всех победил). До сих пор лежит на гитхабе, но не уверен, что старый питоновский студенческий код -- это ответ на заданный вопрос : )",
У меня как раз лежит msi x99 xpower,
"mpavlov: Попробуй вставить всего одну планку оперативки в первый DIMM слот. Желательно другую найти, какую нибудь планку samsung, они менее конфликтные",
"Товарищи, может кто-нибудь обладает информацией где можно найти датасет рентген снимков с переломами?",
Антипаттерны возможно могут быть связаны только со временем обучения и большим количеством параметров сети.  Или там где требуется интерпретация результата понятная человеку. Теоретически нейронные сети могут решить практически любую задачу.,
привет! А есть способ в keras-е мониторить val_loss и автоматически останавливаться когда val_loss начал расти?,
"Скоро будет первый ClickHouse Meetup в Новосибирске. Создатель БД Леша Миловидов расскажет, про новшества и road map, а я расскажу про то, как аналитики в Яндексе используют ClickHouse для решения своих рабочих задач. <https://events.yandex.ru/events/meetings/3-april-2017/>",
А как зарегистрироваться на семинар по когнитивке? Для прохода нужен читательский?,
"привет) Не знаете, как получить из лазаньи матрицу обученных весов между двумя слоями? Про слои известно их поле name.",
"вобщем, получилось, спс.  
<@U14BPHDK6> и <@U17UDV31D> почему спагетти? типа использовать name это не ок?",
"кто знает, в связи с чем поменяли? выходные слои в Keras applications.inception_v3
было
```
if include_top:
        # Classification block
        x = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(x)
        x = Flatten(name='flatten')(x)
        x = Dense(classes, activation='softmax', name='predictions')(x)
```
стало
```
if include_top:
        # Classification block
        x = GlobalAveragePooling2D(name='avg_pool')(x)
        x = Dense(classes, activation='softmax', name='predictions')(x)
```
<https://github.com/fchollet/deep-learning-models/blob/master/inception_v3.py>
в пейперах не нашёл детального описания выходных слоёв, только inception blocks, но помню где то видел именно последний вариант
имхо он имеет ряд преимуществ",
"причем когда лазанья только набирала обороты, и ты вводил что-то типа lasagne layers в гугле было ну просто нереально найти то что надо",
<@U14BPHDK6>: а какое отношение лазанья имеет к спагетти? :nerd_face:,
как раз будет ночь с пятницы на субботу :youknow:,
"А кто знает, какие сейчас на питоне есть либы аналога pyssm (а то он какой-то неживой, похоже)?",
"Ребят, вопрос:
Почему для деревьев OHE не крут (особенно, когда много категорий), а лучше Label?",
"Да, и как раз в зависимости вся проблема. Иначе достаточно было бы с маргинальными разобраться.",
"т.е. просто дольше обучается? ведь если глубокое дерево, то норм?
Я так понял, что от реализации зависит еще.
Короче, пробовать и подбирать кодирование, как всегда",
"Неплохая мысль, но у меня нет разметки.
Я решил, что попробую NLTK для английского, попробую обучить Punkt на неразмеченных данных (хотя что-то не представляю, как это должно обучаться на неразмеченных данных) и попробую ту утилиту от Самсунга.",
"Привет, кто что знает и думает об <https://rasa.ai/>?  Это АПИ поверх <http://API.ai|API.ai> :genious:",
"Есть какой-нибудь rule of thumb относительно того,  насколько можно уменьшить исходный датасет, чтобы произвести подбор гиперпараметров (аля hyperopt и иже с ними) и не сильно отойти от набора гиперпараметров, оптимизирующих loss на тесте?  Понимаю, что все сильно зависит от данных, но вдруг у кого есть интересные мысли по этому поводу.",
"а зачем сжимать, ресурсов мало?",
"я так и не смог разобраться, как пайпить matplotlib плоты в binary, чтобы телепот не ругался, не сохраняя на диск",
Такой вопрос: положим есть сырые данные в виде действий пользователя. причем один пользователь может делать меньше 1 действия в день а другой может делать по 1к действий. Есть огромное желание этих пользователей кластеризовать. Подскажите пожалуйста как вообще подойти к этой задаче?,
"Не знаете, где достать образы для qflash, чтобы обновить биос на матери? На сайте гигабайта есть только exe файл",
"Я бы пробовал относиться к данным в задаче, как к тексту",
"Экономисты, посоветуйте, пожалуйста, что-нибудь почитать/посмотреть про экономику маркетплейсов? Волнуют вопросы типа как выравнивать спрос-предложение при помощи ценообразования и все такое. <@U1NLYUY5N> <@U0Q3USC0Z> <@U1LNBRZ29>",
"сегодня спросили на митинге неожиданно:)
- какая сложность запроса в монго?
- какова сложность merge sorting в монго?
- какова сложность perfect hash-tabling?",
"Да, Роше Тироль хорошие, еще есть <http://www.nber.org/papers/w11603> (по виду расчитано на English majors, а не хардовых экономистов, как все, что Тироль пишет)

surge это скорее dynamic pricing, что-то даже скорее в сторону operations research, N-sided platforms это про баланс между несколькими связанными сторонами клиентов

ну и на всякий случай <https://drive.google.com/file/d/0B3y6Efb4V73TdG9pV1F0VGpsM1E/view>",
"по сути, как я понимаю, это аналог tableau просто с рядом своих фишек. так?",
"Тоже интересен ответ на этот вопрос. Пробовала делать как написал <@U44J2H9K6> , но думаю должно быть более элегантное решение",
"<@U0FEJNBGQ> не пойму, как с телефона начать thread, отвечу так. 
Думаю, нужен либо lowpass, либо тренд от stl, например. Просто коэффициенты linear будут искажаться шумом ряда, особенно, если их брать за скользящий период какой то, а не на всех данных.",
"сезонности нет, тренд от stl не годится. 
глазами есть краткосрочные тренды, когда железка меняет режимы (например разгон), либо что-то постепенно нагревается или ползет (какие-то смещения осей) в одном режиме",
"Правильно я понял , что сейчас в москве 1080ti не купить? и кто знает когда можно будет?",
"тут отдельно что-то не вижу, только в комплекте, но это интернет магазины в мск, к которым я скептически отношусь, меня больше официальный дилер или магазин какой интересуют",
"Какая разница в производительности между 1070 и 1080? Побаловаться на кеггле ради. Вот думаю, стоит ли переплачивать. На Ti бюджета нет =(",
"1080 при выходе Ti как-то теряет нишу в DL: памяти столько же, стоит на процентов 40% дороже 1070, тогда как слегка подкопив получаешь фактически титан за адекватные деньги.",
"имхо, если ты покупаешь даже _одну_ карточку за $500 плюс сопутствующее железо, то отжалеть ещё 150-200 на нормальнный блок питания стОит, чтобы потом не кусать локти, когда китайской ноунейм за тридцатку всё это пустит по некоторому соревнованию на кэггле.",
"Добрый всем день! Есть вопрос про нормализацию. Есть обычный многослойный персептрон, обучаю его обратным распространением. Перед подачей выборки на сеть, для обучения, я провожу минимаксную нормализацию данных. Т.к. функция активации нейронов сигсоида униполярная (0;1) то и данные нормализую к этому диапазону. Но если я после обучения буду подавать реальные данные, выходящие из диапазона 0;1 то сеть конечно же выдаст запредельные значения и не правильный ответ. У меня на входе сети числа с плавающей точкой, а сама сеть классифицирует. Собственно вопрос в том, как быть с нормализацией? она помогает при обучении, но для использования сеть становится не пригодной",
"Т.е. вопрос собственно в том, как обучить сеть на сигмоидах, если тренировочные данные в больших диапазонах. Больше чем 0;1. сигмоида тут потому что используется алгоритм обратного распространения",
но когда я буду подавать данные после обучения. например 2 входа и я подаю 10 и 15. то я не могу тут провести нормализацию.и что подать на сеть не понятно,
так тебе нужно эти 10 и 15 точно так же как для тренировки нормализовать.,
"правильно применять одинаковую нормализацию, в принципе.
не вполне понятно, почему у тебя на входе сети не может быть 1.2 например.",
"меня просто интересует вопрос с значениями выходящими за предел..точнее меня интересовал вопрос как подавать данные на сеть, которая обучилась на нормализованных",
"Нет. Я занимаюсь сетями не первый год и даже есть пару приложения на c#. Но сейчас встала задача такая - сделать маленькую внешнюю библиотеку, чтобы другой человек мог использовать. ТАк нужна возможность обучать и использовать сеть. Вот обучить проблем нет, а потом я подумал как человек будет подавать значения, и сеть выдаст не правильный результат. И решил узнать что делать в таком случае",
а поговорить об этом не с кем было,
"Можете порекомендовать среду или онлайн-сервис для рисования графиков? Нужно рисовать графики функций сложности примерно как |y| = |x| (но разные части графика красить разным цветом). Неплохо было бы, чтобы он сохранялся в векторной форме (но не обязательно). Нужно добавлять также всякие надписи на график.",
"Используют то, что лучше работает. Почему бы и не сигмоиды",
"Я думаю, что в целом понятен use-case, но все же уточню: работаете вы над какой-то задачей (коллективом или самостоятельно) и хотите иметь некоторую витрину-табличку по всем парам модель и метрика чтобы выбирать какая лучше когда нет одного критерия",
"Ну и да, если знаете куда меня послать с этим -- скажите) Задачка возникла в контексте DL хотя понятно это не по адресу",
"Насколько реалистична идея использовать одну и ту же модель (обученную один раз) для предсказания одного и того же процесса, но в различных регионах?Например, имеется 10 областей (географических) и в них мы хотим предсказать кривую спроса. Сезонность данных, предположительно, одинаковая, тренды различные. Хотелось бы обучить на данных из 3 из 10 областей и использовать в остальных. В сторону каких моделей лучше смотреть?",
сезонность и тренды включить как фичи,
"а хотя обычные линейные же тоже подойдут, как мне кажется",
А расскажешь почему не зашло?,
"FGMachine гоняется в докере, а надо было уметь раскидывать эксприменты по нескольким тачкам с несколькими GPU, с этим возникли сложности.
Таких подробностей не знаю, там можно допилить, проект рабочий, но еще свежий.
Скажем Суматра как средство регистрации результатов оказалась еще печальнее",
"как вариант, кластеризовать 10 областей на сколько-то групп, дальше обучать и использовать модель на данных этой группы - для каждой; ну или использовать принадлежность к кластеру как фичу",
"Возможно немножко оффтоп, но никто не подскажет, какие сейчас актуальные проблемы у разработчиков беспилотных автомобилей? В области обеспечения безопасности, в особенности.",
"судя по хабру, самая актуальная проблема сейчас, это кого должен убить робот - трёх негритят или престарелую хозяйку автомобиля?",
странно что это вообще кто то рассматривает как проблему,
"<@U06J1LG1M> интересно твоё мнение. Я вижу это как аукцион по типу показа рекламы в Интернет. Страховые фирмы делают свои ставки, кто проиграл, того и убили. Экономисты потом докажут, что так эффективно",
"как обычно же, в любое время с 9:30 до 12:00",
npetrenko97: а тебе для каких целей вообще? ,
"вообще, непонятна суть вопроса. автопилот должен защищать хозяина, без вариантов. потому как никто не будет покупать автопилота, который чуть что, сделает из хозяина отбивную.",
"Поясните пожалуйста разницу между optimistic и pessimistic bias (""bias"" тот, что имеется в виду, когда говорят bias-variance trade-off)",
"Это когда ты сильнонелинейную функцию приближаешь линейной, например",
"Да это же как раз последнее поколение зионов, которые стандартно гонятся, обычными матерями, вроде за это их и любят",
"А можно и вообще просто - подать в 10 вузов на CS, в 10 на EE - а там уже разбираться куда хочется после того когда понятно куда взяли.",
"1. Как понять, чем занята память на GPU ? Делаю finetune на vgg16 c отрубленной головой (без двух последних fc)  Через несколько итераций из gpu памяти остается порядка 100Mб (из 12Гб), причем никаких внешних shared vars кроме весов vgg16 без fc явно не создаю.  
2. Как посмотреть, сколько памяти требуется theano для весов и промежуточных вычислений? Кажется для весов достаточно посмотреть на вывод `np.sum([p.size.eval() for p in lasagne.layers.get_all_params(&lt;last_net_layer&gt;)])`, а как посмотреть, сколько нужно для промежуточных вычислений ?",
"почему же так часто логи с временными рядами мешают. из логов можно сделать ряды, например, считая определенные события за интервалы времени",
"достаточно широкая область, читать не на одну неделю будет. Что именно интересует, какая прикладная задача решается?",
"я не знаю точно как theano управляет памятью inputs, но регулярно наблюдаю, что после вызова функции остаётся занятая память",
optimistic и pessimistic bias  - а где такие термины вам встретились?,
Как обычно называют Fold или KFold по-русски?,
"А можешь подсказать, пожалуйста, как из theano.shared сделать нечто, чтобы можно было подать в lasagne.layer? Как то неочевидно и документации такого использования нигде не могу найти",
"<@U1D4RRA7K>  USB 3.1, как я понимаю",
"Кстати, если гнать этот ксеон потом, то кулер может быть маловат (правда это я как :sofa_warrior: )",
"она как таичи, туда 3 карты только влезет (",
еще вопрос: какую термопасту сейчас модно брать? :troll:,
"&gt; Теоретически, это задача распознавания образов (поправьте пожалуйста, если не прав)
Это задача детекции объектов

Из всех ответов ниже, самый понятный это faster rcnn, так как есть давно всеми пользуемая авторская библиотека на python",
а где бы найти датасет с открытыми данными по такси? (можно не в России),
"А какие есть инструменты для того, чтобы структуру нейронной сети нарисовать, но так, чтобы все было красиво и не стытно вставить в перзентацию / статью",
Привет! А кто умеет собирать BigARTM? Что то у меня под Ubuntu 16.04 все время загадочный c++: internal compiler error: Killed (program cc1plus) происходит...,
"Народ, есть у кого Яндексовый вариант ""Введение в информационный поиск"" Кристофер Д. Маннинг, Прабхакар Рагхаван, Хайнрих Шютце?",
"ну я имел ввиду логрег и свм, я не знаю как работает ваббит",
"это как в рекламе: “Папа, а существует какой-то еще другой бустинг кроме xgboost и lightgbm?” “Нет, сынок, это фантастика.” или уже можно сказать, что часть истории ml :slightly_smiling_face:",
"<@U040HKJE7> это как то странно( изначально я конечно совсем не так понял суть вопроса и думал речь про One-hot vs label encoding, но вот про то что линейные модели в целом лучше приспособлены под категориальные чем деревянные, этого я не понимаю, откуда такая интуиция/знание?",
"а вот почему деревья не очень приспособлены для категорий и OHE - писали выше (и за последние пару дней вопрос еще раз всплывал). проблема в том, что алгоритму значительно сложнее эффективно строить сплиты",
начиная с ANOVA в какой книжке?,
в том смысле что начиная с и далее в какую сторону?,
"пытаюсь выбрать, где лучше объяснят :slightly_smiling_face:  может у <@U04423D74> и <@U0Q3USC0Z> есть идеи хороших книжек 
на курсе по статистике на stepik точно про работу с категориями будет: <https://stepik.org/course/Introduction-to-Statistics-701/syllabus>",
"всем привет, помогите советом, есть корпус слов и выделено, какая из букв ударная, хочется на этом обучить сетку предсказывать ударения для слов.
я так понимаю, что надо использовать character-level LSTM, но не очень понимаю, каким должен быть аутпут сети, чтобы это работало для слов разной длины?",
"Хотя, как по мне, все топовое железо это петушня с светодиодами, оверклокигном, extreme 66.6, power и т.д.",
"быть может кто-то все-таки сможет разъяснить следующий вопрос: почему линейные модели, конретно СВМ и логрегрессия лучше работают с категориальными признаками чем деревья? во-первых что значит лучше? во-вторых для свма же весьма критичен вопрос где конкретно проводить разделяющую гиперплоскость, а не просто какие наблюдения оставить по какую от нее сторону, а ведь для категорий это совершенно неразрешаемо, на них ведь нет явно заданного порядка. Просто кажется что вот в элементс по этому поводу несколько иное мнение чем у <@U040HKJE7> или я просто что то не так понимаю(((",
и вот эта вот свинья <@U065VP6F7> может уже проснуться и не просто ставить плюсики где попало,
"попробуем-с на пальцах тогда:

1. лучше = обобщаяющая способность выше, вероятность оверфиттинга ниже. на пратике аналогично тому, что построил на одних и тех же данных дерево и glm - на тесте glm и лучше, и падение целевой метрики по сравнению с трейном заметно меньше чем у дерева.

2. как выглядят категориальные признаки, попадая в линейную регрессию? почти всем уровням (кроме одного) назначается свой коэффициент поверх OHE (оставшиеся “ненужные"" уровни со всех категорий уходят в intercept). процесс нахождения этих коэффициентов тщательно отлажен, есть хорошие регуляризации. линейные модели успешно строят по миллионам (карл!) признаков, включая и многоуровневые категориальные. например министерство образования штатов строит линейные модели успеваемости студентов с миллионами коэффициентов именно на категориальные признаки и их взаимодействия (вплоть до школы\учителя\класса, по всей стране). да и хорошие Байесовские регуляризации чисто на оценки в уровнях тоже есть, для случаев высокой кардинальности ~ малого числа точек на уровень (если уж совсем подстраховаться)

3. в деревья категориальные признаки попадают аналогичным образом. но когда начинается построение сплитов, успешно искать среди множества уровней~OHE нужную комбинацию становится затруднительно. перебор долгий, а число остающихся в сплите точек с каждым уровнем глубины существенно уменьшается. мысленный эксперимент: если в задаче будут только категориальные признаки, деревья будут искать только их OHE комбинации-interaction-ы. и они не будут покрывать их всех, как линейная модель. у линейной модели будет (почти) на каждый уровень по коэффициенту. у деревьев будет произвольное комбо с пачки взаимодействий уровней (значительно сложнее оценить грамотно - их мало а такие взаимодействия устойчивы настолько насколько много в них осталось точек).",
"Про то, куда теряются уровни: <http://stats.stackexchange.com/questions/26539/r-linear-regression-categorical-variable-hidden-value>",
"можно и сказать, что потому как расширяется про-во признаков, что повышает дискриминативную способность классификатора. Слухи о curse of dimensionality сильно преувеличены особенно в случае данных линейных моделей",
"это дико крутой материал. как <http://www.r2d3.us/visual-intro-to-machine-learning-part-1/|www.r2d3.us/visual-intro-to-machine-learning-part-1/> 

а еще такие крутые штуки не встречались?",
"мне кажется как раз вдохновлен тем про то что ты скинул. есть еще setosa, но там чуть другое",
А когда еще по питону встречи намечаются?),
"Да, там дисперсия растет экспоненциально. (Для самого хорошего случая ARIMA - как квадрат)",
<@U36Q9NJMD> а где можно о подобных фактах чего-нибудь почитать?,
":book: 
Подскажите пожалуйста понятный ( =не требующий существенной мат. подготовки) учебник по ТВ, где бы освещая подробно метод максимального правдоподобия.
Прочитал Вентцель, но там не было.",
"<@U24MQAZ9P> это простейшие задачи для курса эконометрики временных рядов. Сильных доказанных утверждений по этой теме я не видел. Но на уровне - пусть истинная модель имеет вид ..., тогда ... Очень много. И у меня складывается мнение, что на практике xgboost с интересными фичами/LSTM дает куда лучшие результаты по метрикам, чем что-то сильно обоснованное теорией.",
"Немного не точно выразился. Отправил скриншот в личку, хз как в тред прикреплять.",
"Такой вопрос. Есть данные одной компании, у нее есть множество различного рода отделений по регионам за последние 5 лет. Нужно построить предсказание объемов продаж по месяцам. Итого есть сагригированные по месяцам значения продаж, есть грубо говоря число окошек в каждом регионе, через которые осуществляются продажи, есть исторические данные по разного рода макроэкономическим показателям: ввп региона, средние зп, население и тд, даже с прогнозами на необходимый горизонт(условно 3 года)
Мне кажется это очень стандартная задача. Хотелось бы узнать как это обычно делается, сам я умею только в базовые одномерные аримы-саримы ну и стандартное машинное обучение. Стоит ли на эту задачу смотреть как на задачу прогнозирования многомерного времянного ряда, или это обычный времянной ряд с кучей новых признаков, либо вообще лучше взять месяц и год как категориальные фичи и забыть что это был времянной ряд
В общем есть ли что-то вроде примеров решения чего-то такого, желательно на питоне.",
"Я немного посмотрел профет, как я понял там же нет внешних регрессоров и WARMA - а просто ариму я и на statmodels делать умею",
А какие сейчас лучшие с точки зрения и удоства и популярности - библиотеки или проги для построения байесовских сетей?,
"Кто-то игрался с текстами отсюда (40ГБ либрусека): <http://russe.nlpub.ru/downloads/> ?
Уперся в одну проблемку при конвертировании текстов в токены, иногда склеиваются слова и не понятно как этого избежать",
"Сходу сложно понять, как замиксовать skipgram (пословный ведь?) и crnn",
Читал тут статью про новый способ metric learning <https://arxiv.org/abs/1703.07464>. Если кто-то читал расскажите как они выбирают proxy points? Просто случайные точки на сфере? Или их тоже учат через embedding? Ничего не могу найти про выбор этих точек в самой статье.,
"И про память, думать о совместимости нужно только, если покупаешь ECC REG память, она реально не вся заводится. Смысл её покупать есть, если модели будут считаться не одни сутки, тогда сбоев различных должно быть меньше, ну и запас прочности у неё больше в теории. Если покупать обычные плашки, то разницы никакой нет, какую брать. Но многие производители гонят память за счет увеличения таймингов, вообще не очевидно, что более разогнанная память таким образом дает прирост, особенно там, где нужно динамически постоянно что-то подгружать туда-сюда.",
"модели перехода между состояниями обычно представляются в виде МО + случайных шум, где МО соответствует, например, нейронная сеть, а шум - это какое-то параметрическое распределение с нулевым матожиданием",
"Здесь вроде много раз поднималась тема, какие должны быть спеки для deep learning пеки ? Кроме поста Tim Dettmers'a, есть еще агрегации?",
"Есть ли системы водяного охлаждения, когда 4 1080ti стоят?",
"Описывается два способа назначения координат.
Первый - статический. И тогда, как я понял, координата прокси это просто one hot encoding от id класса. Позитивному и негативному семплу батча назначаются координаты прокси, соответствущей их классу.",
В динамическом методе за проксями не закрепляются семантические метки. Как выбираются координаты прокси - ни слова.,
"вроде можно магистраль через всё, какая разница сколько их там будет
главное жижу в банке подсветить :kekeke:",
как раз таки в простонародии (местном) - кениг,
Я бы чо-нибудь рассказал :slightly_smiling_face: У меня как раз брат туда переезжает 1 апреля,
"я что-то не понимаю как они определяют proxy через distance, когда distance и оптимизируется с использованием proxy",
"кажется (но это не точно), что они начинают с d=l2, фигачат условный k-means на обучающей выборке и выбирают proxy как центры кластеров",
"<@U36Q9NJMD> а на m2 тут ругались, как бы не ты сам, что греются очень, что-то изменилось или я туплю.. :pepe_sad: ",
"Мне кажется прокси выбирается не из эмбеддингов семплов из данных. В уравнении 5 написано только как выбрать прокси для семпла, но набор проксей уже предефайнен и обозначен P.",
"Ну если с заделом на апгрейд до 4х ti — ладно.  Но так как есть там оверкил по матери, оверкил по жестким дискам (но это субъективно, ладно), но при этом дико мало оперативы. Дисбаланс какой-то.",
"Запихай его себе в очко в следующий раз, когда решишь посетить рф",
"После истории <@U041P485A> как два титана блэк умерли от протечки, есть",
Я как раз хочу узнать как решаются такие вопросы,
А почему бы просто не гасить сервер при перегреве?,
"<@U1D4RRA7K> память добавляется по одной планке, мне лень добавлять ещё, да и цены там чуть завышены на многое. Насчет всего остального сборка рассматривается как сервер для небольшой группы исследователей/небольшого dl стартапа. Из-за этого всё решения с хранением данных на уровне Enterprise, потому что терять данные это очень больно. Насчет материнки - серия хорошо себя показала. И собирать кластеры из таких мамок просто сказка и выходит намного дешевле, чем покупать все неинтегрированное, та же сетевая карта такого уровня обойдется очень недешево.",
"понимаешь как выучиваются, если d от θ явно не зависит?",
Примерно понимаю. А почему d от  θ не зависит? d ведь через нейронку считаются. А прокси - тоже параметры сети. Например в керасе статическое назначение можно через Embedding layer реализовать.,
"<@U07V1URT9> ничего себе в какое порно тебя потянуло. 
<@U041P485A> а кто делал водянку? ",
"Сварить/спаять коробку из оцинкованной жести, герметичную, куда поставить системник. Протечет -всё в этой коробк, как в поддоне, соберется. Загорится - будет как мусорный бак на помойке гореть внутри.",
И еще вопрос -- а где самое узкое место в диплернинге? Доступ к обычной ram?,
ovchinnikov: го в треды. А сейчас какая карта стоит?,
"Написал небольшой пост как профайлить tensorflow - возможно кому-то будет полезно. <https://medium.com/@illarionkhlestov/howto-profile-tensorflow-1a49fb18073d> .
Как обычно английский не родной - есть замечания - пишите)",
"какая страна, контекст, что значит зарезервировала?",
Есть датасеты где реально есть WAV-файлы?,
"не, я к тому, датасет из wav в что ты хочешь? Потому что просто wav как бы много где можно взять, есть сайты всякие И так далее.",
"а для каких задач, если не секрет?",
"А можешь подробнее про ""каждому прокси назначается метка класса"". Как вычислить координату этой точки?",
"Я понял, у всех идеальный результат и вопрос как быстрее до него дойти",
"Вот такой вопрос в разрезе задачи на рыбки.

У вас есть картинки с рыбками класса Акула, Тунец

Метрика в задаче logloss по классам Акула, Тунец, other

Если на картинке есть один тип рыбы, то другого быть не может.

Что лучше брать как loss function:
sigmoid по классам (Акула, Тунец) или softmax по классам (Акула, Тунец, other) ?",
"Вот такой еще вопрос - если вы использует pre-trained сеть, чтобы фич надергать, как я понимаю, нормализация не обязательна?",
"Как обычно, балансировать классы итд",
"Есть сеть натренированная на ImageNet. Если мы хотим чисто фичи поизвлекать - надо ли вычитать такой же mean, как был в данных при тренировке этой оригинальной модели?",
"Я свой mean считал, когда датасет был достаточно большой",
"И он сильный, как показывает финальная классификация",
"Каким образом это может быть ""не хуже"" - мне непонятно",
А кто нибудь может будет на конце в Чикаго в апреле ? thedatascienceconference,
"А расскажите какие кто делал эксперименты по архитектуре и процессу обучения, особенно если вы верите что они были валидными.
Вот, например.
БатчНорм. Я понимаю что она сейчас много где используется, но вот я никогда не воспроизводил экспериментально, чтобы сказать как она влияет на итоговое качество, на скорость сходимости, на память и время работы модели в тест режиме (даже часть этой информации была бы полезна)
А есть же еще куча нормализаций. По идее хороший подход в серьезной компании сделать бенчмарки по всем вариантам и иметь табличку. Но это ж дофига вложения ресурсов. А в статьях, как я понимаю, сразу несколько сильных байесов",
"Вот был пейпер, где кучу всякого препроцессинга для CNN попробовали",
"запутался в трех соснах: нужно натренировать лстм, который каждой букве в слове дает вероятность, что она ударная.
то есть размерность Х (количество слов, макслен слова, OHE буквы)
а y (количество слов, макслен слова, бинарный признак ударная ли буква)

подскажите, пожалуйста, как правильно это передать керасу?",
"да, это-то как раз я понимаю)
вопрос в аутпуте сети? какой он должен быть размерности?",
"Я вот хочу понять почему не стоит, понятное дело что не получится оптимизировать градиентным методом. Но по идее разве CRF не куча связанных случайных переменных?",
"ну вроде как softmax размерности maxlen, не?",
"Я из этих оригиналов, где про это можно почитать?",
"Но я так и не разгуглил отчего и почему это, потому что все запросы ""фреймворк занимает не всю память"" выдают ответы на вопрос ""фреймворк занимает всю память"", как я не пытался сформулировать",
"я нашел вот это, думал может кто решение знает: <https://devtalk.nvidia.com/default/topic/901646/-980-ti-windows-10-cuda-7-5-out-of-memory-after-allocating-4-5-out-of-6gb/>",
"<@U298B1T19> добавь обязательно masking, так как у тебя будут появляться нули, если слово меньше максимальной длинны, которую ты выставил ",
"так правильно, надо сделать паддинг, но нужно еще и masking. Когда ты делаешь макскинг, то в лосс-функции не учитываешь эти нули",
Но всякие gpu-z не показывают эту память как занятую,
"Во вчерашнем объяснении <@U040HKJE7> по поводу категориальных признков мне подозрительным странным утверждение
&gt; министерство образования штатов строит линейные модели успеваемости студентов с миллионами коэффициентов именно на категориальные признаки
Я правильно понимаю из объяснения (и из поста, к которому был направлен этот комментарий), что люди успешно тренируют SVM/NN на датасетах с &gt;10000 категориальных признаков? Как же проблема размерности и проблема редко встречающихся категорий? Точность моих NN-моделей здорово падает уже после после пары десятков разбитых на one-hot категориальных переменных. Я освежил свои знания в этой области препроцессинга датасетов, и обнаружил, что существует не так уж много способов скормить категориальные данные NN:
0) (дефолтный) Просто закодировать каждый класс своим числом. Работает плохо.
1) (дёшево и сердито) One-hot. Разбить признак, в котором K классов на вектор из K элементов. Если признак имеет k-тый класс, то в векторе k-тый элемент равен 1, а остальные -1/K. (одна фича превращается в K, большая часть из которых равна -1/K)
2) Бинарное кодирование. Класс представляется вектором своего бинарного представления (одна фича превращается в log2(K)) 
3) Полиномиальное кодирование. Какая-то странная штука.
4) Deviation и helmert кодирование. Сами по себе работают не очень, так как сохраняют лишь информацию о необычности класса, но их можно добавить к любому другому кодированию.
5) Кодировать подпространства признаков. Если категориальных признаков много, то можно попытаться найти кластеры в их распределении, и кодировать их, вместо каждого отдельного признака. Можно воспользоваться как автоматическими методами (кластеризация, генетические алгоритмы), так и априорным знанием о датасете. 
6) NN Entity Embedding - каждый категориальный признак, разбитый на one-hot, сначала предобрабатывается своим суб-слоем нейронной сети, который преобразует его в полезное численное представление, а только затем подаётся в остальную NN. (<https://arxiv.org/abs/1604.06737>) Суб-слои можно предобучить, скажем, автоэнкодером.

Вопрос честному народу: чем из этого пользуются гуру ML? Есть какие-то другие методики, как обрабатывать датасеты с большим количеством категориальных данных?",
"Кто разбирался с svgd, почему медианное расстояние там такую центральную роль играет? Его же дорого считать довольно(сортировка)",
Почему не забить и использовать среднее например?,
"0. как правило, это не работает, но бывают случаи (смотри :thread-please: )
1. дефолтный вариант, линейные модели так и работают (читай комментарии выше)
2, 3, 4. ?
5.  звучит клево :eyes:  если считать принадлежность листу дереву - категориальным признаком (смотри xgboost :xgboost: predict_by_leaf) - это и будет кодировкой “по построению”. по факту, если данные только категориальные - смотри предыдущее обсуждение про поломки деревьев. 
6. такие эксперименты существуют. даже существуюет пара историй успеха на kaggle (и не меньше, а то и сильно больше неизвестных, не взлетевших историй). 

Теперь про способы замены:
А) OHE - самое очевидное, классическое и с чем научились работать. Особенно применительно к линейным моделям - смотри регуляризацию и Lasso, Ridge, а также например вещи вида Glinternet. Про школы - найду ссылку позже. 
Б) Как уже верно сказали, замена на среднее и его робастные версии - крайне успешная частая альтернатива. Можно даже по ответам в этом канале от <@U054DU76Y> <@U054CQTS7> и <@U04422XJL> найти конкретные рецепты (а еще Стас неоднократно на тренировках это показывал и рассказывал)
В) Другой популярный вариант - замена уровня на его относительную популярность. Это альтернативный полезный признак, который в некоторых задачах очень помогает отобрать нормально-популярные уровни и редкую фигню (и деревья уже сами разберуться что делать в каждом случае)
Д) Обобщение 6-го пункта, которое иногда заходит - всякие проекции (SVD TSVD, всякие матричные факторизации) поверх OHE. Это будет осмысленно если категориальных переменных много - поверх одной переменной OHE матрица уже ортогональна и вся информация уже в ней. 

P.S. не знаю, релевантно ли, но неоднократно встречал предвзятое мифологическое мнение, что “нейросетка может все”. и ощущение что она обязательно обыграет линейные модели. миф лечится сталкиванием с практикой и периодическим напоминанием про бесплатный ланч
P.P.S. :thread-please:",
"На нейросетках свет клином не сошёлся, но если задача действительно под них подходит, но в датасете встречаются категориальные данные, надо же знать, как с ними разбираться.

&gt;&gt; Другой популярный вариант - замена уровня на его относительную популярность...
Это вроде и есть +/- deviation и helmert кодирование",
"Пример когда 0 ~ числовое кодирование может сработать. 

Возник на практике изза рукожопости - задача была построить фильтр типа антифрода. Данные были и категориальные, и вещественные - полный фарш. И была важная фича - локация (допустим, название города). По-случайности конвертнул ту категориальную переменную в числа, и бустингом удалось неплохо ее использовать для предсказания. Точность хорошая, все норм. А когда баг нашел, и сделал нормальное OHE - получалась какаято фигня более низкого качества, и вообще с деревьями все было плохо.

Что на самом деле поймала модель? В общем случае, фильтр агрился на редкие непонятные локации, и выучивал нормальную модель для популярных локаций - Москвы и Питера. В случае когда это было просто число, были сплиты, отделяющие эти 2 локации-номера от всех остальных (не очень точно, но в целом разумно, поняли это по Partial Dependence Plot-ам). И много деревьев, зацепившихся за эти сплиты. А в случае OHE, эти важные уровни из OHE попадались деревьям не всегда, зато было очень много левых уровней, за которые деревья цеплялись.

Чтобы качество стало хорошим, несколько топовых локаций были вынесены в свои бинарные  (аля OHE) переменные, а прочий хвост побился на еще 3 уровня редкости-популярности. Тогда я не знал про рецепт “В” из своего списка, но по сути, для задачи нужен был он, ну и из ~рукашек и говна~ веревок и палок тоже по сути сделали чтото похожее на него.",
"<@U3WAPHM5X> :+1: 
А еще поучительный пример про то, когда кодирование числами по-случайности может сработать - в :thread-please: сообщениях :eyes:",
"На большом количестве категорий как раз именно линейные модели очень хорошо заходят, можно смотреть, например, кликовые соревнования на кагле, где с  помощью ftrl очень неплохой скор можно было получить",
"В чат призывается <@U3KTCHLUD> 

Если у кого есть вопросы о том, как учиться девушке в STEM, да и вообще, как поступить в аспирантуру в США =&gt; задавайте.",
"Гайз, а в каких магазинах в мск нормальный выбор/цены?",
"Привет всем, сразу скажу, что я начинающий и вопрос может быть совсем простым, может я просто придумываю себе костыль)

Как в таких пакетах как xgboost или sklearn (допустим randomforestclassifier),  поставить оптимизацию алгоритма по количеству верных предсказаний определенной класса.

Дело в том, что при классификации алгоритм оптимизируется по среднему значению точности и полноты. При этом учитывается предсказание всех классов.

Но мне нужно другое. допустим мои ответы имеют бинарное значение 1 или 0. И мне нужно предсказать именно 1. И как можно точнее. 
Например: Если алгоритм предсказывает 0, а там была 1, это не страшно, за это не штрафуем. но вот если он предсказал 1, а там 0, то за это штрафовать. 
По сути это свой score, но как его внедрить в алгоритм, или может уже есть готовое решения (на примере смещения  f меры)?",
"Можно оптимизировать как обычно логлосс, но смотреть на то, как ведет себя нужная метрика",
"выглядит как threshold логрега выкрученный в сторону, где он дает больше нулей, чем единиц. В мануале <https://xgboost.readthedocs.io/en/latest//parameter.html> есть `error@t`",
"да, эта ""кастом"" метрика есть в более полном виде (где уже учитывается количество верных предсказаний). могу даже код дать, но вопрос в том, как это внедрить в эти пакеты) я сделал предположение, что с подобной ""задачей"" ни один я сталкивался, так-что может уже есть готовое решение и нужно всего лишь написать верное заклинание)",
"<https://github.com/dmlc/xgboost/blob/master/demo/README.md#features-walkthrough> в родном мануале, где ""Customize loss function""",
"еще надо, чтобы она дважды дифференцируема была, а на мануал, как встроить выше ссылку давали",
"не совсем понял по документации, как тогда должен получиться запрос. gbm.fit(ROCtrainTRN, ROCtrainTRG, eval_metric='error@0.2') картину не меняет вообще.",
"В целом я бы советовал поучиться, если еще не совсем осточертело. Один ШАД - это мало, там не настолько много учебы, как в принципе и почти любая магистратура (кроме наверно Сколтеха).",
"ну я знаю очень крутых вроде людей, с хорошим базовым образованием, не поступивших попыток с трех, планировать свою жизнь, ориентируясь на результаты хардкорного экзамена+собеседования - очень так себе идея, это скорее надо рассматривать как крутой бонус вида ""а вдруг получится""",
"Кстати, а что скажете про магистратуру в сколтехе? Особенно по анализу данных. Сильная/нет, как с поступлением и т.д.",
"блин нет это тот же самый json.
когда в нем меняю theano на tensorflow - начинает ругаться что нет такого бэкэнда",
так а зачем он мне,
"Не совсем понятно - если deadline в декабре, то как считать GPA, если диплом выдается весной? ) Или тогда и поступать надо в 2018?",
вот на гарантию там как раз жалоб много,
"Спасибо. 
А как вообще отличаются критерии при поступлении в условный ТОП (Стенфорд, Берли, КалТех) и условное более ""доступное"" (Дэвис, Санта-Барбара)? К примеру если GPA ниже 4.9 то в ТОП не попадешь или Motivatoin Letter должен быть особо проникновенным, может порог по баллам GRE)",
"<https://rpubs.com/bradleyboehmke/weather_graphic>
Доклад B.Boehmke рассказывает о том, как при помощи ggplot2 (пакет для визуализации данных на языке R) можно воспроизвести на своих данных классический график из книги Visual Display of Quantitative Information, 2nd Ed. (стр. 30) by E. Tufte.
В этом проекте во всей полноте раскрываются возможности ggplot2, основанного на принципах Grammar of Graphics (<http://vita.had.co.nz/papers/layered-grammar.pdf>).",
"Ну вот ты шутишь, а я eSXI скачал, когда совсем ушатают нейронки, попробую.",
"Я давненько от этой темы отошёл, но когда более-менее плотно этим занимался, там как раз это научились делать",
уже 2-ой день пытаюсь разобраться в этом примере. можете пояснить что там происходит и как мне увязать это со своим кодом?,
"прям с самого начала, зачем мы заворачиваем данные в xgb.DMatrix?",
"просто не понятно, если это единственный формат который понимает xgb, почему тогда работают такие команды ```
gbm = xgb.XGBClassifier(learning_rate=0.05, max_depth=3, min_child_weight=1, n_estimators=300)
gbm.fit(X_train, y_train)```",
"может посоветуете где почитать, посмотреть?",
"продолжаю работать с примером <https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py>

можете пояснить что происходит внутри функции 
``` def logregobj(preds, dtrain):
    labels = dtrain.get_label()
    preds = 1.0 / (1.0 + np.exp(-preds))
    grad = preds - labels
    hess = preds * (1.0-preds)
    return grad, hess
```
не совсем понимаю как влияют grad, hess",
"Может кто подсказать конкретные кейсы использования nlp? Ну то есть не просто, что сентимент анализ может помочь оценить эффективность( ну да, да эффективность не совсем корректно) маркетинговой кампании, а например такая-то компания использовало для того то, получилось то то. А то я кроме как одной статьи на медиуме и про орео и геев ничего не нашел) ну еще SfiwtKey",
"а неправильно сформулировал я) У них сам продукт на nlp построен. Я имел ввиду, где не nlp это все-таки не основная деятельность",
а к чему вопрос? что-то конкретное интересует? везде где есть текст - можно использовать NLP,
<@U4E1EF5CZ> я имел в виду про случае в конкретных компаниях.  Как это возможно применить то я понимаю. Я искал что-то вроде этого <https://medium.com/xeneta/boosting-sales-with-machine-learning-fbcf2e618be3#.cr60ssq8w>  то есть Xeneta автоматизировала таким образом поиск подходящх клиентов.,
"Подскажите пожалуйста, где можно найти датасет с фотками газетных судоку?",
Или размножающиеся как кролики фичемапы в densenet'ах,
"И еще я не понял, как у них per-pixel сегментация получается",
Но регионы же разного размера - как эти 14x14 в пиксели на исходной картинке превращаются?,
"<https://www.slideshare.net/SparkSummit/feature-hashing-for-scalable-machine-learning-spark-summit-east-talk-by-nick-pentreath> странное попалось на слайде 20, там меряется auc при разном размере выходного пространства feature hashing, но как же получается так, что auc падает при увеличении пространства? 
Ведь должно же наоборот, больше пространство, меньше коллизий, лучше качество модели.",
"Всем привет! Скажите, правильно ли я рассуждаю? 
Результат обучения NN (нейронки) случаен. Однако при одних условиях он более случаен, а при других менее. Соответственно если задача такая, что функция ошибки имеет малое число экстремумов, то решение становится более детерминированным (при одном экстремуме на всю поляну получается стабильное решение с определенной погрешностью при любых начальных весах и при условии оптимального числа нейронов\слоев). Если задача такая, что функция ошибки имеет большое число экстремумов, то чем их больше тем решение становится менее детерминированным. И все это получается собственно не из-за самих нейронок, а из-за численных методов градиентного спуска, так? Ну нейронки разве что добавляют сюда кучу параметров, что как бы портит ситуацию, но не является первопричиной.
Однако, если запустить серию нейронок с разными начальными весами, то при условии оптимального числа слоев\нейронов можно таки найти пару нормальных решений из сотни?
В таком случае вопрос. Насколько много таких задач, где экстремумов дофига? Как-то можно это выявить или так и запускать кучу моделей с различными параметрами? Какова вообще стратегия в использовании нейронок или ее еще нет?",
"Кстати, <@U14CTBLFJ> у тебя тот редкий случай, когда можно посмотреть на AMD Ryzen 7",
"<@U4PTMUZKN>
1. Количество экстремумов -- свойство не задачи, а математической модели.
2. О какой детерминированности идёт речь? Если не вносить в сеть шум явно, то она всегда будет детерминированной. И наоборот ",
"зачем экономить на i7, покупая тачку за 2к$?
1) вряд ли она будет использоваться только для DL, иногда и xgb нужно постакать
2) препроцессинг и нарезка батчей тоже занимают время, не всегда меньше, чем само обучение",
"Особенно это справедливо для машинного обучения, где мы на самом деле не хотим достичь минимально возможной ошибки на обучающей выборке, а хотим иметь хорошую обобщающую способность, т.е. нужно балансировать точность и сложность модели ",
"Ну я тоже не полагал, что модель бесполезна. Просто хотел понять статистическую природу решений (сходимости этого дела). И главное - понять как при этом все же делаются надежные решения. Как совершается вот этот логический переход от недетерминированности решений к ""оно вполне нормально работает, сходиться, можно дофига где использовать"" (надо ведь как-то конкретно оценивать это дело). Ну или узнать, что я вообще себе все не так представляю и матан там устроен иначе.",
"Сложно сказать. Там все достаточно субъективно. 

Для прохождения через Graduate School advisor - публикуют на сайтах университетов - типа TOEFL меньше 95 - не смотрим.
После этого заявки попадают на стол профессуре и они смотрят нравится им кандидат или нет. На что они смотрят - сложно сказать. Я бы смотрел на наличие Kaggle Master, а они могут смотреть на число публикаций, хотя от бакалавров это и не требуется, на то откуда ты, кто дал рекомендацию и прочее.

Одобряют заявки с запасом. Что-то вроде одобрили 80 - реально согласились 20.",
А там как раз в другом треде то же самое советуют :-),
"Почитал про Ryzen, выходит, что в приложениях, где надо в много потоков, делает младшие i7, в остальных вещах(которые в принципе не актуальны: игры, фоточки) отстаёт, но на уровне. 
Такой вопрос: он всякие xgboost сам будет параллелить или это на мне?",
"Народ, у меня как минимум до лета есть доступ к Nvidia Titax X Pascal (12 GB). Если кому-то надо посчитать что-то в Keras или Tensorflow и этот расчёт не займёт более суток, могу вам посчитать модель. Всё работает на последней Ubuntu. Есть 32 ГБ оперативки на хосте. Есть также возможность запустить всё это добро на Windows, но нужен будет мануал как быстро и правильно настроить среду.",
"Про Kaggle и рыбок ещё не читал. В Корее у нас, как ни странно, никто этим не занимается из знакомых рисёчеров. Жду код и датасет:)",
"там нет регрессоров, непонятно, как рекламные кампании заносить",
"привет. подскажите как правильно обучить NER чтобы он мог одновременно правильно идентифицировать `New York`, `California`, `Salt Lake City`?",
"приветик всем
на sberbank ds day Воронцов рассказывал такое: <https://www.sdsj.ru/slides/Vorontsov.pdf>
выглядит прикольно, но не очень понятно, какой от этого всего можно получить профит (и можно ли вообще)

есть ли что годное/интересное на эту тему?
ну или, может, кто пробовал на практике, и может поделиться результатами",
Ну знаем мы как юзвери деньги тратят. Знать бы что они захотят в течение ближайшей недели.,
"Просто туда дофига понаписали, вдруг кто прочитал уже..",
Вектора с вероятностями принадлежности к тематическим категориям можно юзать как фичи при классификации,
"кажется, что там как раз упрётся в самую медленную карту",
"Всем привет. 
Замучился с рекуррентными сетями в tensorflow, прошу помощи.

1) проблема с переобъявлением в iPython Notebook
Предположим, есть код: 

```x1 = tf.placeholder(tf.int32, [batch_size, seq_max_len], name=""x1"")
x1_len = tf.placeholder(tf.int32, [batch_size])
embeddings = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1), name=""embeddings"")
cell = tf.contrib.rnn.LSTMCell(state_size)
cell = tf.contrib.rnn.MultiRNNCell([cell] * 3)
outputs, states = tf.nn.dynamic_rnn(cell=cell, sequence_length=x1_len, dtype=tf.float32, inputs=x1_embed) ```

Этот код можно выполнить только 1 раз. Если я решу изменить в ноутбуке, например, batch_size, то перезапустить эти строки не получится: на tf.nn.dynamic_rnn tensorflow начнет ругаться (Variable rnn/multi_rnn_cell/cell_0/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope?)

Как с этим бороться?

2) если у меня есть x1 и x2, которые я хочу прогнать через динамическую рекуррентную сеть, как это сделать?
на
```outputs1, states1 = tf.nn.dynamic_rnn(cell=cell, sequence_length=x1_len, dtype=tf.float32, inputs=x1_embed)
outputs2, states2 = tf.nn.dynamic_rnn(cell=cell, sequence_length=x2_len, dtype=tf.float32, inputs=x2_embed)```
вылезет ошибка, аналогичная предыдущей",
"Да, время инференса пропорционально размеру батча, так что можно налить батчи как 26:38 примерно или соответственно отношению количества cuda-ядер",
Ответ на мой вопрос -- как минимум mxnet умеет (<http://mxnet.io/how_to/multi_devices.html#advanced-usage>),
"1. ""weights already exists"" -- вставь `tf.reset_default_graph()` в начале этой ячейки.

2. используй `tf.variable_scope(""my_awesome_var_scope"", reuse=False/True)`, где в первый раз `reuse=False` (не пытаться переиспользовать переменные, а создавать их), и в последующие разы `reuse=True` (переиспользовать уже созданные веса)",
"тут где-то недавно спрашивали, где юзают nlp, вот еще кейс <https://br-analytics.ru/>",
"Прикольно! Интересно, бы узнать какие модели у них там за ширмой",
"А знает ли кто-нибудь такую штуку, чтобы быстро, как кистью в графическом редакторе, набросать много точек, а оно бы мне выдало json/csv с их координатами? 2D вполне хватило бы, sklearn-игрушки не очень подходят. Как-то давно видел такое на хабре, но даже не знаю, как загуглить :с",
"что почитать и по каким словам гуглить кластеризацию путей (гео-координаты, время) пользователей, классификацию пользователей по путям, поиск интересных (часто посещаемых по определенному шаблону) точек?",
"Вот наткнулся на либу, как я понял она анализирует похожесть слов на основе их звучания
<https://pypi.python.org/pypi/soundex/1.1.3> (есть еще fuzzy)
Только вот проблема - там есть английский (и вроде иврит), а русского нет. Да и либа какая-то заброшенная
Есть ли что-то похожее свежее и русское?",
И брать ембеддинг как представление слова,
"подскажите, как в керасе считать лосс только по части изображения? 

задача сегментации, есть некая маска. важны предсказания только тех пикселей, которые соответствуют маске, в остальных местах можно предсказывать все подряд.",
"у `sklearn.linear_model.LogisticRegression` есть метод `predict()`, выдающий для каждого объектаего класс -- но если логрег выдаёт *вероятность* принадлежности к классу, почему в методе не задаётся порог, с которого считаем объект принадлежащим классу?",
"~я начал лезть в исходники, и мне стало лень, он predict наследует толи от BaseEstimator, то ли от LinearModel, то ли еще от какого то родительского класса, и вобщем там это типа наверное можно переопределить, но зачем если можно взять predict_proba и выбрать любой нужный порог~ predict наследуется от LinearClassifierMixin",
"Где почитать, как использовать posterior predictive distribution простой модели для построения модели второго уровня? Аля баесовский стэкинг. Загуглить не получилось, там какой-то шлак",
Есть у кого нибудь ссылка на предобученный word2vec для русского?,
"Пока что я пользуюсь эвристиками типа фильтровать входные признаки зная статистики (варианс моды медианы и среднего) целевой переменной от простой модели. Потом взвешиваю достаточные статистики правдоподобия от старой и новой модели. У меня дало желаемое изменение качества, но как дальше качественно улучшать модель хз. ",
<@U0DA4J82H> а как для SEO используете?,
а на какой технологии работает?,
"есть истории от таких людей: <http://udibr.github.io/using-external-gtx-980-with-macbook-pro.html>

возьми точно будет больше. как минус: придётся оставлять макбук на длительные вычисления.",
"теоретически ААЕ можно прикрутить, который как в статье разделяет стиль и семантику, но чот мне кажется такое обучить будет не легко, не мнист все таки",
как я понял из описания там есть ground truth разметка,
"Призывается <@U1SJ2U4V9> 

Ты как GAN к спутникам прикрутил?",
"Если там все картинки, как та, что выше и их просто надо посчитать - задача прямолинейная по самое не могу. Это не рыбки или рак.",
ну смотреть как у тебя шатаются данные в исходной выборке и так же шатать их руками,
"чот мне кажется что результат такого порно будет из серии онеме cool devices, если вы понимаете о чем я",
"Привет. А тут есть кто-нибудь со стороны Яндекса, кто контачит со Сколтехом по поводу стажировок или набирает к себе в отдел оттуда?",
какой must заботать по deeplearning кроме <http://cs231n.github.io>?,
"В требованиях к данным для применения линрег есть некоррелированность ошибок.
Второй вопрос -- самый важный.

[1] Как влияет на результат модели невыполненность этого условия?
[2] Почему именно так? (если объяснять интуитивно)",
"Если ошибки независимы и нормальны, то минимизация квадрата ошибок эквивалентна максимизации правдоподобия. Если независимость не выполнена, то мы будем максимизировать не правдоподобие, а какую-то другую величину. Последствия этого зависят от того, как именно ошибки зависят между собой, кажется, что можно придумать пример, который все сильно испортит. К слову кросс валидацию в таком случае тоже надо делать аккуратно, потому что непонятно, как правильно бить на фолды в таких случаях.",
"Еще в учебниках иногда встречается упражнение на вывести формулу для оценки модели, если известно, что ошибки распределены, как нормальные, но матрица корреляции не диагональная",
как оптимизация без гессиана (хотя возможно кривой пример :disappointed: ),
"воу, круто, я думал сильно хуже будет, а как трейн-тест составлял?",
Jeff Atwood как раз несколько дней назад написал про внешнюю GPU пост <https://blog.codinghorror.com/thunderbolting-your-video-card/>,
"хватит emoji ставить, лучше бы посоветовали что-нибудь кто что ботал :disappointed:",
"ну он устраивает стажировки сколтеховцем, в том числе в яндексе централизовано забарасывая анкеты, хотел спросить подробнее как там чего устроено будет, когда собеседы планируются",
"Ребята, а вот допустим у нас каждому объекту сопоставлен какой-нибудь узел в иерархической таксономии (например, классификации специальностей ВАК). Получается, что в фичи можно добавить метку этого узла и метки всех узлов выше по иерархии. Как говорит ваш опыт, нужно ли как-нибудь специально кодировать такие фичи?",
"а как быть, если таксономия очень большая?",
"то есть типа mean target encoding, но когда категории не взаимоисключающие и связанные",
"и мы заранее не знаем, принадлежность к какому уровню таксономии будет релевантной для задачи",
"ага, и тогда если классификатор вообще нормально со sparse данными работает и есть регуляризация, он должен понять с какого уровня какие категории брать, надеюсь)",
"ну вот у нас пока что LightGBM лучше всего работает, думаем как ещё поднять",
"по идее то же самое, но как будто у нас не одна таксономия, а несколько?",
"Кто подскажет, чем сейчас лучше всего дать мозги простому разговорному боту? Сейчас стоят Alice или Mitsuku, но это скриптовые штуки, может что-то интереснее есть?
Бот не отвечает на вопросы, а поддерживает диалог (то есть тоже должен по-идее задавать их).",
"хотя не знаю где это написано в доках, но сохраняет :slightly_smiling_face:",
"Это же не из ганов. Это как google deep dream примерно, просто визуализируют что сеть выучила. Через gan порн был бы гораздо интереснее. Например как pix2pix, только вместо сумок и коттиков фотки. Они бы вообще не такие абстрактные были.",
"хм, нет - никогда не проверял какого типа инты)",
"То есть ты предполагаешь, что во время :kaggle: Ты читаешь меньше статей, чем без него? По факту скорее больше, но они очень тематически сфокусированы - например по сегментации тебе каждый, кто работал над спутникам даст весь расклад, но за GAN или локализацию не скажет ничего.",
"те например если пешка стоит на клетке впереди, а все остальное так же, то эти позиции схожи (хотя, иногда может быть и важно как пешка стоит)",
"спасибо - не помнишь, как назыается?",
"Всем привет! Кто-нибудь работал с BigARТM и тематическими моделями для задач без текстовых данных? Хочу повторить пример от Воронцова по классификации клиентов Сбербанка методами topic modelling - но перед тем, как ринуться в эксперимент, интересен опыт других аналитиков - работал ли этот метод для вас?",
"На мой взгляд для любой задачи, в которой ""документы"" можно представлять как bow, тематическое моделирование должно заходить. Еще полезно добавлять различные модальности документов",
"диктофон в андроиде? Или звонок с телефона в помещении на другой телефон с андроидом, где стоит приложение ACR, записывающее звонки. Или вы про что-то иное?",
"кстати хорошей adversarial метрикой качества такой схожести могло бы быть умение отличать задачку (т.е. где есть способ поставить мат в N ходов, желательно единственным образом) от не задачки (т.е. можно и на ничью натянуть). В таких случаях как раз каждая пешка может многое решать.",
а кто нибудь знает в матплотлибе можно так-же настроить наложение прозрачностей мультипликативное и тп?,
"привет - а есть ли кто знакомый с bokeh? есть два графика vplot(p1,p2) и на каждом есть по crosshair - есть ли способ заставить их двигаться ""взаимосвязано""  по оси Х? двигаешь на одном и на другом двигается аналогично. Спасибо",
Расскажи потом чё как :) ,
"Привет,  подскажите пожалуйста,  где можно почитать статьи / сайты про кластеризацию,  когда у тебя есть набор объектов про которые ты знаешь,  что они из одного класса.",
"<@U24FQ67C7> если честно, я тут просто как гость. Пока не добрался я до этого на таком уровне.
Единственное, что я знаю - <http://opencorpora.org/>, на этом сайте есть корпус русского языка",
"Спрошу и там, спасибо. Задача имеет смысл так как только у малой части объектов есть метки, при этом только одного класса. В парадигму поиска аномалий это не ложится, потому что класс составляет значительную долю. В задачу кластеризации не ложится,  потому что нет меток не этого класса. Кажется,  что это кластеризация с дополнительным знанием/подкреплением.",
"Привет!  Спросил в топике reinforcement learning, посоветовали спросить и тут. 

Посоветуйте,  пожалуйста статьи/ресурсы про кластеризацию с подкреплением. 
У меня вот какая задача выборка. Из этой выборки есть мало ( допустим 1%)  объектов,  про которые известно,  что они относятся к одному классу. Надо провести кластеризацию этой выборки, с учётом дополнительного знания.

В концепцию поиска аномалий задача не ложится из-за того, что класс, про который мы имеем примеры достаточно массовый ( вилами по воде - процентов 5%),  классификацией тоже трудно,  так как нет примеров не этого класса.",
Привет. Я тут сделаль туториалы как фигачить дата-виз с vue.js . Скажите плиз че думаете,
там прикол в том как это именно дружить с vue,
"На 100% <#C4P7DGAHY|data_breakfast>, где я был (три шт), было как минимум X2 :girl:. Так что ажиотаж на эту тему можно сворачивать, девчули в DS - это реальность. И корреляции с местом чот не заметно.",
"от этого зависит ту, какую регуляризацию в общем, в остальном никаких ограничений быть не должно",
"на какой винде лучше ставить все это на 10 или на 7?
и получается в каком порядке лучше ставить keras theano и TF?",
"А какой практический вывод из этого? Нужно строить ROC на тестовом датасете, в котором (+) и (-) объектов 50/50?",
"""Аналитик должен быть социально зрелым"" - спикер говорил что-то вроде того,что все эти датасаентисты, только вышедшие из универов, хорошего анализа сделать не могут, так как они не знают жизни",
"В эту субботу (без шуток) 1 апреля будет зарешка. Будем обсуждать контест по Hearthstone (<https://knowledgepit.fedcsis.org/contest/view.php?id=120>).

Собираемся как обычно в ШАД с 12 до 15-ти в Принстоне. Регистрация доступна до 14:00 пятницы 31 марта, учтите это, пожалуйста: <https://events.yandex.ru/surveys/4685/>
Схема прохода - <https://yandex.ru/maps/-/CZcsM8YJ>
Приходите!",
"В свое время я потратил кучу времени на разбор регуляризаторов. Так что выслушать мнение людей с опытом как их настраивать в общем случае было бы очень интересно. То же самое про scores. Ну и в целом - так как рычажков в библиотеке много, а на разных задачах есть смысл использовать разные - тоже хотелось бы послушать",
"С практической точки зрания, это полностью зависит от бизнес-требований.
ROC это лишь способ визуально оценить качество классификатора, ROC AUC лишь одна из метрик, притом весьма абстрактная. Ее применяют в (почти) чистом виде только для узкого класса задач, например, для оценки скоринговых алгоритмов через индекс Джини (`Gini = 2 * ROC_AUC - 1`). Как при этом балансируют выборку, может <@U2LGQMC4D> рассказать, например.",
А на интеловском ИИ саммите есть кто в Лондоне? ,
Или еще какого то зверя,
ах как жалко что пропустил :disappointed:,
"<@U3BUXTVM1> 
- здравствуйте, это канал о датасаенсе? 
- да
- подскажите, как собрать ангуляр?
:trollface:",
"<@U44J2H9K6>  вообще не планировал. Я, правда, не очень понимаю формат зарешки (это как тренировка в прошлую субботу или что-то другое?)",
"У нас свой ASR, вопрос не в том, как транскрибировать, а в том, как записать и чем",
"при этом под особенностями можно понимать нетекстовые исходные данные, временнЫе модели, двуязычные и т.п. - тут уж какие кейсы были",
"Привет! Я думаю, здесь уже задавали этот вопрос, но четкого ответа  не нашла ) Какие курсы по deep learning рекомендуются для новичка?",
"Про коммерческие кейсы тоже интересно, но здесь уже можно подискутировать - кто и как видит эту тему в своей отрасли. После обсуждения нюансов метода это будет даже продуктивнее)",
"мммм, не пробовал. Как-то анакондой безопаснее, там уже много скомпилено как надо.",
"Навскидку это как раз не ко мне, а к любителям постакать.",
"versus: это правда так хорошо работает, как пишут в статье? ",
"<@U3PLVL3RD> а почему не взлетело, есть интуиция? ",
"Доброго времени суток! На стажировке столкнулся с такой задачей computer vision: надо найти на скрине веб-страницы все присущие ей элементы (кнопки, текстовые поля, чекбоксы). Можете подсказать, какие способы можно попробовать или по теории материалы какие-то, которые помогут решить именно эту задачу. CS231n уже начал. Спасибо!",
"Да, я пытался такое делать,  а точнее я делил расстояние между одноклассниками на к, где к - сурерпараметр. Но кажется,  что есть что-то более умное ",
то есть хотите автоматически определять когда что-то съехало?,
"Когда keypoints - да, регрессия точек, только без sigmoid",
Когда прям bounding box и регионы - то Faster R-CNN,
Я все понять не могу как этих рыбок локализовать :disappointed:,
"<@U411PKASW> как минимум можно определять пересекаются ли элементы, которые не должны и какие между ними расстояния. Но неясно как идентифицировать элементы - у тебя может быть несколько чекбоксов и полей на странице, будет сложно понять где какой наверное",
"<@U3R32B38W> Надо определить границы всех элементов. Как второй этап задачи уже, это понять что в пределах этих границ находится чекбокс, текстовое поле, кнопка…",
"Данных ~3k и рыбок на самих картинках мало

=&gt; один из подходов кропнуть куски где есть рыбки и уже их классифицировать, как в том году делали, когда китов классифицировали",
"Что-то я никак не пойму, как Faster R-CNN работает. 

Я правильно понимаю, что берется любая FCN, после последнего CNN слоя сигнал идет в два бранча, в каждом из них делается 1x1 convolution, после этого в каждом flatten, dense, а после этого в одной голове sigmoid или softmax на классификацию, а в другой  регрессия на bounding boxes?",
"И почему в скоринге так любят джини, хотя там всегда большой дисбаланс...",
"тут парни из Uber рассказывают как они ищут похожие маршруты с помощью LSH
<https://spark-summit.org/2016/events/locality-sensitive-hashing-by-spark/>",
"Но я не понимаю, какой может быть профит от использования posterior predictive",
"интересно, какой процесс взаимодействия с ""экспертом"" оказался наиболее удобным/эффективным",
и какие политики семплирования неразмеченных примеров в итоге сработали?,
"Подскажите пожалуйста, как можно использовать KerasClassifier в sklearn Pipeline с feature селектором? В nn модель необходимо передавать input_shape, а в такой ситуации количество выбранных признаков мы априори не знаем.",
"Я пилил платформу, которая автоматизирует то, что изначально крауд размечает и потихоньку крауд уменьшает.
Сэмплирование там было качество-ориентированное: для текущей модели выбирали какой-то порог когда качество устраивает, всё остальное процессили людьми, и по мере роста размеченного датасета модель обучали на старых+новых данных.
Но там вопрос семплирования не стоял, размечать нужно было всё, просто что-то можно было МЛ, а что-то людьми, и главное - гарантированное качество, а не трудозатраты.",
"для нас наверное основной профит что эксперт видит результат, насколько хорошо все работает: и AUC на cross-validation, и новые примеры глазами. И если все плохо, может вообще по другому принципу начать размечать, или понять когда остановится. Как я понимаю циклов не очень много, меньше 5. Разные политики сэмплирования не пробовали, но у нас ограничение задачи что выдаются скорее более релевентные примеры (с точки зрения модели) чем в изначальной тестовой выборке - т.е. по идее как раз более сложные для классификации.",
"&gt; Рок кривая ерундовая штука если есть дисбаланс
В который раз это слышу и в который раз не соглашусь : )  Буду рад, если кто-нибудь сможет разубедить. 

Зачем вы используете AUC ROC? 

1. Как относительную метрику (например, при сравнении качества модели в ходе оптимизации гиперпарамтеров). Вне зависимости от классового перекоса AUC ROC будет корректна для такого использования. Оно будет больше (на очень маленькое число) для более хорошего классификатора. До тех пор, пока у вас примеров в выборке &lt; 10^15, вы не получите численную погрешность в AUC, которая смогла бы испортить AUC, как относительную метрику. 

2. Как абсолютную метрику, по которой можно сравнить алгоритм с common sense (ближе к 1 - отлично, вблизи 0.5 совсем плохо). Асболютно некорректное использование любой (произвольной) метрики. Например,  `Accuracy = 0.6` это хорошо?  А непонятно. Если оптимальный байесовский классификатор имеет `Accuracy = 1`, то 0.6 это ужасно. А если оптимальный байесовский классификатор имеет `Accuracy = 0.61`, то вы скорее всего построили одну из лучших моделей в мире.   Как вы это поймете по значению метрики, оцененной по отложенной выборке ? никак.

Тогда какие могут быть претензии к roc auc?",
Например использовать уверенность в предсказании как фичу для второго уровня. И использовать ее и ещё несколько для фильтрации новых признаков,
"Это та, где они использовали десятки тысяч машино-лет и получили какой-то средненький результат на cifar-10?",
"Видел недавно статью, где говорилось, что roc-auc зависит от датасета, на котором он рисуется, и поэтому годится только чтобы сравнить 2 модели на одном и том же датасете.",
"Вопрос - а есть ли что либо опенсорсное, что можно запустить как BI-систему, скормив на вход пару линков, откуда будет подтягиваться статистика в формате JSON и парсить оттуда определённые поля?",
"а как боретесь с переобучением модели на первых итерациях? а то положительных примеров просто катастрофически мало по началу, precision=1, а recall-&gt;0, поэтому тот же UncertaintySampling фигню выдаёт, очень медленно растёт полнота",
"у нас такой нет проблемы - эксперт может найти достаточно много положительных примеров (есть для этого инструменты), а если не хватает негативных то есть источник рандомных заведомо негативных примеров. Ну и эксперту говорится что для начала нужно получить приемлемое качество. Т.е. это наверное по целям очень далеко от активного обучения. Плюс тут заведомо понятно что примеров будет мало (меньше 1000), поэтому модель такая чтобы не переобучалась. Плюс пользователю показывается какие фичи модель использует (на какие слова реагирует и т.п.), так что если она что-то не то начала отлавливать это видно",
"Измеряешь качество модели как среднее на CV. Твоя метрика -- случайная велечина. У нее есть какая-то дисперсия. Если у тебя есть дисбаланс классов, ROC AUC будет не сильно чувствительно к тому что модель начала различать минорный класс лучше. Тяжело будет понять это маленькое колебание просто случайное? или улчшилось? или вообще они там друг дурга перекроют и ты будешь думать что ничего не изменилось. Поэтому надо брать метрики, чуствительные к улучшению качества на минорном классе.",
"с cv отдельная тема, попадалось что улучшение есть, но в пределах sd, сабмитишься - хорошо. И как понять, что оно существенно было, особенно если больше фолдов не нарезать",
"Кстати, насчет Сколтеха, есть кто-то, кто поступает туда/поступил?",
"нет, как раз `value.lower()` выше по коду есть",
"правда я впервые вижу чтобы кто-то делал аргумент функции case insensitive, когда там всего два значения возможно",
"Интересно узнать несколько вещей, самое важное - какого уровня рекомендации нужны? Большой ли конкурс? Что спрашивают на собеседовании на DS программу? Общежитие дают всем, или только по 5-той+ зоне?",
"Не важно, это просто пример где ясно что любое значение метрики это реализация сл.в. Если мерить на отложенной те же самые проблемы с рок аук, что я описал выше",
"Всем привет! Посоветуйте, пожалуйста, как быть, если не доступна полная информация по типам действий. Например, агент может совершать 10 разных типов действий. Есть данные, что было совершено действие, но не у всех действий мы знаем тип (только примерно у половины). При неполностью наблюдаемых стейтах используют POMDP. Есть ли аналог для PO actions? :slightly_smiling_face: Подскажите, пожалуйста, что можно почитать про подобную проблему.",
"В выходные заполнил анкету, вот теперь чекаю переодически, пока пишет что на ревью. Кстати, я не понял почему у них есть набор весной когда многие еще не получили диплома",
"У меня есть множество пользователей, у каждого пользователя много-много атрибутов со значениями 1/0. Также у меня есть результаты кластеризации этих пользователей (на основе внешней информации). Хочу посмотреть какие атрибуты наиболее характеризуют каждый кластер. Пока пробовал смотреть топ атрибутов по соотношению `кол. атрибута в кластере / кол. атрибута во всем множестве`. Куда можно еще посмотреть, что загуглить на эту тему?",
"<@U0AS548A1>  Воу, воу, датасет фиксированный, фолды фиксированы. Откуда случайность? 
Процедура CV с фолдами в смысле обсуждамого вопроса эквивалентна CV на отложенном датасете.  Случайность у тебя может быть только в алгоритме обучения (напр, bagging / random projections). Но эта случайность у тебя сохранится для любой метрики. И для AUC PR в том числе. 
О случайности в данном случае правильно думать, как о случайности из-за распределения над датасетами. Но ты все равно обладаешь только одним набором данных, поэтому случайность из-за семплирования датасета не в счет.",
"Очень интересно, на твой взгляд зачем тогда существует поправа Бонферрони, раз ""воу-воу""?",
"<@U3HM4KY14> убрал девайс. Запустилось. По данным nvidia-smi аллоцирована память ток на одной тачке, CPU-Util в пике 20%. Почему не 100% или около того? Значит ли эта, что карточка не используется? Как проверить, что карточка нагружается? По скорости по сравнению с cpu улучшения заметного нет.",
"<https://medium.com/towards-data-science/howto-profile-tensorflow-1a49fb18073d> - недавно кто-то в чятике пиарился, попробуй посмотреть кто и что и где гоняет, я думаю <@U0H7VBQQ1> прав, у тебя в графе что-то тормозит.",
Есть у кого нибудь опыт деплоя API на AWS ECS Кластер?,
"У меня вопрос такой, когда создается сервис там можно прописать количество копий task definition",
а количетво task definition может как то скейлиться от нагрузки?,
В общем задача что бы ECS service работал как beanstalk на одном инстансе всегда был один докер контейнер?,
"Спасибо, буду читать дальше, документация у aws как всегда в своем духе :grinning:",
"народ, как бы вы стали решать задачку: 
имея
- изображение здания
- координату фотографа и направление камеры при съемке
нужно разметить на фото геометрию видимой части здания (углы и ребра, и прикинуть их координаты)",
А тераформ как cloud formation создает stack на aws? В общем ввопрос в том что если его использовать откаты делать удобно?,
"Есть вопрос по Keras. В последних версияз появились соответственно Pretrained сетки, которые можно вызвать.
from keras.applications.vgg16 import VGG16
vgg16 = VGG16(include_top=False, weights='imagenet')

Вопрос в следующем. Как потом сюда прикрутить в конец, например, Dense? Я это сделал, но через танцы с бубном, вытащив саму функцию и переделав под свои нужды. Мне кажется должен быть элегантный вариант в одну строку.",
"И Dilated Convolutions сразу станут быстро работать, а не как обычно, сразу после установки cudnn 6?",
"Поправка Бонферрони то тут при чем?  : ) В случае с проверкой гипотез тебе нужно сделать коррекцию, когда ты делаешь ""групповое"" вероятностное утверждение располагая только частными вероятностными утверждениями (проверка каждой гипотезы на уровне значимости alpha).  Здесь это внешнее распределение над датесетами при построении модели от тебя не зависит. И сделать оценку дисперсии, а равно как и посетовать на то, что ""что-то наложится"" не будет возможности. 

&gt; Измеряешь качество модели как среднее на CV. Твоя метрика -- случайная велечина.
Качество твоей модели - детерминированная функция от объектов выборки. Если все-таки говорить про случайность, наш датасет - это случайная величина с некоторым распределением, нам не доступным.  В этом смысле метрика, да, -- случайная величина. 

&gt; У нее есть какая-то дисперсия.  
Ее дисперсия зависит только от вида распределения над датасетами. 

&gt; Тяжело будет понять это маленькое колебание просто случайное? или улчшилось? или вообще они там друг дурга перекроют и ты будешь думать что ничего не изменилось.
Тебе не придется понимать, какое это колебание, потому что у тебя нет возможности посмотреть и сравнить с другими значениями метрики, поскольку у тебя больше нет датасетов. 

Поправь, меня, пожалуйста, если я тебя не так понял.",
"&gt;Да даже при дисбалансе 1 к 20
отличное соотношение  для банковского скоринга, где все  алгоритмы сравнивают по 2*roc_auc-1",
"<@U06TZHSSJ>: позже опишу подробнее. Если просто как инженер: ты хочешь мерить качество определения минорного класса не чувствительным к нему инструментом. У тебя лежит ряд инструментов, а ты берёшь самый не чувствительный. ",
"А где доступно написано про регуляризацию градиентами? У меня есть физичный мир: ""чем больше положишь - тем больше получишь"", я хочу, чтобы нейронка учитывала эту физику градиентами",
"А вообще, зачем тебе девушка, когда у тебя есть такой комп?",
"довольно детерменированный процесс есть, который хорошо предсказывается. Но иногда, когда я анализирую свои градиенты, я замечаю, что есть пара выбросов, когда градиент по инпуту отрицателен, хотя физичный мир говорит, что положителен",
Я вот все равно ни черта не понял. Что за физический процесс? Зачем его предсказывать если он детерменирован? Что за сеть?,
"Хотя, я видел пейперы, где нейросети заменяют расчёты, потому что расчёты по мелкой сетке -- долго, а нейронка -- хуякс, хуякс и 98% точности за мало флопсов",
"cepera_ang: так ли мало? И настолько ли точно?) 

Видел пару применений, где довольно сложные мат., модели аппроксимируют НС. Плюс товарищи, которые пытаются модель ошибок всяких акселерометров и гироскопов аппроксимировать ими же. Но тут понятно-  в данных шума полно.

Но мне почему-то это кажется юношеским максимализмом. Неужели нет чего-то получше и попроще для регрессии?",
"cepera_ang: а как через нее диплернить? Кстати, очень странно, что никто не переписал cuDNN на OpenCL, там вроде бы нет исходного кода, но суть ясна. Я в рамках своего обучения по алгоритмам уже начал писать легкие вещи на Cuda, может пересесть на OpenCL, хм",
<@U14GG4E69> можешь описать существенные трудности написания DL фреймворка(Низкоуровневых функций) на OpenCL с названиями функций как в cuDNN. И почему это до сих пор не реализовали?,
"""в лоб"" такую задачу в CV решают, зная: параметры внешнего и внутреннего ориентирования (плюс дисторция), и еще нужно знать ""мировые""   трехмерные координаты трех точек снимаемого: для восстановления третьей координаты. Либо знать что-то о геоиетрии здания (напр., размер дверного проема). Если не в лоб - можно попробовать поискать, как делают и обучают какой-нибудь ""угадыватель"" трехмерки по размеченным съемкам зданий,  но никакой точности там, пожалуй, не будет. Если чего не пропустил из ""новенького"".",
"Добрый вечер! Посоветуйте пожалуйста книжку где подробно и хорошо расписаны доверительные инетрвалы, минимальный размер семпла чтобы получить заданную точность статистики и т.п.",
"Всем привет! Пользовался ли кто-нибудь реализацией tSNE <https://lvdmaaten.github.io/tsne/> или <https://github.com/DmitryUlyanov/Multicore-TSNE>?
Какой максимальный объем данных вам удалось обработать? У меня файл объемом около 1Гб завис примерно на 14 часов  так и не досчитался (хотя комп весьма мощный).",
"<@U0AS548A1> 

аргумент 1: scale метрики не влияет на чувствительность инструмента. 
Если все упирается в визуальный анализ и хочется видеть `разброс` на графике - умножь AUC на `10^n` / перейди в log шкалу / whatever. Хотя мы до сих пор так и не выяснили о каком разрбросе и какой случайности шла речь. 

аргумент 2: auc roc в точности монотонно зависит от количества дефектных пар. Что может быть еще более чувствительней? 

аргумент 3: если я буду знать число каждого класса в выборке (что не кажется серьезным допущением) я смогу биективно отобразить ROC кривую на PR кривую. Что еще нужно, чтобы быть `более` чувствительным?

аргумент 4: известный факт, что если ROC1 &gt; ROC2  =&gt; PR1 &gt; PR2 (где знак &gt; обозначает строгое доминирование, смотри, например <http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf>), поэтому разницы между PR и ROC еще меньше, чем кажется на первый взгляд.  

аргумент 5: Если ты уж начал про статистику и `поправу`.  Площадь под ROC кривой соответствует статистике критерия Уилкоксона Манна Уитни, одного из самых распространенных статистических критериев, который в том числе используется и в случае сильно скошенных классов (<http://stats.stackexchange.com/a/40347>). Поэтому относительно `чувствительности`  как минмум трое  (Уилкоксон, Манн и Уитни) с тобой бы поспорили.  

контраргумент 1 : оптимизация по метрике ROC-AUC необязательно приводит к оптимуму по ROC PR, но это совсем другой вопрос, который больше про то, что в вашей задаче  считается оптимумом.

контраргумент 2: ROC PR более нагляден на графиках, так как он меняется в зависимости от скошенности классов (рандомный классифкатор имеет ROC PR =  P / (N + P) , имея по ROC AUC всегда 0.5). Это аргумент только в том случае, если вы сравниваете _производительность модели на датасетах разной скошенности по фиксированной метрике_. И это ровно то, о чем писал <@U0FEJNBGQ> выше, и что действительно является аргументом против ROC AUC но только в том случае, если у вас действительно такой case – множество датасетов и нужно сравнивать производительность одной и той же модели на данных разной скошенности (что бывает не часто, видимо). 

PS. Взываю коллег к высказыванию аргументированных мнений, подкрепленных знаниями / статьями / ссылками, а не утвреждениям `По рок аук никакие модели вы не сравните между собой`, поскольку такие утверждения не добавляют к ясности аргумента.",
"размерность совсем не маленькая...спасибо! 
начит начну понемногу добавлять данные для определения - где же критическая точка.",
вот я им и пользовалась как раз),
n01z3: а примерную утилизацию CPU+GPU можно? Может mxnet более эффективно использует ресурсы CPU там где оно хорошо подходит,
Как сказал Андрей Киселев: И че? ,
"чат, а какие есть датасеты под анализ реальных видео (youtube8m не в счет). интересуют:
-классификация последовательностей кадров
-распознавание объектов на видео

<@U07V1URT9> <@U1BAKQH2M> <@U041P485A> <@U06K9ELB1> может быть вы в теме?",
"Блин, как не удобно получилось, у меня было уже одно написано и я решил его попровать. Неудобно вышло очень",
Кто должен был это реализовать?,
"Рок аук не показывает род ошибки
Не зная этого нельзя получить никакой информации о качестве модели и сравнивать модели между собой. В качестве аргумента могу дать датасет - попробуйте построить модели с помощью разных алгоритмов, оценить их рок аук кривой и выбрать лучшую 
А потом спрогнозировать каких проблем мы можем ожидать ответа использования этой модели ",
"<@U30A6KZLH> до Яндекса у меня был отдел предиктивной аналитики в банке открытие - это как раз банковский пример, в банках любят эту метрику но она не годится  ",
Это плохой путь как по мне. Люблю все оптимизировать,
"Интересно на сколько большая, неужели на столько, что AMD не могут нанять отдел, который бы за разумные сроки все сделал?

Но я вижу, что в роудмапе много где есть пункты - добавить поддержку OpenCL. Я бы поконтрибтютил даже куда-нибудь, пока обучаюсь.",
А кто может подсказать где искать статьи на тему примерного решения навье-стокса с нс?,
"Из kinect'а? Kinect - это ж ros, он позволяет много каких данных вытягивать) но там облако точек можно либо самому запрогать (из картинки конверт) либо с камеры снять)))",
"У тебя же есть depth map, некоторые люди считают это облаком точек, а кто то нет, по этому и уточняю.",
"поэтому и спрашиваю, может, есть ещё датасеты, где можно повыбирать объекты",
"Стохастическая регуляризация плюс ко всему, так как сетка корректирует предсказание  баесовской модели.",
"Добрый день! Кто нибудь из вас случайно не натыкался на статьи по применению ML в решении/ускорении решения физических задач?) Интересуют решение уравнений Навье-Стокса - перенос вещества, гидро/газодинамика. Что то в этом стиле <https://arxiv.org/pdf/1607.03597v5.pdf>",
А кто какое железо использует? Есть кто на aws или у всех топовые титаны пачками воткнуты?,
"&gt; Рок аук не показывает род ошибки 
неужели ROC PR показывает ? o_O ...  `ROC PR = &lt;fill_value_here&gt;`,  какой `род` ошибки ? 

предлагаю закончить обсуждение, если даже аргумент про биекцию AUC &lt;–&gt; PR  не является аргументом",
"Никто не знает, где можно купить Deep Learning(Goodfellow) в бумажном варианте? Желательно в мск:)",
"Сервер уже есть, так что как раз хочу оптимизировать) <@U36Q9NJMD>, srarse методы подойдут для не разреженных матриц? (возможно вопрос глупый, просто не очень с ними знакома)",
"А какой правильный термин для такой штуки: когда картинку по которой надо сделать предикт, крутим вертим предсказываем несколько раз и потом усредняем и говорим ответ?",
"Вообще когда фраза состоит из одних существительных -- это пиздец, так не говороят, поэтому и читать такое невозможно",
"<@U43FTJQ2V> если для людей пишешь -- пиши так, как в чате бы написал, если для формальных нужд -- похер, множь энтропию, пиши в стиле ""множественная деформация единичного семпла во время выполнения операции предсказания"".",
"Как я поняла без обычно конечно-разностных методов там не обошлось, но некоторые затратные операции они ускоряют с помощью нейросетей. На счет других диффуров не знаю пока. Мне тоже все это интересно, вот и спрашиваю есть ли еще литература на эту тему. Пока это единственная статья, что я нашла.",
"Теперь совсем жесть. Как раз в диссер вставить, лол",
Прям как в изначальном вопросе :joy:,
"+1.  Я думаю, что есть) Без классических численных методов пока все равно во многих областях физики никуда, но если их можно улучшить с помощью ML - так это мне кажется как раз новый виток их развития. Я знаю что в Шлюмберже, которые занимаются моделированием в нефтянке уже много проектов на грани численных методов + ML.",
"взаимное расположение наблюдателя и здания, полагаю, неизвестно? тогда я бы тоже попробовал сначала edge detector'ы. как только есть ключевые точки (углы, дверной проем, т.д.) и их референсные значения (в мировых координатах) - вкручиваешь гомографию и готово.",
"картинок из камеры как раз мало. я думала, мож из интернетика можно что придумать.",
"Друзья, а как выбирать window_size и количество скрытых нейронов в lstm? Как они к друг другу должны соотносится? Если какие-то эвристики? Мне наивно кажется что 1 к 1 норм, но я даже и это нагуглить не смог :dolan: ",
"Пусть есть длинная последовательность (длиной L), которую ты хочешь RNN-кой свернуть, если нет необходимости сделать за один присест, тогда побьешь на  отрезки длиной seq_length, а при обработке будешь сохранять и передавать state RNN. `seq_length ~10-30`, `rnn_size ~0.1k-2k`
Какое из чисел L или seq_length ты называл окном -- не знаю.",
"Хммм. Я подразумевал количество символов/точек на основе которых мы предсказываем. То есть берём window_size токинов типа фичи и window_size+1 таргет. 
Или как обычно эту величину величают? Я может поэтому загуглить не смог. ",
"Только не language, а multivariate time-series. Но кажется для понимания каким должно быть окном тут конкретики задачи не важна. 
Спасибо! :+1: ",
"<@U040M0W0S> мне кажется они вообще про разное, я не понимаю как они связаны",
"сейчас пытаемся понять, как обработать датасет. пока только выкинули очевидно бесполезные фичи. один из членов команды предложил pca сделать. на 17 фич мне показалось это бессмысленным, но вроде дало какое-то улучшение",
<@U342T2PT2> а где печатал и сколько вышло по $?,
А почему hub :r: ?,
"товарищи, подскажите, пожалуйста, если есть ненормальное распределение, и я его бутстрэплю, есть ли какая-то ""теорема"" о том, какие распределения в смысле нормальности в результате бутстрапа должны получатся.",
И все статистические выводы можно делать не прибегая ни к какой гипотезе о распределении чего-нибудь,
"Прикольно, что основная идея простая как две копейки",
"У меня нету иерархичности в данных, у меня есть простая модель с хорошими приорами, которую я корректирую сеткой из-за того, что без понятия, как прочие факторы влияют",
А как FCN получится прогнать?,
"Я думаю патчами получится, как pix2pix делает",
"кстати, MSE упало до как раз примерно std и встало",
"Да, согласен, плохо сказали. При росте оно конечно будет, но вся суть бутстрапа же в том что как раз не совсем и можно оценивать все эти bias параметров и вообще ""бутстрапный мир"" получается",
"Кто-нибудь сталкивался с задачей понимания что делает каждый фильтр в CNN ? Конкретнее, у меня CNN для time-series  c 1D свертками, мб есть какой-то более четкий способ чем то как делают в статьях про CNN и котиков (я нашел что выбирают или даже как-то частично генерят котиков которые сильнее всего активируют конкретный фильтр и рисуют feature map)",
"Кто посоветут сканер, такой чтобы завелся под Ubuntu без плясок с бубном?",
"Полез смотреть где это 1080 по 30к и наткнулся в отзывах на информацию о том, как подсветкой управлять, наконец-то отключить можно )))",
"Есть спецы по лазанье?
Кейс: есть физический процесс, 12 картинок в семпле. 
Я их сую в одну переменную шейпа [None,12,256,256]
Теперь мне нужно предсказать конфигурацию системы через какое-то время (3 класса).
Сейчас я просто делаю сверточную сеть. 
Может вариант как к этому прикрутить rnn/lstm?",
"Разложение по писательскому базису.
Давно (в прошлом веке) была у меня идея - как выработать ортогональный базис в пространстве авторов текстов, чтобы вычислить например, что Хлебников представляется как линейная комбинация Державина, Пушкина и Маяковского. 
С учетом современного развития технологий есть ли смысл говорить о чем-то подобном всерьез? Или это уже сделано?",
alex - направление примерно можешь обозначить - на какой фреймворк обратить внимание?,
"Да фреймворк вообще же вторичен.
1. Берешь книжки (флибуста с торрентов или откуда угодно еще)
2. Чистишь и готовишь (чтобы были просто слова, токенизатор из nltk, лемматизатор -- mystem)
Тут у тебя получается набор файлов с содержимым вида: 
```
среди_PR многий_APRO новый_A явление_S наблюдать_V в_PR россия_S последний_A десятилетие_S можно_ADV отмечать_V
```
3. Качаешь w2v с <http://rusvectores.org/ru/models/> , смотришь как их в генсим загрузить и использовать.
4. Берешь keras/lasagne или любой другой халявный фреймворк, смотришь как сделать классификатор многоклассовый с софтмаксом на выходе
5. Пишешь итератор, который достает фрагмент текста (случайный, небольшой для начала) и в качестве метки говорит автора.
6. Тренируешь, смотришь чтобы оно научилось.
7. Применяешь к целевым фрагментам и смотришь на сколько процентов там пушкин, а насколько ницше.
...
Для первой версии самое долгое -- это вероятно книжки скачать.

Затем вспомнить, что надо кросс-валидацию делать (так ты узнаешь, насколько твой классификатор +- хорош на самом деле).
Организовать подбор гиперпараметров и улучшить модельку.
Потом придумать что-нибудь более интересное, чем работать с готовым w2v.
Завернуть модельку в телеграмного бота и выкатить для в мир.

Я не сказал как из лемматизированного текста получать фичи для классификатора. Простейший вариант -- заэнкодить слова в вектора, а вектора поскладывать. А так вариантов сколько угодно много.",
"Никто не подскажет, как построить такой плот, но чтобы высота всех столбцов была одинаковой и просто смотреть распределение величины по каждому значению параметра?
<http://seaborn.pydata.org/examples/horizontal_barplot.html>",
"в какой задаче? detection вроде говорит, что суслик есть, а localization говорит, где именно",
"Как я понимаю, задачи примерно одинаковые, только метрика разная.",
"<@U040HKJE7> вроде, да, этот тут именно так и называется, хотя когда я гуглил и искал на сайтал матлплотлиба stacked barplot был с разной высотой столбцов",
"Мне он больно поверхностным показался -- многие инструменты давались как черный ящик, без понимания, почему они работают именно так",
"Продожая тему: сегодня протестировала пример <http://nbviewer.jupyter.org/github/bigartm/bigartm-book/blob/master/ARTM_example_RU.ipynb>
на bigartm 0.8.3.
Повторить его не получилось - при задании параметров ругается на наличие use_unigram_document_model
Кто -нибудь сталкивался с этой проблемой? Если этот параметр убрать, результаты абсолютно не совпадают с примером.",
"Для тех, кто подписывался на тред - небольшие результаты эксперимента. 
Попробовала запустить пример без изменений на реальных данных об оплатах картой по MCC категориям - увы, но топики не выделились.
Метрики качества модели вышли очень странные:
Sparsity Phi: nan (PLSA) vs. nan (ARTM)
Sparsity Theta: 0.000 (PLSA) vs. 0.000 (ARTM)
Kernel contrast: 0.000 (PLSA) vs. 0.000 (ARTM)
Kernel purity: 0.000 (PLSA) vs. 0.000 (ARTM)
Perplexity: 1.002 (PLSA) vs. 1.003 (ARTM)
Видимо, без стратегии настроек под каждый тип данных не обойтись. Пойду искать тот самый вариант :slightly_smiling_face:",
"Мне кажется, что исходные данные слишком разрежены...хотя на том же примере, где я слова попыталась собрать в более агрегированный вид (например покупки в grocery stores + fastfood = повседневные покупки и т.д.), результат был не лучше.
Так что и ошибку буду искать, все может быть.",
"кто в Гефи кластеризовал геоданые прям на карте? (мне смутно кажется, что я видела где-то такой кейс)",
"А то получается что-то типа ""а если данные -- картинки, какие архитектуры посоветуете""",
для начала надо понять какие у модальности есть в коллекции и с какими они названиями,
"1) берем pre-trained вектора для слов от google или ещё от кого
2)сопоставляем вопросу и его дубликату сумму всех его векторов, деленную на число векторов",
вы про какой из курсов на степике-то?,
"Всем привет! Кто-то знает, где можно добыть customer service/contact center/call center/customer support корпус для бота?",
"Как известно, resnet50 тренированный на кошках и собаках отлично ловит хоть рак, хоть рыбу :noise:  ",
Так tfidf как раз такой коэффициент и есть. Нужно добавить к нему другой коэффициент полезности? ,
"А где лучше почитать, как визуализировать активации на исходной картинке?",
"Имеет ли в задачах смысл брать несколько холдаутов? грубо говоря один из них вообще не трогать и только проверять периодически скор, а на втором считать какие-то общие характеристики (например важность признаков для отбора)
У меня например был опыт переобучения - в последнем mlbootcamp я нагенерил кучу признаков и начал их отбирать через `get_fscore`, которую я оценил по всем фолдам сразу. У меня cv сильно поднялся, но lb упал - как раз я думаю из-за того, что использовал информацию из своего фолда
В то же время получать `fscore` каждый раз по фолду заново и получать новое множество признаков выглядит каким-то бредом
Мне на ум приходит только вариант с несколькими холдаутами, может еще какие техники есть?",
"Ребята, всем привет! Помогите со следующим вопросом — хочу формализовать для себя понятие среды, по каким ключевым словам гуглить пейперы или может есть какие-то книги, лекции? Интуитивно все более менее ясно, хочется повнимательнее поизучать этот вопрос",
"а чем плоха книжка Саттона? там среда описывается, как некая сущность, на которую идет воздействие actions и которая дает реворды",
"Книжка Саттона отличная, но как некую сущность я среду и понимаю, интересно стало, вдруг есть чего-то еще)",
"каким образом ты собираешься валидировать предсказания, если референсных данных для конца года нет?",
среда как множество всех (интересующих) состояний,
Вот тут что похожее делали <http://www.idiap.ch/~gatica/publications/FaselGatica-icpr06.pdf> специфических слов нет. Как и в статье <http://visual.cs.ucl.ac.uk/pubs/harmonicNets/> Называй как хочешь,
"Это же тема для исследований, причем богатая, кмк. 
&gt; среда как множество всех (интересующих) состояний

Это не кажется исчерпывающим, и, что важно и в чем заключался мой вопрос, * формальным*, описанием)",
\\одна из причин почему с анонсами не торопимся - то что много всего и сразу будет выкачено,
"друзья, снова про `sacred` вопрос. вот я смотрю на эксперимент, как на состоящий из несколько шагов. для простоты  два: fit и predict. 
следовательно я много раз сначала фитю, а потом несколько раз предиктю. 
как такую двухходовочку в сакред оформить? кажется, что для этого и придуманы команды. но выходит, что в базе данных команды между собой не отличаются. то есть в `sacredboard` все они идут одним списком. но я результат несколько иначе представляю -- ну то есть фиты отдельно, предикты отдельно. 
может, конечно, сакредборд просто сырой и надо в него впилить разделение эксперимента на команды. или может есть какое-то другое решение? подскажите, :please:",
как освещают события в спб,
а что значит отрезать? это у тебя какая размера картинка выйдет?,
"Например в рыбах - если не знаешь где рыба, при повороте может быть проблема.",
"<@U1VSJ4KF0> экономические как раз) спасибо за хинт, попробую)",
"почему не слышно каких-нибудь рисерчей про синих китов вконтакта, етц? <@U041LH06L>",
"кто нибудь знает, есть ли бесплатные spell корректоры для русского языка? ",
"кто может покритиковать вот эту paper <http://www.aclweb.org/anthology/N13-1008> (Relation Extraction with Matrix Factorization and Universal Schemas, Riedel 2013)? что хорошо, чего не хватает? я по NLP вообще только начинаю читать Jurafsky and Martin, так что очень хотелось бы услышать от спецов плиз",
Сейчас как минимум 3 на дл и еще долго будут идти,
"не, команды для другого. например, чтоб графики отрисовать или другую мета-инфу, которая как раз не особо нужна для сохранения.
а тебе, похоже, ингридиенты нужны.",
"<@U07V1URT9> ну вот возьмем к примеру Intel &amp; MobileODT Cervical Cancer Screening - идея важная. Но, кому и как поможет моё участие, если мой результат будет где-то в топ 30%",
"Что такое ""общеполезный прикладной проект"" и под какую из этих категорий не подходит керас?",
"добр вечер) народ, подскажите, что там у человечества на данный момент с распознаванием речи? все так же тухленько как года 2-3 назад? какие есть коммерческие продукты, которые реально хоть где-то используются и не вызывают фейспалма?",
"Ну это же фреймворк, ""инструмент"", какую общеполезную прикладную задачу он решает?",
"Как я уже писал в другом среаде, кому будет полезен мой код и мой труд, если мой скор будет на уровне 30% или даже 20%? Как пример, если бы велась сейчас МЛ разработка на Mycroft AI - это совсем другое дело. Добавил какой-то функционал, стянул апгрейд и твой личный voice assistant получил новые возможности.",
"но команды остаются в истории как полноценные запуски -- зачем тогда они нужны, если примерно пофиг на графики?",
"<@U2TNYDLRL> Вы делали анализ рынка перед тем как заявить подобное? Из OOS Kaldi, CMUSphinx,  из коммерческих Google Speech, Microsoft CNTK",
"Поисковые речевые запросы от яндекса\гугла\байды вполне себе прилично распознаются и имеют смысл. Нахождение маршрута в нафигаторе опять же. Всякие ассисты типа сири, которые по сути те же речевые запросы. Поиск по медиа-контенту типа новостного (фильмы и свободная речь - куда хуже). Надиктовка смс и даже иногда текстов (но только при настройке на диктора и использовании спецмикрофона). Узкоспециализированные приложения (типа диктовки диагнозов). Приложения с очень ограниченным словарем (управление\общение с роботом, всякие IVR). Но вообще всё плохо - как только начинаются шумы\речь в необычных канал (телефон\радио\диктофон) качество резко деградирует. IBM хвастается пусть сколько угодно, реально они чуть-чуть получше настроились на switchboard (кстати MS тоже недавно что-то такое заявляли и тоже на switchboard, так что я думаю тут дело чести, типа чем мы хуже). А так у них довольно посредственно всё.",
"Если вы хотите просто использовать ASR как сервис, то думаю, для диалогов <http://api.ai|api.ai> подойдет, или заплатить и купить лицуху Гугла",
"а так, в теории сейчас намного интереснее стало когда перешли с HMM на DNN",
"а потом, когда с DNN перешли на Bi-directional LSTM, но  это для имеющих жирную выборку",
"Те он корректно распознал речь, а потом понял, в какой отдел мне нужно обратится",
"Ребят всем привет! Нужен совет! Очень интересны стали hadoop и spark, думаю тут прекрасно знают что это такое. Сам программирую 3 года на 1с + параллельно учу java, очень не хватает реальных задач и практики(java,bigdata). Имею пару сертов(java) с coursera но сами понимаете там самые азы и учебные задачи. своих мощностей боюсь не хватит для ""жизненных задач"" и следовательно еще 1 вопрос, как создать себе тренировочную площадку. Кто бы что посоветовал в плане развития практических навыков? поделитесь опытом.Заранее спасибо)
P.S. в _call_4_collaboration так же отписался, буду рад вниманию)",
"а у кого из каких-либо компаний они работают на входящих, если не секрет?",
как раз практического опыта не хватает для поиска хорошей работы,
"Спасибо, судя по заголовку как раз для меня )) И давно хотел получше разобраться.",
"Разделение спикеров это другая задача (диаризация) и для числа дикторов больше 3 она толком не решена (кстати, у IBM есть демо-сайт, который заодно это делает, ну так можно оценить качество, хе-хе). Если же это качественно сделать (допустим), то тогда можно применять адаптацию к конкретному диктору, что в данный момент говорит. Но и это малоактуально - реально используют SAT <https://www.cs.cmu.edu/~ymiao/pub/ivec_draft_final.pdf> т.е. заодно с обычными признаками подают некий вектор (обычно i-vector), описывающий диктора - и это дает плоды. Кепстры используют (точнее говоря, обычно только их и используют), MFCC, производные, это как бы стандарт. Еще могут быть всякие дополнительные фичи, кто во что горазд, но это обычно специфично. Используется шумоподавление, в том числе и на DNN, обученных получать чистый вектор из шумного. Что-такое бимформинг - не в курсе, обычно термин ""бим"" (т.е. beam, набор лучших гипотез) используют относительно декодера, а это не акустическая часть, в нем д-но бимом крутят обычно с целью добиться соотношения кач-во\скорость. Вообще, признаки и преобразования их (кроме cmn, lda и всякой другой нормализации и декорреляции) сейчас не в центре внимания, все обленились и пожалуй оправданно - в текущих уже есть необходимая информация, а дальше с акустикой будет работать DNN (LSTM\GRU). Не сказал бы, что такой подход мне нравится, но такое есть. И опять же, сейчас стал моден end-to-end recognition, то есть из спектра (или мел-шкалы какой) - прямо в буквы\слова, но это требует много выборки и все равно там проблемы с языковой моделью.",
"скорее актуальна задача детектировать участки, где говорят несколько дикторов одновременно. Чтоб их выкинуть",
"фазовая решетка, если быть совсем уж точным. Всякие конторы глубинного бурения делают специальный девайс с волноводами и т.п., а на практике гражданской я могу только кинект припомнить, у него такое было. Любопытно, конечно, как сейчас это делают",
"Нубский вопрос, можно ли взять что-нибудь сложное из model zoo, типа U-net, и тренировать его как BNN, или там нужны другие архитектуры? Какие преимущества у BNN перед традиционными DNN?",
"<@U1APAJ3UJ> делает в академии? Я видел эту статью и не очень понял, почему они даже не попытались использовать классические алгоритмы и почему они видят какое то улучшение от градиентов при расчете ошибки от акустической модели. Бимформинг это компенсация путей распространения, и модель компенсации лучей отделена от акустики по определению, они и сами пишут что бимформинг связан с DOA. Короче не понравилась статья - пришили один черный ящик к другому, натренировали их совместно, получили два процента улучшения, а бейзлайн даже и не попробовали",
как их вообще можно сравнивать?,
"Друзья, я столкнулся с проблемой выравнивания изображений по цвету/тону. У меня есть сырое изображение, например, c google maps, и на нем отчетливо видно, что снимки сделаны в разное время при разной освещенности. Мне нужно решать задачу image segmentation, но с такими изображениями это трудновато. Я пробовал equalizeHist, но результат неудовлетворительный. Подскажите, пожалуйста, как стоит выравнивать по цвету?",
"Приветствую. А есть живые люди, знающие CMUSphinx? Хочу получить log-likehood зафорсенных фонем и уже запутался в том, как это сделать.",
"Я вот по этой статье написал свой: <http://norvig.com/spell-correct.html> Нужны только хорошие русские тексти и побольше. Это конечно не полноценный спеллинг корректор, так как основан только на частоте одного слова, то есть контекст никак не учитывается",
"13 апреля будет первый “ClickHouse meetup для аналитиков“. Рекомендую всем, кто задумывался об использовании CH для своих задач. Подробная программа и регистрация на сайте:
<https://events.yandex.ru/events/meetings/13-apr-2017/>",
"Господа диплернеры, какие крутые примеры применения DL кроме распознавания/генерации изображений, аудио, текстов и self-driving cars вы можете подсказать? Мне нужно подготовить презентацию, чтобы впечатлить всех возможностями DL",
"Распознавание лиц/эмоций и когда её обманывают паттерном на очках и она думает, что это Бред Питт.",
вот не надо. Там где нет данных с локальной корреляций между фичами - а это 95% всех случаев xgboost будет рвать нейронки на британский флаг.,
"И как они могут обмануть весь этот ИИ в очках с паттерном Бреда Питта— типа, вот, смотрите, есть куда расти— купите.",
вот тут есть примеры как использовать: <https://github.com/yandexdataschool/tinyverse/blob/deephack-rl/atari.py#L199>,
Да всё как обычно срут,
"Каждый раз, когда пишут про сознание у нейросетей хочется вспомнить сатрое ""shut up and calculate""",
Когда я пытаюсь делать dir(env) — такой переменной не нашел,
"<@U0R55NS1M> расскажи про текущее соревнование data science bowl, где рак распознают по томограммам",
Кто-нибудь видел где можно найти исходники алгоритмов ML реализованные на каком-нибудь функциональном языке? Желательно haskell. И желательно чтобы реализации как можно более прозрачные были без всяких оптимизаций,
"Хочется посмотреть как оно выглядит. Начал уже второй курс Haslell”я проходить и очень мне нравится концепт такого программирования, может я больной конечно, не знаю))",
"<@U0FEJNBGQ> Хм, интересно почему так дело обстоит. А ты вычеркнул хаскель именно или вообще языки функциональные?
Скала то всем подходит, но пока студент — интересно баловаться со всяким и изучать разные концепты:)",
"Ну и на ТФ писать несложно, он же как раз декларативный
<https://github.com/tensorflow/haskell>",
"Кто там хотел примеров сетей без картинок, звука и текстов? <https://twitter.com/KyleCranmer/status/848959715472277505>",
"Всем привет! Появился вопрос - существует ли готовое решение, которое позволяет разделять речь на части, сказанные на разных языках. То есть, на входе есть аудиофайл - на выходе его разметка: когда говорят на английском, а когда на русском.",
А на каком языке слова нужны?,
"А где была регистрация на ""Moscow CV: Учим машину ехать в потоке""? Очень хочется прийти - послушать",
"И снова старые песни о главном.

Пора слезать с Keras - в какой framework сунуться?

[1] Легко прототипировать - не сильно сложнее Keras
[2] Быстро работает
[3] Легко параллелится на 2 GPU
[4] Не жрет память неадекватно.
[4] Много внятной документации, лучше с примерами.
[5] Много всякой инфраструктуры работает если не из коробки - то легко находится в интернете.

Я правильно понимаю, что [3] ограничивает мой выбор на Tensorflow и mxNet, а [4] и [5] ограничивает это до Tensorflow?",
"Говорю как человек, который постоянно хакает керас и попробовал tf)",
"<@U041P485A> рассказывай как успехи с биосами, разгоном и вообще :slightly_smiling_face:",
"Хм. Увидел, обычно о таком пишут крупно и где только можно",
"Хотел отписаться, когда закончу эксперименты, но раз <@U1CF22N7J>  интересуется, расскажу, что пока получается :slightly_smiling_face:
Вообщем я поставил Taichi X99 + Xeon E5-2683 v3 + 4x16 Gb DDR4 2400MHz Patriot Viper Elite (PVE432G240C5KGY)
В стоковом варианте полёт нормальный :slightly_smiling_face:
Пытаюсь разлочить турбобуст на всех ядрах (по мотивам <https://forums.anandtech.com/threads/what-controls-turbo-core-in-xeons.2496647/page-3>)
- Взял из ветки на anandtech версию биоса 1.40 с вычищенным микрокодом проца, но она не позволяет менять множитель 
- Немодифицированный биос 1.10 позволяет менять множитель, но у меня нет версии без микрокода
- Взял 2 версии офицального биоса (1.10, 1.40) и с помощью UBUTool попробовал их модифицировать разными микрокодами, пока это приводит к неработоспособности биоса",
"Да хз чо там, на карточках в основном ethereum майнят и ещё более говно-валюты, поэтому там бывают малопонятные волны, но когда заказывали вообще на 1$ с карты расчитывали, просто жалко что сейчас такой пик по 2-3$ пропускать из-за дурацкой доставки :slightly_smiling_face:",
"Забавно читать такое нытье иногда: <https://habrahabr.ru/company/alconost/blog/325690/>
&gt; Даже если использовать удобное ПО для управления окнами, при работе на нескольких мониторах есть проблема: если у меня два монитора, то нужное мне будет не прямо перед глазами, а слева или справа, и придется постоянно вертеть головой *(что особенно раздражает, если одновременно идти по беговой дорожке, как я часто делаю — см. рисунок ниже). *",
"Усталость от принятия решения на какой монитор смотреть, бляя, прям как в анекдоте про сортировщика апельсинов",
особенно когда это все находится в  кузове едущего пикапа :xzibit:,
"Есть ли новости про Nvidia *DGX-1*?
Видел кто-то что-то кроме PR от Nvidia? В идеале хотелось бы найти бенчмарки с какой-то более адекватной по цене спецификацией.

И еще - как думаете, какой бенефит можно получить от этой железки, если DL-модель будет разрабатываться на TF (с учетом информации у _Tim Dettmers_ о максимальном увеличении скорости в 3 раза при распараллеливании в TF)?",
"Добрый день.

Встречал кто нибудь данные связанные с пассажиропотоком городского транспорта? Загруженность автобусов? Или остановок?",
а как именно должны быть размечены русские быквы?),
"хай, коллеги, подскажите как инструменты для визуализации вероятностного дерева",
"Не знаю, где еще спросить, попробую здесь - почему когда я на убунте делаю mv filename /home/foldername, файл просто исчезает?",
"Я разобраться хочу, потому что вроде как mv должен перемещать файл, если директория существует, и переименовывать, если ее не существует.",
<@U0RV376MC> а тебе для каких целей? handwritten или печатные пойдут?,
"Твою мать, я только что понял, что не понял того, кто меня об этом датасете спросил",
"Это уже после того, как я засунул туда через ctrl+x ctrl+v",
"ну это смотря, видимо, как глубоко проваливаться :slightly_smiling_face: Но если пишешь слой в керасе (и не заботишься о том, чтобы он работал с theano backend), то керасовский тензор -- это tf-тензор, и операции можно выполнять вообще прозрачно.",
"Лично мне что бы съехать на TF не хватает тупо например Kernel'ов на том же Kaggle для известных задач, написанных как можно проще и логично на TF.",
"сложности возникают со служебными вещами, когда хочется добавить assert nodes, или считать что-то помимо вычисления выходов сети",
"а я вот писал на голом tf и очень радовался, когда изучил керас, то есть двигался как раз в другую сторону",
"керас 2.0 (или раньше тоже?) ещё хитро умеет спрашивать у tf размерность всех тензоров, поэтому о согласовании размеров надо думать только там, где это содержательно надо",
"Прикидки на пальцах: в DGX-1 8 карточек P100 c официальной производительностью ~9.5-10 TFLOPS fp32, у 1080Ti заявлено 11.3 TFLOPS, итого получаем даже быстрее вычислительную часть, но меньше памяти и она медленнее и шина типа медленнее. 
В рекламных материалах nvidia делает ход конём и пишет производительность fp16, которая у GeForce'ов полное дно, и которая вообще непонятно зачем нужна, ведь для сетей практически в 100% используется fp32.",
А почему у тебя permission denied и sudo потребовалось? :slightly_smiling_face:,
"кто имел опыт работы с подобными промышленными решениями ( <@U14GG4E69>   ?) - можете как-то прокомментировать расхожее мнение касаемо наличия/отсутствия выигрыша в производительности у *DGX-1* относительно альтернативных конфигураций с 1080Ti на борту?
Помогите пжл разобраться, где мы можем рассчитывать на бенефиты от данного решения?
Озвученное <@U1CF22N7J> мнение встречается и в найденных независимых обзорах по железкам.
Например - стр 111
<https://www.slideshare.net/PetteriTeikariPhD/deep-learning-workstation>",
"как создается энв в джиме и какие туда аргументы передаются * но это просто догадка, сам таким не занимался)",
"гайз, а как лучше подать бустингу фичи, полученные с помощью w2v на текстах?
есть объекты, которые содержат текст + другие фичи. В текущем состоянии на всех фичах без текста наилучшие результаты показывают бустинги, пробовались так же нейросети на w2v эмбеддингах от текстов(хуже)

но кмк w2v  эмбеддинги не очень хорошо подавать бустингу, и поэтому что в таком случае лучше ему подать?
может просто вероятность, полученная нейросеткой на тех самых w2v эмбеддингах? или сразу модельки стакать?",
"Рассылка Mathworks принесла конференцию в бауманке: <http://matlab.ru/seminars/conf2017>, где во второй день (26.04.2017) посвящён анализу данных, машинному обучению и интернету вещей.",
"Привет, я до конца не понимаю чем mkldnn отличается от mkl2017 и почему в моих тестах слои показывают показывает сравнимую производительность. 
Типо нужны совсем илитарные xeon для того чтобы заметить разницу?
У кого-нибудь был опыт? (интересует в первую очередь backward)",
для классификации при помощи word lstm видел как рандомное слово на &lt;unk&gt; заменяли,
"Ребят, серьёзно, чего вы ждёте от представителя компании? :slightly_smiling_face: Как человек, который провёл с той стороны баррикад много лет, могу сказать, что никогда и никто вам напрямую не скажет ""Да, для 90% задач домашний жифорс будет дешевле и лучше dgx-1, а где-то ещё и быстрее"", будет упор на то, что ""это серверное решение"", ""никто серьёзный жифорсы в приличное решение не поставит"", ""там память другая, шина другая, это в каких-то задачах даст прирост огого"" и ""нельзя напрямую сравнивать"" и т.д.",
"Слайды там кстати ещё до выхода ~титана паскаль и~ 1080ti, так что 72% смело можно читать как 100% на последних карточках",
"Не понимаю как после TF IDF сделать hold out по тем строкам что мне нужно(например хочу в холд отложить первые 500 строк) пробовал просто 
```
x_test = finTF[:550]
```
после обучения Xgboosta во время предикта пишет feature missmatch",
"я понимаю что в спарсе, но что делать не очень понял. Не смотря на xgboost как поделить данные на тест и трейн",
"<@U0DA4J82H> Что скажешь, как эксперт по этому делу?",
можно взять сетку с хорошим результатом (кстати как тексты в сетки пихались?) и добавить в фичи эмбеддинг с последнего слоя,
"Есть email рассылка, надо оптимизировать время дня когда посылаем email, так чтобы увеличить CTR на ссылку в email.

Email’ы посылаются с некой регалярностью. 

Вопрос: Как фич наплодить, которые будут хорошо выражать поведение юзера - открыл этот email в это время, кликнул на две минуты позже, так чтобы моделька это правильно использовала?",
"Юзеры разные, под каждого море фич, плюс в идеале хочется чтобы юзер кликнул на ссылку в первом же email’e, указал номер кредитки и мы разошлись. Так что собрать статистику под каждого юзера возможности нет.

Интуитивно хочется сыграть на том, что в среднем кто-то может открыть в течение дня в любой момент - напрмер я, так как на работе и дома за компьютером сижу, да и на телефоне могу открыть в другое время.
Кто-то обычо проверяет по вечерам, после работы, кто-то по утрам.

У кого-то еще какие-то привычки.

Не факт, что на этом можно словить. Но уже лучше какой-то ML, чем чистый рандом.",
"<@U1G303UTW> ""бандитизм"" -- это что-то про ""multi-armed bandit problem""? как это для прогнозирования CTR можно применить?",
"Время-послал, открыл, разница, среднее,  среднее время от послал-открыл по регионам, фичи по темам СТА/шаг воронки, устройство, с которого открывают, на каком шаге воронки сам чувак находится+все что есть про мнего).",
"да. необязательно предсказывать, если нужно максимизировать. если фичи слабые (или ML прикручивать долго/сложно), можно рассматривать временные бакеты как руки бандитов",
"еще бывает помогает выбрать только существительные, а потом взвесить через idf, как <@U1BAKQH2M> посоветовал",
"<@U0H7VBQQ1>, <@U4PKKEN4T>   спасибо, коллеги! Что-то такое я себе и представлял. Лемматазация было для меня новое понятие. Начну с обозначенного пути, но дальше будет интересно решить именно задачу с ""ортогональным"" базисом, это, конечно больше как шутка или игра ума, но быть может что-то интересное получится! Еще раз спасибо за советы",
"я заинтересовался, поясните, пожалуйста, как из второго уравнения получается третье?",
"мб у кого завалялся,или знаете репозиторий-нужна сиамка/triplet наученный на каком-нибудь разнородном датасете фоток вроде Imagenet(лица не предлагать)? преимущественно на тф или керасе",
"Всем привет! а может у кого-то есть вот это <http://dmkpress.com/catalog/computer/statistics/978-5-97060-293-5/>
в электронном виде?
И заодно, есть ли те, кто читал на русском? как оно вообще?",
Зачем русский если есть оригинал?,
"<@U44FD6Q66> Критерий оттока для каждого свой. Например, 7 дней не открывал приложение. Я делал так— кластеризовал всех отвалившихся  пользователей по кол-ву дней в приложении и построил график локтя) там, где он резко начал расти—установил значение слева от этой точки на графике как значение дней churn.  ",
"Это классическое определение черна. Если заказчик хочет чтобы все было как у всех, лучше плясать от такого определения (активность за н дней). Уже отталкиваясь от него можно найти сегменты юзеров, которые не то чтобы уходят а редко пользуются, сегменты которые реально уходят и итд. И уже выделять ключевые фичи про активность с момента установки. 

А если стейкхолдер ты, то можно вообще переформулировать задачу на паттеры поведение юзеров(часть которых ведёт к оттоку) и делать что угодно ",
"<@U0DA4J82H>  &gt;&gt;&gt;ololo [10:34 PM] 
а, прикольно тогда, становится интереснее

[10:34]  
там правда всего 5 тыс примеров

[10:35]  
я что-то не нашел про внешние данные, можно ли их использовать (edited)

[10:35]  
а то можно спарсить весь Buzzfeed

[10:36]  
(я на работе как раз таким способом данные для модели собирал)",
а как валидировали модель потом?,
"<@U14CTBLFJ> количество дней в приложении интерпретировал как "" кол-во дней в которые пользователь проявлял активность""? или просто last_usage_date - first_usage_date",
"привет. может кто подскажет, какие есть актуальные подходы или статьи на тему нахождения action items в диалоге? action item - это некая задача, директива, указание к действию одной из сторон. т.е. то что можно занести в todo list грубо говоря",
"Есть простая (по меркам DL) задача определения облаков и их теней на спутниковых снимках. Есть обучающая выборка, но она не очень большая, поэтому хочется попробовать что-нибудь unsupervised. Наверняка есть какие-нибудь ресурсы, которые помогут сделать это быстро и на коленке как proof of concept, желательно на тф. Спасибо!",
"не понятно, почему ""код"", если спикеры вроде не про ""код""
<https://ladiescode.timepad.ru/event/471400/>",
"Про  deep Q-network и как его можно применить, чтобы играть в игру по типу Neon Drive",
"Я так понял, надо предсказать в какой интервал попадет переменная? <@U3L7M0QVC>  если нет привязки ко времени, то можно делать простую регрессию, а интервал указывать эвристикой. Если нужен только сам интервал, типа, 20-30, то это просто задача классификации(если самих интервалов какое-то разумное счетное количество). Оба случая xgboost может)
",
"<@U44FD6Q66> да, сколько дней (в штуках) проводил более 1 минуты, прежде чем отвалиться. Там игра была типа раннера, но не он— убийца времени отскуки. Вся бизнес-модель была заточена на это. Т е смысла от разницы первого и второго визита у нас конкретно было не много. Это более относится к приложениям, где надо не каждый день. Типа, такси или что-то купить.",
"там можно отдельно разные стадии переопределять (если речь про sklearn), например preprocessor позволяет не трогать токенизацию но как раз добавить нормализацию дополнительно",
"Ни хрена пока что не понял, но спрошу сразу— как это можно прикрутить к временным рядам, где задача сведена к задаче регрессии, а эвристикой выбираем интервал?) Типа, для чего дисперсия? Звучит полезно, но мозга без картинок на бумажке не хватает понять.",
"Да и у Xp 3840 куда ядер, а у 1080Ti - 3584",
"А они просто такие: ""А хули нам кабанам, на фабрике новые чипы испеклись, ещё больше куда ядер, давайте нахуячим очередную карточку""",
А почему у АМД карты стоят 200 000 некоторые? Что в них такого? ,
"Ну, это новый титан, с дополнительными 10% куда ядер",
"а, ща. то есть они взяли титанХ, отковыряли 1 чип памяти и выпустили как 1080ти, а теперь чтобы был снова титан, сделали больше ядер?",
"Ну, как бы хер с ним с запутать, в трёх картах не запутаешься особо, другое дело, что прогресс всё таки есть. Это не по 5% в год как интел добавлять, а всё же что-то",
"Поди до 2Ггц с водой будет тянуть, как 1080ti",
Есть кто на митапе в вышке? :eyes:,
"Кажется, я не сильно фантазировал, когда предсказывал домашний петафлопс в течение 10 лет",
"А где ты возьмешь мамку, в которую вставишь 8 функциональных видеокарт?",
:wat: это когда такое вышло?,
"В теории - да, можно сделать из 1060 titan xp. На практике - нет. Раньше была возможность сделать себе из core i3 core i7, если повезет, с тех времен защита очень сильно возросла и такой трюк сродни разработки своего GPU с паяльником. И лочат не просто так ядра, на старте продаж, обычно лочат глюченные ядра, которые берутся из-за неоднородности песка, дефектов и т.д. Но, когда спрос на младшие модели повышается, то начинают лочить и хорошие.",
"да нет, вполне обчно, я тоже так говорю когда от амазона счет получаю",
"Товарищи, а на каких ресурсах вы новости из мира ML читаете? Поделитесь источниками, плз :)",
Кто нибудь будет на на амстердамской конференции по питону pydata на этих выходных?,
вот кстати в ожидании покупки 1080ti мучает эстетический вопрос “а какую бы мать туда купить” чтоб потом не было мучительно больно,
но ws дома как бы не особо смотрится,
"просто цеперанг не хипстер и не понимает кайф светодиодов, вот в одном стартапе была отличная машинка с видюхами и синей подсветкой, и в комнате где стояла тоже синие лампочки под потолком, максимально красиво смотрелось, даем дорогу светодиодам (ну и вообще в них есть еще и ограниченный практический смысл, ночью втыкать в монитор под минорный рассеянный цветной свет от диодиков приятненько)",
чуваки из uber жаловались кстати что у них в датацентре темно как в жопе и кроме того он в поле стоит и летом их пчелы кусают. вот так их ДЦ весь на титанах им бы подстветка не помешала. а нам она нафиг,
"А ещё я по фонарикам угараю, поэтому вообще не понимаю проблемы ""где-то темно как в жопе"", современный фонарь размером с палец освещает комнату, если поставить вертикально в потолок",
"Ребята, кто может подсказать, в рекомендательной системе, каков может быть критерий того, что рекомандация удачная? я думаю что это должно быть что-то вроде взвешенного User's Product Rating + isFavourite + HasPurchased, кто то делал исследования в этом направлении?",
"Всем привет. У меня очередной затык с rnn-ками в tensorflow. Надеюсь на вашу помощь.

Вопрос: как работает у dynamic_rnn параметр sequence_length? Как брать последний выход и последний скрытый слой? (он копируется в конец массива или нет?)

Конкретная проблема: есть простая сеть
```x_input = tf.placeholder(tf.float32, [None, max_steps, num_features])
x_input_len = tf.placeholder(tf.int64, [None])
y_input = tf.placeholder(tf.int64, [None])
cell = tf.contrib.rnn.LSTMCell(128)
outputs, states = tf.nn.dynamic_rnn(cell=cell, inputs=x_input, sequence_length=x_input_len, dtype=tf.float32) 
output = outputs[:, -1, :]
logits = tflearn.fully_connected(output, 10)
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_input)
loss = tf.reduce_mean(cross_entropy)
optimize = tf.train.AdamOptimizer(learning_rate).minimize(loss)```

Она не сходится (на 10-классовой классификации выдает accuracy стабильно 8.75% и дальше не идет).
Проблема решается, если:
- заменить output = outputs[:, -1, :] на output = states.c
- убрать параметр sequence_length
Не понимаю, как это работает и почему это так. Прошу объяснить.

Я полагал, что последний output (т.е. outputs[:, -1, :]) - это то же самое, что states.c, но, почему-то это не так",
"И еще сразу задам вопрос:
когда мы пишем
```lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm] * number_of_layers)```
у нас эти lstm имеют одни веса, или это разные lstm в количестве number_of_layers штук?",
"хм, там действительно есть: 
csr = scipy.sparse.csr_matrix((dat, (row, col)))
dtrain = xgb.DMatrix(csr)интересно, как реализовано",
"У меня есть переменная, которая принимает значения от 1 до 7000. Нужно предсказать ее значение. Сейчас я делю ее на интервалы и решаю как задачу классификации.  У меня 14 интервалов. Но тогда, если прогнозное значение попадает в 10 интервал, а модель предсказывает 1 или 11 интервал, ошибка получается одинаковая. Но это же не так. Нужно чтобы предсказание было как можно точнее, то есть предсказание 11ого интервала за место 10ого предпочтительнее. Да и хотелось бы предсказывать  значения более точно, чем с шагом 500. При этом у меня нет временной зависимости. Если не разбивать на интервалы, то остается только регрессию использовать?",
<@U1FLG6YR1> А можешь чуть подробнее рассказать про контекстных бандитов? Где можно почитать про применение их на практике? Почему бы не просто руководствоваться предсказанным CTR?,
"Гугл как всегда, максимально к оптимальному производительность/цена. Вообще пора бы уже и в другую сторону двигать науку. К нейросетям, которые требуют большей точности чисел и за счет этого решают проблему затухания градиента.",
Размер весов кстати можно серьезно так уменьшить я просто не задумывался и выодил как {:.12f} там куча нулей и тому подобной чуши.,
Из любопытства - а тебе зачем такое?,
"<@U36Q9NJMD>  Я более склонен к словам Дональда Кнута. Он сказал типа ""Лучше делать хорошую науку, чем популярную. Если что-то популярное, то это по-моему не правильное :whaaaaaat: :wat:"" - хотя звучит как отмазка  :slightly_smiling_face:",
"Подскажите, есть какой-нибудь способ доказать, что на исходных данных нельзя получить модель лучше какого-то уровня?   А то не понятно, когда остановиться в попытках улучшить модель.",
"O, в продолжение вопроса: есть ли какая-нибудь теория, которая может нам сказать, что на этих данных невозможно построить точную модель, потому что не достаточно иноформации? Например, если у нас X это вектор состоящий из одинаковых (очень близких) значений, то ясно, что хорошую модель не построить. Вот есть ли какой нибудь метод количественно оценить это ""ясно""?",
"<https://github.com/precedenceguo/mx-rcnn>
а как вообще сюда корректно подключать новые датасеты? писать самому адаптер(когда то столкнулся в <https://github.com/rbgirshick/py-faster-rcnn>) или есть легче пути?",
"Коротко -- нет.
Чуть более развернуто -- как советует <@U0KQ5M6KX>, нужно ввести формализм, явно обозначить все используемые предположения о данных и тогда, возможно, вы сможете что-то обосновать для конкретной задачи.",
"заедем, когда звезды правильно станут )) за приглашение спасибо",
"мы приглашение будет рассылать в понедельник (первую часть). Кстати, что бы попасть на ивент нужно как минимум написать про свой опыт и хоть немного мотивации. Три слова в мотивации не внушают доверия к человеку (мотивацию можно редактировать)",
"поскольку ивент бесплатный и количество мест органичено, то мы будем приглашать только тех людей, у кого релевантный опыт и хочет прийти к нам. Опыт и мотивацию нужно хорошо описать. А не вот так, как некоторые товарищи
&gt; my name is motivation",
"Разгонять будешь? Хотелось бы увидеть тоже результаты, думаю как раз 1700 взять, все равно с нуля собираю",
"<@U07V1URT9> когда приедут? интересно понять,как они, но это ждем тебя, и понять бы когда они уж появятся в Мск",
<@U07V1URT9> как кстати называется топовый магаз в мск который ты советовал?,
<@U32H8EW2F> а как ты собираешься смотреть видяху вживую? и отличать норм от не норм,
Это пример как получить предсказания с промежуточного уровня в Keras.,
"так же гайд должен работать для других материнских плат (кроме пункта 1, естественно). Для этого надо найти BIOS без микрокодов, или вычистить их самому (я не понял как это сделать, но на anadtech есть информация)",
"Запусти какой-нибудь тест, чтобы посмотреть какая скорость этого дела получается",
скажи какой :slightly_smiling_face: я пока только linpack + cpuburn гонял,
Кто то сегодня на семинар Ветрова идет ?,
а у тебя какой проц?,
"У меня 2*Xeon E5-2670, 2.7Ггц, турбо 3.0 как раз",
что-то не совсем понимаю как результаты интерпретировать... получается E5-2670 &gt; E5-2683 ?,
какие доводы в пользу титана?,
"если кто-то не знает, как скачать ImageNet бесплатно, без регистрации и смс, погуглите ""imagenet  download dataset"" (спс, кэп) и третья ссылка (оканчивается на 5jj5.php) ведет на страницу с датасетом 2014 года. Видимо кто-то там у них накосячил с разрешением индексации страниц",
ну зачем сразу то ногами в живот,
"Когда изучишь общую формулу Стокса по Зоричу чисто для интереса попробуй решить задачки из Демидовича  на тему ""Формула Стокса"" и ""Формула Остроградского"".",
"А где грязь, не могу найти?)",
"Кто подсказать может, где можно достать англо-русский словарь(общая лексика), желательно с примерами контекста на английском, порядка 200к слов. Что сейчас делал - просто взял с гитхаба список 400к слов на английском и слил что можно с Яндекс словаря - получилось ~70к, но там много всяких названий и вообще узкой терминологии. Куды смотреть не подскажите?",
"&gt; Я заказывал в computer universe

&gt; на computer universe же есть гарантия

Хороший магаз для покупки через интернет с доставкой, да? Есть у кого больше подробностей/отзывов?",
<@U1UM6S9KN> зачем решать задачки из демидовича?,
<@U0JJ69UB1> вы будете участвовать в этом как 5vision ^,
"Скорее всего нет, там бенчмарк что-то с простыми числами связанное, зачем там линейная алгебра?",
"Я бегло поглядел, sonnet судя по всему должен быть удобен для генеративных моделек.
Как раз там, где керас очень неудобен.

Для динамических графов есть TensorFlow Fold",
"<@U1R8X3V54> в urss появился, посмотри. правда формат издания конечно не такой располагающий как у Зорича :disappointed: Зато больше всякого из функана",
"Из диалога с моим товарищем:
- Слушай, а читал книгу камынина по матану?
- ты знаешь Каледина? Дмитрий кажется
- неа, а кто это?
- Друг Миши, Он с мехмата. Сильный математик. Он написал рецензию на эту книгу
- а, и что там?
- Привожу полностью: ""Вот говорят — хороший учебник. Открыл. Там плотные множества называют густыми. Охуевши закрыл” :grinning:",
"Питание, вроде, обдувает. Если я правильно понял про какой ты. Оно прилично греется тоже.",
"друзья, а че-то у меня Керас после `load_weights` обучение как будто заново начинает :hm-squared: 
у меня LSTM и RMSprop. нагуглил, что оно у кого-то еще себя так ведет: <https://github.com/fchollet/keras/issues/2378>
но не нагуглил решения :pepe_sad: может кто сталкивался и победил? :muscle:",
"ебаный майкрософт, поставил эту херь, хер знает куда она поставилась, в меню нихера не добавила, блять",
"<@U040HKJE7> так синяя -- это и есть модель (1-слой лстм) обученная на черное. надо еще с размера окон и прочими гиперпараметами поиграть, но пока лучшее из получившегося  по MSE-лоссу
а вот more-layers как раз чего-то не заходит",
"зачем фурье, если есть lowpass фильтры (типа скользящих средних и медиан), которые сразу в time domain работают?",
"Рассудит эксперимент, почему не попробовать ",
"<@U0FEJNBGQ> ну для чистоты эксперимента лучше посмотреть на спектр и отрезать шумную низкочастотную часть. а  так какая разница, в scipy/numpy все равно все в одну строчку делается.",
"А в какую сумму выходит все вместе ,примерно? Билет, виза, проживание, сама школа.",
"Если скользящее среднее - свертка и длина свертки существенно меньше длины массива, то она быстро реализуется как раз через FFT, блоками.
Поэтому lowpass в Фурье области, это низкоуровневый подход к той же проблеме",
а какие алгоритмы сейчас state of art в этой задаче?,
"Я на R небольшой велосипед написал как раз для статистик с графиками сразу: <http://biostat-r.blogspot.com/2015/08/blog-post.html> Но для R много пакетов написано именно для такого вот репортинга, на питоне больше руками писать придется.",
"и все же, почему oob_score_ в sklearn RF дает более оптимистичную величину ошибки?",
"хочу набросать более или менее универсальный sanity check скрипт для простых моделей. давайте в :thread-please: накидаем, какие базовые проверки стоит проводить всегда?",
"например:
1) подходят ли данные для такой модели (отскейлены ли для линейной, не слишком ли sparse для деревьев)
2) насколько разошлись ошибка на трейне и тесте, нет ли оверфита
3) сохраняется ли соотношение классов в тесте и в предсказаниях 
4) как модель перформит по сравнению с очень тупым бейзлайном 
…

набрасывайте!",
"Еще оказалось, что кухня - не самое удачное место для компа, если любишь стейки. Я столько жира с вентиляторов соскреб, словно через них прошли все шутки в рф, когда обсуждали феминизм.",
попробую как они предлагают через новый бэкенд libgpuarray,
"если что-то не компилится, то надо посмотреть, как оно компилится - не инклудящийся файл найти и его папку проверить в include paths `-I`, не линкующуюся библиотеку найти и ее папку проверить в `-L`
переменные окружения еще полезно export-ить на всякий случай, но тут вряд ли этот случай",
как мне казалось этого достаточно,
А кто ставил LightGBM на WIn? Вот после билда на выходе lightgbm.exe— его куда теперь? Чтобы видел Python.,
"Не могу найти тут в поиске :(
Мб у кого-нибудь нужная ссылка недалеко... 

Где можно взять датасетов по болезням каким-нибудь? Есть много вроде данных пульса или экг.
А есть что-нибудь вроде пневмонии? ",
А какую картиночную модель сейчас моднее всего применять для поиска по картинкам? Все ещё везде VGG из-за большого выбора юзабельных боттлнеков или резнеты и прочие инсепшны лучше работают?,
"Скажите, а cudnn с кудой встаёт на Ubuntu 17.04? А то у меня 16.04 LTS, и я хочу обновиться до 17.04, но боюсь, что TF не заведётся :pepe_sad:",
omtcyfz: поставь рядом (или где нибудь на амазоне спот на часок) ,
"И куду сразу 6, чтобы контрольный :gun: дать",
<@U1CF22N7J> амазон споты - это неудобно. а про куду 6 не очень понял :pepe_sad:,
"я качнул, кинь куда отправить в личку",
<@U4D3FH3A7> а как этот лимит подсчитывается? тупо по ip?,
"Ну и все выходные развлекался с тем, что собирал компы. Теперь, когда все руки исцарапаны как после котенка, могу сказать, что Termaltake core V31 лучше, чем Phanteks Eclipse p400.",
"Как жаль, что у меня их 4. Хотя нет, не жаль :kekeke:",
"Ну знаешь ли, к уголкам проще фильтры присобачить какие хочешь,",
"Стальная сетка, как в вытяжках, она притягивает жир ))",
"Как подготавливают данные при задачах локализации?

Для классификации можно просто кропать, для сегментации тоже.

А вот если у нас есть картинка 400x1000 и на ней что-то выделено в bounding box.

Что обычно делают? Перегоняют под квадрат, забивая на то, что изображение сплющит по горизонтали?

Кропают?",
"Например, при локализации plate numbers точно не меняют aspect ratio, так как он имеет значение. Но вот _вписать_ и _отмасштабировать_ во что-то можно.",
"Например я хочу посчитать значение квадратичной формы (mu - mu_0)^T(mu - m_0) в точке mu = \beta/(\beta + n)mu_0 + 1/(beta)x

Руками есть риск ошибиться, а эти формулы нужны для программирования пересчета параметров и потом не понятно где искать ошибку, то ли в коде, то ли эти формулы заново переписывать",
Необходимо на изображении выделять обпределенные объекты в bounding box. Сами объекты могут быть сильно разные по размеру. Какой метод на данный момент лучше выбрать? Желательно чтобы сеть это выполняла в один проход. Пока остановился на сети yolo <https://pjreddie.com/media/files/papers/yolo_1.pdf>,
Может есть какие еще однопроходные решения?,
Вынося из треда к одному из предыдущих сообщений: почему данные для tree-based models не должны быть слишком разреженными?,
"Вот интересно, подойдет такой пример. Решаем задачу регрессии. Пусть пространство признаков двумерное. Пусть есть две точки в обучающей выборке. Одна (0, x12, Y1) другая (x21, 0, Y2). Линейная регрессия тогда вроде построит плоскость перпендикулярно отрезку соединяющие точки. А дерево вроде научится говорить что если ты в плоскости  (0, x2) то Y1, а если в (x1, 0), то Y2. А точкам вида (x1, x2) будет присваиваться Y1 или  Y2 в зависимости от того с какого признака и какой точки дерево начинало перебирать при построении себя",
"Ну и когда данные сильно разряжены, получается по сути такая ситуация как я написал вверху",
"<@U07V1URT9> у меня на работе с такой затычкой pci-слота вместо видеокарты даже новый лидербоард тормозит, как ты с ней вообще живёшь?",
"нет, я думаю что <@U064DRUF4> сравнивал, плюс недавно был <http://www.dialog-21.ru/en/evaluation/2017/morphorueval/> - там выложили больше корпусов, надеюсь что будут и статьи где будет сравниваться все это (не знаю правда будут ли смотреть отдельно точность приведения к начальной форме). Какое-то количество неправильных начальных форм mystem выдает, но оценку качества сходу не могу найти",
"Может теперь ещё один график, где эти величины ещё и на стоимость видюх в долларах поделены?",
Мне прям стало интересно кто как ведет конспект. Пишите ли вы доказательства сразу? Или может быть конспектируете основные моменты док-ва и потом доказываете сами? И кто как читает книги математические?,
"Ребят, есть какие-нибудь советы по 4к гейминг монитору? 30-40к рублей. 
И вообще вот думаю как 4к для например контры(на про уровне:rage4:) (1080ti все потянет вопрос именно моника)",
"• Если книга бумажная, моя, и сложная, непонятные моменты (важные для общего повествования/очень неочевидные) доказываю сам на бумаге формата A6 и вкладываю внутрь на соответствующей странице (на листике пишу номер страницы — пару раз все непомеченные листочки выпадали из книг). В менее важных случаях просто ставлю восклицательный знак карандашом, как пометка, что потом надо доказать (никогда не доказывал).
• Если книга электронная, непонятные моменты доказывал на листах A4. Они обычно терялись. Сейчас я завел тетрадь для этого.
• Если книга очень понравилась, то могу даже конспектировать её (было один раз). Хотя конспектирование существенно увеличивает глубину освоения материала, к сожалению, не всегда хватает на это времени.
• И конечно упражнения, тут уж, без тетради или кода, мне кажется никак.",
"ну для писания куда у меня моники есть, просто вот стоит ли купить 4к ибо это 4к или не париться и купить просто fullHD (но это не 4к:(  )",
"Вопрос по tensorflow, как сохранить / загрузить веса? Поиск по SO особо не помог.
Я выше привел код сниппета - при первом запуске получаем модель (раскомментируем строку), затем снова запускаем - все работает, готовая модель предсказывает нормально. Python 3, tf 1.0.1

Если теперь закомментировать первый “with tf.Session() as sess:” и еще раз запустить, то будет модель загружаться, но предсказывать полный рандом. 
Вопрос: почему так, что вообще происходит?",
Смотря для чего тебе 4к - играть на 4к с йоба ГРАФОНОМ у тебя даже на 1080ti скорее всего вряд ли выйдет (ибо на 1080 тот же ведьмак на ультрах и с волосами проседает до 20 фпс на 4к мониторе). А для контры/дотки - зачем 4к?,
Попробуй сохранять-загружать как тут - <https://www.tensorflow.org/programmers_guide/variables#saving_and_restoring>,
"Ну да, но у меня без global_variables_initializer и созданием saver как там - все работает :idk:",
можешь показать код? а то я как обезьяна тыкаю,
"такой вроде как заработал, спасибо!",
"а не подскажешь, почему все таки в одном случае нормально веса грузятся, а в другом криво (исходя из первоначальной постановки вопроса)?",
"С чего проще начать - учить чистый tensorflow :tensorflow:  или keras :keras: ? Где больше документации, меньше багов, подводных камней и проще порог вхождения?",
"<@U29TG6K9U> прикольно. ""(никогда не доказывал)” порадовало))) жиза жизненная. Еще бывает “Ну, это упражнение потом порешаю…”.
И еще заметил, что для освоения какого либо предмета очень желательно иметь две книги. Чтобы там была одна концепция изложения (похожие определения), но были и разные дополняющие вещи",
"господа, а вот такой вопрос, задача бинарной классификации, в трейне пропорция классов допустим 0.3 и 0.7, а в тесте 0.2 и 0.8 (узнать можно попинговав публичную часть на кегле); ну и вот, по идее при обучении нужно веса семплов выставить в соответствии с распределением тест сета, но нужно ли на трейне выравнивать распределение? вроде как да, что бы оно новыми весами  сравнялось с новым, но как то доказать это хз как; мне чот кажется что если не выравнивать, то получается мы новыми весами двикаем распределение в тестовую сторону, а само распределение примеров в трейне двигает итоговое распределение в свою, и получится, что то посередине",
"Поэтому -- что тебе самому-то нужно? Монитор 4к -- это в 4 раза больше пикселей для всех задач, и гладкая приятная картинка, как на телефонах или айпаде, прикольно даже без игр",
Монитор 4к — это в 4 раза больше пикселей для задач ИЛИ гладкая картинка как на айпаде,
вообще в теории стоит.. так мы получим схожие распределения и особенно эффективно будет когда не будет(или качественных мало) фич которые хорошо предсказывают зависимую переменную,
"Как в bigARTM получить вектор распределения вероятностей топиков для нового текста, который не входил в обучение?",
"<@U47A6MR1R> Он принимает batchvectorizer. Т.е. мне нужно переводить строку в какой нибудь VW формат, а потом делать из него BatchVectorizer?",
"Справедливости ради, pymorphy2 не является контексно зависимым морфером. Он анализиурет только само слово без контекста. mystem3 вроде как действительно контексно зависимый. Я думаю pymorphy2 может работать на одном слове, так как в русском редко одинаково пищущиеся слова относятся к разным частям речи, типа стекло, печь (глагол и существительное), поэтому прокатывает. В английском далеко бы с таким подходом не уехали.",
<@U47A6MR1R> А как это сделать не подскажешь? Он же путь требует к файлу.,
"Кстати, а Intel MKL на процессорах AMD Ryzen как себя ведёт?",
"И этот 1 пиксель -- куда жирнее чем волос или лист бумаги, это раз",
"А еще, не напомните команду, чтобы посмотреть какие видеокарты с какой скоростью друг с другом обмениваются.",
"А второе -- играет роль не только толщина отдельных пикселей, а ещё точность позиционирования, почитай какие сложности связаны с рендерингом текста на таких девайсах ,как современные мониторы с 96dpi",
"У меня сейчас скорость падает в два раза, когда я переключаюсь с одной видюхи на две.",
"я и говорю что как пидор, ну ваще по нему видно что он жучара, не то что :schmidhuber:",
А как это посмотреть в линуксе? :kekeke:,
"То, в каком порядке перечислены GPU в nvidia-smi, и в какой последовательности они представлены для CUDA-программы - не совпадают.",
"Ты можешь изменить то, как CUDA драйвер нумерует устройства посредством выставления переменной среды CUDA_DEVICE_ORDER, см <http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars>",
"после ретины даже full-hd как песок в глаза, честное слово)",
"кто-то файнтюнил ResNet-50? хотелось бы узнать какие блоки лучше дообучать. я сначала навесил dense размером в 2048 (также перед и после дропауты в 0.5). 
первый шаг - обучаю конкретно этот dense 
на втором шаге - позволяю тренирвоатся последнему конволюционному и двум identity блокам. Результаты очень хороши в сравнении с VGG16 и InceptionV3, прмерно +12% на валидации. Но хотелось бы узнать или прочесть как лучше всего тюнить эту сеть. Или все зависит от набора данных и их специфики?",
"не, <@U1BAKQH2M> про то, что туда можно только сами данные кормить, а не попарные расстояния, как для обычных кластеризаций",
"можно наверное пробовать minibatch kmedoids родить, но изкоробочной реализации хорошей я не видел, когда искал такое",
как в спарс то уместить если расстояния плотные по определению?,
а как ты планируешь делать спарс матрицу попарных расстояний?,
"Да, много, если расстояния плотные то как иначе?",
и почему не в треде,
А как в керасе проще всего sampled softmax заюзать? ,
Какой способ доставки стоит выбрать при покупке на natex?,
"Как жить с оптимизацией с помощью лагранжиана, когда ограничения в виде неравенств? Я собираюсь использовать стохастическую оптимизацию, но если ограничение не активно, то я боюсь, что у меня множитель лагранжа улетит в небытие(inf). Не понятно как обрабатывать случай когда оптимум внутренний.
Скорее всего конструкция будет вида 
```
loss += lam*tt.switch(G&gt;=0, 0, G)
```",
Я не вижу проблем сделать выход на 1000 bounding boxes. Я вот понять не могу что туда при тренировке подавать. И в какой из выходов подавать правильный bounding box?,
"при тренировке? да, как и для меток класса
только у одного класса стоит метка 1 и bbox с ненулевыми координатами
во всех остальных метках и координатах нули",
"Привет всем. Подскажите, какие существуют алгоритмы поисков аномальных значений временного ряда (выбросов)?",
"Так это прекрасно. Но это проблемы post processing, до них дожить надо.

Собственно на ImageNet на Localization надо классы + bounding boxes для топ 5 классов,
На detection для всех.

То есть неулевые предсказания мне как раз и нужны.",
"А зачем мутить?

Можно начать либо с rescale, либо вставлять в большую картинку. А потом как что-то пойдет можно глянуть на SPP Net, мужики в статье с Mask R CNN - пишут, что это просто обязательно.",
"Я в общем тоже, поэтому и хочется на этой задаче разобраться кто чей брат, тем более, что с валидацией проблем нет.",
"Всем привет. Можете подсказать каким образом лучше собрать 4 1080 ti? Если например использовать ASUS X99-E-10G WS, то не будут ли перегреваться карты?",
"Так чтобы батч пополам побить и потом собрать корректно - я не знаю, какой фреймворк это прозрачно умеет",
"А если задача и так невыпуклая, то можно попробовать переформулировать задачу как безусловную оптимизацию",
"Задача выпуклая, а как проецироваться непонятно. У меня есть функция `f(X)`, а ограничением является, то, куда она попадает, там интервал. Потери это другая функция `c(X)`",
"не знал в какой канал обратиться, решил сюда. Задача надергать фоток по тегам через инстаграм. Но вот не получается. Что то с правам доступа как я понимаю. Создал аккаунт разработчика.
Создал клиента в Manage Clients. Статус у него Sandbox Mode.
Я получил access_token, по client id, который необходим для аутентификации.
Хотел воспользоваться методом <https://api.instagram.com/v1/tags/> {tag-name}/media/recent?access_token=ACCESS-TOKEN для выгрузки данных по тегу (вот документация: <https://www.instagram.com/developer/endpoints/tags/>).
Им не получилось воспользоваться.
Если отправлять только с этими данными + обязательный параметр scope=public_content вываливается в ответ ""next_max_id and min_id are deprecated for this endpoint; use min_tag_id and max_tag_id instead"".
Если заполняю min_tag_id=MIN-TAG-ID, то вываливается ""min_id must not be a media id""
Если запрашиваю данные, связанные с моим аккаунтом, например <https://api.instagram.com/v1/users/self/?access_token>.. или <https://api.instagram.com/v1/users/self/media/recent/>.., то все Ок. Данные приходят.
Еще пробовал, <https://api.instagram.com/v1/users/search?q=QUERY>... в ответ вернулось
{
""data"": [],
""meta"": {""code"": 200}
}",
"<@U14CTBLFJ> а ты собрал машинку на Ryzen? хотелось бы услышать впечатления. И вообще, какая у тебя конфигурация?",
"Товарищи, а может кто-нибудь помочь обновить excel-файлик Eikon Reuters? Может, у кого на работе есть?",
"Господа, помогите разобраться начинающему, пожалуйста.

Разбираю пример классификации изображений: <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb>

Цель — на своих данных распознавать не 10 классов (как в примере), а 250.
(Картинки как в примере — 28х28,просто чёрно-белые, визуально распознаются норм)
Похоже что не достаточно изменить только размер выходного слоя (сеть просто не сходится), а нужно изменить ещё гиперпараметры (?)

Наверное надо поставить batch_size такого размера чтобы в каждом шаге были примеры каждого класса.
Опытным путём на 200ах классах пришёл к
batch_size = 500
patch_size = 15
Буквально небольшое изменение — сеть опять не сходится.

Ну ок, обучаюсь. Сохраняю граф вычислений для использования в другом месте, а он весит 100 мегабайт, как-то многовато.

Вопрос 1. Верны ли мои рассуждения относительно параметров и сходимости сети? Или причина несходимости сети при увеличении классов на порядок может быть в другом?
Вопрос 2. Почему граф такой простой сети столько весит? Что я делаю не так? Как уменьшить?",
"У меня раньше приложение крутилось в Google App Engine пару лет и использовало API инстаграм, а в феврале кажется 2016 они ввели жесткие ограничения, и в мае моё ""приложение"" перестало получать респонз. Я написал им заявку, но мне отказали. В техподдержке написали, что желательно в заявке присылать ссылку на youtube видео, где будет показано, как работает приложение, которое будет API использовать.",
"попробуй способ выше, вроде размер это тебе уменьшит, я в inception не копал внутрь, не могу сказать, оптимизированную версию они предоставляют или как есть.",
"Собственно решил не переобучивать Inception на своих данных из-за того что посчитал что это слишком сложный инструмент для моей простой классификации. Поэтому оказалось неожиданным что получившийся у меня результат ощутимо больше весит чем оригинальный Inception, рассчитанный на куда более сложные картинки",
"Ребят, кто с телекомом работал, какие хорошие показатели по оттоку precision/recall по классу оттекающих с ежемесячным оттоком в 5-10% от базы?
Понимаю, что вопрос общий, но всё же.",
"Да собрал, что-то жирное из xgboostов не тестировал, всё ещё настраиваю под себя. Я напишу тут впечатления, как они будут)
Я взял Ryzen 1700, 32 Гб ОП Какой-то понтовой от Корсара и 1080ti.",
"медленный через иммитацию ajax requesta который улетает, когда ты новых пользователей подгружаешь",
нужно подумать как с тегами,
"всё, нашел как по тегам сделать",
"<@U14CTBLFJ> очень интересно послушать, У меня тут один знакомый две недели назад собрал на RYZEN 7 1800X + 2Х1080TI весьма доволен, А какую мать использовали?",
спасибо Плиз как только появятся впечатления - отпишитесь,
"да, хотелось бы узнать, как он на не DL задачах, mkl взрывает его и прочее",
я от этого своего знакомого слышал что очень гуд если запускать работу с картинками если вешается задача по предобработке на проц параллельно с сетью на GPU - типа очень шустро выходит как на дорогих Интелах,
"Стою на пороге покупки компа для ML - планирую сначала собрать все без карты, потом докупить уже видюху
Сейчас вопрос конкретно про процессоры - какая линейка лучше для  ML - Xeon или i7 ? 
в процессоре на что обращать основное внимание - размер кэша, частоту или число ядер?",
<@U0H7VBQQ1> ну так DL я все равно пока делать не буду - буду на видюхе когда она появится. Или все равно проц будет важен?,
"<@U0FEJNBGQ> 
Это отдельная (и больная) тема.
Часто хочется видеть причины и какой сегмент уходит (богатые/бедные).
А так да, как описал <@U4P10HEJV>, впрочем, обычно этим занимается уже сам оператор.",
"Я использовал sphinx search в паре проектов. Тоже достаточно просто прикрутить. С русским нормально у него, так как русский разработчик.",
"А как вы смотрите что сподвигло/предшествовало уходу? Фичи импотанс в дереве, например?",
"<@U14CTBLFJ> 
Это тоже, но это неоднозначный подход, особенно, когда фичи относительно сложные или какие-то тренды/агрегаты. Плюс корреляция не значит причинность и т.д. Но поспекулировать и найти объяснение часто можно :)
Обычно делаем какую-то постсегментацию, какой возраст ушёл, какие у него были связи (свой абонент или чужой), на каком уровне арпу (условно) его начинать пинговать и т.д.
И потом уже строятся какие-то теории.",
"я вот тоже не понимаю зачем и веса и семплить одновременно (ты же это понимаешь под ""выравнванием распределения”?). интуиция говорит что одного дейтсвия достаточно",
"<@U0ZHCGY5T> вопрос в том, какие хорошие фичи? а какой вид оттока?
 b2b / b2с / внутренний / ранний ? у каждого своё играет...",
"гайз, а поясните, пожалуйста, для :ololosh: как бы заюзать эдвард для классификации временных рядов?",
"Его нет. Это лично мое мнение, конечно. Как мне кажется бизнес модель AMD - поставлять железки OEM производителям, которые тяп ляп и готово, опенсоурс особо не замачивается с компиляцией своих продуктов под архитектуры амд. Однако, если Вы большая исследовательская группа или компания, где есть ресурсы на комплияцию и отладку чужого кода, то амд - это способ сильно сэкономить на железе, для OEM производителей они заморочились с инструментами разработки, это можно использовать.",
"<@U14CTBLFJ> спасибо, мы тоже в этом направлении думали. По-колхозному мы придумали делать бинаризацию Оцу и считать число кластеров, чтобы определять размытость, и также сравнивать среднюю интенсивность всего изображения со средней интенсивностью его частей (например, поделить на 4 равные части), чтобы определять равномерность освещенности. Думаем еще какие требования выдвинуть к разрешению минимальному.",
"Всем привет!

А кто-нибудь может дать практические советы по использованию дерев решений? В частности:

1) Если на кросс-валидации score высокий, то это говорит о том, что нет оверфиттинга?
2) Как определить факторы, которые реально влияют и будут влиять в будущем? Хочу сократить модель, но без PCA.
3) По опыту, сколько минимум листов нужно, чтобы модель была репрезентативна? Желательно в % от выборки, у меня где-то 800 строк всего.
4) Какая максимальная глубина дерева? Есть способы определения оптимальных параметров?",
"ага, там как раз перед morpheval диалоговским добавили много, видимо релиз еще не вышел, так что master наше все",
"Привет, возможно вопрос - дубль или оффтоп, но задам:
Порекомендуйте большой мануал, а лучше книгу по CNN. Хочу разобраться с различными слоями типа pooling например, как работаю каждый, когда и зачем нужен. Вокруг все по крупицам есть конечно, но может кто-то уже находил клад.",
"вроде бы в ESL писали, что при большом K модель начинает вести себя как линейная 

но да, называть её линейной нельзя",
"ребята, вот я взял resnet50, отрезал последние слои (include_top=False), но вектора, которые получаются - не решают мою проблему (детект фоток ""близких"" вещей), как я понимаю, мне надо дообучать сеть для моей задачи, правильно? Порекомендуйте релевантный материал на данную тему.",
"подскажите, как подступиться к такой задаче:
есть item и его представление в двух пространствах - item|ourTag1,ourTag2.. и  item|otherTag1,otherTag2.. 
нужно найти взаимосвязь, чтоб с минимальной погрешностью можно было конвертить из одного в другое. если это еще и красиво визуализировать можно вообще огонь.  
сейчас пробовал кластернуть к-минсом смерженные вместе теги, но получилось что-то не очень.",
<@U364MEXR9> <@U4CKAGJMT> ребят а можете скинуть как и что делать? Что получилось в итоге,
"Я предложил эмулировать ajax request который уходит от инстаграмма, когда ты в браузере данные подгружаешь (например когда ты сделал поиск по тегу, и нажимаешь кнопочку ""показать еще"" - уходит такой запрос, который мы хотим эмулироать в headles mode). Т.е. ты посылаешь POST запрос на получение данных, парсишь нужное тебе из ответа (в данном случае это картинки, и endCursor (индикатор текущей страницы) из предыдущего запроса), и потом  в цикле используя данные из предыдущего запроса подгружаешь новые данные.",
"Разве к тем, кто покупает десятки карт не особое отношение у продавцов? :)",
"Каждый раз, когда пересаживаюсь на Xeon 2630 (10 ядер по 2.2 ГГц) не покидает ощущение тормознутости и неторопливости. У меня нет замеров, но mxnet компилится вечность. Я вчера упел стейк приготовить и вернуться пока он компилился и потом выпал с ошибкой. При том, что я делал make -j20.
Либо я не поставил какую-то библиотечку, которая умеет использовать все ядра через тайные протоколы общения - я хз",
"Есть графовая инфа вида ```(node_start, node_end, x1, x2, ..., xn)```, где `xn` - вероятность принадлежности пользователя к некоторой категории. Хочу предсказывать для пользователя с заданной `node_start` и вектором  `x` вероятность переходв по нодам, не соображу как расписать модель",
"<@U064DRUF4> наоборот бы еще :imagination:. правда, мне показалось что там у вас не очень корректно. почему POS идет через пробел. хотя в версии от syntaxnet вся строка FEATS через |",
"У меня есть 150к английских слов, нужно для каждого слова три синонима. Первая идея – взять из w2v top 3 для каждого слова. Но работает очень долго, 4 слова в секунду, параллелить не вариант, так как нужно передавать w2v в каждый процесс, а он 4гб. Пробовал wordnet, но чет вообще не очень. Что посоветуете?",
"Есть ли какое-н исследование, которое показывает как различные глубокие архитектуры (fully convolutional, resnet, inception, etc) работают в задачах обучения embedding'ов? <@U041P485A> , <@U06K9ELB1> ?",
"пиши дальше умные слова плс, node_start, node_end - просто соседние вершины, переходим по ребру, как дальше писать?",
"Объясни подробнее, что x значит. Я пока не пойму, как он на задачу влияет",
"все же, как мне кажется, в гугловском w2v очень много весьма специфичных векторов, думаю, можно взять что-нибудь с меньшим размером словаря",
"Окей
Пусть `s` – текущий товар, `t` – товар, на который пользователь может перейти, `g` – пол пользователя (неизвестная величина), от которого зависит вероятность перехода: `p(t | s, g) = P_{g,s -&gt; t}` -- тензор вероятностей перехода размера `len(genders) x len(nodes) x len(nodes)`. Увы, `g` мы не знаем, но знаем, что `g` распределена как категориальная величина с вероятностями `x`: `g ~ Cat(x)`, т.е. `p(g = male) = x_{male}`. Тогда нужно просто маргинализовать `p(t | s, g)` по `g`: `p(t | s, x) = ∫ p(t | s, g) p(g | x) dg`",
"генсиму все равно, как вектора были натренированы, можно и вектора от glove подсунуть, там только в первой строке надо добавить количество слов в словаре и размерность",
"То есть, тебе нужно сделать reshape P в тензор `len(nodes) x len(nodes) x len(genders)`, а потом посчитать dot product с вектором x, получив матрицу маргинализованных вероятностей перехода, где `p(t|s,x)` будет равно средневзвешенному `p(t|s,g)` с весами `x`",
"наверное если матрица перехода из одного пространства в другое построена с помощью факторизации, то получится в некотором роде кластеризация, как в NMF",
"Есть поток событий, с координатами. Пусть координаты распределены как смесь гауссиан. Кроме того, я делаю предположение что условное распределяете числа точек в каждом кластере, при фиксированном числе точек в остальных кластерах пуассоновское с параметром интенсивности лямбда. Хочется отсюда выцепить какое-то взаимодействие лямбд. Не понимаю, как сделать. ",
"еще подозреваю может сильно влиять какой blas стоит и т.п., например на ubuntu из коробки вроде медленный идет достаточно",
<@U14CTBLFJ> когда планируешь затестить xgboost на ryzen?)),
"я всегда делаю по частоте, так удобнее - но я не знаю как сделано в конкретной гугловской модели",
"Пусть X_k число элементов в k из K кластере. X_k ~ Poisson(lambda_kt|X_{K\k}). Хочется физически сказать следующее: если новых точек нападало в какой-то кластер много, то в другие прийдет меньше, но только я не знаю заранее какая там зависимость. 

Веса гауссиан не очень подходят. Кластера я выучил один раз и предполагаю их стационарными. Новые точки буду приписываться по наименьшему соответствующем смаханалубиса расстоянию.  А количество точек в них хочу моделировать.",
"на всякий случай, restrict_vocab 100 это только для проверки как упорядочен словарь, если нужно получить синонимы то так конечно не надо делать",
Я хотел в тупую попробовать так. Сказать что общее количество распределено как Пуассон с интенсивностью лямбда^ТWлямбда. Но выразить W не вышло ,
"привет) Как сделать, чтоб нейросеть не только выдавала ответ, но еще и уровень её уверенности в этом ответе?",
"Если argmax от вероятности, тогда уверенность -- как раз вероятность",
"В общем случае делать дополнительный выход, вопрос только в том, как его тренить.
С генеративной все сложно :slightly_smiling_face:
Для регрессии задача похожа на что-то с байесом: предсказывать величину и разброс, штрафовать за большой предсказанный разброс, штрафовать за несоответствие тому, что должно быть",
"Именно такого исследования не встречал, попадалось в некоторых статьях сравнение разных архитектур. В целом тенденция такая, что более современная архитектура оказывается лучше более старой :slightly_smiling_face: так какое-то время назад с AlexNet-ов перешли на GoogleNet/VGG, сейчас в ходу в основном разные ResNet-ы, ну и гугл свои усовершенствованные Inception-ы использует.

Fully convolutional - видел вроде для какой-то специфичной задачи, где embedding-вектор - это не вектор, а такой embedding-feature-map (т.е. каждый ""пиксель"" feature map - это embedding-вектор), подробностей не помню.",
"вопрос: вот у нас есть некая функция M, которая отступ от разделяющей плоскости. И у нас есть ступенька, которая говорит, что мы даем штраф, если M&lt;0. Мы эту ступеньку аппроксимируем чем-то гладким, чтобы было легко искать минимум. Все вроде бы ок, но! В градиентном бустинге делаем то же самое, но я не понимаю, что является функцией M? Там же деревья. M должно отвечать за нашу уверенность в правильной классификации. Но как его посчитать для дерева?",
"И ноль обсуждения в илитном сообществе профессионалов, все кто хотел обсудили на реддите или hn неделю назад",
Как вариант можно использовать софтмакс и распределение по уровням предсказывать.,
"Кто-нибудь встречался с задачами ""оптимального сэмплирования""? Задача следующая. Есть гипотеза относительно плотности распределения случайной величины на плоскости. В данном случае речь идет о распределении вредителей по полю. Необходимо выбрать точки сэмплирования так, чтобы с минимальным количеством точек достичь определенного качества (точности) аппроксимации результата. Предполагается, что значение в произвольной точке будет интерполироваться по результатам сэмплирования. Качество можно определить как процент отклонения интерполированного значения от реального.",
"Привет, может быть кто то встречал размеченный корпус с диалогами, тоесть чтобы напротив фраз стояла какая то разметка, например ID тематики разговора, или еще какой то атрибут, наподобии как размечают тональность в твитах, спасибо",
"Если ещё подумать, то я бы попробовал предсказывать мат ожидание и десперсию случайной величины как в VAE только на выходе. Интересно что получится :slightly_smiling_face:",
"В обычном бустинге минимизируется число ошибок. Как такового отступа в бустинге нет (в классическом смысле - расстояние до разделяющей плоскости). Можно провести аналогию: отступ здесь - это число ошибок \sum_i w_i[a(xi)&lt;0] -&gt; min . Здесь тоже ступенька, которую апроксимируют гладкой функцией. А в грудиентном бустинге - там чуть сложнее... там каждый следующий алгоритм ""оптимизирует остатки"" (residual fitting, вроде бы так называют)... но опять же отступа в классическом смысле нет...",
"ну и не факт, что именно это надо, т.к. тут речь о том, как уменьшить число точек, сохранив форму визуально",
"это справедливо только для адабуста. Где сложение экспенент превращаются в перемножение. И из этого можно аналиически найти веса. В градиентном бустинге с произвольной функцией нет весов. Я все еще разбираюсь, как же там считается отступы. Такое ощущение, что отступ - это функция аппроксимирующая отступ М, который варьируется от -1 до 1",
И почему твои любимые LSTM не зашли?,
"<@U36Q9NJMD> у меня нет последовательностей как таковых, на юзера в трейне в лучшем случае два ивента, обычно меньше, всякие црф и лстм мимо",
Насколько сильно? У меня просто слот под неё как раз рядом с процом и видюхой.,
Ну у меня у прошло ноута практически взорвалась батарейка когда я считал всю ночь на ней,
"У меня у знакомого на самом первом macbook air расплавился шлейф к дисплею, когда он компилил что-то",
"Такой вопрос - если у меня есть датасет на ~миллион наблюдений, около 60 фич - половина категориальные (от 2 до 15 уровней) половина непрерывные. Когда делаю OHE количество фич возрастает до 350. Все фичи анонимные. Лес из коробки и логрегрессия жмут около 0.52 AUC. Как можно выкинуть мусорные фичи, не зная их смысла? (SVM пока не пробовал)",
<@U04725QK7> а какой аук на трейне?,
"Короч делай уже кагл ин класс, ставь пиво за топ и ты пройдешь тестовое задание как никто",
<@U436BP343> вот это глянь. тут тебе автор (бывший президент Кеггла) покажет в Экселе (sic!) как эти слои работают: <http://course.fast.ai/>,
"Насколько я понял, схема такая: строим распределение, например гистограммой, целевой фичи при фиксированном значении категориальной переменной. И вектор значений этой категорипльной переменной где такие значения где зафиксировал заменяешь вектором семплированым из гистограмки. Это на самом деле все равно семплировать как бутстрап, так что и не так уж долго",
На замену ему в названиях статей или как работающему механизму?),
у кого референс 1080 ti? Есть смысл ждать когда появятся нереференсы?,
"<https://arxiv.org/abs/1506.03134> - это как seq2seq, но на каждом таймстемпе декодера он аттендится на входную последовательность",
Согласен на 100% лишь бы не заоверфитить :slightly_smiling_face: но вопрос был в том как с результатом выдавать ещё и увереность модели.,
"Мне кажется, что если мы хотим, чтобы модель умела оценивать свою уверенность, то по-хорошему, ей надо бы иметь возможность свои оценки проверять. То есть нужны не просто данные вида ""для входа x ответ y"", а ""для входа x ответ может быть от a до b"",  иначе нужной информации просто нет (кроме случая когда один и тот же вход x повторяется в выборке несколько раз с разными y)",
"<https://openreview.net/forum?id=B1KBHtcel&amp;noteId=B1KBHtcel> -  у нас не очень понятная табличка была, наша joint модель, которая больше, чем просто pointer network, была в табличке обозначена как PN - я думаю ревьюверы нет понял вообще, что именно joint model устанавливает state of the art :disappointed: Ну и один ревью был вообще ни о чем, 1 строчка - это ужасный ревью",
"Звучит как магия -- не понятно, почему работает",
"а как бы построить классификатор, который бы отличал осмысленные предложения от бессмысленных?
лстм с кроссэнтропией над ворд2век-векторами?
или может сконкантенировать ворд2веки, западдить нулями и, например, SVM обучить над SVD? :thinking_face:",
"Вот тут тоже затупил. Это в каком случае надо?) Т е при установке CUDA для tf я скопировал файлиуи cuDNN, обновил PATH и по идее следующий шаг- установка самого tf. Речь про приложения с поддержкой CUDA, которые компилируются через nvcc которые возможно в будущем будут тут компилироваться?",
"Сейчас поставил версию ядра линукса 4.10. Поставил какие-то микрокоды для для процессора. И рядом поставил компилиться opencv с одинаковым конфигом. начал на ксеоне. Потом, когда уже 66% прошло, запустил на i7 6700k. Закончили одновременно",
<@U040M0W0S> Вопрос очень неконкретный и потому не корректный... что есть смысл? С какой точки зрения подходить к осмысленности? Классическое пример-предложение про глокую куздру осмысленно? Речь магистра йоды осмысленна? Вот ваш вопрос осмысленный :slightly_smiling_face: ?,
"Ой, а поясните для тех, кто не особо в теме, но очень хочет)) Знаю что такое лес, а вот про взвешенные классы и LE не слышала(. Можно ссылочки или хотя бы полные названия вместо сокращений, чтобы нагуглить))",
"Без логирования использования CPU не очень ясно. У меня есть два предположения: 1) Из-за большего количества потоков в первом случае не хватает оперативной памяти и используется swap, что может сильно снизить производительность. 2) При компиляции на ксеоны начинает включаться какая та дикая оптимизация кода на уровне инструкций. Это долго, плюс может быть написано не особо эффективно.",
"<@U47A6MR1R> справедливое в целом замечание, но какая для ответа разница? давайте представим, что я как-нибудь смогу решить этот вопрос. 
но я при этом совершенно точно не планирую отвечать на ""что есть смысл?"". вот уж где неконкретно))) в лингвистика это тема известных дебатов. интересная тема, конечно, но не в моем скромном прикладном смысле.
а вот примеры конкретны:

-- глокая куздра -- бессмысленна
-- речь йода -- имеет смысл.
-- мой вопрос -- как ж тонко! -- имеет смысл
-- ваш вопрос -- имеет смысл

большинство предложений в этом чате осмысленны -- это положительные примеры в задаче
негативные примеры -- я буду генерировать сам.",
"Вообще, читается как довольно straightforward задача, не без трудностей, но и не видно ничего такого, для чего нужен был бы PhD по математике скрещённый с Корменом, как это обычно выглядит, когда читаешь вакансии...",
"&gt; а как бы построить классификатор ...
... когда нет критерия что осмысленно, а что нет, но есть осмысленные примеры.
Например вспомнить про GAN. Который как раз для этих целей служит.",
"1024. Ну везде проигрывает же CPU. Но зато видно, на каких фремворках реализация сеток написана не школьниками для CPU",
И почему 16 потоков совсем не ясно. Нужно делать 10 или 20...,
"Почему столько потоков -- хз, может под другой проц изначально писали бенчмарк",
"Так какая разница, где тотал рантайм меньше, там же в скрипте всего четыре эпохи для GPU и две для CPU, как я понял. Ну и что, что инициализация быстрее на CPU, это копейки времени в обучении",
"Ну и порядок цифр ты нашел) Там обычно что-то в стиле 210s против 47s и это с разницей в два раза по эпохам. Нужно смотреть только на цифры сколько секунд на эпоху. Даже разница в 10 минут на инициализацию, когда ты алекснет учишь пару суток - ничто:)",
"я как раз про ган, подумав/почитав, и пришел к такому вопросу))) 
но вопрос-то  более прикладной -- чо попробовать первым делом)",
"LE - Label Encoding, OHE - One Hot Encoding, взвешенные классы - в функции потерь потеря с каждого класса домножается на число, пропорциональное количетсву этого класса в выборке. Про ОНЕ и LE лучше так и загуглить, сходу нормлаьные статьи даст.  А про классы хз даже, здесь просто надо понимать, как функции потерь работают. Elements of Statistical Learning в помощь",
Купляй E5-2699 или какой там самый частотный*ядра,
У тебя там железа столько -- хер поймёшь про какой ты говоришь,
"Тот же 5930к лучше по цене и производительности будет. E3 - это для производителей различных сложных датчиков, где в реал тайме что-то считать нужно 24/7 прямо на месте",
"Я поставил 17.04 и снёс. После того, как он сказал мне вставить диск с убунтой в дисковвод, чтобы поставить Intel microcode ",
"Есть интересный вопрос. Я сейчас смотрю на загрузку GPU, особенно это видно на Inference - она близка к 0, потомучто почти всё время уходит на генерацию батчей. Как бы это сделать в параллели? Кто-то писал код на Keras который это решает? Мб ссылочки есть?",
"<@U040M0W0S> какая для ответа разница? очень большая! в зависимости от того, что для вас осмысленное предложение, зависит то, какими инструментами нужно пользоваться :slightly_smiling_face: вы же не формулируете, что для вас ""смысл"", но спрашиваете подойдет ли конкретная связка инструментов... Я удивлен, что ""глокая куздра"" бессмысленна... там все грамматически и синаксически верна... что с того, что в нашем лексиконе нет ни одного слова в  предложении? 
Вот несколько примеров того, как можно понимать ""смысл"" в узком прикладном, извините, смысле:
- ""мера удивленности"" language model поданному на вход предложению, то речь йода - скорее бессмысленная :smiley:
- ""мера удивленности"" синтаксического парсера
- совсем просто: доля известных слов в предложении 
- непротиворечивость некоторому заранее заданному набору фактов
- непротиворечивость фактом, которые есть в самом высказывании
- релевантность фразы для некоторого разговора/сценариия (если вы разрабатываете, например, диалоговую систему для общения с телевизором, то фраза ""включи горячую воду"" нерелевантна и, значит, бессмысленна, хотя все формальные проверки с точки зрения правильности языка пройдет)

Еще несколько примеров пограничных по осмысленности высказываний: 
- знаменитая речь кличко про ""сегодня в завтрашний день"" 
- знаменитая речь о ""не так дорог подарок как дорого внимание""
- предложение ""Д.И. Менделеев - известный попуас, открывший Америку""",
"в моем ворд2век не будет ни ""глокой"", ни ""куздры"" -- какой в ней для меня смысл? 0",
"В твоем w2v это будет `&lt;unk&gt; &lt;unk&gt;` и как оно пойдет дальше зависит от того, как ты это обрабатываешься, и что думает классификатор.",
"Ппц, как же медленно работают жёсткие диски, уже и забыл совсем про этот ужас. Скачал ImageNet, теперь чтобы эти 350гигов как-то поворочать с hdd -- это всё по три часа занимает, аааа",
"Смотрите: ""Кривоножкин отмстил Криворучкину"". В предложении нет ошибок, слово ""отмстил"" написано именно так намеренно. В вероятностью 95% в вашем w2v нет ни одного слова... Что ж теперь, признать бессмысленным предложение? Это уже не искусственный пример, как в глокой куздре )
Вообщем, для того, чтобы решать задачу, желательно ее четко сформулировать и поставить. Если же у вас есть просто уже размеченный датасет и вам его нужно проклассифицировать, то желательно понять специфику. Вообщем как всегда... А слово ""смысл"" - ну очень расплывчатое :slightly_smiling_face:",
"<@U0AS548A1> как определить, что ""уровней много""?",
"Надо было брать рапторы, когда тут в чате кто-то почти нахаляву предлагал",
"я не понимаю, что ты хочешь мне доказать? что значение слова ""смысл"" неопределенно? да. я с этим сразу согласился. 
я не решаю глобальную задачу. и тем более такую глобальную как его определения. моя задача скромнее. я в ней не очень, но-таки представляю, что осмысленно, а что нет. и это представление у меня закрепленно в примерах :slightly_smiling_face: я тебе его попытался передать. ""глокая куздра"" не имеет смысл в этом моем случае. если для твоих задач имеет -- здорово! :thumbsup:

специфику типа такая: есть художественные тексты  -- они осмысленны. я нагененирурю на основе них кучу бессмысленных текстов. на основе синтакстических/морфологических правил, например. я хочу классификатор, который будет их отделять. затем я попробую применить его к лстм. как-то так.",
"Ну ладно, пусть это будут те самые wd gold или кто там у них самый быстрый и пусть это будет 200МБайт/сек, тогда да, читая линейно можно будет насладиться 800Мбайт/сек",
"Но стоит при этом зайти в папку с каким-нибудь имейджнетом, где миллион файлов и они начнут читаться, как сразу скорость этого линейного процесса упадёт до абсолютного нуля )",
"Короче, сравнивать богомерзкую механику с божественным кремнием нужно по чему угодно, но не по производительности, это я тебе как сторадж инженер работавший с железками за миллионы (иногда баксов) говорю",
а как же корзинка из уголков на  +100 к брутальности?,
"Тогда один шаг обучения будет выглядеть как-нибудь так:
```
loss, _ = sess.run(model.loss, model.train_op)
```
Тут не будет feed_dict и это даст немножко скорости (кажется, размер передаваемого не очень важен, важно передаешь или нет)
Когда заработает, можно в QueueRunner все запихать и лишний код выкинуть",
"<https://opendatascience.slack.com/archives/C04N3UMSL/p1492036395434351?thread_ts=1492030069.446008&amp;cid=C04N3UMSL>
Изначально это ваш вопрос :slightly_smiling_face: Я объясняю, ""Шо не ясно"" именно мне ) Что я пытаюсь доказать... Да в принципе ничего не доказываю. Главное, чтобы перед тем, как задавать вопрос где-нибудь в ""интернетах"", люди думали, задавали эти вопросы сами себе, а потом формулировали нормально свою мысль :slightly_smiling_face:",
"ну прости, Саш! сложный для меня вопрос. был бы простой, я бы и решение загуглил, скорее всего. а так вот даже сформулировать нормально не могу)))) 

прихожу к выводу, что стоит говорить не про ""осмысленные"" и ""бессмысленные"", а про ""хорошие"" и ""поломанные"". хорошие берутся откуда. поломанные я генерирую тем или иным образом. 

но оценивать их хочется с точки зрения семантики. синтаксически они может быть будут норм.  как ""глокая куздра"" -- синтаксис окей, семантика нет. 

вот и думаю, что ворд2век тут поможет.",
прикольно! а как вы это делали?),
"засовывали текст в анализатор (синтаксис+вариант srl) и смотрели, а какие связи должны были установиться, но не установились",
в целом же задача выглядит как language modelling (типа когда тренируем сетку предсказывать следующее слово и при эксплуатации смотрим на её удивление),
"прикольно! спасибо. а откуда брали валидные тексты? много их было? чем парсили?)) как вот узнавали, что связь должна быть, но ее не было?",
"А есть ли тут те, кто имеет доступ к instructor solutions manual на <https://www.pearsonhighered.com/> ?",
"Коллеги, а вот нубский вопрос. Есть веб-сервис, который собирает логи действий всех посетителей. Нужно определить принадлежность каждого действия конкретному посетителю, чтобы попытаться выявить паттерны поведения. При этом понятно, что человек может заходить с разных устройств, под разными аккаунтами, использовать адблок и скрываться другими способами — то есть ни один признак нельзя считать 100% достоверным. В какую сторону тут копать, что гуглить? Или, может, задача изначально неверно поставлена?",
"а как узнавали - сначала искали просто предложения с низкой связностью, из них находили какие-то сочетания морфологических признаков у соседних слов, обобщали, писали правила на распознавание таких ситуаций, когда сочетание признаков такое, а связи нет - ну и так далее как снежный ком",
"И ещё один нубский вопрос в копилку. Недели три назад проходил решающие деревья в курсе ""Введение в ML"" К. Воронцова, и там была несложная задача с помощью готовой функции из scikit построить решающее дерево по данным о пассажирах Титаника и выдать важность признаков относительно предсказания того, выживет пассажир или нет. 

Предлагалось использовать всего 4 признака из всего ассортимента:
```
clf = DecisionTreeClassifier(random_state=241)
features = data.loc[:, ['Pclass', 'Fare', 'Age', 'Sex']]
clf.fit(features, data['Survived'])
```

Я, значит, всё это сделал, но ответ оказался неправильный из-за того, что я из датафрейма признаки не в том порядке выбрал, т.е. вместо того, что выше, я написал:

```
clf = DecisionTreeClassifier(random_state=241)
features = data.loc[:, ['Pclass', 'Age', 'Fare', 'Sex']] # Swap Age and Fare
clf.fit(features, data['Survived'])
```

Так вот, почему порядок колонок в датафрейме влияет на то, как scikit определяет важность признаков?",
как говаривал капитан очевидность - что--то тут не так. Правильно ли я курю табличку что 1080Ti и Титан по сути есть одна и таже производительность?,
"почему тогда линейка ты считаешь лучше? (я вот хотел сходу линейку заюзать, но чот призадумался)",
"статья верно говорит, но как я понимаю это касается только того случая когда пространство дохера большое",
"Ну, это часть ""цикла"" в котором он обещает вот-вот раскрыть все секреты и показать, как наши нейронные сети безнадёжно устарели",
"Особенно когда читаю комментарии типа ""Гениально! Продолжайте, продолжайте!""",
"<@U1CF22N7J> нет не бред, просто графомания, не пости это сюда. когда будет про deep learning то кидай. Все ведь просто",
а какие у тебя температуры при этом?,
"<@U1G303UTW> где почитать подробнее? пока для меня выглядит как магия, поэтому использовать на практике трудно",
разбирался кто с новой вариацией триплетов proxy loss от гугла? <https://arxiv.org/pdf/1703.07464v2.pdf>,
"Кто нибудь знает реализацию KMeans, который бы считал не L2, а cosine similarity?",
"Я так разобрался, что до сих пор стыдно :grinning: 
И написанная за два вечера имплементация на theano не до конца работает. Но вроде понятно как закончить.",
"А можно пару примеров, когда вещественные признаки очень желательно запилить в категориальные? Сорян за некропост :smile:",
"Я с <@U0ZJV6E5Q> уже в кружках обсуждал, но так до конца и не понял как учить сами прокси",
"<@U4U5DNNPJ> в lasagne я положил прокси в embedding layer, как Александр предлагал. Но можно было просто матрицей обойтись.",
Саша в кружках хорошую картинку выкладывал с пояснением как это должно быть. Видел?,
"Я просто может быть что-то путаю, но я либо где-то читал, либо видел в одном из видео с <@U054DU76Y>, где он рассказывал как раз про перевод из вещественных в категориальные и в каких случаях это нужно делать, а в каких нет. Или мб это было на датафесте последнем, где он рассказывал про одну из задач :thinking_face:",
"Дан батч картинок и их меток. Для всего батча вычисляешь нейросетевые эмбединги. Метки классов картинок используешь, как индексы, чтобы взять прокси картинки из embedding слоя.

Имея эти данные можно напрямую записать NCA loss.",
"Мне тут посоветовали для предсказания ""насколько новый документ отличается от всех предыдущих"" использовать pca, а потом смотреть на reconstruction error. Это имеется ввиду что? Если я разложу документы (tf-idf вектора) через pca, как мне потом сравнить новый документ? Если я просто вызову inverse_transform, то там несовпадение размерностей, как и если новый документ transform. Или это я должен добавить один документ и заново сделать fit, а потом посмотреть на reconstruction error?",
"<@U07V1URT9> Тебе Титаны по работе нужны, у Семена тоже десктоп под это дело. Тут же мне не объяснить будет зачем мне это надо. У меня начальство в ML не шарит, но не настолько.",
какие рассылки без LSTM в наши дни-то? :more-layers:,
"Тут никого не волнует какие алгоритмы я использую, волнует чтобы A/B testing стабильно показывал результаты, поэтому я тут фигачу то, что даст наилучший результат, а не те, алгоритмы, что мне интересны. Плюс на мне  и прототипы и в production, так что до безумия все усложнять я не хочу. Поддерживать сильно муторно, особенно учитывая что у нас “hypergrowth phase”",
"сделаешь прототип на LSTM, а дальше LR в прод, все как у всех",
"Сейчас все на так как в 2007, когда надо было виндовые драйвера перебивать через ndiswrapper - это факт, но всякие грабельки все-равно вылезают.",
Обязательно бери видюху как минимум на Maxwell чипе. Кеплер не бери,
"Почему бы ""в лоб"" не взвесить схожесть между новым текстом и каждым текстом выборки? 

```
similarities = []
for d in vectorized_documents:
    similarities.append(new_doc_vec * d.transpose())

mean(similarities) ---&gt; [0,1] 
```",
"Юзеры приходят в систему разными путями =&gt; может получиться что один и тот же юзер представлен несколько раз. Если карта ляжет плохо - за последствия этого могут засудить =&gt; на мне модель, которая для каждого юзера предскажет наиболее похожих, ну и хорошо бы с оценкой близости, чтобы можно было отсекать по порогу.

Заниматься экзотикой, как это было в <https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections> не надо, про юзеров известны, имена, адреса, телефоны и т.д.

Все может быть с опечатками или написано по разному. Телефонные номера могут отсутствовать.

[1] Какие есть подходы?
[2] Какая есть литература на тему?",
"Вроде как полнотекстовый поиск неплохо подойдет для таких целей, для порога можно использовать релевантность результата.",
"Мне кажется, что тут более инженерная задача. Если надо при добавлении нового пользователя определять, существует ли уже такой, то попарно их сравнивать как-то не очень. N-квадрат выходит. А вот с полнотекстовым поиском намного все проще. Зависит правда где данные хранятся и насколько есть возможность их проиндексировать. Т.е. стоит отталкиваться от текущего хранилища данных, возможностей прикрутить к нему full-text search плагин или заиспользовать стороннее хранилище с full-text search.",
"Господа дата саентисты и ML инженеры, интересует ваше мнение по поводу ML продакшена.

В общем, я тут почти дописал generic платформу для выкатывания уже натренированных алгоритмов в продакшен (<https://opendatascience.slack.com/archives/C0SGCGB52/p1489662336503772>) и думаю над generic же платформой для расчета фичей + тренировки алгоритмов + time travel (см ниже) валидации на исторических данных + сохранения результатов в какое-нить хранилище.

В связи с чем вопросы:
- А как у вас в продакшене все устроено? 
- Кто настраивает пайплайны обучения моделей и как? 
- Вообще понятно ли что я ниже написал, или это просто каким-то трешем выглядит?
- Сэкономит ли время DS чувакам и пригодится ли коммьюнити если опенсорснуть, как думаете?


Я вот представляю идеал примерно таким:

1) Нам приходят события `E` про, например, как мы посылаем письма пользователям, как они их открывают, и как потом возвращают свои долги (<@U34Q3KU8H> делает это тут: <https://opendatascience.slack.com/archives/C044B7CSQ/p1492016239130495>). Именно события, а не просто фичи, что у пользователя возраст 30: а) каждую неизменную фичу вроде места рождения можно представить в виде события `birthplace=London` б) большая часть фичей, например, тот же возраст может меняться, и для time travel хорошо бы иметь историю изменений.

Эти события присылает например, фронтенд, чтобы нам добавить новый тип события, нужно программеров просить.
Хранятся они в неком логе событий `L = [E0, E1, E2, ...]`

2) Мы (датасаентисты, наверное) определяем некую функцию `F`, которая может идти по логу событий, менять некоторое состояние `S` в процессе (`S_N = F(S_N-1, E_N)`), и периодически сбрасывать рядки в датасет `D`. Ну т.е., говоря человеческим языком, мы отправили письмо - увеличили каунтер отправленных писем для этого пользователя; чувак письмо открыл - увеличили каунтер открытых писем, когда пришло время платить по долгам и чувак заплатил/не заплатил, сбросили рядок `sent_emails_count, open_emails_count, paid_yn`. Если вдруг функция ассоциативна и коммутативна, такой проход по логу легко параллелится на несколько машин.

3) Сгенерив датасет, мы можем теперь строить на нем модели. Наверное, разные модели потребуют разные преобразования фичей, поэтому мы попросим от дата саентистов предоставлять не только алгоритм тренировки, но и функцию трансформации исходных фичей в те, на которых треним модель / делаем предикт. Как модель построится, сохраняем под определенным ключом в хранилище вместе с функцией трансформации.

4) Когда хотим отдавать чего модель там предсказывает, достаем ее из хранилища, применяем функцию трансформации к сырым фичам из датасета `D` для пользователя, отдаем результат.

5) Если для построения моделей нужно построить другие модели, мы можем задать эти зависимости по типу как в фейсбуковском FBLearner сделано, и строить их распределенно. Ну т.е., упор на то, что дата саентист определит алгоритм, а выделение ресурсов, шедулинг тренинга модели на выполнение, параллелинг того, что параллелится платформа возьмет на себя.

6) Time Travel. Ну, типа, вопрос – как чекать что модели хорошо работают: можно, да, делить данные на train / validate наборы, и выводить ошибку, но есть же еще a/b тесты. Если иметь историю изменений из лога событий, то можно делать примерно так: у нас есть лог E0, E1, ..., E1000, мы строим модель исходя из E0-E100, проверяем ее на следующих E100-E200, строим дальше модель исходя из E0-E200, проверяем на E200-E300 и т.д. Возможно, можно еще что-нить классное сделать с тем, что можно достать датасет на время в прошлом, а не только на текущую дату. Такую фичу Netflix, например, использует: <http://techblog.netflix.com/2016/02/distributed-time-travel-for-feature.html>",
"Ну и пилим на luigi :muscle:, как и <@U1G303UTW> ",
"А вопрос тренировки на biased данных ты, кстати, как решаешь?",
":yeah-sure: Это как раз на тебя повесят, там фичи плодить не надо, это больше в твое поле.",
Да ни у кого на её написание времени нет :good-enough:,
"я-т вообще не против побольше матана, но мне сейчас именно платформу хочется, где можно будет грабить корованы",
"Ну и вообще, кто сказал, что БД первична? :stuck_out_tongue: <https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/>",
"у нас все на луиджи, в последней инкарнации пайплайна если очень высокоуровнево смотреть то все как ты написал",
"Поэтому часто какие-то данные пускают по рандому, без моделек. У Lendup - это 1%, у меня где как.",
"но как вариант выбрать из сотни моделей, для каких нескольки гонять a/b тест - может быть удобно с time travel",
с каких пор рядовые ds пользуются математикой?,
<@U1QN13664> а вы таски как генерируете для новых моделей? Паметризацией воркфлоу? ,
"В принципе, пох какой процессор, если видюха одна :foreveralone:",
было бы интересно посмотреть как АМD свежий себя бы показал,
"а можно ликбез, почему пофиг?",
правда как на xgboost скорость памяти влияет я не знаю,
"Друзья, никак не могу врубиться в принцип работы рекуррентных сетей, в частности, LSTM.
Допустим, у меня есть временной ряд длины m и мне нужно предсказать следующее значение этого ряда. Есть выборка из таких рядов и соответствующие им отклики.

Если бы мы рассматривали этот временной ряд просто как вектор признаков, то как бы он проходил по нейронке с полносвязными слоями, я понимаю. Не ясен принцип, как строить рекуррентную сеть для указанной задачи. 
Сейчас будут глупые вопросы.
1) Если первый слой LSTM, то сколько в нем должно быть нейронов? Весь временной ряд проходит через каждый нейрон этой сети?
2) На выходе этого слоя что мы имеем?",
"когда кому-нить не нравится какая-то фича, и ее хотят переделать - не дропать ее",
"То есть если члены моего временного ряда  - это просто действительные числа, то стоит один нейрон, через который последовательно проходит этот ряд. Далее либо на каждый член ряда есть некоторый ответ, которые далее также обрабатываются как последовательности, либо последний ответ, и тогда мы можем прикрутить, например, полносвязный слой. Так получается?",
"я чот не могу найти ссылку - но там смысл в том, что архитектура Ryzen как-то завязана на память и процессор может работать на более высокой частоте при более быстрой памяти",
А как тогда будет тренировка выглядеть?,
"Сейчас думаю куда 2 картинки воткнуть, там менее минуты всё работает",
"<@U07V1URT9> не поможешь с каким-нить тестом, да еще может сравнили бы с интелом каким",
"Кто может напомнить, почему в VGG / Resnet на вход идут BGR, а не RGB?",
"а где больше интересных датасетов найти для text classification (желательно multilabel). Есть датасет с медиума, вики, news groups. Что-нибудь еще?",
"А если делать каждого юзера как 'name_phone_whatever', то можно будет сравнить векторную близость",
<@U34Q3KU8H> скорее всего как раз связано с использованием OpenCV. Там по дефолту BGR,
а есть вообще разница для сети в каком порядке ей каналы кормить?,
"Я когда гуглил самый энергосберегающий дистрибьютив linux наткнулся на то, что много кто ругается за занижение частот проца в рабочем режиме.",
"Вот не надо. Интуиция в работе ученого теоретика - это 70%. У экспериментаторов все по другому, но это к Артуру - он у нас физик эксперементатор. 

А если серьезно - я тогда создание батча распараллелил, но как оно вообще с чем-то связано я понять не могу.",
<@U0SBLSTJ4> как лучше подготовиться к интервью? К чему нужно быть готовым в первую очередь? ,
"В первую очередь нужно хорошо рассказать о своем бэкграунде, учебных и научных успехах. Также важно понимание, зачем вам все это надо, почему именно Сколтех. А дальше вам будут задавать вопросы в окрестностях вашего бэкграунда, знаний ML. Задачки решать вряд ли придется, а вот заявлять что-то, чего вы не знаете на хорошем уровне, я бы не советовал.",
"вот , я кажется понял как правильно спросить. есть батч векторов который прилетает в лосс , из embedding по соответствующим label получаем прокси, тогда для каждого примера будет положительный прокси а все остальные отрицательные, то есть если батч 128 то для каждого будет одна положительная прокси и 127 отрицательных , так ?",
поэтому и спрашивал куда ошибку то складывать если они из embedding не придут,
"В этой статье <https://arxiv.org/abs/1611.01236> для inception применили, но там в основном рассматривали именно устойчивость к adversarial examples, а не как регуляризацию.",
"Тут сверху писали, что для батча нужно брать 1 прокси как положительную, а остальные как отрицательную. Имеется ведь в виду, что брать остальные прокси (то есть для остальных классов), а не остальные прокси для всех остальных элементов из батча?",
"ну, простейший пример- онтология
Онтология - это дерево понятий. Вершины - слова/понятия, а связи -это иерархические отношения между ними. Возьмем онтологию, ну скажем какую-нить классификацию зверья.
Вопрос: какой смысл в ""тигре""? Ну, это млекопитающее, кошка и т.д.
Т.е. указывая на положение тигра в иерерхии/графе мы даем ему смысл",
"word2vec - это такой прием, к-й позволяет искать куски текстового графа похожие на другой.
Когда мы задаем word2vec вопрос:
Франция - Париж
Россия - ?
И он нам отвечает: [Москва, Питер, Воронеж]
Это оно и есть. Он нам как бы говорит, что кусок Франция+ Париж похож на куски Россия+Москва, Россия+Питер, ...",
"Для word2vec, между словами существует связь - это когда они употребляются  рядом. Наверное, можно тут разнообразить понятие связь. Ну, там сильная -подряд идут, и слабая -через слово,  совсем слабая -просто в одном предложении употребились. Типа того",
Как быстро мне реакшенов наставили из-за опечатки,
"Возник такой вопрос, можно ли использовать датасеты от Сбербанковского контеста для учебы и каких нибудь своих исследований?

И выкладывает ли кто-нибудь из банков РФ ещё такие не агрегированные данные?",
вот такую в оперсорсе где можно б получить? а то там стата,
"Или может идеи есть, как перевести",
"Всем привет! Есть вопрос по обучению LSTM. Вот допустим у нас есть некоторая обучающая выборка, которая сама по себе состоит из последовательностей. Каким образом я могу сказать сети, что началась новая последовательность и сети необходимо ""забыть"" свое текущее состояние (не связывать новую поступающую последовательность с предыдущей)?",
Например когда есть временные разрывы в последовательных наблюденях,
"можно, но у меня скорее всего какой-то средне-древний проц (и я даж хз какой, так как он на удаленной рабочей машине)",
<@U14CTBLFJ> а у тебя как дела с рузеном?,
tensorflow получается хранит state как отдельный variable <https://www.tensorflow.org/tutorials/recurrent>,
"Что значит получаем из сетки? Они представляют собой просто веса сети, учатся обычным backprop'ом как и остальные веса сетки.",
"Так себе примерчик, надо что-то посложнее, где есть время прямых конкурентов Intel",
"Как отдельный Variable, который сохраняется даже между вызовами пока не поменяешь было бы что-нибудь вроде:
```
persistent_state = tf.Variable()
...
output, state = lstm(current_batch_of_words, state)
assign_op = tf.assign(persistent_state, state)
```
и этот assign_op дергать.",
"Кто тут, разбирается - это нормальный блок питания?
<https://www.amazon.com/EVGA-SuperNOVA-Modular-Warranty-120-G2-1000-XR/dp/B00CGYCNG2/ref=sr_1_2?ie=UTF8&amp;qid=1492179861&amp;sr=8-2&amp;keywords=1000w&amp;th=1>",
Вот тут можно узнать какой БП норм или не очень <https://www.youtube.com/watch?v=cXktzNPv7b4>,
"У тебя какие блоки питания стоят в своих компах? Особенно интересуют те, где много GPU понатыкано.",
А в каком контексте оно используется?,
"А вот такой вопрос - вот вдруг неожиданно оригинальное охлаждение слабовато. На что его обычно заменяют? Я последний раз заморачивался на смену охлаждения у компа, когда в школе разгонял свой Pentium со 100 MHz =&gt; 120 MHz и там это было очень важно.",
"Эти современные видяхи так устроены, что выходят на полную скорость только тогда, когда температура на чипе лишь чуть выше комнатной, с хорошей водянкой будут у тебя на двух ггц кочегарить",
"Не знаю как 1080, но 1080ti имеет такую же плату как титан",
"Плюс, как я понимаю, обвязку под это дело надо.",
"О! Спасибо за видео (ничего интересного), но канал! Этот чувак оказывается собирал на базе делловской мамки систему с зионами, блин, жалко, что я его не нашёл, когда свой комп собирал :slightly_smiling_face:",
"Ну и также как weakly supervised, unsupervised можно сказать distantly supervised",
Я все жду когда Артуру набор 1080Ti прийдет в его звероферму и он будет вопросы задавать почему производительность не такая как ожидалась.,
"Просматривается нечеткая корреляция - когда выше температура - выше производительность. Это понятно. А потом и то и другое падает, причем синхронно. Это непонтяно.",
"Вова, сам жду когда карточки придут, чтобы нормально потестить и сравнить производительность с титанами и 1080. Ну и понять какой прирост от нескольких видеокарт.",
"У тебя же видно, что узкое место где-то не в карточках, на это указывает и ""выше температура -- выше скорость"", т.е. на самом деле наоборот и просто они остывают, когда начинают простаивать из-за этого узкого места",
"Делать то что? Причем ладно бы это были мои изобретения - чужой код, почти как есть. Я бы может на SSD гнал, но там по идее скорости чтения с запасом + у меня есть надежда, что mxnet батчи готовит на CPU параллельно с GPU.

```
==============NVSMI LOG==============

Timestamp                           : Fri Apr 14 09:26:53 2017
Driver Version                      : 378.13

Attached GPUs                       : 2
GPU 0000:01:00.0
    Performance State               : P2
    Clocks Throttle Reasons
        Idle                        : Not Active
        Applications Clocks Setting : Not Active
        SW Power Cap                : Not Active
        HW Slowdown                 : Not Active
        Sync Boost                  : Not Active
        Unknown                     : Active

GPU 0000:02:00.0
    Performance State               : P2
    Clocks Throttle Reasons
        Idle                        : Not Active
        Applications Clocks Setting : Not Active
        SW Power Cap                : Not Active
        HW Slowdown                 : Not Active
        Sync Boost                  : Not Active
        Unknown                     : Active
```
Кто соображает, вот тут никакого криминала нет?",
"разрезы будут тогда, когда этот поток событий будет превращаться в фичи",
"т.е. когда будем по логу событий идти и ""менять некоторое состояние `S` в процессе (`S_N = F(S_N-1, E_N)`), и периодически сбрасывать рядки в датасет `D`""",
"<@U07V1URT9> Скинь фото пожалуйста, как у тебя комп с 2+ GPU выглядит изнутри?",
"стрим джойн какбе не проблема сделать как раз через шареное состояние (по иронии судьбы, я это на собесах спрашиваю почти все время :D)",
"А что можно почитать про NLP? Не про машинку последних лет (w2v, rnn, всякое такое), а про то, как работают всякие стеммеры, лексеры и т.д.

Желательно с intro в лингвистку.",
как раз ее в бумажном варианте осиливаю сейчас,
"Jurafsky, martin как раз такой - там и история есть и все уже ""решенные"" задачи разбираются, и лингвистики полно",
"Кто-нибудь подскажет ссылки на работы, когда авторы добавляли batch normalization к VGG и как это что-то меняло?",
"Какую статью не читаю, за редким исключением все тренируют с Relu, хотя есть целый зоопарк чуть более адекватных функций активаций из этого семейства. Или же как оптимизатор используют SGD + momentum, хотя у меня всегда Nadam / Adam сходились заметно быстрее. Хотя иногда можно было полирнуть в конце через SGD.

Понятно, что выбор optimizer’a сильно зависит от типа данных и функции потерь.

Вот вы все тоже только SGD используете при тренировке сетей?",
"Хотя если можешь себе позволить: `LR is reduced by 10x after 1.8M steps`, то, наверно пофиг как быстро что сходится.",
почему deep гауссовские процессы не стали популярными?,
"Критическая масса людей / публикаций еще не сформировалась, вот и нет хайпа
Есть пара групп энтузиастов - и они клепают там что-то потихоньку
Основная проблема - результат не так легко показывать, как котиков, лица и прочие сгенеренные картинки
Мало есть гиков, которые будут радостно втыкать в узкий график плотности предсказанного распределения с малой дисперсией",
"ну вроде как если ты выведешь model.layers у тебя первым слоем будет инстанс model, который как раз emb, ну и можно для него делать  load/save",
как обычно проверяют сходимость параметров?,
Adam юзаю только когда непонятная ебанина и соответственно не ясно когда уменьшать LR.,
"Может что-то с температурой, без нагрузки, когда включаешь сколько?",
"а подробнее про это где почитать? т.к. пока впечатление, что зафитить GP - это n_samples^3, поэтому фишка для очень маленьких данных",
А в каких случаях нельзя полагаться на эти критерии для бинарной классификации?,
"Всем привет!

Не успеваю вернуться в город на ""DS: погода"" в яндексе, кот стартует чз 30мин. Если кто хотел, но не получил приглос, представьтесь Славой Смирновым, только напишите сюда, чтоб другие люди знали, что вы взяли проходку",
"посоны, а поясните ньюфагу, чем tf.layers от tf.contrib.layers отличается и зачем так?",
"`tf.contrib` - это contribution от community (типа, мы (гугл) посмотрели, вроде все норм работает, да и штуки полезные, но никто ничего не гарантирует)

`tf.layers` входит в основную часть tf и _вроде как есть надежда_, что из версии в версию будут поддерживать совместимость API, да и внутри все оптимально выполняется

слои из `tf.contrib.layers` постепенно переезжают в `tf.layers`",
Как вариант из головы: сохранять скользящее среднее раз в k итераций и смотреть на np.allclose с соответствующим rtol. ,
"Такой вопрос: кто как держит в голове теорию машоба/алгоритмы? По себе скажу такое - когда я проходил курс по алгоритмам на курсере - сделал все домашки, даже решил некоторые задачи ""продвинутого"" уровня. Но спустя месяц не могу с точностью до деталей реализации вспомнить даже бинарный поиск/сортировку слиянием. Если посидеть немного - то все ок. С теорией та же фигня.",
"&gt;В целом, Ryzen 7 1700 выглядит как отличный вариант для тех, кто профессионально занимается игровыми трансляциями технологичных новинок
Уровень аналитики зашкаливает",
"Остается общее знание, что и как работает, когда что надо применять. Детали остаются только там, где есть прикладной навык. Собственно, если хочется ничего не забывать, то надо иметь работу, которая использует эту теорию и практику.",
"С классическими алгоритмами у меня тоже так -- вылетает, если не пишу раз в месяц.
А куда и зачем писать сортировки пока не нашел.",
"Куда - на вайтборд
Зачем - чтобы взяли на работу
:idk:",
"Хороший вопрос. Я помнится когда-то этим вопросом мельком интересовался и помнится у меня осталось впечатление что у психологов такая модель не от балды взята - эксперименты какие-то на эту тему были. Но т.к. я этот материал не повторял :slightly_smiling_face:, я конечно же забыл о каких конкретно результатах речь.

Чисто по себе (и по некоторому собственному опыту преподавания) я чувствую что именно так всё и работает, хотя конечно объективных замеров не делал.",
"Ну я когда понял принцип и отправлял код на проверку, постоянно на каком-то крайнем случае валился. В итоге все, что менялось в программе - штуки вроде range(n) на range(n+1) и т.д.",
"Я думаю тут вопрос субъективного ощущения, что такое ""отлетает"" и как это замерять. Я, например, не чувствую что у меня вещи, в которых я более менее разбираюсь, прямо вот ""отлетают"".

Отлетают мелкие детали, причем чем лучше ты что-то знаешь, тем медленнее они отлетают, прямо как в тех экспоненциальных графиках.",
"При этом я понимаю ситуацию, что бывают вещи ""понятые"" (в смысле что у тебя в голове достаточно полезных ассоциаций и всё красиво по полочкам), а бывают ""выученные"" (это когда ты на самом деле нифига не разложил себе по полочкам).

Вторые действительно ""отлетают"".",
"У меня что хорошо, если есть сходимость, то сэмплы судорожно дергаются на итерациях, поидее можно это воспринимать как вариации одного и того же распределения. А стандартный подход ""трэкать каждый сэмпл"" и следить за нормой изменения не катит, они дергаются и все ломают. Мб дело с большом Learning Rate, но мне достаточно субоптимальной вменяемой сходимости, так как о файнтюнинге речи нет. Надо сделать как можно меньше итераций",
<@U04ELQZAU> это как на пикче чтоли? Раз в t повторять наиболее важное/забывающееся?,
кто имел дело с карточками palit? есть смысл брать? как у них с сервисом дела?,
"Это да. Хотя когда речь о 40+ измерениях тут уже что 1Д, что 2Д - одна фигня.
Хотя в моем предложении выше рассматривать маргиналы можно конечно и попарные включить, хотя это тоже всё плохо скажется на силе.",
"По-моему, все эти алгоритмы нужно помнить только до первой полученной работы, а дальше все равно будешь фокусироваться. Есть люди, которые десять лет строго фильтры Калмана пилят, ну и зачем им бинарные деревья?",
"<@U24FHECDD> С одной стороны, понизить размерность и применять к результату КС - это вариация всё того же метода (только в моем случае ""понижение размерности"" делается покоординатной проекцией, а в случае PCA - ортонормальной матрицей). С другой стороны, идея найти ""одно оптимальное преобразование"" исключительно для нахождения различия двух распределений - это красиво. Только чтобы такой подход реализовать по-честному нужно, во-первых, придумать как найти это (одинаковое для обоих наборов) преобразование, чтобы оно наилучшим образом подчеркивало различия, а во-вторых, придумать каково будет распределение целевой статистики (скорее всего оно будет не тем же самым что у К-С, т.к. мы фитим какое-то конкретное преобразование).",
"Или сразу производителю, спустя 5 лет использования, как у меня случилось с ноутбучной памятью",
"Ооо, спасибо большое!!! Наконец-то всё прояснилось. Как думаете, если попробовать другую реализацию Faster-RCNN не из mxnet, тоже с этим можно столкнуться?",
"Эта реализация быстрого обучения только для правильных плотных структур точек (типа сетка). Для произвольных данных это не зайдет. 
fast GP обучают быстро с помощью inducing points (пример либы  <http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/sparse_gp_regression.ipynb>, там же ссылка на статью). 
При этом это хорошо работает только для низкоразмерных пространств (как у них в одномерных примерах) 
Как только размерность начинает превышать 50~100 все становится _очень_ плохо. 
Поэтому вроде бы быстро и хорошо их обучать  так до сих пор никто и не научился.",
"Да там 30к, вдруг кто больше выкачивал.",
"Но если ты считаешь, что твои наблюдения адекватно описывают пространство признаков (ну то есть как-то более-менее равномерно его заполняют - не знаю, как точней сказать), то можно набутстрепить себе еще наблюдений. Причем бустрепить по каждому измерению (переменной) независимо. Это еще более костыльно,  конечно.",
"Всем привет. Сейчас занимаюсь проблемой распознавания шоковых изображений (человеческая расчленёнка) и надо это сделать в виде одноклассовой классификации (нельзя использовать бинарную классификацию, так как множество всех не-шок картинок бесконечно и неизвестно как делать репрезентативную выборку). У меня достаточно скромный датасет - 1500 для обучения, по 500 для тестирования и валидации. Первоначальная идея была использовать свёрточные автокодировщики, где ошибка восстановления шоковых изображений будет меньше, чем ошибка восстановления любых других изображений и можно будет установить некий порог ошибки. Первоначально за кодировщик использовал VGG16, но сработало плохо. После множества экспериментов хорошие результаты показала сеть, где по 4 слоя tanh/maxpool на кодировщике и декодировщике (120 эпох, есть data augmentation), но проблема в том, что ошибка восстановления вообще не зависит от класса. Мне советуют использовать VAE, но боюсь на таких данных он будет работать ещё хуже. Подскажите, пожалуйста, какие-либо статьи по одноклассовой классификации изображений. Знаю про BoW+SVM, но хотелось бы использовать глубокое обучение.",
"<@U041P485A> каким образом? cnn не рассчитан на обучение по одному классу. я читал на stackoverflow идею обучить сеть на одном классе и поставить сигмоиду вместо softmax на последний слой, но нигде в статьях этого не читал. Фактически это выглядит как сомнительная бинарная классификация без одно класса.",
<@U1YK4SV6W> а где датасет шок-контента собирал :youknow: ?,
"Если я правильно помню, в случае с One-Class SVM идея упирается в то, чтобы регуляризацией давить вообще все предсказания в ноль, но при этом штрафовать ошибки на сампле как обычно. Т.е. роль ""второго класса"" играет условный ноль, к которому модель пытается стащить все свои предсказания кроме сампла.

Мне кажется что тот же принцип мог бы быть применим вне зависимости от выбранной модели, дип она или не дип.",
"В случае с deep-ом наверное идеологично оставить все промежуточные слои как обычно, а давящую регуляризацию весов привинтить в самом конце.

Получится по сути то же что и в случае с SVM только фичеры будут браться (и возможно выучиваться) из глубокой сети.",
"если хочу гонять проц, то что лучше брать: водяное охлаждение all-in-one, или воздушку попродвинутее? Ничего неприличного по разгону не хочу, просто хочу, чтобы проц в загрузке на 4ггц держался где-нибудь в районе 60 градусов. Сейчас есть воздушка, на которой он на 75 градусах работает. 
Судя по тому, что я вижу, водянка и топовая воздушка по цене примерно совпадают, но в воздушке нравится то, что она проста, как топор",
"С тем, что нужен хоть какой то кругозор, никто не спорит, это довольно общее место

Но хотел бы я посмотреть на человека, способного написать статью высокого уровня и о фильтре Калмана, и о highload.
Даже глубокое обучение уже сейчас сильно специализируется: те, кто пишут чатботов, мало что знают о распознавании лиц",
"И я удивлен фразой ""залипнуть на фильтрах калмана"". А как еще наука делается, по вашему?)) чтобы написать приличную работу, нужно несколько лет заниматься одной задачей. Это учить известные алгоритмы можно по одному в день, а придумать что то можно только сфокусировавшись",
"Может я неверно понял ваш тезис про ""ну и зачем им бинарные деревья"". Конечно когда ты работаешь по определенной теме, на ней приходится фокусироваться, и нередко углубляться в столь нишевые детали, которые больше нигде особо не нужны (пока) и о которых мало кто знает.
Но я не вижу почему при этом должны вылетать из головы азы типа каких-нибудь бинарных деревьев, фильтров Калмана или прочего материала вводных курсов.
Я так же не соглашусь про то, что ""те, кто пишут чатботов, мало что знают о распознавании лиц"". Это всё-таки соседние области, понимание одной не мешает понимать другую, есть примеры переноса подходов из одной в другую, поэтому, по-хорошему, здесь надо стараться следить за обеими.

Более верное утверждение было бы: ""те, кто пишет чатботов, не обязательно разбираются в самых нишевых и последних результатах на тему распознавания лиц, которые сложно переносятся на их задачу"", но это уже не относится к проблеме забывания методов бинарных деревьев.",
"А наука делается так, что залипая на фильтрах Калмана приходится помнить и о бинарных деревьях. Ситуация, где можно залипнуть на первых и забыть о вторых - это, как я заметил, скорее ""ремесленно-инженерный"" нежели научный подход.",
"<https://opendatascience.slack.com/archives/C040HKJF1/p1492339850507115> как можно было бы задиплернить штуку, собирающую осмысленные картинки вроде этих из каких-то суперпикселей, представленных маленькими картиночками?",
"Какие методы кластеризации текстов на практике хорошо работают? Я пробовал KMeans, в моем случае отработал он ужасно + количество кластеров нужно заранее задавать. Affinity Propagation уже лучше, но все равно фигово. Лучше всего получилось вообще без ML: построил граф, в котором ребро соединяет два документа только если между ними расстояние меньше трешхолда, а потом выделил связные компоненты ДФСом, получилось достаточно неплохо, но как то это не тру, особенно если документов много. Подскажите в какую сторону копать, что еще посмотреть?",
а почему все же нельзя использовать бинарную классификацию? какая-то выборка не-шок картинок все равно ведь нужна чтобы оценить качество работы,
"Всем привет! Есть кто-нибудь, кто учится в магистратуре/PhD/преподает в топ20 в США?",
dbscan как замену не пробовали?,
"а именно dilated LSTM
<https://arxiv.org/pdf/1703.01161.pdf#page.5>
...у меня есть пару страшных имплементаций,
но не сосем очевидно как тестировать, пробую пока с каноническим char-rnn",
"коллеги, у кого есть проплаченный эккаунт на <http://barchart.com|barchart.com> ?
нужно скаать неск. файлов истории
заранее спасибо",
"Я пробовал Clockwork RNN, что почти тоже самое - на задаче классификации последовательностей работало не очень, хуже, чем LSTM. Есть еще PhasedLSTM на таком же принципе, и ClockworkLSTM - их не пробовал, хз как они будут работать.",
"решил заняться самообучением, так как бросать работЫ из-за магистратуры/доктарантуры не получится. Выхожу с чата",
А по какому поводу интересуешься если не секрет?,
"Я зашёл к ним на сайт, нашёл где rma запрос подавать, все заполнил, они дали инструкцию чего куда, отправил в обычном письме, через пару дней после получения прилетела замена",
"Коллеги, подскажите какие есть варианты решения следующей задачи. Есть набор фотографий разного качества с лицами людей (разных полов и возрастов). Нужно обучить некий лагоритм, чтобы он предсказать, встречалось ли неизвестное ранее лицо в обучающем наборе.",
"При использовании леса и линрега скоррелированные ""иерархические"" признаки (например, гео: регион-субрегион-страна, или время: квартал-день_года-неделя_года-...) ухудшают качество модели? Если да, то как с ними поступать?",
"а если категориальные, как гео?",
"сделайте бинарный вектор из событий для каждого дня, где каждое измерение будет соответствовать событию",
"зачем вам тогда вообще рекуррентная сеть? сформируйте из дней вектор фиксированной длины, сделав паддинг нулями. Ну а потом ваш любимый классификатор.",
"котаны, а какой наименее геморройный и наиболее быстрый способ перестать бояться и начать файнтьюнить инсепшены/резнеты в :tensorflow: ? slim?",
"от этого как раз и :bombanoolo:  , так то можно все и самому собрать, но хочется понять максимально быстро, имеет ли смысл в таких времязатратах:)",
"Я для двух авитовских конкурсов успешно файнтюнил и инсепшны и резнеты из двух разных подрепов гугла, но не разобрался при этом ни в одном из них, понятней стало только когда начал руками делать обучение и вывод картинок в тензорборде для u-net с нуля",
"Я бы не заморачивался, какие-то признаки всегда скорелированы друг с другом. Для нелесных моделей ответ регуляризация. С лесами еще проще, можно просто скармливать данные.

Как отметили выше, добавленое разбиение если и поможет, то несильно, а если ухудшит то тоже не сильно.

Так что я бы не добавлял - сложнее поддерживать.

Но! Если добавлять фичи, которые вычисляются через агрегированный таргет, то 

```
groupby(’weekday’)[’target'].mean()
```
это не то же самое, что и
```
groupby(’day_of_the_year’)[’target'].mean()
```
И если идет работа в этом направлении, то можно добавлять.

Короче да, я бы сказал `it depends`",
"Как вариант можно через bootstrap посчитать:

mean difference
std difference
median difference
и прочее

Для каждого посмотреть на p-value и от этого плясать.",
"На практике что у кого лучше работало:

[1] CountVectorizer с ограничениями на число фич.
[2] Более расслабленный CountVectorizer + PCA
[3] Feature Hashing

?",
"лучше работало для чего? все ж зависит от многих вещей, например, что за характер фич, какая размерность, нужны ли взаимодействия",
"на практике у меня под разные задачи каждый из этих пунктов был лучше остальных 

1 - лучше для классификации текстов
2 - для кластеризации
3 - для очень большой размерности, когда есть ограничения на память и когда нужно добавить взаимодействия",
"Я для каждого юзера делаю mapping:
actions =&gt; sentense, который эти actions описывает, например какую страницу посетил или на какой email кликнул, потом это скармливаю tf-idf + PCA и добрасываю к набору фич, и потом все в xgboost

Использую sklearn, но он у меня ложится по памяти, приходится резать все это на батчи =&gt; муторно, поддержка всего этого дела тоже на мне. Хочется сильно все упростить и не потерять сильно по силе фич. Понятно, надо взять да сравнить, но может кто похожей фигней занимался и Feature Hashing работал так же, как и все =&gt; это добавит мне мотивации под него перебить",
"для конкурса по выявлению вандалов в викидате я как раз HashingVectorizer использовал, но потому что у меня словарь токен-&gt;индекс не вмещался в память, может у тебя такой же случай",
"<@U0DA4J82H> Как эксперт - какого уровня взаимодействия используешь? Когда фичи такого плана добрасываются в дополнение к всяким другим вроде city, state, income, и т. д. и потом скармливается xgboost сколько чисто фичей брать, так чтобы мало, но сильно?",
"Кто подскажет, что у нас мэйнстрим с точки зрения anomaly detection?",
"Эта книжка в :pirate: виде доступна тоже. Но она в основном про outliers, а есть еще changepoints.
В какой предметной области аномалии интересуют?",
Где можно посмотреть презентации с прошлых семинаров?,
"<#C45CUFESK|true_story> Тема следующего занятия — ритейл. В гостях: *Diginetica* — международная технологическая компания, разработчик продуктов и решений для индустрии розничных продаж.

Мероприятие пройдет 25 апреля, Вторник, в 19:00. Как обычно, в ВШЭ на Кочновском проезде.
Подробности и форма регистрации: <https://goo.gl/forms/mL4eHnLEIbVfUFQb2> (закроется в день мероприятия).

У Diginetica есть много интересных исследовательских датасаенсовых задач, на встрече можно будет обсудить возможности стажировки.",
"ребят, можете подсказать, как устроить vcs для нескольких депелоперов на удаленном сервере. кейс выглядит так:
есть dev-сервер, на котором происходят вычисления и лежат все данные.  есть несколько человек, которые на нем работают, и могут пилить одновременно разные задачи (т.е. менять бранчи). как разбить юзеров/проект чтоб можно было нормально версионность поддерживать?",
"Добрый день. Подскажите, плз, как в lasagne наложить на слой требование разреженности активаций ( sparsity penalty) ? Показалось, что надо смотреть lasagne.regularization, но там нет этого.",
использовать так же как L1 или L2,
"у нас сейчас luigi + teamcity, каждый работает в своей ветке, когда надо - запускает руками билд ветки, если все ок - PR",
"<@U041P485A> а я правильно понимаю, что здесь аргумент функции это матрица весов слоя? Если так, то я не очень понимаю, как оно заставит быть разреженными активации",
"<@U4PKKEN4T> Дело в том, что классы или центры классов меняются постоянно, пример - яндекс новости: происходит новое событие (новый класс) и нужно новые данные отнести к нему или каким то другим классам, которые так же появляются и исчезают. ",
"Вопросец. В свертках для классификации текстов я в туториалах часто вижу conv1d и реже conv2d. Можете объяснить почему тот или другой вариант нужно использовать? Например, я хочу char-cnn. Каждую букву закодировал через ohe и закидываю в свертку. По идее правильно юзать conv1d, так как у меня нет высоты, а предложение я разворачиваю в длинный формат.",
И в каких случаях нужно conv2d свертку для текстов использовать?,
"я видел только чтобы conv2d использовали как poor man's conv1d, размером 1 на n, пока conv1d не добавили в tf",
"<@U0P95857C> можешь простыми словами объяснить почему conv1d нужно использовать, а то я вроде бы понимаю, но не уверен на 100%",
"меня смущает, что у нас эмбеддинги. Например, одна буква – 70 размерность. Мы ставим кернел 5, тогда мы 65 раз пройдемся сверткой по одному слову. Где у меня прокол в знаниях?",
наткнулся на такой пример: <https://github.com/korobool/winter-ml-nlp/blob/master/cnn-classifier/prima/charlevel_cnn_run.py> . Тут conv2d. Почему у них тогда работает?,
"чтобы понять нужно найти документацию к той версии keras которую они использовали :slightly_smiling_face: потому что актуальным докам это не очень соответствует (ну или код странный, непонятно зачем делать кучу 1x1 conv2d со stride=3)",
"Всем привет. Есть ли  корректный способ обработать nan в LightGBM? Я имею в виду не замену nan-ов средним или нулем, а именно обработку nan, как отдельную категорию. Просто, если оставить nan, как nan, то результаты сильно падают.",
"Как и во все другие недели, хотим напомнить что завтра утром будет проходить Data Science завтрак. С 9:30AM  по адресу Проспект Мира 26, кафе под зданием входа в Аптекарский Огород! Приходите, у нас интересно, весело, а еще безлимитный кофе/чай. 
Также присоеденяйтесь в наш <#C4P7DGAHY|data_breakfast> канал",
почему нельзя заменить `nan` до того как подаешь матрицу в lightgbm?,
где найти русский дата-сет для обучения чат бота,
и будет как с ботом от майкрософта),
"<https://github.com/tomrunia/ClockworkRNN> - я делал свою имплементацию на базе вот этой, только, насколько я помню, там был какой-то баг с периодами, непомню точно какой :disappointed:",
"Ещё есть куча сайтов, где хранится история из IRC каналов",
Ну ты не отбрыкивайся от топикмоделинга так уж. Рассматривай его как один из множества способов эмбедить документы.,
"Посмотри как это реализовано в xgboost и примени те же методы. Есть пакеты, которые делают тоже самое из коробки. (Если переменная категориальная, то рассматривают nan как отдельную категорию, если непрерывная, то заменяют медианой и делают дамми по замене и т.д.)",
"ну вот в прошлый раз ты меня так и позвал, а на месте сказал типа ""иди отсюда с такими вопросами и возвращайся через года через 3, когда станешь таким же умным как я"". 
я все еще под впечатление от такой схемы! :joy:",
"xgboost, если верить дампам использует nan как отдельное значение.",
"привет, может кто то сталкивался, ищу информацию по оценке сложности текста, нагуглил кучу ссылок на разные компании которые каждая по своему делают такую оценку, но нигде толком не описано как. Можно конечно изобрести свою, например, средняя длина слова, средняя длина предложения, среднее по гистограмме слов и как то это вместе сплести с весами, но хотелось бы не изобретать велосипед и взять что то уже используемое, спасибо",
"Вчера на Митап ходил, там мужик работал на anomaly detection для IOT. И делал он это так - прогонял LSTM, делая предсказание для каждого следующего шага. И смотрел как `y_true - y_pred`  относится к распределению `y_true - y_pred`, которое было получено до этого.",
А где про это почитать?,
"маководы, как примуантить папку по ssh?",
"Коллеги, существует ли где словарь русского языка с фонетическими транскрипциями (в электронном виде)? Мне нужны транскрипции русских слов в разных склонениях.",
"Ребята, а подскажите, пожалуйста, у кого опыт есть. 
Хочется собрать мини-монстра для DL снарядив его четырьмя 1080ti. 
Собрал такой вот комплект. Очень пргодились бы ваши комментарии и советы что на что лучше поменять или на чем можно было бы сэкономить. 

Интересуют следующие вопросы. 
1. motherboard Gigabyte GA-X99-UD4 имеет 2 x PCI-E 3.0 x16, 2 x PCI-E x16. Насколько я знаю, особой разницы в производительности я не почувствую из-за разницы в версии PCI-E. Верно? 
2. Можете сказать что-нибудь про надежность GA-X99-UD4  ? 
3. Думал поставить  Ryzen 7 1800X, но не нашел под него материнку с четырьмя слотами для GPU. 
4. Еще хотел спросить про проц, как думаете, справится ли  Intel i7 6900k  c 8 ядрами  гонять  tensorflow на 4 картах 1080ti при 100% утилизации? 

Ну и если есть предложения по тому, что на что можно поменять, то не стесняйтесь, пожалуйста  : )  В первую очередь интересует цена / качество и надежность.",
"С практической точки зрения, если знать устройство объекта, на котором аномалии ищутся, то можно расставить ограниченное количество определялок выбросов (простейшие outlier detection), обычных регрессий, эвристик типа забитых порогов значений и задача решается достаточно хорошо. 
А вот в общем случае, когда объект неизвестный, по нему орудуют операторы, переключая из режима в режим, задача плохо решается.",
"Про порты непонятно, как они на одной матери могут быть разных версий?",
Я бы попробовал как гипотезу слова синонимы позаменять.,
"Потому что ты сэкономил на том, где лучше не экономить. Вот корпус можно и дешевле купить более хорошего качества",
"Судя по тому, что я скачал все данные от nltk `import nltk;nltk.download('all')`, то тут только для en-US. Но можно проверить в директорий, куда все данные скачались. На Linux - это по дефолту хранится в `/home/user/nltk_data/stemmers/`. Тут есть только `porter_test` для инглиша. 
Думаю, для русского языка можно использовать `pymorphy2`",
"когда я послдений раз смотрел стемминг делался через такую жопу, что после этого я вообще nltk перестал воспринимать за библиотеку. Он транслитерировал русский в английский, потом делал стемминг, затем конвертировал обратно",
"А можешь ссылку подсказать, где они просят за свое поделие 13k ? а то я вижу только много много большее, как <@U36Q9NJMD> справедливо заметил.",
"У нас тут есть эксперты по awk? Не пойму как сделать фильтр по времени в ISO формате, без указания даты. Пример `2017-04-15T14:30:36.218870Z`",
А где можно про Soft спросить есть канал отдельный?,
"Тогда здесь спрошу: возникла проблема с установкой Ubuntu (desktop). Юзал образ из репозитория записанный на загрузочную флешку. После нажатия на Установить Ubuntu. Выдалось много ошибок, которые ползли плавно. Я подождал 10 минут и вырубил.

Ощибки вида: 
bcma bus0 No SPROM available 
nouveau pfifo sched_error 08 

Пробовал разные образы, ошибки отличались.
Может кто посоветует что делать?",
"<@U43FTJQ2V> Отпишись, пожалуйста, как потестишь, любопытно",
"Тут рекомендовали устраиваться на работу через backdoor, а можно делать запрос о доступе к информации через эту дверцу? <@U1CF95PP1>  как считаешь Y сможет поделиться инфой?",
Всем привет! А какой сейчас самый быстрый фреймворк для RNN’ок? Tensowflow все еще отстает от торча и теано? Или уже нагнал и перегнал? Не смог найти свежих бенчмарок на эту тему.,
"Если ты не в Яндексе, то врядли у тебя это получится, как мне кажется",
И как большинство всего от любителей коров ни о чём конкретно и вперемешку.,
"я в задаче с пробками нахерачил иксгбустов, а сейчас страдаю, почему оно такое хрупкое :loh:",
"привет, играюсь с задачкой Intel Cervix с Kaagle, и думаю над таким вопросом - поиграться с размером batch при тренинге при этом меняя learning_rate для SGD, и вот какая там зависимость получается ? если батчи побольше, то lr тоже побольше ? например был batch=8 и lr=0.001 , а если batch=16 то lr=0.01, есть в этом какой то смысл ? спасибо",
"я для себя тематику определил как ""стартап для ФБ""",
"Как мне недавно ответили: ""Аналитик за 2 секунды поймет""",
"это какой не глубокий линейный аналитик. 
вообще сайт действительно образец аскезы. но зато без булшита",
кто нибудь пробовал MR-RNN ?,
"кстати если вдруг кто не видел
<http://rawgraphs.io/>",
А какое это имеет значения? ),
"<@U3PETUSSE> как вариант под твою задачу PageRank KeyWord extraction, и кластеризацию(например dbscan или kmeans), можно центры кластеров можно проклассифицировать еще",
"товарищи, а как в Keras взять модель, загрузить в нее заранее посчитанные веса и присобачить наверх другой модели?",
"я посмотрел на то, как автор предлагает расширять модель - <https://github.com/fchollet/keras/issues/4040>",
"<@U0SBLSTJ4>  ну когда занимаешься тем и другим, а выбираешь CSE, то хотелось бы знать с кем придется говорить) наверное, если собеседование одно для всех, то это удобнее)",
"В любом случае будет 2 потока, вас слишком много - так что непонятно, кто конкретно будет в вашей комиссии. А так да, CSE будет вместе с DS.",
"summon <@U1CF22N7J> У тебя, помнится, тоже была куча проблем когда ты убунту ставил.",
я кстати попробовал по-быстрому перейти на второй керас но там с весами стандартными под resnet вообще дичь какаято. теановские веса не добавлены а присутствуют только  для tf но при этом все равно используются когда у нас бэкэнд th. в итоге обучение замедляется в десятки раз...,
"Никто не знает, как вытащить и скомпилировать определенную версию xgboost on Ubuntu? auto-sklearn требует определенной версии xgboost (0.4a30) но поставляемая с анакондой не работает (wrong ELF header for libxgboostwrapper.so) . При апгрейде устанавливается xgboost-0.6, но она не принимается auto-sklearn. git clone --recursive <https://github.com/dmlc/xgboost> вытаскивает, понятное дело последнюю версию. Как вытащить 0.4a30?",
"спасибо, я не понял как найти именно ту версию 0.4a30?",
"если она присутствует в дереве, как это будет v0.4a30?",
"```как разбить юзеров/проект чтоб можно было нормально версионность поддерживать?```
На самом деле это 2 разных вопроса не имеющих между собой ничего общего
```как разбить юзеров/проект?``` и ```Как нормально версионность поддерживать?```
Под версионностью - ты понимаешь версионирование приложения или как работать в GIT?",
"может кто концептуально сказать когда лучше использовать логлосс как метрику, в каких случаях?",
"когда важнее вероятности, а не сами классы",
"имхо логлосс как метрику вообще лучше не использовать, потому что сложно интепретировать",
"<@U0DA4J82H> а как иначе, если работаешь с каким-нибудь CTR?",
"возможно для CTR подойдет, но вот как понять, логлосс 0.4 это хорошо или плохо? аук или какие-нибудь другие метрики из IR проще понять, как мне кажется",
"если сравнивать две модели, то понятно, какая лучше. 
хорошо или нет - сравнивать с какой-то заглушкой типа DummyClassifier.",
"ну вот для бинарной классификации надо выбрать наилучшую(что значит наилучшая?) метрику. аук не очень так как классы разбалансированы, вот и думаю про MAP F1 или Logloss. как интерпретировать все кроме логлосса понятно, вот и спрашиваю.",
"так а как определить ""Лучшую"" метрику, какие критерии",
для 1к6 как раз auc стоит взять и его оптимизировать,
"а можете посоветовать какое-нибудь практическое руководство для нубов о том, в каких случаях какие метрики предпочтительней?",
"Если цель искать классификатор, то логлосс (она же cross-entropy) - это идеологически красивый критерий, т.к. напрямую соответствует правдоподобию данных под вероятностной моделью.

Если важна интерпретация, можно думать о кросс-энтропии как о ""недообъясненной информации в битах"" (только её надо перескалировать в терминах log2 (другими словами умножить на 1.44)

Например, логлосс 0.7 после умножения на 1.44 становится равным 1биту. Это значит что ваш классификатор тупит на один бит информации. В частности, если он бинарный, он вообще бестолков.

Если вы предсказываете 8 равновероятных классов, энтропия предсказания 3 бита (или, в терминах натуральных логарифмов, 2.07 ната). Соответственно таков максимально возможный логлосс максимально тупого классификатора.",
"Все эти метрики, завязанные на теорию информации, довольно сложно поддаются интерпретации. Хотел бы я посмотреть на тех, кто попытается все эти биты и наты людям из маркетинга  объяснить",
"Не важно как разбалансированы классы, всегда можно учитывать что логлосс самого тупого случайного классификатора будет равен энтропии распределения классов.",
"Биты и проценты оптимальности модели, как заметил <@U0DA4J82H>, в доллары обычно так просто не переводятся.",
"Ну ""наш метод вызывает 20% респонс рейт тогда как раньше он был 10%"" стейкхолдер тоже поймет.",
"Кто как сравнивает эффективность разных моделей (например, бинарных классификаторов) между собой в условиях не слишком больших выборок? Понятно, можно провести кросс-валидацию, получить cv auc mean и cv auc std, потом построить обычные confidence интервалы для среднего, но последние из-за небольших выборок будут широкими и будут сильно накладываться друг на друга для разных моделей. Еще что-нибудь придумали на эту тему?",
"есть ли здесь те, кто делал siamese network, в которой каждая из двух веток - resnet или какая-то другая подобная (по размеру) сеть без последних полносвязных слоев?",
"как я понимаю, это все очень затратно и по памяти и по времени, правильно?",
"как думаете, какой объем картинок будет достаточен, чтобы добиться более-менее точности?",
"<@U1BAKQH2M> ну вдруг выходной слой жирнее становится, но это как организуешь",
а какие ресурсы нужны на finetuning resnet'а?,
"подскажите в юпитере какие есть команды в селах типа Markdown типа там увеличение шрифта, изменение шрифта, булетпоинты, вобщем редакторские штучки, где такое поглядеть?",
"и еще, какой объем картинок нужен для обучения siamese с двумя resnet?",
а где то есть какой нибудь tips list?,
"вот я такое нашел, но оно почему то не работает(",
"да, это странно, потому что у меня сейчас два селла оба в маркдауне, в одном все спец символы воспринимаются и рендарят как надо в другом нет",
<@U041P485A> 4гб - это resnet50 с каким размером батча? 8?,
и такой вот нубский вопрос: как правильно курится resnet50? картинки грузятся в память батчами? а если учиться только на CPU?,
"эм, от фреймворка же зависит. но вообще обычно готовят батчи на CPU а потом грузят в GPU, где делают forward-backward и шаг оптимизатора",
"Всем привет! 
Еще немного докладов прибыло в программу конференции <http://datascience.in.ua>, которая состоиться в Одессе 13 мая;) Напоминаю о скидочном промо-коде для нашего чата на 10%: FlyElephant10. Присоединяйтесь;) 

А если вы преподаватель ВУЗа, то как получить бесплатный билет можно узнать здесь - <https://www.facebook.com/photo.php?fbid=10212499098569921&amp;set=a.10200648969644104.1073741825.1156281111&amp;type=3&amp;theater> :wink:",
и в догонку вопрос: как распараллелить resnet.predict? keras+theano. Заюзать tf?,
А 1070 как оказалась в этом списке? На сдачу попросил положить?)),
Как будет что-то запощу в канал,
":wtf: моё лицо все ещё выглядит так, когда я вспоминаю, что Роман собрал комп с 2x 1080Ti, 1080 и 1070",
"Народ, если кто манускриптом Войнича интересуется, гляньте статья норм или не очень? <http://keldysh.ru/papers/2016/prep2016_52.pdf>",
"Хочу посмотреть на твоё лицо, когда я соберу на 8 титанах и алюминиевых уголках :troll:",
"Вопрос; Если сложность сортировки в Монго `O(log n)`, то как высчитать отсюда время?  Какова сложность полной индексации (14 млн документов)?

Может ли помочь тут частота процессора, скажем 2.2 GHz?",
"ээ, полной индексации или одного инсерта?
(на практике только померять конкретный случай, т.к. кучи косяков и оптимизаций одновременно борются друг с другом, и кто конкретно победит, заранее трудно угадать)",
"это правильно ли будет считать гипотетический, что скажем индексация занимает `O(n)` - это кол-во операции, то как отсюда высчитать время?",
"Оффтоп от темы параллельных вычислений. Немного нубовские вопросы интересуют. Подскажите, как выбрать метод оптимизации для задачи классификации изображений, используя модели типа resnet50, inception v3? Различные методы находят минимумы с разной скоростью (Adadelta, Adam). Какие методы вы используете и почему? И как правильно подобрать learning rate, на скольки эпохах?",
В mxnet вроде всякие такие штуки можно делать ещё <https://arxiv.org/abs/1604.06174> а как с этим в tf?,
Простовато как мне кажется для Хабра... ),
"Есть задача на работе, Надо предсказывать юзеров у которых ACH как payment не пройдет. Вот сейчас я попробую xgboost натравить на фичи и посмотреть что он напредсказывает, но есть подозрение, что толку будем мало.

Как я понимаю, задача похожа на fraud detection. Кто подкажет, что у нас по этому делу state of the art?",
А где взять бенчмарк Артура? Я бы тоже запустил.,
"хз таких постов много, вот если Роман это напишет это разрезе:

[1] Встала задача: Я на винде, железа мало, мир жесток.
[2] Купил железа на спутниковые деньги, но не как все, а с изподвыподвертом (тут удивленное лицо Артура)
[3] Стал собирать не работает ни хера - но с божьей помощью все слошлось - и вот как: детали
[4] Детальное описание как на этом компе можно заехать в топ на задаче про рак легких.

Вот тогда зайдет. Но на 99% из-за пункта [4]",
<@U06K9ELB1> а почему это важно?,
ну ты войдешь в историю как чел которому распространение информации важнее каких то зеленых бумажек -),
"нет, я так и не выяснил как 0.4a30 найти на гитхабе, а локально ковырять не охота. своих багов и проблем хватает :slightly_smiling_face:",
"Почему-то мне кажется, что тут как раз xgboost лучше всего подойдет, это ж бинарная классификация получается, но нужно над фичами подумать. Например, всякие меры непохожести, которые в обнаружении аномалий используются, можно попробовать как фичи использовать. Но я с дивана, сам таким не занимался",
А как канонично усреднять лосс? До размеров батча или до 1 сэмла?,
Вот у меня есть набор фоток лиц с лендмарками. Как их лучше разбить на трейн/вал/тест ? Чем вообще нужно руководствоваться?,
"offtop: ""ящик с усами"" - не перестану удивляться тому, как в русском звучат английские термины",
"<@U0H7VBQQ1> собственного нет, как и нет собственного кластера из машинок, если бы был, то и код бы был. В туториалах тоже рабочий код есть. Лично я видел бенчмарки lstm-мо подобного кода на три машины, каждая с 8 GPU (причем там могло быть 3*dgx-1, поэтому всё хорошо было с сетью и обменом информации между GPU, но я не могу сейчас найти точную ссылку). И выглядел он понятно.
А вообще можно засамонить <@U0XR20SA1> . У него есть код:) ",
аналогично тем постам как уже есть,
"В задаче прогнозирования временных рядов для формирования фич беру значения, скажем, k предыдущих итераций. Для двух соседних строк в Х получается, что k-1 данных дублируюся со сдвигом. Из-за этого получается матрица Х с большим количеством дублирующихся данных и поэтому не влезает в память. Как с этим боретесь кроме батчей? Хотел генератор сделать, но xgb.DMatrix его не примет",
"Господа, а не подскажите - изобрели уже виртуальную машину, которая дает доступ к ГПУ, или надо еще подождать? Хочу поставить Ubuntu виртуалкой(хост - Windows 10), но как вторую операционку ставить не хочется...",
"я могу про deeplearning4j накатать, но им точно мало кто пользуется :java:",
"&gt;Хочу поставить Ubuntu виртуалкой(хост - Windows 10), но как вторую операционку ставить не хочется...
Обычно всё с точностью до наоборот :trollface_peeking:",
"<@U0H7VBQQ1> для тех, у кого есть кластер и тех, кто смог distributive TF поставить - проблема реально для левой пятки:просто запустил, указал ip:port воркеров, сказал эту считать тут, а это тут с помощью with (ну да, тут нужно небольшое понимание архитектуры, чтобы не сделать глупость) и готово",
"Довольно интересное предложение для тех, у кого нет своего компа для deep learning. Выглядит даже интереснее AWS p2. <https://www.paperspace.com/ml>",
артур раскажет как мхнет-ом решать эту задачу?),
"<@U1CF22N7J> да ну, почему? на амазоне больше сильно было, когда я последний раз смотрел",
"о чем вопрос?
данные шарить не могу, карты достаточных фич есть в открытой литературе, как пользоваться прогнозами тоже",
"<@U1D4RRA7K> споты по 0.2$ же были, а это цена такая же как на azure за аналогичный сервер. Дешевле хосткея ничо вроде нет, там 1080 за 4200 (75$) в месяц, или ~5 суток вот это штуки",
"споты по 0.2$ видимо в какие-то очень хорошие дни, потому что когда я заглядывал там загрузка была под 100% и цены упирались в цену аренды (1-2$ кажется в час)
Про хосткей не слышал, но надо бы, видимо :slightly_smiling_face:",
"А в ods нет канала, купи/продай железа? 
я вот хочу рассмотреть варианты покупки б/у частей, поэтому интересуюсь 
1) стоит ли вообще так делать (в плане надежности)?
2) продает ли кто (проц,память, мать)? :drouz:",
"1) конечно, кремний это же натурально камень, что ему сделается :slightly_smiling_face:
2) Ебей велик, чатик мал, шансы найти там тогда и то, что нужно куда выше",
"Вопрос стоял такой - если левой пяткой параллелить на уровне батча, какой overhead.",
"из ноутбука норм работает. я как раз оттуда и запускал. 
только вопрос возник -- он же фичимапы рисует не по-настоящему выходит? т.е. он берет картинку и применяет только фильтры из выбранного слоя, игнорируя преобразования которые идут до?",
"<@U4ZKGPVEF> Да, парсинг так себе, сейчас как раз решаю эту проблему. После получения более менее чистого референса уже понятно, что делать)",
"И если писать не а фрэймворке, а как, например используя его, с примерами кода подъехать к задаче про амазонские леса - это будет более востребовано, как минимум потому что халявщиков, которые хотят натянуть амазонию гораздо больше, чем специалистов по NN которым это может быть интересно.

+ байки про классификацию амазонок для бонуса.

Артур, вроде хотел показать как используя mxnet можно взгромоздится на тюленя и уехать в топ кагла, но замотался.",
"Хайп начнется, когда мы с Романом разберемся что к чему и сможем задачки закидывать железом, через mxnet",
":likeaboss: когда раздел хайп вокруг mxnet, не показывая с его помощи никаких результатов ",
<@U34Q3KU8H> какой у тебя полный конфиг компа? ,
"Что меня, что его останавливает, что на Керасе все между просто и очень просто, как в силу привычки, так и замечательного API, и работая с ним я чувствую себя умным, а читая доки по mxnet я начианаю сильно сомневаться в своих умственных способностях.",
"Я пока загорелся mxnet, ну и тот факт, что существует такой человек как Артур мотивирует в нем таки разобраться, надо просто сильно себя озадачить.",
"А кто-нибудь mxnet в проде использует? Мне tensorflow как раз нравится не только скоростью разработки, но и простотой развертывания в проде (через serving). Я на текущей работе продвинул serving, теперь его используем. До этого были модели на mxnet, но там все так неудобно было - свой враппер на плюсах, подключается как либа, еще и модель каждый раз заново с диска грузится, короче неудобно.",
"Будет работа где надо будет развертывать в prod - будет tensorflow,

Но чисто под :kaggle: так чтобы бысто и на несколько GPU из коробки без боли - это вроде mxnet",
"Посмотрел тесты тернауса. Чет у меня только i7 + Z170 + 1x 1080Ti выдает такую же скорость (чуть большую). А когда 2x GPU, то скорость падает.",
"да, давайте считать, что она репрезентативна, стратифицирована или как там еще правильно сказать",
"у меня 2 карты на компе. как это поможет, я не понял...",
"А вот тут я не понял, это как твой 1080Ti обскакивает мой Титан? В теории он же шустрее.

Проц в теории пофиг, батчи же на CPU собираются.",
"Просто такая мысль, мало ли уже кто-то реализовывал такое. Хотел бы узнать, если реализовывали, как кормили и обучали (особенно нюансы с калибровкой весов).",
"Вопрос #2: кто-нибудь здесь применяет метод генетического алгоритма в оптимизации весов? Какие либы его поддерживают, эффективен ли он по сравнению с SGD, adam?",
"Посоветуйте/покритикуйте пожалуйста. Собираю домашнюю бюджетную машинку. Идея в том, чтобы на некоей минимальной конфигурации отлаживаться, а если действительно все упирается в производительность железа и нужен многократный прирост - переносить в облако без изменений кода. Что хочу брать: 
MB Asus Z170-K
CPU Celeron Skylake
GPU GTX 1060 6Gb 
Memory 2x16Gb DDR4
диски и прочее уже есть. По нынешним ценам выходит порядка 35 тыс руб за все. Как думаете, адекватная конфигурация для этих целей или стоит что-то поправить? Что процессор тут непропорционально слабый я понимаю, но у меня есть еще ноутбук с Core i7 и что-то можно считать там, а эта машинка больше под расчеты на GPU планируется.",
"Это верно. Тут не поспоришь. Тогда так. Транслитерация nickname на русском - хочется с большой, ибо в `Посмотрел тесты тернауса` - обращение звучит как имя собсвенное. А по английски - похеру как,.",
Спасибо! Где можно по этому поводу почитать поподробнее? МБ есть статьи на arxiv?,
"<@U04ELQZAU> , спасибо! А как называется статья? За ссылку отдельная благодарность безумная!",
"Вопрос #4 (за недельку накопилось порядочно так): а какой метод кросс валидации лучше и в каких ситуациях: k-fold или бэггинг (случайное разделение выборки на трэйн и тестовую выборки с заданным % доли тестовой, повторение n раз)?",
"Насчет меньших затрат ресурсов ты не угадал.
Я могу объяснить более популярно:
Нельзя оптимизировать сразу и дерево, и сеть. Из-за этого два выхода: сначала обучать сеть, а потом брать фичи из последних слоев и обучать деревья на них. Это сейчас популярное направление, перспективно в нём - найти такую архитектуру сети, которая будет выдавать наиболее правильные фичи для деревьев (мы же знаем, в чем деревья сильны, а в чем - не очень), причем можно костылить еще последний слой в нейронке.
О втором способе: сначала обучать дерево, а потом нейронку для него никто не пишет, потому что сейчас это слишком затратно по ресурсам. В теории необходимо для каждой гипотезы сплита вершины дерева переоптимизировать всю сетку, что идет до него, то есть мы получаем новую модель. Дерево в этом смысле можно рассматривать как недифференциируемую нелинейную активацию, приближать её дифференциируемой функцией. Итого, если гипотез сплитов k, высота дерева d,  то нужно обучить k^d моделей :( 
PS. Я тоже задавался таким вопросом и озвучивал его тут, даже придумал некоторый свой велосипед, который вряд ли будет работать
Проще сделать просто несколько слоев с активацией сигмоида, которые в теории также должны улавливать логические связи, я вроде бы слышал, что три fc слоя с сигмоидой могут давать любую логическую операцию.",
"Вопрос #5 (извиняюсь, кому уже надоел): допустим, у меня есть имбалансная выборка для классификации, где `{0: 800, 1: 40}`. Для того, чтобы мой алгоритм предсказания работал корректно (пусть это будет decision tree), мне нужно по идее задать классу 1 какой-то вес. В теории, нужно задать ему вес 800/40 = 20, это должно максимизировать мой F1. Но допустим я ставлю вес 30 и получаю, что 0 у меня предсказывается верно с 30% вероятностью, тогда как 1 - с 90% вероятностью, причём на жёсткой кросс-валидации. 

В качестве результата мне нужно, чтобы accuracy по предсказанию класса 1 был максимальным.

Вопрос: нужно ли мне брать вес 1 как в теории, или я могу ""играться"" с ним для повышения моего accuracy rate по 1, жертвуя F1? Не рискую ли я в таком случае попасть на оверфиттинг, даже с кросс-валом?",
"Я пробовал, когда оно только начиналось, но моих потребностей она не закрывала, а в остальном не превосходила ipython, так что забросил.",
"<@U0AS548A1> 
1. Что за распознавание? Классификация? С большой вероятностью из фич такой сети хороший перцептивный хеш получить не получится (это не требуется для классификации и если специально сеть не заставлять, такого свойства она не приобретет)
3. В качестве расстояния брать не L2, а cosine? не сильно отличается
4. Какими полезными свойствами ты ожидаешь, оно будет обладать? Так-то это такой Knowledge Distillation, только для сиамской сетки",
"Есть ли multi-output regression для XGBoost (когда предсказуемое состоит из нескольких связанных величин)?
Гугление только feature requests нашло, как будто нет такого. Можно обойти построением нескольких моделей для каждой из предсказываемых величин, но тогда косяки с их (неучтенной) взаимосвязью могут вылезти.",
"Френз, в статье про U-Net авторы всё пишут, что ей нужно ""very few training images"". Кто реально использовал, very few - это какого порядка размер датасета?",
"Да. Т.е. я выучил сетью  phi(x). Теперь хочу выучить ядро по определению K(x, x') = &lt;phi(x), phi(x') &gt; Получится можно использовать такое ядро во всех ядровых методах для похожих задач. Вместо того, как делают сейчас: берут сетку обученную на много классов, отрубают ей голову и доучивают на нужные. А тут надо будет брать только такое ядро ну и обучать с ним что-нибудь ядровое",
"1. Да. А почему тогда вроде делают так что берут отрубают голову сетки и доучивают? 
2. Ну суть не в расстоянии, а в том чтобы выучить ядро соответствующее скалярному произведению в выученной сеткой пространстве
3. Го в тред выше, чтобы в одном месте было",
"прошу прощения, а если я юзаю, скажем, word2vec, но полная матрица эмбеддингов(тем более с паддингом) для всех-всех текстов выходит далеко за пределы оперативки - как сократить объем потребляемой памяти? есть открытый код, где подобная проблема решалась?",
"Зависит от степени упоротости данных, если как в спутниках, где у снимков адский динамический диапазон и вообще глобально разные виды на картинках, то лучше побольше.",
"По описанной тобой процедуре построения получается, что phi(x) есть в точности выход сетки ""без головы"". А какой тогда дальше смысл городить ядро?",
"Ядра особенно хороши тогда, когда они проще/естественнее, чем придумывать фичи. Вообще хороший вопрос, как это через сетки делать, может и вариант Жени сработает.",
"Да, жрёт память в Mac и процессор как не в себя",
"&gt;адский динамический диапазон
<@U1CF22N7J> что имеется ввиду? Dynamic range как количесство полутонов картинки?",
"Скажите, а как в сверточную сеть (картинки на входе) подмешать ещё своих признаков? На ум приходит только выдирать признаки с последнего слоя сети и обучать отдельную модель на этих признаках + свой вектор. ",
"у менять есть похожая задача, только там один источник - картинка, второй - временной ряд
если я обучаю все сразу, результаты получаются такие же как если бы использовал только временной ряд
если обучаю отдельно, потом вставляю веса и дообучаю - получается норм",
<@U49NJJXNJ> Как обычно все зависит от задачи.,
<@U49NJJXNJ> На какую именно тему?,
как объединять разнородные данные в dl,
"а как загуглить? data fusion? 
или может были на кагле подобные соревнования, как вот авитовское?",
"Друзья, а как ретрейнутый граф тф в керас загрузить? :confusedparrot: ",
"<@U0H7VBQQ1> а мне понравилось тулза, которые слои визуализирует. для кераса она работает. а для тф -- не знаю. подумал, что проще будет сконвертить, чем разбираться с тем, как оно с тф работает-то",
"Господа, если я аспирант, то я могу регистрироваться на конференцию как Academic participant или имеются ввиду настоящие академики?",
"Строю countplot в Seaborn, величина двухкатегорийная. По умолчанию 1-я категория имеет на графике синий цвет, вторая - зеленый. Как задать кастомные цвета для каждого столбца? Palette не предлагать, так как там набор цветов и нельзя задать нужный мне конкретный цвет для каждого столбца",
"Коллеги, а есть кто-нибудь, кто реализовывал/воспроизводил результаты статьи ""No fuss distance metric learning Using proxies"" (<https://arxiv.org/pdf/1703.07464.pdf>)?",
"Всем привет. Кто нибудь может помочь с поиском книги: Recommender Systems: The Textbook
Charu C. Aggarwal. Буду благодарен. Может у кого нибудь есть пдфка или линк? Мои поиски не увенчались успехом :(",
"<@U042UQC96> привет, привет! :smiley: а в каком месте не воспроизводится? у меня на данный момент получилось, что NMI и R@1 на CUB200 +- такой же (значения топовые для модельки), на Stanford CARS 196 где-то чуть лучше уровня стандартных. Мой NCA loss без нормализации в отличие от того, который ты скидывал. И ещё из неприятных моментов - если у авторов статьи R@1 непрерывно растёт,то у меня он растёт на протяжении 4 эпох, а потом начинает падать. Аналогично и с NMI. Меня смущает, что не указано для CUB, Cars196 там они используют static/не static + количество проксей. А у тебя как дела обстоят на текущий момент?",
"Вяло делаю в свободное время и пока ещё ищу баги. На cars NCA loss снижается на протяжении 50 эпох, затем растёт. На новых классах r@1 стабильно ниже 0.1.
А есть способ понять место где не воспроизводится?",
"Не воспроизводится в результатах, хехе) плюс в поведении r@1 как на графиках, ну, эт то что на поверхности. Поигрался с регуляризацией в знаменателе nca loss-не особо влияет на общую картину. Но,повторюсь, у меня ненормализованный вариант - в статье они говорили, что на практике на результаты эт не оч влияет(дословно не помню). Вообдем надо будет еще попробовать с нормализацией, посмотреть что из этого выйдет",
"если там еще есть отдельная цена для студентов - регистрируйся как студент, аспирант == студент",
"<https://www.amazon.com/2011-3-Motherboards-WS-USB-3-1/dp/B00XUDLXJG/ref=cm_cr_arp_d_product_top?ie=UTF8> - на амазоне как плохие, так и хорошие отзывы есть",
"Ну так. С рейдом и 4мя карточками она тупит на первом старте после изменения настроек или конфигурации и долго щелкает биосом.

А еще с ней был прикол: не видела половину памяти.
Решилось перестановкой проца (видимо контакт плохой был)
А ну и какая-то фигня с настройками таки была: эпизодически не стартовала, приходилось сбрасывать настройки. 
Подозреваю, что когда напихиваешь кучу железа -- это обычное поведение)",
"Здравствуйте! Тоже вопрос по одной работе :slightly_smiling_face:
Знаком кто-то с этим алгоритмом (<https://arxiv.org/pdf/1506.04878.pdf>)? Можете объяснить как точность меряется?",
"чат. я всю неделю буду фигачить ультра-мануал по бустингу в рамках <#C39147V60|mlcourse_open> 
что бы вы хотели разобрать про бустинг?

например, частые вопросы:
-что какие параметры значат и как друг с другом завязаны
-как правильно тюнить гиперпараметры
-…

есть возможность “на заказ” чтонибудь доразобрать :thread-please:",
"по мотивам популярных тредов в канале:
- применимость бустинга к OHE и прочим разреженным данным
- [failconf style] как (неочевидно) выстрелить себе в ногу бустингом",
"ох, с кастомными лоссами на самом деле можно целую лекцию фигачить. которая будет связкой из того, что по большей части, что если вы очень хорошо знаете че где нужно, почти наверное хватит и вектора весов либо преобразования таргета. и что вы можете упороться и зашивать в лосс сразу регуляризационные компоненты (на страх и риск). ну и что можно поехать так далеко как захотите (для ранжирования надо группировки вводить, и если надо делать многомерные предсказания\вывод, это реализуемо но едрить геморрой и надо четко понимать нахрена вы это делаете - например для многошаговых предсказаний ряда)",
"и с кастомным лосем потенциал для отстреливания  ноги особенно высок (впервую очередь сходящийся к тому, то ли вы вообще предсказываете как таргет)",
"Ну и рекомендации, как хгбуст тюнить",
"Я думаю, не стоит цепляться к словам. Конечно, из двухсот стран мира в какой то да есть академия, имеющая некое сходство с нашей. Всем же понятно, каких академиков имели в виду.",
"Подскажите, как в Seaborn управлять отдельными графиками на subplot? Пример ниже, есть 2 графика, мне нужно для каждого вручную задать xlabel, ylabel, а также поменять подписи данных как это сделать?",
"Привет

А если у меня стоит видюха Intel® Haswell Desktop , и у Intel есть OpenCL для работы с GPU, но только для Виндоус :disappointed: а у меня Дебиан Линукс, то как быть? У вас у всех стоят GeForce? (аренда инстанса на Амазоне пока не расматривал)",
"правильно ли я понял, что в модель как фичи заносилось 4000 лагов предсказываемого значения? Это слишком много, оригинальных лагов в пределах 10 обычно заносят, а остальное агрегатами и сезонными фичами добивают",
"<@U0DA4J82H> попробую както про именно значимые взаимодействия а не фичи задеть. просто как перейти от простых значимостей к значимым взаимодействиям - нетривиальный обьем шаманизма не-вводного уровня статьи. тоесть на PDP можно посмотреть и в первой статье, как и нашаманить аналог переора значимостей для пар фич… а вот нормальный пример с разбором всех деревьев ансамбля по листьям-бинарным фичам, прореживание его лассо-моделью, и сбора с него тех пар фичей, которые были более важны для оставшихся после лассо… люди охренеют короче :feelsgood:",
про PDP и пары фичей - да (и тизер куда смотреть за реальными взаимодействиями). вообще про выдергивание значимостей надо именно научную статью писать. тема еще недоделана и как раз для статьи на конференцию типа ICDM\KDD - самое оно,
"В интеле точно делали свой фреймворк для DL, и там вроде как заявлена поддержка OpenCL. Не знаю правда, как у них сейчас дела. ",
подскажите пожалуйста как дообучить готовую модель gensim,
"что-то не работает? дообучить можно просто загрузив модель и потом делать так же как при обучении с нуля, насколько я помню доки. словарь можно оставить прежним, наверное можно и попробовать добавить слова (но я не пробовал так). еще важно чтобы препроцессинг (лемматизация, токенизация и т.п.) были максимально похожими",
"Привет) А не подскажете такие статьи, в которых нейросеть ""широкой специализации"" управляла бы нейросетями ""узкой специализации""?  Как, типа, менеджер, который знает, кто лучше справится в данной ситуации - и тому кидает задачу. Ответ берется как функция ответов экпертов, назначенных ""менеджером"" на этот таск. Обучаются все с нуля.",
"Берем простую двухслойную сеть и смотрим на неё под следующим углом:

Каждый нейрон первого слоя суть ""эксперт узкой специализации"". Нейрон выходного слоя - это ""менеджер"", который знает ""кто лучше справится"", и ""кидает задачу"" именно им (это определяется весами, которые влияют и на влияние каждого ""эксперта"" на предсказание, и на то, кто из ""экспертов"" будет больше платить за ошибки).",
"Ну тогда разберись чего ты хочешь добиться, потому что исходя из твоей спецификации так вполне ""идет"".

Более того, одно из самых понятных обоснований, поясняющих почему нейросети осмысленны - как раз вот такое.",
"В областях вне нейросетей многослойность не является столь естественным аспектом, и там можно найти более специфичные методы, где действительно ""отдельно тренируются эксперты или фичер-экстракторы, отдельно делается комбинация оных"". Самое простое - тот же стекинг. Из более специфичного сейчас приходит в голову клевый метод из kernel methods где бралось несколько кернелов и тренировался новый кернел, являющийся взвешенной комбинацией оных. Это всё впихивалось в SVM и выглядело довольно наукоемко.

Но опять же, с колокольни именно нейросетевого диплернинга - это всё просто end-to-end learning глубокой сети.",
"под словарем подразумевается как раз те слова, которые попадают в модель. максимальный размер словаря - все слова, которые встретились в корпусе. но его можно уменьшить, отказавшись от редких слов",
"Странно кстати что они софтмаксом веса скалируют, логичнее имхо их рассматривать как отдельные бинарные классификаторы.",
"Кто понял из сайта, будут записи?",
"Вообще по-моему такого типа input-dependent stacking можно куда угодно прикручивать (пусть не везде получится end to end, всё равно лишняя простая нелинейность). Надо чтобы на кагле кто-нибудь попробовал, хватит уже линейно стакать хгбусты, нужна революция!",
"можешь поступить как некоторые физики: возьми корень из своего единственного значения и это будет в очень грубом приближении доверительный интервал. ЗП вроде имеет лог-нормальное распределение, так что для получения реалистичной оценки лучше взять log(30000) ~ 10.31 +/- 3.21 -&gt; доверительный интервал 744714 - 1211. Такая оценка смотрится реалистично, однако доврять такому интервалу можно весьма условно, поэтому если есть возмонжность, стоит узнать хотя бы примерный размер выборки, тогда доверительный интервал сразу станет лучше. Конечно, этот трюк работает не всегда и только на определенном масштабе. Попробуй догадаться сам, почему :slightly_smiling_face:",
"Всем привет. Возможно кто-то встречал какой-то ресурс с различной статистикой связанной с человеческой жизнью. Например, тебе кто-то говорит, что средняя продолжительность жизни уменьшается, а ты ему такой хабась ссылку, а там как раз таки обратное, со статистикой. Или кто-то говорит, что в авиакатастрофах погибло больше чем в автомобильных авариях, а ты тоже ссылку.",
"Вот-вот, расскажи, как деревья строятся",
У кого нибудь есть опыт работы с яндекс толокой?,
"Еще домашку надо бы. Как раз первую часть думал сделать а-ля «бустинг своими руками», но если это в статье будет, то видимо, просто на применение хгбуста к каким-нибудь данным, хотя тут сложно придумать что-то оригинальное. <@U35GH0DDH> готова помочь с заданием, так что курируй",
"потому что после лекций про бустинг, основной рабочий инструментарий в каггло-подобных задачах у людей уже появится. и дальше уже надо давать задачи на применение всех накопленных знаний (а тут как раз будет и бустинг, и фича инжиниринг, и временные ряды, и знание прочего материала про другие модели)",
"А зависимость известна? Можно костылить предсказания одних переменных, как фичи для других",
"Да, почему нет. Но там в основном 2-ядерные i5 или i7, но задорого же. И памяти 16 Гб, как правило. Это, в принципе, стало основным фактором для меня купить декстоп. Насчёт многокарточной системы кмк покатит, если драйвер понимает.",
"я не уверен, что кому-то это может быть интересно, но допускаю такую возможность.
Shopify выложила стайлгайд + набор компонентов для веба, и в стайлгайде в том числе описано, как рисовать графики. Что-то вроде правил типографики, но для плоттинга.
<https://polaris.shopify.com/visuals/data-visualizations#navigation>",
"На саму конфу не отпустят с работы, а вот пересечься с теми кто поедет за рюмкой чая я бы пересекся.",
"подскажите, в каких случаях на графике boxplot может не показываться медиана? я так понимаю, что она совпадает с Q1 или Q3 и поэтому на графике ее не видно, но не могу себе такое распределение представить",
"Зависимость выбора метода при решении задачи сильно зависит от задачи. Из описания ""только числа на входе и выходе"" вообще ничего не понятно: ни почему тут нужен Q-learning, ни что-либо ещё.",
"мм, а почему именно 5-10% ? Логичней измерять относительно размероности каждого вектора",
Как тестить gradient flow в керасе7,
"вот насколько малую? просто в данный момент у меня вся обучающая выборка - 200 семплов. Её, как по мне вообще дробить не надо",
"Если не вдаваться в детали у меня есть бинарная матрица - результат опроса с 1000 ответами (строками)  на 3000 вопросов. Как выделить 'мусорные' ответы? Есть ли теоретическое название этой задачи? Пример мусора - ответы на все вопросы только '1' или только '0'. Интересно численное решение в виде статистики индикатора мусорности или графическое, как представить такой датасет вообще визуально?",
"кто писал кастомный генератор для Keras есть вопрос, падает после нескольких эпох и причина не понятна?",
"может быть, понижение размерности (PCA?), а потом поверх плотных векторов уже какой-нибудь DBSCAN. и те кластера, которые состоят из единичных (немногих) точек можно уже рассматривать как подозрительные.",
вот еще возможный подход <http://scikit-learn.org/stable/modules/biclustering.html> как думаете?,
<@U36Q9NJMD> какой конкретно алгоритм предложите для кластеризация без снижения размерности?,
"ну да, засчёт гейтов там возрастает кол-во параметров. но, когда соберёте модель целиком, посмотрите сколько в ней параметров. может быть, это не так и критично окажется.",
могу попробовать помочь как дойду до ноута ,
"<@U3PETUSSE> я правильно понял, что те у кого комбинация ответов сильно отличается от среднего - тех будем отсекать?",
"<@U505GQJ6M> Если ответы респондента выглядят как куча нулей или единиц (выброс), то такой респондент мало информативен (маленькая дисперсия). ",
"как раз хотели с осадчим похожее сделать, а потом увидели, что уже кто-то запилил ",
"да насчёт неполных батчей определённо идея, но вот сейчас не упало почему то… конец эпохи я не знаю :disappointed: или пока не нашёл метод который за это должен отвечать",
"Пояснить зачем применяется разложение в ряд Тейлора, зачем нужна гессиана, ну и в целом разжевать objective(касательно xgboost).  Пояснить чем gbree отличается от gblinear.  А ещё как применять feature engeneering , и почему добавление столбца со средним откликом для категориальных данных хорошо работает.",
"<@U36Q9NJMD> задача в общем наверное стандартная, в смысле что ее в любой маркетинговой фирме можно найти: выделять в результатах опросов странные ""outliers"". Я ее решаю в виде тестового задания, так что в данных я ограничен. Но идеи всякие пригодятся. Я думаю тем, кто задал задание - все-равно откуда я возьму идеи; сам придумаю, в книжке прочитаю или здесь нахватаюсь. Так что большое спасибо за идеи!",
"Как не знаешь? Твой же генератор (функция trainGen) по данным идет, как до конца дошел, вот эпоха и закончилась",
<@U04URBM8V> где завтрак то будет ,
<@U36Q9NJMD> а какие плюсы/минусы по сравнению с альтернативами типа жадного перебора или выдергиванием важности фичей из моделей?,
может уже было - но вдруг кто пропустил <https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5>,
"<@U1G303UTW> я должен был сам догадаться, что это iv это не инструменты совсем не выспался) 
Плюсы: быстро посчитать, есть статистические тесты на значимость, реализовано везде, где есть автоселекшн
Минусы: может делать полную фигню, статистические тесты sucks. Нет никакой информации о взаимодействии фич.
Вообще вроде как это приехало из Information retrieval и люди(особенно экономисты) обычно используют это, чтобы разбивать переменные на бины, а потом считать woe, например. PS. К отбору фич это не очень относится, но все эти методы очень критичны к выборке, можно даже рассматривать их как модель первого уровня, поэтому если делать woe, то потом могут заходить простые модели в стиле логрегрессия/линейной с регуляризаторами. Это скорее способ подтюнить результат к выборке. Ну и есть некоторая аналогия, когда это применяется к отбору фич, все очень сильно зависит от выборки в трейне. 
А еще метод изначально для классификации и для категориальных фич, так что либо непрерывные фичи разбивать на категории (например, алгоритмами для деревьев) и тогда возникает вопрос о том, а правильно ли мы разбили+разбиение сильно зависит от данных, либо фитить непрерывные распределения и интегрировать вместо суммы, что вычислительно сложно(и скорее всего мало где реализовано)",
"Есть догадка, что они все это делают в контексте BCLT, но почему теоретические результаты не использовать в асимптотике?",
"тут уже советую погуглить по порисерчить литературу :slightly_smiling_face: 

с расстановкой банкоматов вся магия только в том, что считать “оптимальным“, изза чего большинство наработок будут в неудел для публикаций, так как метрики там могут быть недостаточно competition-friendly :eyes:",
"Да, верно, но это ничего не меняет, новый параметр это старый параметр // размер батча.

Но твоя-то функция которая батчи генерит, она-то знает когда данные закончились (она не может не знать, она по ним в цикле идет).

И да, неполные батчи керас не умеет ""All arrays should contain the same number of samples.""",
"xgboost умеет warmstart? если по ссылке как раз этот warmstart, то при каждом таком warmstart'e количество моделей растет? как-то не очень это
<https://github.com/dmlc/xgboost/blob/master/demo/guide-python/boost_from_prediction.py>",
"<@U04422XJL> проблема такая: нужно обучить на одних данных xgboost, а потом периодически дообучать его на новых данных. Насколько я понял, xgboost не дообучить кроме как добавить еще деревьев. Значит, xgboost не годится для того, чтобы его потом многократно дообучать. Верно?",
"до-обучать можно, как раз с помощью новых деревьев, но мне кажется, это не то, что нужно в этом случае",
"Всем привет. Хочу спросить у опытных бустеров -важно ли чтобы train loss был как можно ближе к valid loss?
У меня есть два примера и я не очень понимаю какой модели доверять больше. Помогите разобраться

1) Запустил со стандартными параметрами
Early stopping, best iteration is:
[913]	training's binary_logloss: 0.164564	valid_1's binary_logloss: 0.269751

2) Пытался подкручивать bagging_fraction, feature_fraction и lambda, alpha b learning rate так, чтобы train loss не убегал быстро от valid loss
Early stopping, best iteration is:
[4445]	training's binary_logloss: 0.19813	valid_1's binary_logloss: 0.269121 

В итоге результаты на холд-ауте почти не различаются",
"не, пулинг тоже по времени делаешь, просто либо глобальный (один фильтр - одна координата), либо макспулинг как в картинках с каким-то сжатием (у меня получались на чар-бейзед свертках какие-то приятные цифры при использовании maxpooling -&gt; lstm)",
а для русского языка базы для сентимент анализа есть какие нибудь?,
"На работе в папочках нашел 2 интересные книги:
Pattern recognition and machine learning by Bishop
Neural networks for pattern recognition его же.
Вроде классические книги. У меня вроде как понимание машинного обучения есть,  стоит почитать, чтобы систематизировать знания? Или там ничего особенного?",
"хм, как то странно обучать что то на базе размеченной автоматически",
"ну первая книга очень хорошая, мне понравилась. Её и ESL постоянно как справочник использую",
"почему кеггло-митапами поломали? мне скорее, например, непонятно как ML задачи рождать без данных: просто что-то накодить ладно, но мы-то знаем что настоящая нефть нашего века — это не алгоритмы, а данные. И что тогда на хакатоне делать?",
"самим скачать? помимо того, что на большинство задач есть датасеты, все примеры выше сами качали данные: актеры с wiki, база порнозвезд, посты с хабрахабра. в этом плане хакатоны не такие домашние-простые, когда все бьются над одним и тем же",
"Ребята, расскажите в общем про сферу NLP в её текущем состоянии: какие задачи решают те, кто работают в бизнесе (а не академии), с помощью NLP?",
"могу привести пример Google Translate, который всем ясно какую задачу решает",
"ну да, машинный перевод как одна из групп задач -- понятно",
"Чем отличается апи xgboost станартное от sklearn интерфейса в gxboost  и lgbm, заметил что есть параметры отличающеся хочется понять когда какое лучше юзать",
а какой у тебя корпус размером?,
"Ну я когда обучил w2v, он очень даже адекватным выглядел. Жалко в общем, что сверточные не зашли ",
когда ты в лосс-функции не учитываешь нули (когда padding делаешь и у тебя длинна отзыва меньше твоей максимальной длинны),
<@U042UQC96> а в каком это бизнесе применяется?,
"<@U042UQC96> а где здесь работа с _естественным языком_? кажется, что это в чистом виде распознавание изображений -- опознали, что на нём, и прилепили заранее заданную метку",
"Всем привет! Мы в DeepSystems запустили сервис рекомендации фильмов на основе Deep Learning.

<https://movix.ai/>

В ближайшее время планируем опубликовать технические посты и лекции о том, как мы это сделали.

Не стесняйтесь, тестируйте. Будет рады узнать ваше мнение и критику :slightly_smiling_face:

Вот немного о сервисе:

<https://thenextweb.com/apps/2017/04/26/movie-reccomend-artificial-intelligence/#.tnw_Y76TLVpI>",
а где La La Land?:confused:,
И я все еще не понимаю почему так падает перформанс на VGG. В то время как на остальных сетках буст почти 2х,
"<@U54E3UVNK> А можете в двух словах про деревья пояснить, как бы вы ставили задачу? Не решение а именно постановка, типа чего искать? Спасибо за советы, еще раз!",
"пожалуйста, рад помочь. типичная задача обучения с учителем: есть вектор ответов на вопросы и человек ставит оценку ""мусор"" / ""не мусор"". ну и обучаем бинарный классификатор. собственно, какой угодно, в том числе (по стандарту) какой-нибудь древесный ансамбль.",
"Можно, конечно. Но nlp хорошо встраивается в эту задачу. Тут все зависит от того, какие данные есть. Можно структурировать интересы пользователей (через tm, например), по текстам платежек, если речь о банке, или по профилю в соцсети и персонализировать предложения.",
"Рeбята, eсли кто знаком с speaker recognition/diarization, можeтe пожалуйста глянуть вот этот вопросик? 
<https://opendatascience.slack.com/archives/C1UEK73H6/p1493232956958506>",
"В презентации есть детали — какой был размер окна, метрики расстояния, и т.д.",
У тебя расписание на обороты какие стоит? Или 70-85 это при крыльчатках на взлетном режиме?,
"Обычно в биосе есть какая-то опция на ""прощупать"" напряжение вертушки, чтобы мать понимала, в каких пределах можно шатать напругу для управления охлаждением — по крайней мере в асусах так, а потом можно выставить шедулинг оборотов на самый нещадный по громкости",
"Ну, т.е. это не критично для твоего компа, но это как раз та разница между ~60-70 и ~70-85 градусами",
"Ребят. А кто из ученых на кафедрах в МГУ, МФТИ и Мисисе занимается nlp и делает крутые исследования?",
Сколько у тебя на щитке кВт тратится когда вся звероферма на 100% работает?,
"Я на них запустил обучение сеток по работе. Как раз пока буду катать, там что-то разумное получится.",
Привет! Не подскажите как меряют точноть в задачах object detection? Есть координаты правильных bounding box’ов и предсказанных.,
"Друзья, для проекта по Dota 2 и CS:GO на <#C55JFRG3G|spacehack> нужен бог визуализации данных (на самом деле и полубог и идущий к этой цели сойдет), увлекающийся киберспортом. Без подробностей, так как идеи будут доформулированы на месте. Хорошенько поанализируем реплеи турнирных матчей. Пишите :thread-please: или в личку.",
"Всем привет! Решаю задачу выбора лучшего предложения. Предложений много, для каждого строю свою модель (0/1 - откликнется/не откликнется). Модели получаются разные: для одного лучшее качество показывает Логистическая регрессия, для другого - градиентый бустинг. Теперь вопрос:
1)	Как для каждого клиента проранжировать полученные оценки? Изначально соотношение 0/1 для каждого предложения свое. Ну и модели разные как по структуре, так и по качеству.
2)	Если мне надо выбрать не одно предложение, а, скажем, построить цепочку из трех. Они могут влиять друг на друга. Как это сделать? То есть, я хочу добиться не сиюминутного увеличения число продаж, а получить максимум продаж через год. Не факт ведь, что жадный алгоритм будет самым оптимальным.
Буду очень рада ссылкам на статьи по этим вопросам. Хотелось бы начинать с best practices, а уже потом придумывать что-то свое. Ну и если есть какие-то способы оптимизации доходности, а не числа продаж, то тоже будет очень интересно почитать.",
Кому-нибудь попадался туториал для чайников как перебить в mxnet SSD или Faster RCNN чтобы это тренировалось на другом датасете? Там какие-то нюансы с созданием rec файлов и что-то где-то у меня идет не так.,
"<@U07V1URT9> Как эксперт по mxnet, тебе ничего такого не попадалось?",
"Пока что просто задача стоит так, что надо сначала найти нужные объекты (они разных классов, но это пока неважно). Можно было бы использовать и сейчас mAP и потом, когда уже придем к распознаванию классов, просто использовать ту же метрику.",
"Кто-то встречал работы в которых объясняется как одноцветный фон изображений  влияет на процес обучения в сверточных сетях? Например, у нас есть датасет где много изображений не квадратны, соответственно чтобы их поместить в квадрат мы вертикальное или горизонтальное оствшееся место заполняем каким-то цветом. Иногда фон может занимать больше 50%. Я вот заметил, что черный цвет это плохо, белый - хорошо (у меня были почти все изображения где фона больше, и при черном я получил оверфит тупо в один класс, в то время как с белым фоном все было ок). Моя нубская догадка в том что при белом цвете (из-за максимального значения), мы находимся в максимуме из которого проще скатиться в минимум и наоборот для черного. Что вы думаете?",
"Ок, попробую на своем датасете все три(noise, mirroring, full scale) подхода и отпишусь какой дал лучшие результаты",
"<@U0G29N5U4> Я так понимаю, когда сдвигают и есть пиксели вокруг - то конечно надо брать их",
"Ну, почему для этого хочется вырезать?",
"бот, ты почему это вчера написал? :epta:",
"Коллеги, подскажите, где выгоднее всего брать 1080Ti",
"Гайз, привет! Не знаю в какую ветку написать.. Есть у меня есть потребность лить данные из хранилища в гугл док через SAS, как это можно настроить?
Вариант отправлять почтой не самый оптимальный.",
"Кто нибудь собирается на событие 16-го мая в москве от Intel и Nervana ?
<https://aisummit2017.ru/>",
"<@U1G23QR1S> не очень понимаю, в чём трудность сравнения цен между нашими магазинами и забугорными. Это так же просто как и между местными сравнить, только учесть доставку (50$ и две недели), вот и вся разница. За бугром будет ~40k + доставка, у нас 50k и без доставки",
Цепи Маркова как раз превратят задучу в обучение с подкреплением,
Я как раз про интерпритируемость топиков.,
"Всем привет! Как и обещали, выкладываем первый пост с техническими аспектами о найшей рекомендательной системе фильмов Movix

<https://medium.com/deepsystems-ru/movix-ai-%D1%80%D0%B5%D0%BA%D0%BE%D0%BC%D0%B5%D0%BD%D0%B4%D0%B0%D1%86%D0%B8%D0%B8-%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D0%BE%D0%B2-%D0%BD%D0%B0-%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%B5-deep-learning-6f8917666ad9>",
кто что знает про Running Minimum stopping criteria?,
"Может кто знает, как в keras/tensorflow решить проблему, когда мне в кастомной функции лосса нужны дополнительные Variables, которые участвуют в подсчете значения лосса и по которым также должен идти градиент?",
"<@U1BAKQH2M> как это решит проблему с доп. переменными? я их включаю в лосс, но они остаются константными",
"К вопросу о бедных студентах, у кого нет денег на видеокарты. С начала года, комп в фоновом режиме намайнил ~300$ (да, помню ещё пару недель назад, я говорил ~200$, но с тех пор ещё чуть-чуть и из-за курса они как-то подросли)",
"запустил nicehash miner (:windows:), поставил галочку ""включаться при простое"" (и сам включаю, когда просто работаю за компом, без тяжелых нагрузок)",
"Под убунту есть все компоненты, но как всегда, чтобы всё запустить нужно копаться в параметрах командной строки и прочее",
"когда +27 на улице, то да, майнить немного проблематично, но зимой я так экономил на тепле что в беларуси, что тут в британии, и деньги в хату и железо не простаивает :slightly_smiling_face:",
"<@U1CF22N7J> хммм, а где ты видел rx470 за 150 долларов?",
"Но больше что-то не вижу :slightly_smiling_face: Видимо разлетаются как горячие пирожки, на фоне дикой профитности в марте-апреле (до 3$ прыгало с карты, окупаемость 2 месяца, конечно разметают)",
"извиняюсь за дэбильный вопрос, давно не следил за тем, что в мире железа происходит
писали тут не раз, что 1070 - самый сок по соотношению цена/производительность для тех кто не хочет много капусты сливать на DL-комп. а какие проц-мать можете посоветовать?:) для DL-каглоупражнений",
"С другой стороны истерика в мейнстримных сми улеглась (ещё полгода назад, когда 1000$ первый раз с 2013 пробили -- вою было..., а сейчас уже 1400$ у биткоина, а везде тишина)",
"Вообще началось всё с шутки, когда я предложил ему  дом вместо электрокотла компом майнящим греть :slightly_smiling_face:",
"Приветсвую! Такой вопрос, предположим есть абстрактный сайт где мы хотим дать пользователю выбор и редактирование (совсем базовое) разных графиков и т.п. Есть какое-нибудь embedded-решение такой себе редактор D3 для веба? Смотрел RAW, но не совсем то...",
"Всем добрый вечер. Пишу курсовую по _Quora Question Pairs_ (но в контексте русскоязычных текстов). Надо сделать обзор литературы (существующих подходов), и, как я понимаю, в русскоязычных источниках такая задача называется `задачей поиска нечётких текстовых дубликатов`. Но при этом во всех статьях, которые я смог найти, используется _TF-IDF_ или вообще _метод шинглов_, и нигде не описано использование _Word2Vec_ (не говоря уже о более крутых семантических моделях). Это я плохо ищу, или это я неправильно формулирую задачу? Или и то, и другое?",
"Всем привет! Кто-нибудь может посоветовать толковых статей про предсказание оттока клиентов? Тема вроде должна быть избитая, но большая часть того, что я пока находил - вода.
Было бы совсем идеально, почитать про пример конкретного кейса, где затрагивалось бы не только решение, но постановка задачи: что было выбрано в качестве флага оттока и почему, какую метрику(и опять же почему) использовали для оценивания качества.
Но на самом деле любая инфа, которая вам кажется полезной по данной теме крайне приветствуется.",
"Как мне кажется, лучше рассмотреть и английские статьи, это ничему особо не противоречит, т.к. большинство методов от языка не зависят, а на русском хороших статей довольно мало.",
"хммм <@U1CF22N7J> два дурацких вопроса
1/ где ты обитаешь что у тебя такие аццкие перепады температуры (у меня за окном примерно та же хрень с перепадами - просто интересно где так еще встречается)
2/ а тема с майнингом мне интересна что-то стала - а какие майнеры под убунту и мак можешь насоветовать?",
"Можно попробовать  разбить на подтемы: исходные данные, формирование признаков (feature engineering), выбор модели. С выбором модели все достаточно просто. Можно, например, рассказть про градиентный бустинг, так как XGBoost рулит в последнее время. Формирование признаков, тут уже посложнее, например, в морфологии, надо учитывать лексемы.",
"Недавно был курс Stanford NLP with DL и там студенты делали финальные работы, многие из которых как раз по quora. Они используют в основном Word2vec+LSTM или что-то в этом ключе. Возможно что-то окажется полезным. <http://web.stanford.edu/class/cs224n/reports.html>",
"ну и не уверен, что в мск майнинг так же рентабелен, как в сибири",
"Чот большая разница, хотя это же регард, там как-то дорого всё, на я.маркете от 45 тысяч 1080ти",
"А ты спать в комнате с батареми, где внутри бывает перегретый пар под давлением в несколько атмосфер годами гоняет, не боишься?",
"Я уже придумал себе влажных фантазий как в середине лета закончатся все конкурсы, будет жарко и я займусь водянкой.",
"ок) в любом случае, хозяин - барин, как говорится",
ну почему же - если ты говоришь что за 4 месяца 300 накапало то “истина где-то рядом”,
"Ну да, как амортизация части затрат на так или иначе нужный инструмент -- отлично, как заработок -- я бы сильно подумал.",
"ezkhrv: что считать оттоком решает бизнес. Для кого-то ""3 дня не заходил"" очень важно, а кому-то ""2 визита в год"".В среднем, от 2 недель до месяца и далее- это уже отток. Или- не оплатил следующую подписку- тоже отток. Это еще и важно выбрать правильную метрику для бизнес-модели, чтобы в правильный момент включать следующий за оттоком бизнес-процесс Вернуть чувака обратно. Вернуть почти всегда дешевле, чем привести нового платящего.
Решается как и любая задача классификации. Сейчас модно много деревьев xgboost гонять.",
"что за хрень, почему lasagne так отстал от жизни? там депенденси theano 0.8.0 до сих пор",
"Сейчас Сережа скажет: Фреймворки переписывают только те, кто сразу хорошо писать не может :keras:",
"Большое спасибо всем за ответы, но я имел в виду немного не то :slightly_smiling_face: Наверное, неправильно сформулировал вопрос. Я знаю, что для английского языка существует множество разных методов, но я не смог найти русскоязычные статьи, где описывалось бы их применение именно для русского языка. Мне кажется, это довольно важный момент, поскольку качество работы алгоритма на языке другой языковой группы может сильно измениться.

Если для русского языка таких статей нет, то это здорово, значит буду заниматься относительно неизученными вещами :slightly_smiling_face:",
"<@U32506X36> Ну, я бы не сказал, что с выбором модели всё просто :slightly_smiling_face: Многое зависит от того, какие использовать признаки, и по-хорошему XGBoost надо сравнить с классическими классификаторами, чтобы иметь полную картину. Но в идеале я бы рассмотрел ещё LSTM и BiMPM, поскольку именно в этой задаче они показывают самые крутые результаты. Но это, опять же, для англоязычных текстов. 

Ну и да, формировать признаки надо тоже с помощью какой-то модели, и тут есть два разных типа, word embeddings и sentence/document embeddings; каждый представлен совершенно разным набором алгоритмов, и их всех надо посмотреть и сравнить. 

То есть, как я понимаю, надо выбирать две модели для двух разных этапов, и их сравнение может по сути быть ключевым моментом исследования.",
"<@U09BY2N3X> Да, некоторые ещё формулируют задачу как paraphrase identification/detection, text semantic similarity, semantic analysis и semantic verification. Но это всё для английского, а на русском ничего кроме ""нечёткого поиска"" я не нашёл.

Кстати, а в чём твоя работа с библиографиями заключается?",
<@U3TF60E65> а работать когда тогда?,
А кто нибудь пробовал openSUSE + 2 x 1080 ti ?,
"Коллеги, добрый день. Подскажите, пожалуйста, почему Glove модель весит в десятки раз больше чем модель word2vec? Разве там не просто маппинг слово-вектор? На одном и том же корпусе обучил - w2v 1.5 гб, Glove 16 гб. ",
"какой формат в том и другом случае? насколько я помню word2vec гугловый делает бинарники, а glove plain text",
"mtrofimov: 87-95 где то, это можно посмотреть в настройках",
"Насчет текстов, можно ещё обучать супер короткие пни и делать create_features из xgboosta, и пихать в логрег должно хорошо заходить. Но у меня пока не заходит почему то",
n01z3: водянка как целая видяха дополнительная,
"В последние недели мучаюсь со стэнфордским курсом cs231n, где учат, как писать сетки с нуля на numpy и без крутых библиотек.
Если feed forward я вполне осилил, то на CNN я сдался - forward pass для convolutional layer ещё смог написать, но с backward уже не смог справиться.
В самом курсе это описывается совсем мельком. Про backprop для этого случая вообще одно предложение написали - типа это легко, надо только повернуть фильтры...
Что можно почитать, чтобы научиться понимать это? Или учиться самостоятельно писать backprop на практике особо не нужно (достаточно посмотреть как это сделали другие и понять)?",
"какой лосс может быть лучше, если не закапываться в метрик лернинг?",
"может как раз быть достаточно для того, чтоб тротлинг не наступал",
"Всем привет, есть вопрос, но не знаю в какой тред писать, поэтому напишу сюда. Работаю с match3, стоит задача автоматизации тестирования уровней. Если коротко, то для каждого уровня необходимо вычислять показатель сложности (&lt;количество удачных прохождения уровня&gt;/&lt;количество попыток прохождения уровня&gt;), считается для всех пользователей, игравших на уровне. Для получения адекватного значения сложности необходимо большое количество отыгрышей, причем по несколько раз и разными людьми. Собственно, суть задачи, автоматизировать процесс отыгрыша уровней, т.е. условно написать скрипт, который бы отыгрывал уровни. Написать скрипт, который бы смог сделать последовательность ходов на уровне до проигрыша (кончились ходы) или до победы (выполнены цели уровня) не сложно, а вот сделать так, чтобы он “эмитировал” игру среднестатистического пользователя уже не так просто, по крайней мере я не вижу с ходу решения. За сим обращаюсь к коллективному разуму за советом. Может кто знает как решать подобные задачи, или хотя бы с какой стороны подойти к задаче. Или может кто решал подобные задачи и может что то дельное посоветовать, или хотя бы направить.",
А как водянка установлена сверху и нагнетает горячий воздух в корпус?,
"<@U4ZRFG1V0> так по аналогии сделай, там же надо суть уловить, а не то, как он применяется к конкретному кейсу",
Я бы порекомендовал сначала решить что вы будите делать с теми про кого вы знаете что они уходят,
"Никто не подскажет, как называется такой график и как его строить в питоне?",
"<@U37CABUAZ>, Tableau я пробовал, но в бесплатном режиме я бы не сказал, что там все очень просто. Если будет все еще интересно, я могу скинуть пример в googleVis (понадобится R), где по сути только нужно сразу в микро-программу прописать загрузку файла, а дальше юзверь сам определит, что нужно строить ему, и как все обозначать. googleVis оказалась достаточно умной штукой, она также умеет определять столбцы и говорить (на экране), какой тип графика наиболее релевантен под входные данные.",
"звучит так как будто AlphaGo длжно помочь. Смотри оригинальную статью DeepMind, ну или просто обычный tree search, что-то такое, все зависит от количества возможных ходов в игре",
"Допустим фича делается неправильно, так что для неё протекает таргет. В результате при обучении деревья будут сплитить по ней как бещенные и на другие подзабьют. И качество общее может упасть ",
"Широкий вопрос. Помогите плиз сформулировать фразу для поиска инфы по след. теме: Как онлайн-ритейлеры строят свою инфраструктуру данных? То есть условно говоря, я хочу узнать как там какой-нибудь сферический Walmart работает в онлайне. Какие технологии используют, что трекают, какие проблемы решают, о каких объемах инфы идет речь, сколько людей нужно для этого. Я пока рисерчу по ключевым словам online retailers engineering data warehousing infrastructure e-commerce, и подумал может быть что-нибудь важное упускаю?",
"<@U32J3CRP1> да, почитаю как у них все устроено, благодарю",
"<@U1J7A5067> да, тестовая выборка по отыгрывающим пользователям - само собой, я только ен знаю что пока собирать, какие данные нужны.",
"agzamovr: зачем такой мощный бп? Даже если планируешь поставить две видюхи, то хватит 850 Вт",
"видела, как для такой задачи адаптируют Metabase (<https://github.com/metabase/metabase>)",
"Если я в классе для модели на Pytorch сделаю (в том числе) слой Linear, который input произвольной размерноcти переводит в двумерный, это не вызовет трудностей с тренировкой сети? Если вызовет, то как вообще надо работать с CNN для произвольной размерности, чтобы это не приводило к проблемам?",
"<@U1D4RRA7K> Например, с какими сталкивался ты?) Я вот сейчас lgb гоняю, чтобы погонять потом xgb, перед которыми было дофига numpy. Не встречал проблем.",
"Ну у меня не работало банальное перемножение матриц без шаманских действий с компиляцией numpy из бинарника и отвязки её от openblas, а сейчас столкнулся с тем что не работает pca  из sklearn - похоже какая-то из инструкций падает.
Это, правда, серверный проц в виртуалке ещё (и я даже не знаю какой конкретно) но при тех же вводных когда до этого у меня был интел -- все работало без проблем.
В общем я и сам бы рад поддерживать конкуренцию на рынке, и все такое, но я ленивый и не хочу решать такие проблемы - пусть хотя бы геймеры и видео-блоггеры поддерживают амд и заставляют интел волноваться",
"Как делают сейчас не означает, что это правильно. Они все начинали строит когда-то с доступными на тот момент инструментами. А так, сейчас многие на AWS мутят. На сайте Амазона есть много кейс-стадис с примерами инфраструктуры. Можно почитать про бекенд Localytics, Mixpanel. Ну и про обычные метрики e-comm. Так целостная картина сложится более-менее.",
"Как вариант можно взять другую материнку и ECC память, по цене тоже, а лишним не будет. MSI ECC не поддерживает для Райзена, точнее поддерживает, но в non-ecc режиме.",
"Что-то мне места на компьютере стало не хватать под эти активно растущие датасеты, тут 100Gb, там 100Gb, хочется добросить 1TB SSD. Они все на одно лицо, или есть какие-то интересные места, на которые стоит обратить внимание?

Пока я планирую там хранить распакованные данные со всяких задачек, ну и тренировать модели, которые будут их считывыть. По идее и HDD будет за глаза, но с другой стороны - если можно SSD, то почему бы и нет?",
"Бери самый дешевый вариант от нормального производителя, какой-нибудь Crucial обойдётся тебе как полтора похода в баню: <https://www.amazon.com/Crucial-MX300-Internal-Solid-State/dp/B01IAGSDUE/ref=sr_1_2?s=pc&amp;ie=UTF8&amp;qid=1493656259&amp;sr=1-2&amp;keywords=1tb+ssd>",
"ну вот второй от sandisc греется как больной, на нем система",
"Скажите, а есть примеры success story задач/алгоритмов, где классика ML и DL не работает, но хорошо работают баесовские методы? ",
"Хочется разобраться, но непонятно пока зачем это нужно. А так может уверовал бы. ",
"vadimadr: везде, где хорошо себя показывают гауссовские процессы. Посмотри доклад с недавнего PyData в мейле",
m.yurushkin: тоже вае на текстах хочу. а у тебя какой датасет?,
"Всем привет!)Можете подсказать книги(желательно учебники) по аналитике, в которых она будет рассмотрена как дисциплина со своей теоретической базой и методологией. Отмечу что под аналитикой имеется ввиду дисциплина с помощью которой можно познать любой аспект реальности - и  вебаналитика и аналитика требований здесь только как частные случаи, и следственно книги по ним не интересуют. Возможно данная дисциплина может называться както иначе , например системный анализ или чтото в этом роде.
Нашел что то похожее <https://www.livelib.ru/work/1000954351-analitika-metodologiya-tehnologiya-i-organizatsiya-inoframatsionnoanaliticheskoj-raboty-pavel-yurevich-konotopov-yurij-vasilevich-kurnosov> - оглавление многообещающее, но внутри такое...- короче это не учебник. 
Спасибо за помощь)",
"Всем привет!)Можете подсказать книги(желательно учебники) по аналитике, в которых она будет рассмотрена как дисциплина со своей теоретической базой и методологией. Отмечу что под аналитикой имеется ввиду дисциплина с помощью которой можно познать любой аспект реальности - и  вебаналитика и аналитика требований здесь только как частные случаи, и следственно книги по ним не интересуют. Возможно данная дисциплина может называться както иначе , например системный анализ или чтото в этом роде.

Нашел что то похожее <https://www.livelib.ru/work/1000954351-analitika-metodologiya-tehnologiya-i-organizatsiya-inoframatsionnoanaliticheskoj-raboty-pavel-yurevich-konotopov-yurij-vasilevich-kurnosov> - оглавление многообещающее, но внутри такое...- короче это не учебник.

Спасибо за помощь)",
"Бороться надо, большая выборка не поможет, PCA как правило не поможет (поскольку признаки коррелируют в подмножестве выборки и это подмножество надо еще найти).",
"Как у Keras c tf backend сказать tf, что вот это GPU можно использовать, а вот это трогать не надо?

А то сейчас tf резервирует под себя оба GPU, а считает только на одном.

В th это решалось ` THEANO_FLAGS=mode=FAST_RUN,device=gpu1,floatX=float32 python resnet.py`",
тогда какие могут быть альтернативные решения?,
"Spatial? Я смог понять, как это делать, но никак не мог красиво реализовать. Написал с циклами, сильно больше 5 строк. А потом нагуглил решение в 2 строчки с использованием transpose, о чем я раньше не знал.",
"Товарищи, рассматриваем и сравниваем имеющиеся сервисы для визуализации KPI-метрик (по заранее отобранным данным, но с возможностью кастомизации со стороны конечного пользователя - не программиста). На данный момент используем CubesViewer (<http://www.cubesviewer.com/>) *Собственно вопрос*: а есть ли у кого опыт использования сервисов Tableau, Sumologic, Qlick в продакшен-среде (ну или хотя бы дальше учебных примеров) - очень было бы интересно узнать и обсудить особенности. Со своей стороны моге про CubesViewer рассказать и показать возможности. Спасибо.",
"Подключаются когда всему, начиная от текстовых файлов до апи и sql в облаке и внутри сети.",
"dmitrijsc: А не могли бы вы рассказать, какой объём (порядок) данных задействован и какое количество пользователей? Хорошо ли это дело масштабируется?",
"PowerBi себя больше позиционирует как self-service, то есть сделай дешборд себе сам. По количеству пользователей сказать не могу, там их не много, до 50. По объему данных - тянут от мелких датасетов до 100 мб до истории транзакции ~10гб. ",
"<@U4QF4S91U> порядок разработки, делается всё в десктопной версии, так как она более функциональная, аплоудится в веб интерфейс и далее шерится с пользователями на основе прав. ",
"В BI можно и в вебе все делать, главное данные подгрузить.
Один из ощутимых минусов, это если нужна ppt, то нужно спера загрузить в клауд и от туда скачать ppt.

В остальном как инструмент визуализации очень хороший.

Ради интереса скармливал декстому 50млн строк, полет нормальный.",
<https://www.autodeskresearch.com/publications/samestats> очень наглядно (с датазаврами :crocodile:) зачем нужна визуализация данных,
"andrei-kondakov: А как понять как разделять числа? ""400р"" -&gt; ""400"" и ""р"", но при этом ""20.03.04"" -&gt; ""20.03.04""",
"<@U56HBCAAX> nltk, да, это лучшее что я пробовал, действительно наверно нельзя будет разделять сразу и даты и цену, тогда просто как быть со случаем ""400р"", ""400рублей""? nltk.word_tokenize не справляется с этой задачей, самому ручками разделить полученные nltk токены?",
"да, цена вроде хорошая - на порядок ниже маркета
А почему устарел? Вроде новые ксеоны стоят под 30, Там 3.33 ГГц, 6 ядер",
А зачем тебе два зиона?,
"Эмпирическое, ведь пиксели на картинках сильно локально коррелированы. Правда и там применяется subsampling слои и т.п. Вообще, хороший пример выборки с такими неприятностями - MNIST, вот смотрите какие методы и как применялись, что применялось для декорреляции признаков (может, выбор нескольких, LDA или подобное). Статьи с формализмами упомнить не могу, в вопросе о мультиколлинеарности признаков для регрессии что-то такое было.",
"<@U36Q9NJMD> а почему ""бюджетный вариант не на ксионах"" лучше?
я вот наоборот начал ксионы искать, потому как вроде они быстрее в цене теряют",
"avasilev: Очень этот вопрос тоже интересует. Я понимаю, как раскидывать различные операции по разным GPU, но вот так чтобы одной строкой как в mxnet или pytorch, так я пока не умею. :disappointed:",
"привет, один товарищ спрашивает, можно ли решить задачу распознавания цвета волос (по шкале от 1 до 10) по снимку обычной камерой в обычных условиях. датасет готов собирать, какой нужен будет (например, снимок волос и рядом образец известного цвета). я так понимаю, тут проблема в текстуре и балансе белого может быть. кто-то что-то подобное делал или слыхал?",
"Зависит от того, какие настройки и какой white balance",
"Всем привет, подскажите как воспользоваться отчетом который выдаёт тулзовина xgbfi. Что подразумевается под 2-interactions? суммы, разности или что?",
"но вопрос в том, сталкивался ли кто с этим уже? может, баланс белого уже сетки научились определять на снимке",
"тестовые - нет. тренировочные - как скажете, я так понял. я не знаю почему, но не хотят давать палитру, хотят сфотать и чтобы все само",
"Кажется на Керас в качестве основного фреймворка для глубокого обучения рассчитывать не стоит :disappointed: Почти все реализуют свои идеи в чём угодно, только не в нём, а сторонние реализации все как одна кривые до ужаса. Видимо сказывается лёгкость освоения и привлекает тем самым всех индусов, как легкий вариант по умолчанию.",
"а ткните носом где почитать про современные методы для trend detection во временных рядах? туторы, книженции, блогпосты и просто названия методов",
"<@U0AD1L5NC> да с калибровочной шкалой и я бы осилил, но поэтому и спрашиваю тут, что чуваку хочется сервис без нее. я сам далеко не уверен, но почему бы не спросить",
"Ну если датасет есть, почему не попробовать",
"а так же какая вообще принципиальная разница между trend detection, anomaly detection и novelty detection?",
У меня нескромный вопрос -- где там батч-сайз устанавливается?,
"Короче надо на MXnet все это пилить, но я никак не пойму как данные скариливать сети.",
"я одно не пойму: почему не запустить генерацию данных на диск, а потом с него читать, когда гоняешь с разными параметрами? тормозит же жестко...",
"Привет всем. Застрял тут над одной задачей по теорверу, может кто-то подскажет что-то дельное. 
Есть N корзин с шарами. На каждом шаге берётся шар из случайной корзины и откладывается. Какое матожидание числа шагов, после которых из каждой корзины будет отложен хотя бы один шар.",
"Блин, какой ужас, там весь код делает не то, что написано",
Да чот не особо дохрена...,
"Я пока не смог допилить. На уровне подавать данные правильно. Плюс там вроде аугментации нет через D4, как есть в Керасовской, а данных маловато.",
"Меня не отпускает тот факт, как быст mxnet и как хорошо он параллелится. Я еще не отступился от идеи натравить его на ImageNet и на AWS на 8 GPU, ну и хоть сабмит сделать.",
"<@U0AS548A1> <@U2GTUS0CB>  да, спасибо, как раз пришёл к этому решению с гармоническим рядом. Задача оказалась гораздо после того, как переосмыслил. Посчитал матожидание ""лишних шагов"" на каждом этапе : когда уже вытянули 1 уникальный шар, 2 уникальных и т.д. и как раз получается гармонический ряд + 1",
"nyamhyy: смотря что, какое пространство состояний, лосс и т.п. 
...тебе так никто не скажет, проще запустить и посм. тк только ленивый не написал свой pg или q-learning",
"Я сейчас смотрю как раз в стороу rl, так как задача именно не в том чтобы предсказать количество ходов, а сделать на текущем состоянии поля тот ход, который с наибольшей вероятностью сделал бы среднестатистический игрок (по-настоящему должно быть несколько типов игроков: что то типо хардкорщик, средний донатер, новичок, …).",
имхо переход с nano-/meso-scale на macro-scale как раз та область где ML может активно применяться так как детерминистическое осреднение однозначно обречено на провал,
"народ, а где на common crawl можно посмотреть список спаршенных сайты? что-то никак не могу увидеть",
"ребят подскажите, а как с метрикой AUC можно узнать баланс классов для теста, для бинарной классификации, делая сабмиты на лидерборд?",
"Поддался таки на уговоры и поехал опять на линукс, третий раз за этот год. Последняя убунта отказалась грузиться на том же железе, где раньше работала, магические заклинания из гугла помогли. Даже шрифты норм и гигантские картинки больше не зависают. Всё-таки, несмотря на всю ""свободу"" опенсорса, лучше держаться строго линии партии, и от убунты ни шагу, даже чуть-чуть (в прошлые разы ебся с минтом, который ""та же убунта, только обои не скучные"", ан нет)",
"Привет! Помогите, пожалуйста. Как установить tflearn для анаконды?",
"как ни странно, но согласен  :100:! я под утром заглянул в сорцы и понял, что там ничего и нет. лучше прямо crfsuite-врапперы юзать. для склерна того же.  

просто, когда на глаза попалось, были другие дела -- и я не успел разобраться))",
А какая альтернатива линуксу была?,
Увы не могу вспомнить как решил это =(,
"еще можно попробовать google stree view попарсить, смотря какие у них ограничения.",
"господа, а кароч амазон пишет что у них к80, но в версии p2.xlarge доступно 12 гб, на сколько я помню в к80 просто две к40 каждая по 12, это они типа программно блокируют доступ ко второй половине и кто то юзает вторую на одной железяки? или чо?",
"народ, подскажите - кто как организовывает проекты по анализу данных? какая структура? копнул, и вышел только на cookiecutter, после явы, где все хорошо организовано и структурировано, этот инструмент показался ужасным,  кто чем создает в проме проект? и как организовывает?",
"Только собрал под этим вашим линуксом yolo, начал разбираться как этим пользоваться и переучить на другие классы, как оказалось, что единственная нормальная документация у порта на винду :but_why: (и что такой порт вообще существует, где ты раньше был!)",
и что процесс обучения идёт нормально? у меня линукс версия крашилась по непонятным причинам… где то в недрах Yolo,
"Спасибо. Как вижу, это тоже методы семплинга",
В догонку к предыдущим вопросам - какие таит опасности покупки инженерного образца Xeon ES ?,
"Возвращаюсь к вопросу выше: кто-нибудь пользуется какими-то утилитами для создания структуры? и вообще, какую структуру можно бы создать для удобства? а то что-то пытаюсь организовать работу с несколькими тетрадками, на команду, и что-то не понятно, как workflow сделать",
"вопрос про то, какая структура правильная, или как создать автоматом нужные файлы/папки?",
"скорее первый вопрос - как удобнее организовать можно такой проект, чтобы можно было разделить между людьми, и организовать workflow, выше кидал обзор из силиконовой долины, но там как-то кучу всего слили вместе.....автоматом я так понимаю cookieskulptor можно настроить",
<@U0P95857C> а вот у вас какая структура проекта чаще всего?,
"exotol: особо никакой - главное чтобы было README, дальше обычно command line scripts, ноутбуки я не очень люблю - они просто как инструмент посмотреть/поковырять, из них сразу переносить в питоновские файлы стараюсь",
"ребята, какие стоп слова (русские) используете, для обучение классификаторов?) в nltk по-моему слишком мало их",
"<@U15V81K6V> ну я к тому, что обычно стремлюсь разделять adhoc и stable штуки. пока речь о нечеткой задаче (какое-то исследование, например), то можно хоть гусей ебать, лишь бы человеку было удобно. а когда задача выкристаллизовалась и стала полностью прозрачной, тогда контекст и диктует структуру. попытка насадить структуру заранее для меня выглядит карго культом",
"я понимаю, что пока в этой сфере не особо компетентен, может вы слышали, в java например, есть maven/gradle, создают некоторую структуру проекта, которая позволяет дальше разрабатывать, и воркфлоу нормально идет,  в ds я смотрю похоже нет, и пока только попытки что-то похожие привнести, вот и хотел узнать, может кто личным опытом поделится, как он обычно проект организовывает, вот по примеру как здесь <https://habrahabr.ru/company/google/blog/325896/>",
"народ, хочу обучить word2vec на корпусе с отзывами о ресторанах, использую библиотеку gensim для python'a. Хотел спросить как правильно его обучать, что подавать на вход: а) слова нужно приводить к нормальной форме? б) корпус нужно подавать как список набора слов в одном отзыве: [[word_1_in_review1, word_2_in_review...], [word_1_in_review2, ..., word_n_in_review2]? или как список набора слов в каждом предложении отзыва [[word_1_in_sentence1_review1, ... , word_n_in_sentence1_review1], [word_1_in_sentence2_review1, ...,  word_n_in_sentence2_review1]]? в) нужно ли удалять стоп слова из корпуса? Заранее спасибо за ответ на любой подпункт!)",
"<@U34Q3KU8H> Я здесь, капитан

Упаковка в `venv` и `setuptools`, в принципе, идеологически правильна, но мы не стали так заморачиваться. 

Тут правы и <@U1G303UTW>, и <@U0P95857C> : надо отталкиваться от команды и того, как разбираемся с новыми тасками. Мы прошли некоторое количество неудачных попыток организации, среди которых:
1) пишем некоторую, казалось бы, кастомизируемую функцию, тестируем, складируем ее в скрипт, подцепляем скрипт в тетрадке. *Pros*: тетрадка очень маленькая в объеме, где все по делу, визуализация там и проч. *Cons*: Требования все равно меняются, хочется что-то модифицировать примерно всегда, и гонять код в тетрадь и обратно в скрипт - адово неудобно. Отбросили.
2) Пишем чуть лучше, по ООП (да, мы пробовали ООП в проекте). Аккуратнее код стал, багов стало меньше, но модифицируется он так же плохо. Отбросили.
3) Все переводим на пайплайны. Все трансформеры лежат в скриптах. *Pros*: Можно удобно включать/выключать любые шаги в обработке данных. Видно, что в данными происходит. *Cons*: Пайплайны вносят свои ограничения, когда дело доходит до отчетов и графиков, - генерация таких вещей на их рельсы сложно ложится. 

Поэтому мы определились, что пайплайн хорош как первый этап обработки данных, который все чистит, добавляет сторонние фичи, все такое. Дальше все уходят с этим пайплайном по тетрадочкам, где проводят свои эксперименты. Там уже можно дать себе волю и писать, как хочешь, лишь бы был толк. 
В конце объединяется лучшее, строим графики-отчеты, смотрим на результаты. Как-то так.",
"Еще очень важный, по моему мнению, совет: обязательно надо всеми силами уходить от итеративщины в работе с датафреймами и писать на `pandas`-функциях, пусть мозг и трещит иногда, размышляя, как это в dot notation выразить. Плохо, когда вы написали плохой код, но результат, например, закешировали: проблема не решена. Хорошо, когда код работает быстро, и никакие кеши не нужны. 
Это и место в репозитории сэкономит, и работа будет проходить без лишних задержек.
<@U15V81K6V>",
"<@U342T2PT2> это получается, вы пишите основной pipeline, какую-то элементарную очистку, а потом вся команда берет этот pipeline и каждый берет по модельке, добавляет/убирает уже фичи какие нужны, и на выходе вы получаете N возможно хороших решений...разве не вызывает сложности потом это интегрировать?",
"с какой целью это делать? если просто взять алгоритм, который пропуски понимает?",
"<@U0FEJNBGQ>, ну, обычно ведь пропуски расцениваются как отдельный класс",
"<@U065VP6F7>, вот интуитивно понимаю, что можно оверфитнуться, а можешь пояснить, каким образом?",
"Про оверфитнуться это эмпирические наблюдения с конкурсов на кеглах, ирл это зависит от данных. Таргет ты не учитываешь, но важно делать это внутри кросс-валидации, как стэкинг.",
"в конкурсах, где специально наделали пропусков - работает",
"привет, занимаемся machine translation, и есть потребность в параллельных корпусах, другими словами переводы с одного языка на другой, может быть кто то сталкивался и подскажет линк на такие корпуса, Opus/Taus/DGT уже были в работе, сейчас вот не хватает Иврит-Английский,Фарси-Английский, Корейский-Английский, спасибо !",
"Мы с другом интереса ради пытаемся под кальянчик предсказывать курс рубля(если точнее пойдёт ли он вверх или вниз) с помощью рекурентных нейронных сетей. Но диплёрнеры из нас не очень. Может кто посоветовать как правильно выбрать архитектуру сети? Сколько слоёв lstmа брать? Пока что лучший результат 0.55 accuracy на валидации, но это во многом благодаря случайной инициализации весов. Тестовую выборку пока не делали.",
"привет. вопрос к тем, кто шарит в керасе. там есть какой-нибудь функционал, который позволит разделить натренированную сиамскую сеть? хочется написать это одной-двумя простыми строчками",
"привет. Есть вопрос про кросс-энтропию. Вот у меня есть 100 классов. И задача мультилебл-классификации. Если я возьму и посчитаю бинарный лог-лосс для каждого класса (мол, есть ли собачка на картинке или нет), то будет проблема: ведь, представителей остальных классов аж в 99 раз больше. В итоге мой алгоритм проведет черезчур пессимистическую разделяющую поверхность, которая будет считать, что этого класса на картинке нет. 
Но если я буду решать задачу обычной классификации, когда нужно выбрать только 1 класс из 100, такой проблемы не наблюдается. Там я использую лог-лосс на 100 классов. И почему-то получаю что-то вменяемое. Хотя, кажется, что проблема никуда не ушла: чтобы провести каждую из 100 разделяющих линий, мне нужно противопоставить 1 класс против 99 - т.е. выборка все еще несбалансированна.",
"есть ли какой-нибудь относительно небольшой практический курс молодого бойца в классификацию русских текстов? в идеале ноутбук, где такая задача разобрана от начала до конца.",
"там есть какие-то заморочки с выводом понтового более чем 8битного цвета, вроде фотошоп умеет рисовать крутые цвета только на квадро и фаерпро, как я понимаю они могут быть для этого нужны",
А под какие framework'и есть имплементация Mask R-CNN ?,
"Коллеги, подскажите, пожалуйста. У меня есть выведенные эмбеддинги символов из неких последовательностей. В последовательностях на трейн и тест есть символ “Х”, который показывает что типа неизвестно какой там символ, или что символ пустой. Я думаю, что давать модели в случае этих иксов. Если символ неизвестен, то можно ли давать типа средний эмбеддинг по всем символам? Если символ пустой, то насколько адекватно давать модели нулевой вектор? Как вообще здоровые люди поступают в таких случаях? В качестве моделей я буду пробовать что нибудь с CNN.",
ну так пока asus рулит так как его в пакете с процом за 430 отдают в одни руки,
"<@U49A8JCKW> ну да, похоже asus тут слегка обосрался, причем и с более дорогой матерью не все гладко. но если это проблема биос то в принципе она решаема. А как впечатление от MSI - там народ некоторый ругался что не видит 3000 память",
"я думаю можно попробовать учить эмбеддинг только для Х end2end, но не знаю как это сходу приятно в коде написать ",
"Насчет 3000 памяти, то это больше проблема amd. они максимум DDR4 2400 MHz поддерживают, выше будет работать как 2400 
DDR4 2400 MHz and higher memory module will only run at maximum of DDR4 2400Mhz due to AMD® chipset limitation when using 7 th Gen A series CPU
<https://www.msi.com/Motherboard/support/X370-GAMING-PRO-CARBON.html#support-mem>",
<@U49A8JCKW> спасибо!!! а если не секрет какую память ты взял и охлаждение,
"Не-не, мне вообще дообучать  Х нельзя. Есть последовательность, и в ней просто тупо неизвестен символ на какой-то позиции, а я хочу аппроксимировать его как-то. В случае word2vec на словах мы можем сделать RNNку которая будет предсказывать embeddings для неизвестных слов, а тут же только один символ, я поэтому и подумал про среднее как аппроксимацию.",
"Ну или неизвестен, либо он пустой (я про это точно знаю, какой он, т.е. у нас на самом деле два таких спец-символа).",
"Или для вычматов, где ошибка имеет свойство накапливаться",
"<@U0M7UMFDF> так тебе надо представить просто все символы в виде эмбеддингов, чтоб подать в нейронку, и ты не знаешь где взять эмбеддинг для символа, которого изначально в датасете не было, так?",
"Практически да, просто эти два неизвестных символа имеют определенный смысл - либо это типа пустое место, либо неизвестно, какой именно из известных символов там есть.",
<@U43FTJQ2V> мне кажется в README будет круто добавить про то как pretrained веса помогают быстрее сходится,
"Вопрос по tf.

У меня есть список bounding boxes и список соответствующих вероятностей.

У каждой картинки разное число bounding boxes.

То есть на входе что-то вот такое:
```
new_probs	shifted_boxes
0	[0.97138309, 0.89845979, 0.9296, 0.8547526, 0....	[[48, 48, 96, 80], [448, 0, 480, 32], [432, 10...
1	[0.99968529, 0.9976089, 0.99978441, 0.89575303...	[[96, 320, 144, 400], [96, 314, 144, 394], [11...
2	[0.99892837, 0.85288972]	[[1490, 1804, 1538, 1900], [1484, 1804, 1532, ...
3	[0.88441133, 0.99754095, 0.99913651, 0.9104014...	[[400, 32, 448, 112], [176, 1016, 224, 1096], ...
4	[0.96327102, 0.98539644, 0.88011599, 0.8040579]	[[224, 1400, 272, 1512], [224, 1394, 272, 1506...
5	[0.81249487]	[[692, 516, 756, 628]]
6	[0.93350941, 0.86235744, 0.89811134, 0.8266245...	[[48, 368, 80, 416], [64, 368, 80, 400], [176,...
7	[0.95371836, 0.938133, 0.99871385, 0.89310324,...	[[16, 1192, 112, 1240], [160, 522, 240, 570], ...
8	[0.9980374, 0.98452294, 0.99228573, 0.99924421...	[[1224, 1128, 1272, 1240], [1240, 1256, 1288, ...
9	[0.99193442, 0.87001282, 0.89822716, 0.9991089...	[[304, 32, 352, 112], [272, 1192, 320, 1272], ...
10	[0.92838627, 0.99157351, 0.90044743, 0.9988871...	[[1416, 32, 1464, 128], [1288, 1208, 1336, 130...
11	[0.91346675, 0.95700151, 0.99657744, 0.9861850...	[[416, 1602, 448, 1682], [416, 1596, 448, 1676...
12	[0.99615979, 0.98410827]	[[1288, 224, 1400, 288], [1298, 224, 1394, 288]]
13	[0.82114112, 0.93216109, 0.92051238, 0.8621713...	[[0, 1314, 32, 1346], [1320, 320, 1352, 368], ...
14	[0.80408633]	[[682, 16, 730, 96]]
```

Как это пропустить через `tf.image.non_max_suppression` ?

проблема в том, что число bounding boxes в каждом ряду разное, и вследствие этого я не пойму как определить Variables, которые идут на вход `non_max_supression`",
"Хочется картинку, про val loss как функцию от числа итераций. С одной кривой за dice, с другой за softmax, ну и третий за сумму :slightly_smiling_face:

Там сразу будет видно кто быстрее сходится и у кого итоговый результат выше. (Я сам думаю такую набить, но все руки не доходят.)",
"Бог с ним, а разрезе tech report про спутники, я сам прогоню на данных с нормальной валидацией. Уж больно интересно как добавление dice меняет поведение на задачах сегментации. У нас сетки похожие, сразу и посмотрим.",
"вопрос про multinomial logistic regression: 
в scikit-learn есть 2 опции: LogisticRegression(multi_class=‘multinomial/ovr’)
c ovr все понятно: оно считает бинарную логистическую регрессию для кажого класса, объединяя остальные классы в один класс. Но вот как действует multinomial ? 
Я почитал несколько статей, вроде даже вкурил идею про “пивотный” класс. И вроде как мы выбираем пивот, строим K-1 разделяющую плоскость с ним. Замечательно. Но что потом с этими плоскостями делать? Мы бы могли их смешивать, получая кусочно-линейную границу, как в подходе с One-vs-All. Но здесь они ведь будут все разные, в зависимости от выбора пивота, разве нет?  
И еще есть контр-интуитивная картинка:
<http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html>
Вот там есть красная штрихованная линия, которая не отделяет вообще никакой класс. Как ее понимать?",
"хмм, теперь осталось понять, как найти scores от этих профильтрованных bboxes…",
"bboxes_selection это не отфильтрованные ббоксы, и их индексы, так как tf.image.non_max_suppression возвращает не сами ббоксы, а именно индексы, gather потом выбирает по индексам ббоксы в ячейке 8",
"Всем привет! Подскажите, пожалуйста, как лучше и правильнее делать, применять K-Means к отмасштабированным данным с помощью StandardScaler или к неотмасштабированным или к нормализованным относительно среднего в случае, если имеется порядка 50 признаков одной размерности и 1.5*10^6 наблюдений. Большинство признаков содержит много 0 и значений меньше 10, а также есть признак где встречается меньше нулей и сами значения больше в несколько раз. Спасибо!",
ух ты как в тему мне. Спасибо,
"yellowduck: Я могу наврать про то, что именно делает sklearn, но в общем виде multinomial logistic regression это считай однослойная нейросеть с K выходами, софтмаксом и лог-лоссом, тогда как ovr это набор однослойных нейросетей, каждая с 1 выходом и логлоссом.

Количество параметров одинаковое, но лосс-функция слегка разная, а результат в первом случае должен получаться более правильно калиброванным.",
"<@U04ELQZAU> если карта может кушать данные быстрее, чем линейно читает хдд -- хоть как готовь, не успеешь. А хдд читает всего 100-200мбайт/сек (зависит от модели/места на блине)",
"настоящую асинхронную подготовку батчей сложно сделать, не обойдешься тыканьем async/await, как с сетевой асинхронностью. кажется, подход <@U0ZJV6E5Q> должен быть оптимальным трейдоффом между простотой и эффективностью",
"вот да, я пришел к ровно такому же выводу. Спасибо что подтвердили мои догадки. Но все же хочется найти какой-то физический смысл в этих линиях. Например, как рассчитывается вероятность принадлежности точки к красному классу? Кажется, что для рисунка это e^z_красная / [e^z_синяя + e^z_желтая + e^z_красная]  , где z_красная - расстояние от точки до красной штрихованной линии. Но тогда рисунок будет не таким. Чем ближе точки к красной линии - тем больше вероятность красного класса. А тут вообще не так (",
"Пятница, время выплат. Как вы все знаете, курс криптовалют стремится в небо и благодаря этому зарплата компьютера растёт не по дням, а по часам, и спустя неделю 300$ превратились уже в 390$ :troll:",
"Возможности вывода есть, разнообразные, правда я пока не заморачивался, лежат себе как заначка на совсем уж голодный год",
Мы опубликовали материалы встречи rMoscow 27 апреля. Кто не смог прийти — смотрите: <https://events.yandex.ru/events/ds/27-apr-2017/>!,
"по каким кейвоордам имеет смысл прогуглить что-то по анализу хостов, посещенных пользователями? (есть юзер и все хосты, на которые он отправлял запросы, надо сделать какие-то плотные фичи для каждого юзера)",
"Добрый день. Есть те у кого был опыт использования h2o с питоном? Пытался запустить локально чтобы проверить качество + производительность, но он даже просто scikit-learn уступает по обоим параметрам. Датасет 811080 объектов с 1068 вещественными признаками. Модель random forest classifier 1000 деревьев, макс глубина 8. Параметры сервера: 4 cpu (2.40GHz, 14 cores), 256 gb ram. h2o кластер запускаю через init из ноутбука, ядра он все загружает, но все равно обучение модели гораздо медленнее (раза в 4) чем у scikit learn работает. Есть идеи как с этим бороться?",
leonid.danilchenko: с какой целью у человека может возникнуть желание читать такое? ,
"По мере того, как различные составляющие этой методологии начинают работать на серьезных данных, все больше и больше людей начинают их использовать",
"Я тоже это имел ввиду, когда отвечал. Я это делал через languagetool",
"Подскажите, есть ли где-то статья-обзор по методам анализа текстов? Нужна инфа по всевозможным задачам с анализом текста, в идеале с примерами датасетов. Или подскажите, где можно найти максимально общую инфу на этот счёт.",
"Кто мне расскажет, как можно аугментировать данные для локализации / детекции.
Сходу в голову приходят:

[1] D4 group
[2] translation
[3] zoom in / out
[4] random channel shift

Как обстоит ситуация с поворотами? На небольшие углы, наверно, можно bounding box для повернутого изображения брать как максимальное вложение от повернутого исходного прямоугольника. А если хочется на 45 градусов? По новой вручную обводят?",
"Ну, ладно, половина фрейморков раз в полгода ломают апи или меняют/добавляют/убирают всякие промежуточные уровни, как пример -- в релизе 1.1 тензорфлоу всё поменялось, керас 2 -- много поменялось, теано переходит на либгпуаррей, торч съезжает с луа на питон, про менее популярные вообще молчу",
"Вот тут много интересной аугментации - в 2.2. и в 3.6. <https://arxiv.org/pdf/1512.02325.pdf> - растяжения-сжатия, дорисовка полей вокруг картинки средним цветом (чтобы объекты относительно полного изображения более мелкими были) и тп
Ещё вот тут <https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf> и тут <https://arxiv.org/abs/1312.5402> разные цветовые искажения, яркость-контрастность, блюр и прочее
<https://github.com/kevinlin311tw/caffe-augmentation>
<https://github.com/ShaharKatz/Caffe-Data-Augmentation>

Повороты на небольшие углы - обычно хватает просто брать новый bounding box, так чтобы ""повёрнутый"" в него полностью входил, можно подзапариться и оставлять в разметке повёрнутые боксы, помимо координат предсказывая угол поворота (что-то такое <https://arxiv.org/abs/1612.02742> (upd: ссылку обновил на правильную)). Можно также (если это делается на более поздних этапах обучения, когда сеть уже что-то детектирует) просто повёрнутые изображения пропускать через детектор и брать наилучшие предсказанные окна (в плане уверенности и в плане пересечения со старой разметкой) в качестве новой разметки.",
"Я к Packt Publishing слегка скептически отношусь, т.к. их редакторы регулярно спамят предложениями типа ""я смотрю вы в гитхаб выложили пару строчек на питоне, а мы как раз хотим издать книгу про использование Питона в гитхабе. Не хотите ли стать автором?"".",
Имя автора как из Италии или Испании -- что не сильно лучше :kekeke:,
"в смысле спасибо, я нихрена не понял, но когда обломаюсь с lstm - попробую погуглить",
а какой ты векторайзер юзаешь?,
"и ""уверенность"" подросла. Или как правильно называется оценка моделью точности предсказания своего? 
Категорий больше сотни, вот её и размазывает.",
"<@U1CF22N7J> Дай расклад за mining. У меня часто то одна видяха простаивает, то обе, а комп я один фиг не выключаю никогда. Что там надо запустить, но только, чтобы как у белых людей, под линуксом, чтобы мне потери энергии чуть компенсировать?",
Как перевести на английский комитет нейронных сетей?,
А как понять что мне выгоднее?,
"Там же форма есть, цены на люстричество свои вбивай и видеокарты какие есть, он из выгоды от продажи вычтет цену за свет",
Титан паскаль как 2-3 1070 будет,
"7. Через некоторое время фрустрируешь, но потом понимаешь, почему переводы в криптовалютах по полчаса идут",
"При нынешних ценах, пол года майнинга - это как веник для бани купить. Так что это так, для успокоения совести.",
"Про потери энергии — комп в простое ничего не жрёт (десятки ватт), при майнинге будет жрать как надо :)",
"Привет, а где можно найти, что значат теги в аннотациях PASCAL VOC? Интересует sub-tag &lt;depth&gt; в &lt;size&gt;, &lt;segmented&gt;, &lt;truncated&gt;, &lt;difficult&gt;.
Хочу сгенерировать аннотации для своих данных, но непонятно что записывать в эти теги",
"<@U411PKASW> на сайте PASCAL есть PDF, с каждым датасетом идёт вроде как",
"Вот сейчас покупаю , как спецификация для дл/мл? Добавил 16 памяти , летом ещё 1080ti добуду",
Купил как есть + 16 оперативы. Могу отписать позже тесты /впечатления ,
как возраст контрпримера влияет на его доказательную силу? на теорему Пифагора ты тоже просишь более новые выводы?,
"Современного потребительского железа, которое сверкает сгорает от перегрева — нет. Или точнее, я такого вообще не встречал. Пример с атлоном не противоречит, так как это не современное железо. ",
"Но низкоуровнево, как всегда в линуксе",
"Сиди сам выбирай алгоритмы, ищи хорошие пулы, изучай какая там строка для запуска, пиши автомат, который будет текущие цены мониторить и переключать на самое выгодное",
"когда через 5 минут я сообразил, что что-то не так и вернулся к месту его падения - не нашел вообще ничего, никаких следов",
"священники помогают только тем, кто в них верит",
"<@U0FF52P7D>, а как в питоне такой кеширующий меммпа сделать? 
Вообще по хардкору - это просто загрузить весь датасет в оперативку да и всё :slightly_smiling_face:",
"а ты на каком языке писал изначально книгу - русском, немецком, английском?",
Может кто может подсказать есть ли какие то особые методы кластеризации разряженных матриц . Или подсказать в каком направлении копать. Спасибо,
ololo: где ты столько времени нашёл :thinking_face: Как долго писал её?,
"народ, всем добрый вечер. Кто знает, есть ли в сети датасеты фотографий с лейблом ""в полный рост"" ?",
"Подскажите, а как правильно строить граф в tensorflow, если есть разные части для train'а и test'а (но различия очень маленькие)? 

В документации есть такой код:
```if mode == ""train"":
  helper = tf.contrib.seq2seq.TrainingHelper(
    input=input_vectors,
    sequence_length=input_lengths)
elif mode == ""infer"":
  helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
      embedding=embedding,
      start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
      end_token=END_SYMBOL)```

Как этим пользоваться?
что такое здесь mode? Это переменная, которая передается в функцию построения графа? Или надо написать tf.cond для mode (или что-то в этом роде)?",
Как правильнее настраивать гиперпараметры нейронок и какие из них важнее?,
"Подскажите, почему у меня высасывает всю оперативку когда создаю батчи через np.load",
"ну я когда убрал из списка стоп-слов такие слова как слишком, очень, назад, снова, нет, ни, тогда стало немного получше, я вот думаю, может есть какие-то практики уже для этого, что еще убрать, а что добавить",
"<@U3ULV5ATU> Чуть выше кидали <https://pypi.python.org/pypi/many-stop-words> . Либо, как вариант, сортировать словарь по DF и смотреть вручную.",
"&gt; А если хочется на 45 градусов?

А почему каждый угол известного bounding box нельзя просто повернуть на тот же угол?",
"нет, сюда я пришел после того как прочел доку Кераса. Просто по писанию оттуда я не совсем представил как оно работает. Я понимаю что такое обычная свертка, но впервые слышу про spatial convolution и pointwise convolution.",
Видели бы вы какие дебилы способны например кондиционеры монтировать (А там герметичность нужна полная)...,
"Коллеги, хочу спросить совета касательно анализа временных рядов. У меня есть лог коммуникации агента в сети. Делая срез за три месяца и строя гистограмму,  я получаю что-то в виде распределения на прикрепленной картинке. Мне нужно узнать, насколько давно сложился такой частотный профиль у агента. Другими словами, я думаю взять те же три месяца с отступом в один день и сравнить распределения, потом в два, и так до тех пор, пока не разница преодолееет заранее заданный трешхолд. Хотел узнать название такого метода и где можно посмотреть его применения (а если есть реализация в R/python, то вообще здорово!)   Заранее благодарен!",
никто внутренностями Probabilistic programming компиляторов не интересуется? Какие в них возникают сложности по сравнению с классически ЯП и как их решают?,
"Спасибо, так и сделаю. Думал, может какой более хитрый способ есть.",
"Никому не попадались попытки научить сетки решать задачки вроде этой? Я думаю, что кто-то пытался, скорее интересно, есть ли какие успехи в этом напрвлении?

<https://opendatascience.slack.com/files/ternaus/F3J3V3X0T/matrix15n.gif>",
"на AWS многие мутят, но строить на них критичную для бизнеса инфраструктуру для ситуации, когда час простоя приводит к сильным убыткам - опасная идея. Вон недавно coursera лежала из-за отказа амазона. И регулярно у них что-то крепенько так ложится, примерно на сутки.",
"Хочу немного побрэйнштормить немного приложения для DRL алгоритмов и их подобных.
Небольшое коммюнике:
- АльфаГО победила человеков -&gt; но куда двигаться именно с этим не совсем понятно...
- Также тут ребята недавно хвастались неплохими рез-ми в лимитном холдеме -&gt;
это поинтереснее, но все равно особо никуда ""не пришьешь"" кроме как ботов с вытекающими...
- Рукохватательные экспериметы (экзо) -&gt; продвинулись, можно как протезы (здесь мб решение есть и лучше) или военное (careful)
- Трейдинг (лонг) - кто-то уже что-то делает, есть стартапы -&gt; уже более осязаемое
- Фрауд и тп -&gt; банки в основном, что тоже не оч. интересно
- Генерация последовательность реалистичных анимаций / сложные взаимодействия (2+)
- Гейм ИИ (можно даже в будущем кибер-атлетов тренировать наверно)
...пока устал вспоминать &gt;&gt; предлагаю перейти в тред для дальнейших идей :wink:
- вспомнил еще -&gt; гугл электричество
- в эту же стороны -&gt; сложные системы оптимизации различных (в том числе и производственных) процессов",
"кажется, всё что угодно где есть последовательное принятие решений. другой вопрос конечно что рл в большинстве более-менее сложных задач работает мягко говоря не очень хорошо)",
"1) куча приложений есть там, где есть недиффиринцируемые функции (подкласс ntm, аттеншн и тд)
2) дипмайнд искал архитектуру оптимальную по сифар через рл, но это уже больше похоже на извращение)
3) беспилотные тачки, но скорее в контексте сим2риал
4) помимо рукохватательных экспериментов есть другие роботы, например в 2000-ых Ын с коллегами демонстрировали вертолет делающий трюки
5) видел как-то что-то вроде оптимизации трафика (переключения светофоров) через рл",
"После вчерашнего похождения с <@U3R32B38W> на танцы в церковь я вспомнил, что давно хотел улучшить звуковую систему дома.

Вот сейчас у меня настольные колонки <https://www.amazon.com/gp/product/B0042F3K9W/ref=oh_aui_detailpage_o05_s01?ie=UTF8&amp;psc=1> которые подключаются к строенной звуковухе.

Хочется:

[1] Чтобы когда я смотрю кино на своем 50"" телевизоре, все не просто красиво мелькало, но и всякие рычания тигров и прочие звуковые эффекты будоражили кровь.
[2] Музыка, которая играется на фоне, доставляла удовольствие звучанием и вообще была похожа на то, как оно слышится в SF Symphony.

Это вроде как две разные задачи. Так что, видимо, решать надо отдельно.

Не смотря на то, что я скорее аудиофил, то есть мелкие недочеты в качестве звука меня скорее бесят, чем просто раздражают, вопрос звуковой системы встает впервые - раньше денег не было.

Какие есть варианты? Что читать? Что покупать?",
"И скорее это не он нас обошел, а мы его пытались догнать всю дорогу. В отличие от остальных участников, которые каждый день меняли места на LB, он как в начале ушел на первое место, так оттуда и не слезал.",
Кто именно ты имеешь в виду под режимами изображения?,
"хм как бы пояснить - ну вот есть в мониторе настройки которые меняют яркость, контраст и так далее",
"Может в мониторе какой-нибудь режим динамической контрастности стоит и меняет, когда резко открывается большое бело поле :slightly_smiling_face:",
"мне кажется подобные вопросы здесь уже мелькали - как установить нормально xgboost? прошелся по прикрепленным файлам и что-то туториала не видно ((( может кто знает в какую ветку глянуть? Проблема вот в чем - юзаю докер для установки, но анаконда идеально ложится на debian 8.5, а xgboost более менее на ubuntu 14.4, и никак не могу заставить это все заработать на одной машине, на дебиане xgboost либ не находит, а на убунте anaconda отказывается работать",
а как RL в антифроде применяется?,
"Добрый день коллеги.

Нужен Ваш совет.
в Базе задача бинарной классификации со смещенной ""важностью"" одного из классов.
Сейчас собираю ансамбль моделей (XGBoost, LightGBM). 
Посоветуйте:
1. Какую модель лучше использовать на выходе? (почему спрашиваю, тот-же LightGBM на выходе дает результаты хуже, чем простое усреднение результатом всех моделей :slightly_smiling_face: )
2. Как лучше делать отсечение от уровня ""уверенности"" модели? только на выходе или перед метомоделью тоже?

Заранее Спасибо!",
"концептуально то принцип вроде неплохо ложится, но как среду/дату берут/генерять/что подсовывают под обучения не сосем понятно",
"так и из докер хаба, xgboost, тоже нормально ложиться и работает (убунта), в анаконде вроде как следят чтобы пакеты не конфликтовали друг с другом - типа вы ставите N либ разных версий, и эти либы не стабильны и могут конфликтовать, а если вы ставите анаконду, то там подбираются либы определенных версий, типа будет меньше конфликтов и больше стабильность в работе",
"Доброго времени суток! Подскажите, пожалуйста, по поводу образования для data science. Стал задаваться вопросом как образовываться в ds, если есть первое (экономическое). По работе периодически использую эконометрические методы, сижу на курсах на степике, но хотелось бы более фундаментальных знаний, чтобы со временем скорректировать профессию. Стоит ли идти на вечернее на прикладную математику, например? Возможно, есть иные пути? Описания опыта и мнения приветствуются) Заранее спасибо!",
"Коллеги, подскажите пожалуйста, хочу десктоп собрать, цель соревнования и для общего образования в DL/ML. Бюджет 2k +- 200$. На данный момент предпочтение примерно такому варианту <https://pcpartpicker.com/list/RLkmjc> Отталкиваюсь от 1 gtx 1080ti. В перспективе, может в течении года еще получится одну такую же прикупть. Что лучше в плане процессара i7-7700k vs i7-6800k? Какую matherboard, RAM и блог питания? Есть ли смысл в Liquid CPU Cooler или стандартного кулера достаточно?",
Есть вероятность что наш OpenDataScience спутают с каким то другим OpenDataScience который захватил хештег и твиттер акканут,
"ещё можно поискать в этом треде по ссылкам как раз на pcpartspicker — тут уже с десяток было подборок, с обсуждениями каждой",
"Алексей, спасибо большое за советы! А почему такая разница в картах?",
"Везёт же Вам) 60 баксов переплаты там за дело: они добавили две фазы питания, сделали хорошее охлаждение. Плюс, можно старый монитор подключить без переходников. В среднем эта карта на 15 градусов холоднее референса, но и сильно толще. Три таких в данную материнку не воткнуть (да и не нужно, проц их не выдержит по PCI линиям, а две влезут, но придется убрать в любом случае карту для usb 3) Я сам на неё раскошелился, так как регард их завёз по очень хорошей цене(хз как, спасибо ему, что не платит налоги). Работает почти круглые сутки на 100%, в заводском разгоне в 2000. Температура выше 65 градусов не поднималась",
"Этот блок питания, что в сборке за свои деньги хороший. Только нестандартно длинный, нужно смотреть как внутри корпуса поместится, и если менять кулер проца на менее шумный, то блок этот питания будет слышно (судя по обзору <https://www.overclockers.ru/lab/72793_5/testiruem-tri-bloka-pitaniya-vysokoj-moschnosti-fractal-design-newton-r3-1000w-cougar-gx-1050w-v-3-i-evga-supernova-1000-g2.html> )",
"Да в целом нет, модель хорошая, у меня такой же ещё из 2012 года. PS. Он довольно большой войдёт не в любой корпус, нужно смотреть.
Но в целом сейчас хотелось бы что-то на 20% тише
На амазоне со скидкой есть <https://www.amazon.com/MasterAir-Cooler-Continuous-Contact-Technology/dp/B01M5K6DKW>
Новая версия, отзывы нормальные и он как раз немного тише. Но тогда нужно смотреть другой блок питания, иначе нет смысла переплачивать 7$, блок питания будет слышно",
"А почему хочется взять квадратный мид? По отзывам не очень модель, но я в руках не держал. Пластик, шумоизоляции 0, сеток для пыли мало. Я бы не экспериментировал и взял full tower из нормального сплава. Совета конкретного дать не могу, можешь поискать на ютюбе. Например, канал thesellhard или просто в поиске",
"А посоветуйте, кто сталкивался - обзоры, пейперы, etc. на тему какой сейчас пилят DS/ML в оптимизации AdWords и прочего PPC. Спасибо!",
"Приветствую, кто может подсказать метод для удаления выбросов в данных, в pandas? Сейчас, считаю среднее отклонения и удаляю за пятью средних отклонений. После заново пересчитываю среднее отклонение и выбрасываю все, что не входит в 2 сигма.",
"Подскажите, пожалуйста, где можно почитать про подходы реализации чатботов в области консультаций/продаж (напр. онлайн магазины)?",
"Подскажите, пожалуйста, где можно почитать про подходы реализации чатботов в области консультаций/продаж (напр. онлайн магазины)?",
"В процессе участия в `Space App Challenge` возникло куча вопросов по `NASA API`, особенно после хакатона, официальный чат быстро стух и теперь не ясно, где вообще задавать вопросы. 

Простой пример вопроса: в доках по мероприятию указывалось, что следует работать с GIBS снимками через офф API <https://wiki.earthdata.nasa.gov/display/GIBS/GIBS+API+for+Developers>, однако уже после, потеряв неделю, я случайно выяснил, что есть намного более удобное API, нигде не описанное в wiki: <https://gibs.earthdata.nasa.gov/image-download>. Причем в NASA github или stackoverflow мне так и не помогли :disappointed:

Где лучше правильно задавать вопросы по NASA API?",
"На счет квантиля почитал, да как раз думал о чем-то подобном, но не знал как это называется спасибо.  Правда пока не понял как получить нижний квантиль .",
"сейчас обкатываю две куды в разных папках, stay tunder.",
"Там же можно переменную окружения указать, где куднн лежит",
дыа. но тфло глючило. если поставить куду в две разные папки и сделать там разные куднн - всё работает.,
В какие фрэймворки поддержку cudnn6 впилили?,
"<@U0P95857C>  ты как будто предыдущие посты не читал, для дебиана xgboost упорно не хотел ставится, всегда были косяки, на убунте, норм, мне нужно было поставить 2 вещи на одну систему - анаконду и xgboost",
"но на убунте анаконда не вставала как следует, а на дебиане xgboost",
"Это только в Linux так, в винде ищи можно с одной кудой два cudnn, но придется в ручную переключать переменные окружения",
А где все дата сайнтисты? ,
"У меня пара вопросов ко всем, кто шипит в прод ML на Python, а бэкенд на Java/Scala/Go/C++ и вам нужно результат из модели отдавать в бэкенд в риалтайте. 
Как вы это делаете: Kafka, rabbitmq, protobuf? Какой промышленный стандарт, как в тех же Яндекс/Мэйл/Сбер? На чём ещё вы пишите ML-модели, кроме Python, какие фреймворки тогда используете?
<@U0DA4J82H> , <@U1G303UTW> , <@U3Z5KPTRQ> ваше мнение особо сильно хочется услышать)
Спасибо.",
"Сори за оффтоп
<@U1G303UTW> как NATS по сравнению с другими MQ? И у вас NATS Streaming юзается?",
"float'ов вполне хватает, это стандарт при работе с нейросетями. Более того, как правило, оптимизационные задачи, вознимающие в машинном обучении и не нужно считать с такой высокой точностью, есть риск переобучиться под обучающую выборку",
"я просто не знаю работают ли они с NVidia или нет, у меня GTX Titan, задача довольно глобальная, а я знаю С++, планируется все данные размещать целиком на графическом процессоре (ну или большую часть), т.о. минимизировать потери при обращении к памяти.  это существенно повышает скорость вычислений, особенно когда решение  исключительно под задачу с минимальным уровнем абстракций. а как работают те библиотеки понятия не имею. За список спасибо, погуглю)",
"Вроде бы это классическая формулировка soft margin SVM, но есть p как дополнительный параметр для оптимизации",
"Почему он есть в loss, а в обычном SVM его нет?",
"<@U1FLG6YR1> я не могу обоснованно рубиться за плюсы/минусы натса, т.к. выбор делали архитекторы, а не ДС. но вроде проблем с ним пока не было; хотя надо немного думать, когда работаешь с ним, т.к. гарантий нет.",
"<@U04ELQZAU> наверно и так (но на освоение фреймворка тоже уйдет некоторое время), а какой тогда посоветуете для backpropagation? (для питона тоже годится)",
"Почему это надо сделать здесь, но не в стандартной формулировке?",
Но почему это именно так выглядит - я пока не понял,
"<@U0AD1L5NC> как-то так. Тут взрывает немного, что мы хотим всех в одну сторону оттеснить. Есть такой же подход, но где вокруг 0 строится шарик, задача выглядит понятнее",
"нет все во feature space, поэтому и работает) что там прямая не пряма и получается не так наивно как на рисунке)",
"Bishop, p.334: как я понял, эти формулировки неэквивалентны в общем случае. Разница в том, что в случае C-SVM мы ограничиваем сумму a_n &lt; CN (т.е. сверху), а в случае nu-SVM -- сумму a_n &gt; nu (т.е. снизу).",
"Вопрос про прод: где вы храните модель? :thread-please: 
Вы обучили модель и хотите зафигачить ее в вебсервис (сереализуете скорее всего)
0) вы не заливаете ее? (Поясните комментом почему)
1) вы заворачиваете ее в пакет?
2) вы просто переносите ее на прод сервер?
3) вы заливаете ее в бд (напишите какую)?
4) вы заливаете ее в WebDAV?
5) вы используете git lfs ?
6) s3
?) ваш ответ в треде (добавлю для голосования)",
"Насчет где хранить - интересно. Я гитом каммитил, пулил с сервера, потом удалял из гита :genious: Но у меня модель меньше 100 Мб весила. Думаю, да, либо в облаке, S3 как писали выше, но это вроде будет по HTTP проходить, не? :thinking_face:, тогда лучше сразу через SSH с твоей локалкии копировать (scp, sftp, rsync) - думаю, так будет быстрее и как-бы всегда есть связь с твоим компом.

Ответ <@U2Z07JW4T> лучше звучит - более инженерный и правильный по принципу",
а какие максимальные объемы у вас самой модели?,
"<https://opendatascience.slack.com/archives/C047H3N8L/p1494392839446736>
стоит посмотреть на загрузку GPU в процессе обучения если она равномерная и высокая вряд ли поможет, если нет то можно оптимизировать… на практике с ImageDataGenerator бывает по разному, + где то валялся код для утилизации всех ядер CPU на лету",
Я почему про один образ говорю. Потому что не нужно синкать код и модель. Ну и код относительно модели мало занимает. Поэтому удобнее в нашем случае целиком это делать. А какую модель брать из обученных у нас как раз в конфигурации сборки определяется. ,
"В случае одной никак. Да и в случае двух деградирует не сильно для задач не похожих на рендеринг. Как оно в ml я не знаю, <@U1CF22N7J> говорит, что всё хорошо и на x8 вместо x16 на карту. Но зависит, конечно от специфичности задачи(архитектуры нейросетки) и остального железа.",
"как-то кровавенько. А как всякие там А-В тесты, опыты на людях, вот это вот всё?",
"<@U1FLG6YR1> Изменение чего угодно - сборка, тесты, тесты и релиз. Кроме того, обучение на CI гарантирует повторяемость. Мало ли кто чего накрутил. Модель только с CI",
"Я думаю, тут usecase как раз для анализа моделей. На одном проекте, юзер мог выбирать кликая на фичи модели для тренировки, потом визуально это отображалось A/B testing. Модели хранили в MongoDB под одной коллекции.  Мы сначала по дефолту делаем аналитику со всеми фичами и показываем вес фичей после Feature Selection. Тогда у юзера есть представление о данных. Если такой usecase, то я бы с интересом узнал другие ответы, т.к. наши модели небольшие, но все равно хранить физический саму модель в БД - как то не айс по мне.",
"Так вот:slow:. Идея с \rho примерно такая. В стандартном SVM используется hinge loss l(x) = max(0, 1 - x). Единица говорит по сути с какого расстояния до разделяющей плоскости мы перестаем быть ""уверенными"" в предсказаниях. Хочется, чтобы эта величина была как можно больше (у нас высокие стандарты уверенности) поэтому ее выносят в часть с минимизацией. В данной постановек есть еще ряд ништяков, если выписать лагранжиан и внимательно в него повглядываться, то станет заметно, что параметр \nu является нижней границей для кол-ва опорных векторов и верхней границей для точек из обучающей выборки, которые будут отмечены, как аномалии",
"Котаны, решаю задачку детекции нескольких объектов на одной картинке, где ground true боксы пересекаются друг с другом (например, отдельные люди в толпе людей) . Детектор тоже дает россыпь баундинг боксов, которые тоже пересекаются между собой.

Какие метрики для такой задачи лучше/можно применять?
Если IoU, как ее правильно считать (все со всеми/в окрестности/с наибольшим пересечением)?",
неделю назад имел эротические отношения со всяким железом в итоге: корпус лучше брать большой чтоб все влезло аккуратно. В старый не смог упихать нормально 1080ti - упиралась везде где могла. Плюс не завелась с бп на 750 - пришлось взять чуть помощьнее. Короче - не экономь на корпусе и количестве вентиляторов внутри,
"Очень странно, что не завелась с бп 750. А про корпус :i_know_that_feel: Кстати, какой максимум оперативы на разенах? 64 гига? Как же бесит то, что они на оф.сайте все интересные характеристики скрыли непонятно куда.",
"согласен, но когда стоит бутерброд из 3х таких, получится ли так же эффективно охлаждать?",
"не, их по плану должно быть много, это как раз норма :slightly_smiling_face:",
"Я думаю 80 это вполне приемлемо. Тем более для референса с гарантией. Через три года всё-равно менять скорее всего. 
Но я просто сам очень удивился своим температурам. 
Я когда старый комп покупал этим озаботился и у меня корпус серьезно так продувается, там вентиляторы везде, где можно. Я ещё так удачно воткнул, что один фронтальный чисто дует на сам радиатор видюхи, а второй даёт воздух кулерам на GPU. Так она ещё так плотно вошла",
а почему не ставишь водяное охлаждение,
так были когда то статьи сколько стоит обучить разные Deep Learning модели на amazon какие то парни всё в деньгах подсчитали,
"Играл сегодня с новыми видеокартами и обнаружил, что грёбаная нвидия определяет монитор по HDMI как телевизор и подаёт на него сигнал с обрезанными тенями и светами (16-235). Сравнил картинку -- ппц, я страдал всё это время, сам не зная того",
"Думаю взять ASUS X99-E WS, она выглядит как приспособленная для таких изъёбов :troll:",
"А какие видюшки то? Я видел скоро хотят выпустить мамку для фермы как раз на райзене, но там жестоко должно быть по софту(драйверам), чтобы там всякие pci-e линии для m2 перепилить под GPU",
"А ещё подключал сегодня две видюхи для майнинга через ихние райзеры с питанием, но БП для них ещё не пришёл, оказалось что мой тихонький блок питания на 750 ватт может орать как ебанутый, под полной нагрузкой",
"Ну так я о том, что 1080ti ~250W в разгоне, о каких 75W идёт речь?)",
И вопрос у меня остался такой - какой смысл в этом случае имеет минимизация |w|?,
Почему же мы все еще хотим уменьшать веса?,
"Расскажите мне про размер изображения, который подается на вход сети при детекции

Как я понимаю, когда делается детекция, картинки рескейлятся до какого-то разрешения, и часто это разрешение ниже исходного =&gt; downsampling =&gt; теряется информация и поэтому Faster RCNN в оригинальной статье используется (1000, 600), а в статье про SSD результаты (500, 500) выше, чем при (300, 300)

На задаче про машинки ситуация другая, там разрешения и так маловато (5см на пиксель), поэтому кропаются куски из исходных 2000x2000 картинок.

Я пробовал кропать 1000x1000 и 500x500 и кормить Faster RCNN. Есть нестойкое ощущение, что 500x500 работает лучше. Сравнение не совсем честное, и фрэймворки разные и параметры тренировки коих для Faster RCNN море тоже, чуть отличаются.

Но в целом точность заметно разная.

bounding boxes по размеру варьируются от 16 пикселей (короткая сторона мотоцикла) до 256 (длинная сторона автобуса)

Кто мне расскажет, как это надо делать по уму?",
"<@U3NTG7CCS> Как эксперт, что скажешь?",
"Тут еще может быть такой эффект, так как куски на которых нет объектов нет не используются, может получиться что среднее число пикселей, которые не background, в тренировочных данных при 500x500 кропе выше, чем при кропе 1000x1000",
"В оригинальном Faster'е что делают: берут картинку и рескейлят её меньшую сторону до 600 пикселей, а большую сторону пропорционально меньшей и подают на вход сети. В твоем случае надо понять: кропы 1000 и 500 идут уже на вход сети, или там еще какой-то препроцессинг делается? Если идут сразу на вход, то разницы быть по идее не должно, т.к. масштаб то не меняется. Если там делают как в оригинале, то получаем, что кроп 1000 уменьшает средний размер объекта в ~1.5 раза, а кроп 500 увеличивает на 20%...поэтому кроп 500 может работать немного лучше в твоем случае.",
"cepera_ang: :scream: А как ты это определил? У меня один монитор идет через hdmi, но nvidia-settings его правильно определяет.",
"Кто шарит как красиво объединять сетки? Обучил несколько сеток, собрал фичи с последнего слоя в csv и у той и у другой. Потом пробовал и новую сетку обучить на этих фичах и xgb, но результат получается даже хуже чем у любой отдельно взятой. Даже просто усредненные вероятности дают лучше результат. если вместе учить, объединив через ConcatLayer полная беда со сходимостью (кажется,  что одна сеть просто быстрее учится и забивает другую). 
",
"А ни у кого случайно в линуксе нвидиевская карта яичницу не подогревает просто так? У меня как убунта с гномом стартуют, так она с 25 до 60 нагревается, на которых вертушки стартуют, и так и висит",
Не совсем так. Ну точнее не только так. На это можно смотреть также как на регурелизатор. <http://www.ccas.ru/voron/download/SVM.pdf> пункт 1.1.3,
<@U1FLG6YR1> серьезная заявка! почему так думаешь?,
"ну т.е., когда меняется API начинаются проблемы, а без изменения дальше не пойти",
"я немного запутался, честно говоря

есть один вариант иметь /api/v1/method, а при изменении набора параметров делать /api/v2/method
есть второй вариант иметь /api/method - и при изменении набора параметров помечать какие-то параметры как дефолтные/необязательные/етц",
"первый вариант можно сделать практически всегда
второй вариант - ну, иногда не получается так сделать, когда все совсем жестко меняется",
"а – ну да, ну да (хотя изменение типа данных тоже можно поддержать)
протобаф вроде как это поддерживает, и grpc на его основе, стало быть, тоже (хотя я сам не пробовал)",
"Когда я щупал, то, вроде, при банальном удалении поля терялась совместимость",
"Мне кажется, на эту задачу надо смотреть, как на попытку чуть-чуть поменять стандартный SVM, а не как на что-то специально выбранное с глубоким геометрическим смыслом. Да и на сам SVM можно смотреть просто, как на задачу классификации через минимизацию hinge loss на обучающей выборке с добавлением l_2 регуляризации. Не знаю будет ли полезно, но вот <https://www.csie.ntu.edu.tw/~cjlin/papers/nusvmtutorial.pdf> Хотя там они тоже никакого глубокого смысла не накручивают, скорее просто говорят, что можно еще так делать",
"У меня, наверное, к протобафу/трифту напряжение скорее потому, что нужно писать протофайлы какие-то, генерить схему отдельной тулзой. Потом выясняется, что генератора схемы под твой язык нет, или что старая версия библиотеки не поддерживает новую версию схемы. 

Геморрой на ровном месте, честн говоря, хотя готов представить случаи, когда это отлично работает. Json в этом плане намного универсальнее.

Мы тут на работе сделали проект, в котором дата саентисты могут просто написать функцию `def compute(x: int, y: Dict[str, List[float]])`, и потом она принимает через REST апишку данные вида `{""x"": 5, ""y"": {""k1"": [1.0, 2.0], ""k2"": [0.0, 3.0]}}`. Долго смотрел на grpc, но так и не решился, руками все нафигачил :slightly_smiling_face:",
"В смысле, подкрутить так, чтобы остался изначальный вид с |w|^2 как основной term, и поэтому добавляют в loss что-то так, чтобы оказалось что надо?",
Подключил обратно и понял как меня наебывали :),
"я заметил на Amazon p2 инстансах высокую загрузку GPU даже если не запущено никаких процессов
фиксится последовательностью команд Optimizing GPU Settings (P2 Instances Only)
можно посмотреть у себя на загрузку в nvidia-smi и попробовать пофиксить как в статье",
"<https://www.tensorflow.org/api_guides/cc/guide>
Как раз учить модель с готовым графом можно было в C++, а вот графы создавать только на python.
Теперь в С++ есть какой-то минимум и для создания графов.",
"<@U34Q3KU8H> можешь сфоткать как выглядит твой сетап с прихуяренным огромным вентилятором? 
Ну и как ты решил защищать внутренности радиаторов от пыли?",
"их имеет смысл сравнивать с HTML, или какими-то делкаративными DSL как выше пишут",
Вольта конечно суровая железка. Интересно когда увидим 2080ti на её базе? :slightly_smiling_face:,
как собрать торча без конды?,
"На флагманской тесле тоже 16 было, а на условной V40 наверное 24 будет так же как и на P40",
и не совсем понятно как из python пользоваться :kekeke:,
"Парни, о чем может говорить следующее поведение. Есть 2 выборки: Т1 и Т2
Метрика roc_auc
CV на T1 ~ 0.8
CV на Т2 ~ 0.62
Обучение на T1, предсказание Т2 ~ 0.62
Обучение на T2, предсказание T1 ~ *0.78* !
То есть вроде выборка Т2 трешовая, но если на ней обучиться и предсказать более качественную выборку, то скор получается почти такой же как и при обучение на качественной выборке",
А какой процент 1 и 0 в целевой переменной в T1 и T2 и сколько CV от (T1 + T2)?,
"Опачки. Заметил такой паттерн!
Когда обучаюсь на Т1 получается верхнее распредленеие, когда на Т2 - нижнее
<https://ibin.co/w800/3M5I0V7ZQ3fj.png>",
где про pointwise ранжирование почитать? или это просто вероятности тюненого под auc логрега?,
"Народ, а никому на глаза не попадались годные статейки, как строить ""модель"" голоса. Т.е. как по записям человека заставить модель эмитировать его голос?",
"Что-то по итогам тестирования Tableau у нас не прижилось - мы попытались подружить его с postgres и построить небольшие графики по аналогии с имеющейся системой. Может конечно нехватка опыта, но пока решили отложить это решение и рассмотреть конкурентов. А у вас какие основные шаблоны использования: какая-то команда готовит дашборды и далее остальные участники пользуются, либо каждый готовит набор метрик для себя?",
<@U0L8R7092> попадалось демо <https://lyrebird.ai/> + предположения на основе каких статей сделано <https://www.reddit.com/r/MachineLearning/comments/6776pf/d_lyrebird_copy_the_voice_of_anyone/dgocyh2/>,
"Товарищи, поясните, как такое может быть. обучаю lstm и всегда падает после первой эпохи с ошибкой out of memory (использую :keras:  + :tensorflow: ). Это баг что память не чиститься вовремя? целая эпоха то проходит. в интернетах на такое даже сам франсуа отвечал мол делайте меньше батч. а оно падает что с батчем 2048 что с 256 на титане с 12гб памяти. (оригинальный код у человека обучался с батчем 2048)",
<@U0L4KM9R9> у меня такое было когда начинает считаться валидация и используется другой code path (например честный softmax вместо sampled softmax),
"если такой тензор то это не похоже на softmax, видимо производные какие-нибудь - не знаю как это отладить нормально в keras. я считал валидацию на CPU",
"<@U0P95857C> о, как раз сидела размышляла на основе чего этот сервис сделан. Благодарю.",
"Ну например в обоих выборках может быть примерно одинаковый сигнал вида
 ""если (легкий пример) then (угадывай хорошей моделью) else (кидай монетку)""
Выборки могут различаться только соотношением ""легких примеров"", при этом не важно на какой тренировать, модель для легких примеров более-менее одинаково зафитится.",
"вообще в Tableau с postgres все очень лайтово. Очень его советую, это выбор Гугла и Амазона, а также ФБ, в общем передовиков производства. Что до шаблонов, то тут у нас было так. Я его юзала как средство для аналитической визуализации. Например, делала довольно глубокие исследования аудитории, и не знаю, как еще их можно было бы сделать также быстро и легко. Опыт дашбордов и метрик под себя тоже у нас есть, и он успешный",
а как ее отдельно на цпу можно кинуть?,
"вряд ли есть такая прям опция. если модель для валидации/трейна одна и та же примерно, то и на gpu должно работать. я бы попробовал для начала не использовать встроенную валидацию, а запустить ее руками. при валидации не нужно считать градиенты, и памяти наоборот должно меньше потребляться. как это сделать на tf/keras не подскажу",
"не догоняю, как перевести в 2D 100 измерений",
а можешь подсказать как это сделать ?,
"хотя я ставил сервеный вариант, потом туда вкрутил драйвера, а когда ставишь  libcupti для того чтобы tensorflow работал, он притянул lightdm и всё завелось :slightly_smiling_face:",
"куда их поставить, если система не грузится даже?",
"После того как заполнил пропуски медианными значениями количество нужных размерностей сильно возросло. Т.е. теперь оставить не более 100 не получается. Кстати, а как кластеризация дружит с большой размерностью? Ведь не очень должна (проклятие размерности).",
"Да, хорошее замечание. И вот почему её выбирают как |w|^2 обязательно? Почему не другую? Чтобы было похоже на изначальную формулировку SVM?",
ну она вроде как не критично стоит,
"gambler_max: не критично, но прилично :slightly_smiling_face: Тут как всегда -- одно за другое тянет.",
"Нам надо занимать все три первых места, взять <@U14DA0Y4U> ещё с собой и чтобы избежать позора, когда пришлось всех трёх лучших финалистов обламывать -- им придётся правила менять :slightly_smiling_face:",
"Тоже все больше думаю о водянке. Но пока создается впечатление, что подавляющая часть пользователей водянки это дрочеры, которые куда больше сосредоточены на том как это выглядит, чем на температуре. 
т.е. мне не встречались гайды типа: ""вот это самый брейнфак фри набор с фиттингами-хуитингами. температура упала на столько-то. выглядит хз как - не смотрю на корпус""",
а есть ли где “заслуживающие доверия” тесты водянки vs воздух для gpu? я встречал только отзывы что вода дает -20-40 по сравнению со стандартным охлаждением,
"Есть, нужно смотреть по оверклокерским форумам.
А что у тебя сейчас стоит? 
Все очень сильно зависит от сетапа.
Если 1-2 карты, то хороший продуваемый корпус, хороший блок питания+разместить максимально далеко карты даст очень много.
По личным ощущениям плохое водяное охлаждение работает сильно хуже, чем охлаждение воздухом на нереференсных 1080ti при длительной, близкой к максимальной нагрузке: по сути за пару минут вся это водяная система нагревается до предела и разница между тем, чтобы прижать радиатор к чипу или прижать радиатор к трубке, которая температурой как чип теряется. Ну вот и можешь сравнить, какие по площади радиаторы на воздушном охлаждении стоят, а какие на дешёвых водянках.
Другое дело, если есть 3-4 карты, тогда да. И то в таком случае придется находить специалиста, который сможет собрать кастомную обслуживаемую водянку и дать на неё гарантию. И стоить это будет дорого, там один водоблок для одной карты под 150$
  ",
"n01z3: lol, попробовал поконфигурировать, мне как раз их и предложили. Перешёл в адванс мод Your estimated heat load &gt; Cooling Power
890W&gt;656W, лол",
"Да, дефолтные турбины гудят. У меня был выбор купить дешево референсную 1080ti, или асус, упоминаемый выше, тоже 1080ti за адекватные деньги. Выбрал второе и не жалею, теперь у кого не стоит вопрос о 3+ GPU советую этот вариант.
С обслуживаемой кастомной водянкой очень много тонкостей: от выбора термоинтерфейса до схемы расположения. 
Кстати, в тему недавно асус выпустил как раз материнку с полным водяным охлаждением и наставил датчиков от протечек, интересная материнка в тему)
",
"Мне моя MSI Gaming X тоже нравится, но это вариант для пары карточек, не больше. Или делать как майнерский риг, разносить пошире и дуть комнатным вентилятором впридачу )",
"Я пожалел, что взял 2630 v4. Даже после того, как он завелся на нормальных частотах, он был медленней, чем 5930K для DL задач",
"Всем привет! Подскажите, плз, где почитать про анализ и предсказание Time Series данных? Причем как регрессию, так и классификацию. Грубо говоря, есть множество фич, значение которых отслеживается по времени. И надо по из изменению предсказать имело ли место определенное событие и постараться определить, в какой момент оно произошло.",
"предсказать, когда произойдет в будущем, и или просто отметить, когда это случилось в прошлом?",
звучит как нечто похожее на вот это <https://opendatascience.slack.com/archives/C1CEM43TJ/p1494489176487536>,
"vradchenko: какой-то язык непонятный, как такое лайкать?",
"<https://www.youtube.com/watch?v=T_YWBGApUgs&amp;t=7476s> доклад хороший, как применять ml для timeseries",
"<@U0FEJNBGQ> просто хочу ещё раз спасибо сказать, видео как раз то что нужно !",
Ждем когда он сдохнет по невероятной причине :popcorn:,
"Всем привет! Появилась задача предсказания результатов соревнований (скачки, бега), кто нибудать сталкивался с похожим? Подскажите, пожалуйста, в какую сторону копать. Интересует не только ""победитель"" - но и вероятности по в всем участникам.",
"<@U0DA4J82H> а кроме кагловских решений, неужели не сталкивался с такими задачами? Планируется применение в проде, буду благодарен любой инфе, т.к. вообще сложно представляю с чего начинать и что пробовать. Есть набор данных за пару лет, начну с их обработки и анализа - а дальше что, какие лучше выбрать модели? Любой информации в какую сторону смотреть - буду очень рад, спасибо!",
"Однако же, на GPU, где ядра тысячами работает быстрее, чем на быстрых cpu, нет? ",
"Я только что для себя открыл hdf5:slow:. Я правильно понял, что когда сохранишь numpy матрицу на диск, можно работать с определенными слайсами, не прогружая всю матрицу в оперативку? Насколько это медленнее работы напрямую из оперативки?",
"Помню, где то читал, что если кто то в компании не в курсе как выглядит СЕО, значит, контора больше не стартап",
"случаем никто не знает, где можно взять batch iterator для unbalanced классов?",
"какие книги или ресурсы можете порекомендовать для изучения feature engineering? нашел такую книгу, кто нибудь читал? как она? <https://www.amazon.com/dp/3540354875?tag=inspiredalgor-20>",
"Парни, вопрос
У меня сейчас 32гб памяти, работает в 4-х канальном режиме (4 плахи по 8гб). Рядом с каждой плашкой есть еще по свободному слоту
Как я понял, если я сейчас воткну еще `n` плашек, то память будет работать уже в `n` канальном режиме? Это вообще как-то влияет на производительность заметно или нет? 
Допустим если я докуплю и вставлю 1 плаху на 16гб или 2 на 16гб",
"да, вот как пример - <https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/discussion/18754#107511>",
что за тренировки ML? я имел в виду какое-то систематическое описание как делать feature engineering. просто так добавить новые поля в pandas и так умею,
"<@U0DA4J82H>: ну я собственно и спрашиваю, чтобы как то подтянуть свои навыки для успешного участия на kaggle. Иногда народ такие не очевидные на первый взгляд вещи придумывают и тоже хочется этому научится",
"Вот, например, в конкурсе по сберу, который сейчас идет, была проблема с валидацией, когда локальный cv и lb сильно расходятся. И там чувак нашел способ как эту проблему обойти и я не понимаю как он вообще додумался до такого :slightly_smiling_face:",
"для участия в кагле лучше всего помогает читать форумы уже законченных соревнований, особенно треды, где все участники делятся решениями",
"Всем привет!
Посоветуйте, пожалуйста, как эффективно действовать в моей ситуации.
Банк, задача предсказания отклика клиента на предложение ему кредитной карты при условии, что клиент уже имеет банковский продукт (так что есть достаточно информации о клиенте). Нюанс в том, что ""нормальной"" целевой переменной нет. По итогам обсуждению пришли к тому, что надо делать предсказания в несколько этапов.
Вначале предсказать вероятность того, что клиент сам активирует карту в течение первых N дней. Затем, для клиентов, которые это делают, предсказать вероятность того, что они хотя бы один раз используют эту карту. И для сделавших это предсказать либо вероятность того, что они используют карту хотя бы M раз, либо какой процент от лимита они используют.
Если же первое событие не произошло, то предсказать вероятность активации карты при условии того, что клиенту позвонят и напомнят/предложат активацию. И опять же для активировавших предсказывать вначале вероятность хотя бы одного использования, а потом вероятность M использований или процент от лимита.
А затем, для тех, кто не самоактивировался в первые N дней предсказывать всё то же самое для следующего периода и т. д.
И делать всё это для двух каналов отправки, а также для двух групп клиентов.
Делаю это пока на python, но потом придётся в SAS запихивать, но это уже другая история.
Собственно говоря у меня 2 вопроса:
1. Как работать с моделями, которые работают на предсказаниях предыдущих? У меня такая идея: разбивать тренировочные данные на три части. На первой части тренировать модель для предсказания активаций, затем применить её на второй и третьей части. Затем на второй части, там где был предсказан успех, тренировать модель для предсказания хотя бы одного использования. Применить её на третьей части, там где был предсказан успех по активации; затем уже тренировать последнюю модель. Ну и соответственно для предсказания на тестовых/реальных данных использовать эти модели по очереди. Это разумно или что-то не то?
2. Получается довольно много моделей. От 24 для каждого из периодов (которых несколько). По идее для каждой надо отбирать признаки, настраивать параметры и т. д. Видимо стоит просто написать код, который будет для каждой модели брать данные, использовать greedy методы для отбора признаков и подбирать оптимальные параметры?",
"я убрал первые 2000 итераций, так как там 2e+5",
Кто поможет разобраться в чем дело? Могу скинуть код ноутбука,
"Казалось бы 
1) максимизируется ELBO, ADVI прекрасно сходится, вроде как максимизировали и все окей. Как такая оценка может быть побита другим методом, который максимизирует ровным счетом то же самое?
2) Почему возрастает тогда??",
"Иногда ещё заходят туториалы самих пакетов, которые люди используют. Но это совсем для новичков. Иногда полезно читать даже из других языков( на R, например, много всего хорошего написано в том же caret и mlr, h2o)
Есть ещё интересные идеи тут: <http://www.win-vector.com/blog/> , но книжка из не на столько полезна, как их блог.
Это, конечно, всё в последнюю очередь нужно читать, если есть время нужно читать и нет идей.
",
"Подскажите пожалуйста вот по какому вопросу. Есть задача в выделении ноды/селектора карточки товара из хтмл'а страницы. Страницы с разных сайтов. Есть обучающие примеры ввиде страниц с верно извлеченными селекторами. У меня возникает проблема уже на этапе решения о том в виде какой задачи это представить. Стоит ли делать, грубо говоря, бинарный классификатор отдельных хтмл элементов и натравить его на все сотни нод на странице? Или представить как задачу anomaly detection? Или сперва каким-то образом сузить место поиска ноды до разумных пределов и только тогда пускать в дело бинарный классификатор?",
"<http://flightconnections.com|flightconnections.com>, не знаю правда как вытащить",
"если кластеры с хорошим контрастом найдутся, то какая разница сколько PCA потеряет информации? (шашечки или ехать :-)",
"Кто не читает треды, но хочет почитать про FE книгу, вам сюда- <http://shop.oreilly.com/product/0636920049081.do> . Триал без карты 10 дней, можно повторять до бесконечности :pirate:",
"n01z3: огого, где такое обещают? я только про v100 видел",
"<@U4WMPCB7W> я вот запутался. А зачем информацию теряем? Вот скажем человек зашел в сентябре ноябре и декабре. А остается только в декабрьском бине. Или мы интересуемся только тем, как долго этот зареганный в августе человек продержался?",
"Кто как разворачивает новые машины в облаке? Хочу постакать модели, раньше я брал машину в DO и в ручную устанавливал анаконду и другие нужные вещи. А сейчас почитал про kaggle/python докер образ и вроде в пару строк можно все что надо развернуть, но выглядит too good to be true честно говоря.",
"Че в среднем стоит собрать скомный домашний сервак с 1080ti, для ML на картинках,и какие рекомендации будут?(раньше не приходилось сервак собирать)",
"<@U0DA4J82H>  у меня вечно xgb как то криво ставился, приходилось переустанавливать несколько раз. Он же не включен анаконду вроде",
"Привет всем! Подскажите, как можно оценивать положение (+неопределённость) локальных экстремумов функции, которая смоделирована гауссовым процессом? Т.е. например я могу нагенерить сэмплов из GP, в каждом найти все локальные максимумы - но не очень понятно, как по-хорошему потом объединить ""соответствующие"" максимумы в разных сэмплах, чтобы для каждого из них получить положение+-ошибка.
Как-то ещё может лучше формализовать задачу?",
"<@U4LSAJSQM> это хорошо. Я сам в свое время хитро запросил его на последнем году.
Сейчас хочу снова поступить/отучиться + мб как можно скорее защититься, тк в принципе там просто бюрократии всякой много)
...У нас просто товарищи начальники думают еще одного чел-ка  в команду взять, 
кот-ый мог бы и более менее системный код на питоне писать, также и реинфорс алг-мы имплементировать (мб не оч. сложные)
&gt;&gt; но вот им как всегда PhD хочется и пока не ясно что они там порешали
-&gt; с питоном как-то попроще, а вот людей, кто бы в реинфорсе хоть немного бы разбирался - найти гораздо сложнее)",
"чат, а подскажите как мне в tf динамически сконструировать shape на основе размерностей 2х входных тензоров?
я хочу что-то в таком роде сделать:
```
        n, c_main, h, w = tf.shape(tensor1)
        n, c_other, h, w = tf.shape(tensor2)
        padding = tf.zeros((n, c_main - c_other, h, w), dtype=tensor1.dtype)
        other = tf.concat([tensor1, padding], axis=1)
```",
"я делаю так, но скорее всего, как это всегда бывает c tf, есть какой-то более правильный но незадокументированный способ",
"А кто мне расскажет, как заставить вентиляторы на GPU крутится шустрее?

А то температура идет вверх, а nvidia-smi мне говорит, что частота вращения 45% от максимальной.

Я тоже хочу как у <@U1CF22N7J>, чтобы можно было конец эпохи по звуку определять.",
На винде всё просто -- запустил какой-нибудь афтебёрнер и двинул ползунок куда надо,
"Винда - низкий стиль. :slightly_smiling_face:

Кто-нибудь знает как в линуксе это сделать?",
"господа, кароч такая тема хз чо делать не шарю, чот проц стал перегреваться (судя по логам сначала тротлит, потом все вырубается изза перегрева проца выше трешхолда, 87 градусов, раньше когда я его грузил на полную выше 50 не поднималось); чот смущает что когда пальцем трогаю радиаторы на проце или крышку от водянки то как то далеко не 87 градусов

ну тип очевидное что мне может в голову прийти что водянка не работает, может может быть что то не очевидное?",
а как понять качает или нет? :ololosh:,
"ясн спс посоны, обнадежили -) я думал что может это пизда какая пришла, но раз вы не высказали таких гипотез )",
а чо кстати радиаторы тогда около проца не такие горячие как ожидалось бы при 87ми?,
"ну как там эти железяки около называются, не радиаторы наверное, а может они и не у проца, ща кароч фото пришлю",
"По-моему 99% применимости пайчарта упирается в ситуацию ""я дезайнер щас я вам нафигачу постер с мутными визуализациями"" (слово какое-то умное было для этого, из головы вылетело), и там повставляю пайчартов в каких-нибудь пузырьках.

1% реальной применимости - это когда все данные, которые нужно показать - это одно единственное разбиение неких условных 100% на две-три-максимум пять частей. Тогда пайчарт хорошо показывает о чем суть.

Но уже два пайчарта рядом - это неочевидная визуализация.",
Звучит как самое правдоподоное объяснение,
"Привет! Такой вопрос: пытаюсь улучшить разрешение карты загрязнения воздуха (из 34х34 в 102х102) с помощью сетки для сегментации (тирамису с двумя входами для картинок размерностью 2 и 6 (а не как 3 у ргб)). По пути у меня несколько решейпов на данные ( для таргета из (102, 102, -1) в (-1, 102, 102, 1), например), потом получаются такие картинки (слева таргет, справа предсказанное). Почему такие разбросы в значениях, откуда артефакты (и как от них избавиться)? Спасибо",
"Всем привет! Обучаю inception подобную сеть на cifar100 с батч-нормализацией. Проблема в том, что алгоритм adam странно себя ведет и время от времени делает скачки, которые резко увеличивают значение функции ошибки, затем опять ошибка начинает медленно уменьшаться. Сталкивался ли кто-то с такой проблемой? Или, может быть, кто-то обучал такие сети, расскажите, какие методы и параметры использовали для обучения.",
"а вот где или кто может помочь с перетаскиванием существующего хозяйства в новый корпус? например, в тот же define s?",
"я, как бывший тру программер, не очень разбираюсь в компьютерах",
"Нет, вот скрин из tensorboard. Находил у других людей графики, где adam ведет себя точно так же, если использовать батч-нормализацию в сети.",
Я рекомендую обучать SGD с Nesterov Momentum с понижением LR когда лосс выходит на плато. С картиночными сетками с имаджнета этом работает лучше всего для меня. ,
"Спасибо! попробую. Можешь подсказать какой точности тебе удавалось достичь, если ты обучал на cifar100?",
какую максимальную длину ты брал?,
"ну это полнятно. но мне на самом деле нужно ухватить как можно большую длину, наверное",
"Неудачно запустил что-то мультипоточное из юпитера, сам юпитер-сервер убил, но куча процессов, пожирающих тонны памяти отвалились (парента нет, ppid=1) и сами умирать никак не хотят. Как мне красиво найти все процессы, пожирающие больше чем N гб памяти и отправить им sigterm? Неужели нет готовой утилитки для этого? (понятно что можно погрепать и посплитить вывод топа по колонкам, но это как-то топорно)",
какие там тогда внутренности если без стенок? все будет продуваться...,
"Мне иногда кажется, что тут гуманитарии собрались. Особенно после того как Паша трогал декоративные радиаторы на питании и удивлялся почему процессор 87 С, а они не теплые.",
<@U0H7VBQQ1> какую максимальную длину в одном патче можно захватить?,
"<@U041P485A> Серега, расскажи детали водного сетапа, который протек на черные титаны. Какие трубки, какие фиттинги юзали? В каком месте протечка произошло?",
"вид ансамблирования говорит о том какие модели можно объединять, стекинг позволяет объединять разные модели ( в отличии от бустинга и бэгинга), точность, конечно, тоже не всегда растет, плюс производительность из-за обилия моделей может быть хуже, но точность может поднять. Прикол в том что разные модели могут обучаться на разных подвыборках, использовать разные признаки и т.д. + обучаешь на разных подмножествах (+кросс-валидация) должны уменьшить переобучение ( но не исключают его !)",
"А когда накидаешь больше фильтров, там число параметров уже распухнет",
"ну вот да. поэтому и встаёт вопрос, какую максимальную длину рецептивного поля разумнее всего брать",
Проверь на какой нибудь эпохе что она там вообще выдаёт на валидации,
"хотя я знаю много true story, когда народ в серверные просто бытовые кондеи ставил как раз с вентиляторами",
"Ты задачу то и не описала толком. Если последовательностей много, но разной длины, тогда сама длина может быть дурацкой суперфичей.
Какая природа у последовательности btw?
ДНК какая-нибудь? Тогда можно почанкать и word2vec посчитать.",
"выглядит мило, спасибо, но как-то маловато, да и больше обозначено как темы, хотелось бы поток конкретных вопросов, оценить пробелы",
"а поток конкретных вопросов реально поможет? `kl divergence, other divergences ` - могут быть как довольно общие вопросы так и доказательства всяких теорем. Зависит от упоротости мне кажется. Пните меня если я слишком наивен",
"я в этом чатике столько разных страшилок про liquid cooler'ы прочитал, а когда покупал комп продавец о них как об обыденности говорил. кучу собранных компов показал, со всяким моддингом. правда сказал, что один раз действительно сожгли видюху",
"а зачем вообще использовать tf-idf, если есть w2v и даже более совершенные модели эмбеддингов?",
"<@U065VP6F7> у меня получится потом каким-то образом развернуть то, что свёрнуто dilated convolution в последовательность такой же длины как на входе?",
"dilated -- это точно такая же свертка, `mode='SAME'` или как в твоем фреймворке задают, и выход будет такого же размера, что и вход",
"советую Kaldi ASR (C++) для Speech To Text. А с Text to Speech не работал, но слышал такие фреймворки как Lucida на Java",
"Подскажите, а вообще актуально использовать таргет кодирование для задач регрессии, то есть например для каждой каттегории каттегориальной фичи считать регуляризованное среднее, как Станислав Семёнов неоднократно рассказывал (только он всегда применял это в контексте классификации)",
"videodanil: ""таргет-кодирование"" (оно же ""weight of evidence"" в олдскульной терминологии) - это в первую очередь метод регуляризации модели. Применять его надо когда категорий или фич так много, что банальный one-hot взрывает размерность модели.

А то, применяешь ли ты эту регуляризацию в классификации, регрессии, кластеризации или еще чем-нибудь - без разницы.",
ну в общем это всё равно не объясняет почему размерность останется той же,
"Вот прямо так и спрашивали, типа что такое KL divergence, зачем нужно, когда использовать, какие недостатки. Довольно поверхностно, естесвенно, без теорем и прочего",
"Когда размерность меняется? Когда ядро сворачивается на краях тензора. 
Можно западдить, так чтобы размерность не менялась. Именно это и происходит, когда `mode='SAME'`, размерность уменьшается когда `mode='VALID'`.
Прочти уже про арифметику сверток",
"Я просто предостерегаю от ситуации, которую сам недавно наблюдал. Коллега начиталась про WoE (это у статистиков в банках почему-то популярная штука) и давай его вкорячивать в датасет, где всего десяток категориальных фич с тремя-пятью категориями.

То, что она по факту выкидывает данные, и тупой one-hot здесь даст заведомо лучший результат ей в голову не пришло. Может тоже Станислава наслушалась :slightly_smiling_face:",
(Ну и особенно забавно когда поверх такого таргет-кодинга фитят линейную модель),
Каждый раз когда видишь новую проблему попробовать применить к ней все свои любимые методы решения. В упор сейчас не могу найти где это у Фейнмана сказано было но по-моему именно у него (по крайней мере откуда-то у меня и коллег такое выражение зародилось уже давным давно),
"cepera_ang: При том, что я в общем тоже скорее считаю что слабое звено в технике — люди, тем не менее твой пример звучит так, словно ты никогда не сталкивался с текущими трубами, холодильниками/кондиционерами с вышекшим хладагентом и прочими проблемами, которые распространены ровно так же везде где есть такие системы)
(это правда всё ещё не отменяет “нормально делай — нормально будет” принципа, но _вероятность_ есть всегда)",
Вообще выбор таких огромных корпусов не очень большой как мне показалось.,
а ты как вентиляторы по нему раскидал?,
"а на боковую крышку ты не ставил? с охлаждением норм, как я понимаю?",
"woe как раз часто улучшает скор даже, когда переменная непрерывна и пихать потом это в логистическую/линейную регрессию разумно, особенно, если есть ридж регуляризатор.
Но я сам всегда начинаю с ohe :troll: ",
у меня линейный свм порвал как тузик грелку Xgboost на ворд2век + bigARTM. ,
"Ну я вот не видел чтобы WoE положительно влиял на скор, когда атрибутов и так мало - всё больше наоборот (в описанном выше примере терял порядка 4% точности, что существенно).
Тут и простая логика подсказывает что чем навязывать модели свои коэффициенты гораздо лучше дать ей выучить их самих, лишь бы данных хватало.",
"Когда данных мало или атрибутов/категорий много, тогда конечно все методы регуляризации полезны.",
"Да, это довольно много, у меня цель - достичь значения 1 на валидации. Сейчас нашел исследование ребят из Microsoft, у них лучше всего работал SGD c Nesterov, как советовал n01z3 и RMSprop c learning rate 0.001. Сейчас запустил с этим алгоритмом, посмотрим.",
"Я бы поменял:
Мать на Asus X99-E WS
6 Тб на 2х 3 Тб
Друго кулер как советует Женя
Ну и видюхи надо максимально разнообразные, конечно. А именно: Titan X (Pascal), 1080Ti, 1080, 1070. Ато это не дело, что две одинаковые.",
"n01z3: у меня как один жесткий раскручивается, я кричу немцы и под стол прячусь, а ты целых два предлагаешь",
"Есть проблема с дообучением xgboost-а. Хочу проверить, как растет точность при добавление данных. Датасет - `california_housing`. Бью на трейн-тест (70/30). Бью трейн KFold-ом на 20 фолдов, обучаю на первых 10ти (проверяю шейп, половина трейна, все верно). Сохраняю бустер. Потом в цикле от 10 до 20 добавляю к данным по одному фолду (проверил шейп, все верно), дообучаю так:
`model = xgb.train(params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=5, xgb_model='./xgb_dump.model', verbose_eval=True)`
В конце каждого цикла сохраняю модель.
Проблема: первые цикла 2-3 все норм, потом начинает глючить early_stopping. Останавливается все время на одной и той же итерации, причем пишет:
`Stopping. Best iteration: [60] eval-rmse:0.491039`
Тогда как в 60ой итерации написано:
`[60] eval-rmse:0.493182`
Начинается новый цикл, модель дообучается, а ерли стопинг останавливает на 60й итерации и т.д. оставшиеся 7-8 циклов. 
Может кто сталкивался с чем-то подобным?",
"Неа. Я умом понимаю, что нормально все будет, но тот факт, что у меня скорость тренировки сбрасывается без всяких видимых причин и я не знаю почему, заставляет меня сомневаться в своих умственных способностях. А так как ты со всем этим уже игрался - уточняю.",
"Ну а так я с тобой согласен в том, что
&gt; некоторый процент брака\проблем и т.д. есть, но в целом, он достаточно ничтожен
единственная проблема в том, что в автомобиле у тебя под двигателем асфальт и тебе в целом пофиг (если не порвало шлаг в мясо на трассе, но это опустим), в ванне — у тебя плитка какая-нибудь и ущерб будет опять же только если всё ппц как зальет, а вот в корпусе у тебя может стоять три титана — и это ой как неприятно может быть.
В общем я за матожидание ущерба топлю (oh irony)",
"вынужденная некрофилия - я тут кое-кому в слаке обещал кой-чего посмотреть еще два месяца назад и это “кое-что” никак не хотело работать на windows 10,7 так как писалось во времена диназавров. Вот пришлось XP в торрентах копать (жду когда FBI  в дверь постучит)....правда и на XP тоже херня какая-то получается, поэтому буду дальше рыть могилы на кладбище ОС",
"Блин, судя по виду баночки, её даже не нужно покупать канистрами — вдвойне не понимаю почему все воду заливают (и не только производители — есть же комплекты, которые ты сам заполняешь)",
Или трансформаторное масло -- вот уж где точно проводимость какая нужно ),
"Вот да, мы тут совсем видимо не сварщики
(я-то точно, у меня вообще только ноуты, где я максимум термопасту меняю :kekeke:)",
"Попробуй любое из вышеперечисленных, где есть реализация на фреймворке с которым ты знаком нормально",
"Правда довольно поеблись, но если что можем подсказать чего куда",
"tuzoff: я знаю true story, когда народ вешал наружный блок такого кондея в этой же серверной :slightly_smiling_face:",
"Мы взяли тут и правили то, как оно жрёт данные: <https://github.com/precedenceguo/mx-rcnn>",
"зачем тебе думать про разрешение, если можно ходить через ssh :kekeke:",
"Как думаете, с помощью чего можно добиться похожих результатов как в этой статье? <https://geektimes.ru/post/288548/?mobile=no>",
"И кто имеет опыт работы с голосом при обучении нейронных сетей? Очень интересуюсь  этой темой сейчас, отзовитесь кто занимается, пожалуйста )",
"Вот несколько фоток как обещал:
<http://soloro.ru/images/2017/05/Korpus-1.jpg>
<http://soloro.ru/images/2017/05/Korpus-2.jpg>
<http://soloro.ru/images/2017/05/Korpus-3.jpg>
<http://soloro.ru/images/2017/05/Korpus-4.jpg>
<http://soloro.ru/images/2017/05/Korpus-5.jpg>",
"""Грег Коррадо — старший научный сотрудник и технический директор исследовательского центра в Google, один из создателей команды Google Brain Team.

Грег занимается разработками на стыке искусственного интеллекта, нейроинформатики и масштабируемого машинного обучения. Перед тем, как приступить к работе в Google Brain Team, Грег работал над внедрением машинного интеллекта в пользовательские продукты, как, например, Rank Brain и SmartReply, а также для разработчиков при помощи программного обеспечения с открытым исходным кодом (например, TensorFlow и word2vec).

 Место проведения: Технопарк ""Сколково"" конференц зал на 3 этаже.
30 мая c 15:00 до 18:00
 Участие бесплатное по предварительной регистрации""


<https://robotics.timepad.ru/event/492985/>",
"а как работает Сири или Гугл Хелп? Ты говоришь и аналоговый сигнал конвертируется в цифровой, енкодируется и потом посылается асинхронно HTTP на сервер Сири? или все это происходит на самом телефоне?    Соответсвенно, другие приложения с Deep Learning, например как Prisma и тд.  Или модель тренируется на сервере, а потом когда скачиваешь прилагу, то только модель распознает на самом телефоне, а на сервер посылается данные для доп. обучения",
"не уверен, что корректный канал для вопроса, но редиректните, если не прав:  в 2015 был вот такой проект, и даже более-менее работал - <https://en.wikipedia.org/wiki/Prismatic_(app)> (ты читал или смотрел посты, и он предлагал тебе другие посты из неизвестных источников схожей тематики), но денег заработать он не смог и закрылся,  
1) может кто в курсе если что-то похожее щас в живых ? 
2) навскидку по описанию можете накидать, как бы его собрать себе на коленке ? я так понимаю, это что-то вроде topic classification + по прочтению/лайкам/шарам дается скор каждой статье + через какой-нить хгбуст все новые статьи прогоняются через все теже топики выдается финальный вес и в топ твоего фида попадают самые лучшие статьи.",
Хочется сделать по аналогии с тем как они реализованы в самом tf,
"есть еще всякие гибридные варианты, когда на телефоне считаются какие-то ембеддинги и улетают на сервер",
<@U07V1URT9> откуда инфа? Я как-то смотрел спецификацию там как раз нечетные слоты самые быстрые. И вроде как две карты предполагалось как раз засовывать в 1-й и 5-тый слот. Так что между ними еще и свободное место будет.,
"Призма, как известно, работает прямо на телефоне offline. Второй самый известный пример инференса на устройстве - это OCR, гугловый переводчик вроде как может переводить текст на фото офлайн",
"Я сувал в разные слоты и мерил скорость. Самый быстрый варик, когда две карточки в верхних близких слотах",
"Как повезет, у меня в ноуте две разные плашки разных производителей с разным числом чипов заработали в двухканальном, одного объема конечно",
"yaxeen: но с одинаковыми таймингами? 
Вообще можно забить на канальность, не так много она и дает, особенно в ML, где большинство софта опенсоурс, разработанный не суперпрофессионалами.",
"Интересно, а почему Сири у меня всегда проглатывает первые слово-два? Кривая реализация стэка или особенность алгоритма?",
Нужен один код производителя чипа памяти как я помню. ,
"<@U34Q3KU8H> смотря какой ты хочешь себе корпус. Если размеры не важны, бери самый большой и устанавливай любую от сюда: <https://pcpartpicker.com/products/cpu-cooler/#c=28,29&amp;W=10360>",
"Да, она размером как 2.5, но не дает поставить следующую карту в третий слот",
<@U58H63MMW> какой сейчасть State of art на имаджнете?,
А как из него удалять дообучение? :grinning:,
<@U58H63MMW> какой лучший результат на imagenet?,
"товарищи, посоветуйте, как вообще решать задачу оценки скорости и направления движения мобильного девайса по его датчикам (акселерометр, гироскоп, гравити, магнетометр). мысли пока: получать всякие фичи типа среднего, мин, макс, стд из показаний в пределах какого-то окна и по ним делать регрессию. можно еще рекуррентные сети попробовать для end-to-end.",
"как же чешется начать над ним издеваться, но канал жалко",
"Всем привет,  кто - нибудь использует на практике что-нибудь из области A.I.  в автоматизированном тестировании? Статьи по теме читал, даже применял  байевские модели в A/B тестировании, интересует именно практический опыт участников ODS .",
где то можно посмотреть на код ?,
<@U07V1URT9> Ты какие видяхи собираешься ставить? Так чтобы быстро все было и без сброса скорости?,
mephistopheies: Как у тебя с силовыми видами спорта?,
"Мы хотели бы напомнить что завтра как и в любую среду утром будет происходить Data Science завтрак в Москве, и мы будем рады видеть любых участников сообщества и старых и новых.

Кстати заведение в котором проходят завтраки (проспект мира 26) тоже самое где в выходные мы будем проводить первый хакатон целью которого будет создание приложений для нашего слака.

Так что не раздумывая регистрируйтесь и помогите сообществу стать лучше )

PS: у нас ещё куча наклеек с python модулями

<https://goo.gl/forms/qJ8JQsOfqzpxkN5m2>",
блять как жеж я ненавижу ковыряться в железяках,
"<@U1CF22N7J> 
да я менял термопасту под процом и тестировал водянку, для этого снял все платы памяти, все аккуратно как мне кажется делал -)

ну ща всунул и один слот не работает, а платы все ок

причем у меня оператива с диодами, и диоды светятся когда плашка воткнута, но память не видно",
"Подскажите, пожалуйста, куда копать подсчет объектов на фото/видео? Есть однотипные объекты (в одном случае люди, в другом машины), хочется их считать. Считать хочется нейронной сетью, конечно :wink:  Хочется запихать в нейронку фото (кадр с видео), получить число машин. Посмотрел работы зарубежных товарищей, попался подход с оценкой плотности (но это приблизительный подсчет для огромных толп). Как делают знающие люди? Что читать? Где смотреть?",
"<@U411PKASW> Что-то как-то это звучит дорого, не уж-то по  проще не детектировать? генерировать например, id для дебага, много гуи фреймворков кто предоставляет такое при опции дебаг",
всем доброго времени суток. Такой вопрос у кого какие предпочтения; macbook или windows. вопрос связан с выбором для Python and R,
kohrah: а как был получен новый shape изображения? Вообще канонично к маске применить такое же преобразование,
а почему плохо просто маску отресайзить?,
"это уже зависит от того, какое преобразование было применено к исходному изображению.

например, у вас есть картинка и маска к ней
вы ресайзите картинку, например, сплайном
но маску так ресайзить нельзя - она перестанет быть маской, а станет каким-то странным изображением непонятно чего",
"я нубас, поэтому торможу, как это так в одну строку сделать, чтобы область np.array(), заполненная единицами (маска), отресайзилась в тот же shape, что изображение",
"Вопрос про SQL.

Есть две таблицы с колонками 

`A B`

Хочется одну, которая их concatenation. Как?

Что-то типа того, как в pandas делается через:

`pd.concatenate([df1, df2])`",
<@U4E1EF5CZ> а какие более совершенные модели эмбеддингов ты имеешь в виду?,
"<@U0KQ5M6KX>  я не до конца понял идею, о каких параметрах маски идет речь?",
"если, например, вот такая маска: 
<http://i.imgur.com/L0KOmvN.png>
как расчитать её по преобразованию картинки?",
"Сам относительно недавно начал использовать мак как рабочую машину, до этого лет 8 в windows сидел ",
"Возник такой вопросик: есть `ctr` характеризующий кликабельность поста: `ctr=clicks/views`. Проблема в том, что некоторые посты будут иметь, скажем, 2 clicks / 3 views, что даст им высокий ctr, но мало дают нам инфы о качестве. Как составить score функцию получше, чтобы учитывать сколько реально людей накликало и смотрело пост? А так же какие вообще разделы математики могут помочь достичь просветления в решении подобных вопросов? 

До использования threshold я со своими тремя классами церковно-приходской школы додумался, но хочется чего-то подушевнее.",
Почему не в пятый все-таки?),
"А черт, там наложенные png, хрен знает тогда как загрузить с телефона. Короче, на оф. сайте они пишут, что для 28-40 линий такое расположение оптимально",
"Как видишь, в мануале не так.",
"я вообще не понимаю зачем нужен Виндоус для IT, когда есть Unix.  Ну только, чтобы игры для Винды писать, да Ajure попользоваться, потому что Amazon дорогой :troll: , еще вроде популярно хранить копроративные учетки в Винде, чтобы потом с Linux серверов Керберосом и Самбой к ним подключаться",
"ну вот, к примеру, датасет Cars (<http://ai.stanford.edu/~jkrause/cars/car_dataset.html>) интуитивно мелкие детали играют очень большую роль, но при этом объекты крупномасштабные, как тут быть?",
"Это ML детка, какие тут абсолютные правды",
"У кого-нить есть опыт с интерфейсом для наружных видеокамер наблюдения.  Имеется ввиду, если есть сервер, который может слушать по HTTP или по TCP сокетам и делать Image/Face Recognition, то как можно получить видео в Real-time режиме с видеокамер и обрабатывать их?  Какой интерфейс у видеокамер?  Где можно почитать  или лучше в кратце объясните :slightly_smiling_face:",
"Писал на плюсах многопоточную библиотеку с конвейерной архитектурой с использованием intel tbb и её класса pipeline где были этапы: захват кадра, детектирование лица, трекинг, детектирование антропометрических точек и нормализация, вычисление дескриптора, запись в БД. Трекинг позволял сильно экономить ресурсы и не детектировать лицо на каждом кадре.
Real-time или нет зависит от алгоритмов, числа потоков и fps/разрешения камеры.",
"Вопрос к тем, кто занимается машинным переводом. Какие метрики качества используете? Пока ориентируюсь на BLEU <https://en.wikipedia.org/wiki/BLEU>",
"<@U411PKASW> Интересно.   А почему был выбран именно такой подход,  а не стандартный - бизнес спека-&gt;продуктовая спека-&gt; тест ?",
"Парни, а подскажите как получают character-level embeddings? Тренируют skipgram модель с минимальной длиной char ngram 1?",
"Ну да. Я как бы и не писал что для всех сетей это так. Для большинства из тех, с которыми работал, это так",
"skipgram можно добавить как дополнительный loss в обучении, но обычно не нужно",
"Если посмотреть на то, как распознает картинку человек, то, если разрешение маленькое, он может увидеть только контуры и определить объект, но, если разрешение большое, можно будет создать сети, которые, даже, определяют, например, авиакомпанию и модель самолета, плюс множество других, более мелких объектов.",
"Это какие-то философские рассуждения пошли. 

&gt; просто, меньше параметров было в модели и она в память GPU помещалась
 овермного видеопамяти съедается для обеспечения вычислений, редко в какой модели весов на гигабайты.

&gt;  создавать дополнительные слои с размерностью 71х71 и 147х147,
&gt; Я говорю про размерность картинок(тензоров подаваемых на вход слоев)
Эм.

Можно запилить сетку, которая хавает 2к х 2к картинки и делает что-то полезное на 1 GPU.",
"Можно, но архитектура будет как у обычной сверточной сети. Приведу реальный пример с параметрами, у inception их, примерно, 22 млн. Соответственно, чтобы ее обучить на 1 GPU с 11Гб памяти, потребовалось взять батч - размером 100.",
как с баесовской точки зрения подходить к фильтрации выбросов? Выбросов довольно дохрена они сделаны по вине разметки,
А на чем допиливаешь Wav2Letter? Я как раз хотел сравнить CNN-RNN с wav2letter,
"<@U041P485A>
&gt;  о каких параметрах маски идет речь?

Если маска имеет какую-то определенную геометрическую форму (прямоугольник, эллипс и т.п.), то лучше хранить параметры этой формы, а маску перерассчитывать при преобразованиях

&gt; как расчитать её по преобразованию картинки?

Если строгой формы нет, то, вообще говоря, никак. В таких случаях можно применять то же преобразование и к маске, а затем ее бинаризировать по заданному порогу: 
`mask[mask &gt; threshold] = 1`
`mask[mask &lt; 1] = 0`",
"я правильно понимаю что каждому символу в этом случае выставляется в соответствие некий вектор (инициализированный рандомно скорее всего), который подается на вход сети, а затем после forward pass, когда считается backprop, туда придет антиградиент от лосса и изменит этот рандомный вектор в нужную сторону?",
"Да, но в этом нет ничего необычного, просто embedding layer, есть в любом фреймворке. Если не укажешь явно `trainable=False` или как он у тебя, вектора будут учиться.",
не ясно как правдоподобие навесить,
<@U15V81K6V> Мне интересно зачем и что это дает к такому же(или другому) подходу без IE,
"Ну как гауссовская смесь, только распределения будут не гауссовские. Эквивалентно введению бинарного латентного индикатора ",
"Привет! Сегодня вечером будет популярная лекция от прикольных транспортных гуру:

«Как BIGDATA помогает городу бороться с транспортными заторами»
 
Спикер: Штейнардт Герман Вадимович - Советник руководителя проектного офиса перспективных проектов в сфере  IT ГУП «МосгортрансНИИпроект»
 
За последние несколько лет транспортному комплексу Москвы удалось существенно улучшить транспортную ситуацию в российской столице. Достичь позитивных результатов во многом помогают транспортные модели, базирующиеся на технологиях класса big data. О том, как обрабатываются большие данные, что такое статическая и динамическая транспортная модель, в чем их главная функция, и как принимаются решения по управлению транспортной ситуацией в городе,  расскажет советник директора в сфере IT научно-исследовательского и проектного института транспорта Москвы ГУП «МосгортрансНИИпроект» Герман Штейнард.   
В рамках своего выступления эксперт продемонстрирует работу динамической транспортной модели, а также интерактивную карту дорожной ситуации в режиме реального времени.

Республика на Возвдиженке (напротив Библиотеки им. Ленина), начало в 19:30",
"Как говорится умом выделяться надо, а не железками. Железки купить любой может",
<@U3PETUSSE> а как ты соединил ворд2век и BigARTM если не секрет?,
"ясно, а я сначала подумал что ты вектора слов как-то в BigARTM сумел запихнуть как токены)",
<@U55E8EA03> можно поподробнее для тех кому 13? Как этот процесс будет выглядеть в данном случае?,
"Где-нибудь пролетал бенчмарк о том, что по скорости при параллелизации на несколько GPU мизинцем левой ноги на различных фрэймворках? Как я понимаю, паралеллизацию мизинцем на данный момент поддерживают только mxnet и pytorch?",
хмм. Я думал там надо вручную расписывать какую операцию на каком девайсе делать.,
А зачем веса на cpu?,
"<@U0H7VBQQ1> веса на *CPU* размещать не обязательно, а если есть быстрый DMA между картами, как в случае NVLink, то и не желательно. Подразумевается, что input pipeline уже есть нормальный, иначе CPU быстро станет bottleneck-ом, если использовать feed dict-ы",
"привет всем!
подскажите, пожалуйста, кто сталкивался с такой ситуацией - задача бинарной классификации, делаю кросс-валидацию и очень сильно скачет AUC от одного фолда к другому -  вплоть до +-0.15 в зависимости от фолда. Стратифицированный семплинг ситуацию не сильно улучшил (а так хотелось). Попробовал построить кластеры и из них уже семплить - тоже самое(",
"Видяху если что я другую куплю. А вот то, что у меня по соревнованиям отчасти плохо шло из-за этой мути с потерей скорости во время тренировки меня раздражает уже сейчас.

Никто не сравнивал как разгон GPU влияет на скорость тренировки моделей?",
"(Но о какой именно статье речь я не в курсе, не нашел времени выкопать пока сам)",
"ivantrusov: 1. чем и как разбивал?
2. какой размер фолдов относительно всей выборки?
3. какой размер самой выборки?",
"Не видел, но мне кажется справедливый бенчмарк тяжело сделать, так как всегда есть нюансы у либ + зависимость от конкретной задачи.",
"Если они примерно одинаково работают то и бог с ним, а если у одоного отставание, как у TF по ссылке выше - то это везде проявится.",
"Но любой пословный эмбеддинг вполне дополняется фичей ""язык"" и тренируется на параллельных корпусах почти так же, как и изначальный.",
А как character embedding тренировать мне неочевидно.,
Просто как language model - предсказывать следующий character.,
"zfturbo: блин, я даже не подумал, что так может быть :disappointed: Думал как обычно глючит какой-нибудь блокировщик рекламы",
неочевидно как туда прикрутить несколько языков,
подскажите как мне можно выход contrastive loss преобразовать в вероятность принадлежности пары объектов к одному классу?,
"У кулера на CPU вентилятор дует на радиатор (ну когда кулер простой, а не как мы в этом канеле любим).",
"Как оно на практике лучше - ну я хз, наружу, наверно лучше. А еще лучше с одной стороны корпуса внутрь - с другой наружу, чтобы бы неприрывный поток.",
"Задача вентиляторов в корпусе -- обеспечивать максимальный воздухообмен. Как ни ставь вентиляторы, хоть на отток горячего, хоть на приток прохладного -- один хер результат заключается в том, то вентиляторы создают разницу давлений между внутри и снаружи и гоняют воздух",
Вот у тебя как сделано?,
"Ещё хорошо когда потом дует непосредственно на горячие компоненты, это эффективнее, чем ""тянуть"" воздух от поверхности, так как там всякая турбулентность, срыв потока и всё такое",
"У меня ""корпус"" -- это рама из уголка алюминиевого и продувается вентиляторами процессоров ориентированными в направлении спереди-назад и парой вентиляторов сверху (когда заказывал положил в корзину, потому что дешево было, дуют на память, которая как оказалось неплохо греется в таких объёмах)",
"В идеале делать как в серверах -- мощный непрерывный продув в одном направлении. Но в десктопе я бы сделал -- вентиляторы впереди на забор, на проце в том же направлении и на жопе вытяжку",
"И еще выясняется, что у меня почему-то шурупчиков нет эти вентиляторы привентить :disappointed: Вот где их взять, кроме как дойти до компьютерного магазина и поклянчить?",
"при выкрученных на максимум референсных вентиляторах температура GPU не уходит выше 70 градусов... Прикольно. Правда шумят они как самолет. И сразу хочется их разогнать.... Надо будет на выходных озадачитисья. Кто знает, это тоже через coolbits?",
"да, чота он давно не обновляется",
"alexzenin8: Это хороший вопрос, я бы сам почитал.

У меня на лучше всего заходил he_uniform / he_normal.

Но, как я понимаю есть три градации:

[1] Pre trained. Причем лучше чтобы она была pre trained на похожей задаче.
[2] he_uniform, grlot, etc
[3] Криминал вроде все в нули, или все в непонятный рандом.

И что конкретно ты используешь в [2], на сложных сетях вообще не важно. Batch Norm улучшает неидеальную инициализацию. Я думаю, что весь зоопарк активаций в семействе relu, тоже во многом работает в этом же направлении.",
"Я сейчас тренирую inception подобную сеть, где после каждой свертки Batch Norm. 
В итоге из трёх вариантов - 
1. He_uniform
2. He_normal
3. Ничего

Лучше всего работает второй.",
"Так и все - вопрос решен, для keras + inception - he_normal - самый нормальный вариант. :slightly_smiling_face:

Если бы мне денег за эта платили я бы и теорию какую-то под это подвел с касивыми уравниями, но если, по честному, эмпирический факт.

А почему не pre trained инициализация?",
"Всмысле наружу или внутрь? Там же должны быть в идеале вентиляторы обеих конфигураций. Притом дальше обсуждения каких больше: для создания избыточного давления или наоборот, пониженного.",
"Подскажите, пожалуйста, как решать следующую задачу: есть набор пользователей, у каждого список покупок за все время. Нужно предсказать его следующую покупку. Категорий товаров ~20k (распределены почти равномерно), пользователей ~140k, в среднем 15 покупок на пользователя (товары могут повторяться). Заранее благодарен!",
"возникает вопрос, тупо взять маленькую отвертку и выпрямить пины или есть какой то более интеллектуальный способ?",
"(кто не в курсе это трагической истории, у меня не работает один слот оперативы, вынул проц, а половина пинов чот согнута)",
"Только матрица user-item, где на пересечении будет эта самая доля :slightly_smiling_face:",
"Хотел, хотел, и нашёл. Теперь есть несколько вопросов нубских. Суть задачи: есть датчики на машинах, есть сырые данные с них (акселерометры, расход топлива, обороты, прочее). Есть сколько-то аналитики уже готовой (резкие повороты, остановки, разгоны, экстренные торможения). Клиент хочет идентификацию водителя, причём в 2х вариантах 1/0 если к машине привязан только один водитель, и многоклассовую, если несколько. Порядок количества машин &gt;10000. Время на обучение ~500км. Теперь вопросы:
1. Реально ли запустить в продакшен это всё за 3 месяца c нуля, если из тех кто знает про мл есть один я, и опыта у меня - только чатик и курсера ))) (Вряд ли, да)
2. Какие есть хорошие/готовые примеры для подобных задач и где глянуть стек на котором это всё в продакшен выкладывается, если есть
3. По модели: нашим начальникам кто-то из местных универов уже рассказал, что надо всё через нейросетки делать. У меня есть большие сомнения в этом. С одной стороны. С другой стороны, если они хотят детектить по первой минуте езды, то проще скармливать сырые данные прямиком куда-то и смотреть, что получится. Но тогда вопрос: сколько будет в граммах каждая модель и как много она будет жрать ресурсов?
4. С какой стороны подступиться вообще? Я так понимаю, надо пособирать для начала то что уже есть и посмотреть на данные, чтобы слепить из них какие-то вменяемые фичи
Да, вопросы нубские, но заказчики индусы и проект они уже продали, крутись как хочешь.",
"Ну они загнуты когда ты видишь, что среди их наклона прям разброд и шатание. А если просто все одниково наклонены на одной стороне и также одинаково наклонены с другой, то все ок.",
"вот уверен ща соберу когда все, опять чонить откажет",
"Паша, может тебе индуса какого нанять?",
"schertov: 
1. Нет. Смотри как надо сроки оценивать:
  - Сначала подумай, сколько бы это заняло времени ""на глаз"". Здесь адекватна оценка в виде:
      - Разглядывание, очищение данных и фиттинг бейзлайнов - 2-4 недели влегкую.
      - Фиттинг чего-то получше бейзлайна - 1-4 недели, зависит от качества, везения и целей.
      - Шлифовка чтобы можно было ""в продакшене попробовать"" - 1-4 недели, зависит от продакшена.
Итого ~2 месяца по оптимистичной прикидке, для не самого клевого результата, предполагая что никаких тестов, рефакторингов и итераций не нужно, данные не очень злые, и ты понимаешь что делаешь.
В реальности нужны и итерации и тесты и рефакторинг, и данные плохие, и кластер запустить, и модель ты сначала сделаешь какую-нибудь неиспользуемую потому что не подумал о перформансе, итд. Поэтому всё надо умножать на константу реализма - это е или пи, в зависимости от твоих скилов. В некоторых случаях пи, как известно, равно четырем.

Итого, 6-8 месяцев до ""нормального продакшена"" здесь легко уйдет. Вполне возможно, что у индусов константа реализма равна 1, тогда ты справишься за 3, но есть ненулевой шанс что выйдет какое-нибудь говно, работающее на баш-скриптах, предсказывающее константу в 90% случаях, и еще падающее время от времени.

2. Пока писал, уже указали на кегл, вот его посмотри, да. Но по моему опыту примеры из кегла спасают на 10%. 90% времени уходят на унылую специфику продакшена типа ""ой, а у нас вот эта фича приходит только по понедельникам, а по вторникам она равна пяти, и из-за этого твоя модель которая работает прекрасно на тренингсете оказывается не пашет в продакшене, давай-ка еще раз сначала всё"".

3. Начни с бейзлайна. Бейзлайн - это что-то типа логистической регрессии. Она в принципе тоже как бы нейросеть, поэтому начальники не обидятся.
Дальше всё зависит от количества и качества данных.

4. Посмотри на данные, как же ещё. Горячо рекомендую убедиться, что показываемые тебе данные действительно то, что будет приходить в продакшене, а то можешь потратить три месяца на фиттинг ликов ""ой, мы тут добавили пару фич, которые в конце дня высчитываются, а модель должна в начале всё предсказывать"".",
"Как раз первым делом я оставил там один из пулингов, который уменьшал картинку до 15х15, работало быстро, но не очень хорошо.  Теперь картинка, как была 32х32 (cifar100) , так и остается. Вопрос в том, оставлять ли там несколько сверток, которые стоят в линию или сразу же отправлять вход в inception блок с параллельными свертками.",
"Это преждевременное теоретизирование на деталях до хорошего не доведет.
Изначально сетка именно с такой архитектурой хорошо работала на другой задаче. 
Если твой тюнинг даст буст на cifar100 то отлично, но это частность и на иной задачке не полетит.
Именно поэтому очень странно взывать к чужому опыту насчет такого небольшого изменения. 
В ряде статей есть обсуждение входного каскада, например в ENet авторы рассказывают, как и почему делали входной steam именно таким.
Там есть чуток теории, которая натянута на эмпирику.
Только чтобы теория была содержательной, они более осмысленные изменения делают (в частности prelu вместо relu и анализируют обученный наклон в зависимости от номера слоя).",
"Ещё про 2, ""стек, на котором всё в продакшен выкладывается"". Надеюсь у твоих индусов есть ответы на этот вопрос, т.к. тема ""а как нам использовать машинлернинг в продакшене"" - это больное место множества проектов, где на выбор ""правильной"" технологии может уходить немало времени и набиваться шишек, и на занятие ей следует выделять отдельного инженера-архитектора и отдельные сроки.

Самый простой подход (""от датасаентиста"") обычно подразумевает что-то ""я напишу вам функцию f(x) на вашем языке программирования, а ва уж вызывайте как-нибудь"", но в проектах, которые строятся вокруг датасаенса рано или поздно возникает желание как-нибудь удобно поддерживать, настраивать и мониторить эту самую функцию f, ибо версии софтвера меняются, приходящие данные появляются новые, модель нужно обновлять и доучивать, иногда нужно поддерживать несколько версий модели, иногда нужно делать А-Б тестинг, иногда нужно делать кешинг вычислений, отчеты, мониторинг качества, складирование данных. .... и вот вокруг вырастает целая инфраструктура, которая конечно же похожа на космический корабль из спагеттти-велосипедов. Начинаются разговоры о том, что делать и кто виноват, надо ли закупать очередной модный сервис у какого-нибудь вчерашнего стартапа, ""а давайте попробуем"", приходит представитель стартапа, все пробуют, через месяц понятно что это говно, ничего не спасающее, потому что блин легаси, начинается рефакторинг своего космокорабля, всё ломается, у самых горячих возникают идеи ""смотрите как мы хорошо покрасили свой космический корабль, это же стортап получше тех что в гости приходили"", ....

Ну в общем ты понял - сразу спрашивай как клиент хочет деплоить модель, и что ты должен ему дать. Если видишь намек на то, что от тебя помимо датасаенса хотят и архитектурного инжиниринга - будь готов к попадалову, обговаривай сроки, итд.

Ну и конечно всё может быть еще сложнее если у тебя модель должна работать на машине, ибо одно дело поддерживать вебсервис, другое - какой-нибудь оффлайн-девайс на мутной архитектуре в запорожце.",
"Ну и условно просто рассуждать о сетках со свертками, как только появляются residual connections все становится совсем не тривиально.
Интересно было бы послушать мнение [теор]физиков насчет сверток и срезок, похоже ли это на вторичное квантование, но не уверен, что из этого можно какие-то полезные следствия вывести",
"Я рассуждаю с позиции того, как выглядят свертки, если их визуализировать. Получается, что слой stem из картинки 299х299 выделяет немного признаков из областей большого размера свертками 3х3. Гугл провели небольшое исследование и сравнили как влияет размер на точность. 

<https://arxiv.org/pdf/1512.00567.pdf>
пункт 9",
Подскажите если у кого было - если биос  видит систему не с первого раза(причём можно отличить по звуку когда видит и когда нет)+крашится бунта с ata errors - это умирает хард?,
"Коллеги, подскажите плиз куда копать. Есть клиент - потоковое радио и они хотят рекомендательный сервис, чтобы на основе пользовательских “лайков/дизлайков“, прослушанной им и другими пользователями музыки подбирался плэйлист подходящий пользователю. Есть ли уже готовые решения и/или алгоритмы, куда копать? И кроме того не совсем понятно как работать с отклонениями, если юзер, который любит джаз послушал несколько раз шансон или рок, то как работать с такими ситуациями. Стоит ли использовать для реализации TensorFlow или есть что-то более подходящее?",
"Да, всё так. Я же на самом деле как раз джавист, и мл - это у меня в данный момент больше сторонний интерес, который хотелось бы затащить применить и перетащить проект к себе. 
Пока что из разговоров понятно, что на серверной стороне они хотят спарк/хадуп (наслушались где-то). Со стороны машины будет достаточно мощный донгл, в который можно залить свой софт. 
У меня пока из идей -покопаться в сырых данных, Проанализировать. Посмотреть какие фичи можно выделить, чтобы в дальнейшем подготовку данных скинуть на донгл, а анализ уже на бэкенде, если они хотят идентификацию за поездку. Либо простую модель, обученную на серверной стороне, пихать в этот донгл, если они захотят быстрое детектирование 1/0",
"С R не работал, но сейчас у меня такая система - стационарник на убунте(который еще использую как сервер) +  macbook pro. 
Предпочитаю макбук именно из-за удобства интерфейса и физических элементов (клавиатура, тачпад, экран), на других устройствах подобного комфорта мне достичь не удавалось",
"На работе подкинули занимательную задачку посмотреть в бэкграунде. Суть - выделение в тексте новостей пассажей, которые могут коррелировать с последующим повышением/понижением цены продукта (нефть). Кто-нибудь сталкивался с подобным, в какую сторону смотреть? Пока кажется, что мне нужно будет замутить корпус, в котором будет определена потенциальная корреляция содержания новости и изменения цены (ручками?), а уж потом думать как подкрашивать в тексте важное (визуализировать Attention?)",
"Выборка перемешана? Что за модель?
(такое эпизодически бывает на RNN-ках, когда градиенты взорвались)",
"а это лосс на чем? средний по батчу в эпоху, как в керасе считается?",
"А можно в этом случае внутри модели сначала бросить n нечестных монеток, где n число фич, и потом результаты использовать для выбора нужной компоненты смеси? Какие при этом возникают технические проблемы? (я нуб, если что) 

Если предположить, что выбросы в разных компонентах могут быть взаимосвязаны, то тогда можно обойтись одной  бинарной монеткой и двумя компонентами смеси (обычная плотность и плотность для выбросов) ?",
"ну как, это не лучшее решение. но все зависит от качества данных и конкретной специфики. 
например в стримингах не совсем традиционная рекомендация, пользователь по факту не может выбирать айтем, а может проставлять только свою реакцию на него и при таком формате надо неучитывать временную структуру прослушивания.
и в основном вся сложность систем заключается в иерархии и как все организовать. забор данных, обновление моделей, метрики  и тд",
"по крайней мере, я сейчас занимаюсь именно этими вопросами, а не тем, какую модель затюнить",
"npetrenko97: а следующий пост какой должен идти после оперативки? 
Если выдает ошибки памяти любые - это очень плохо в любом случае. Может не оперативка сломалась, а проц или материнка",
"Может кто нибудь подсказать по критерию Стьюдента?  Проверяем H0: равенство нулю среднего против H1: среднее не равно нулю - тут все понятно. Но если берем H1: среднее &gt;0, не пойму как в таком случае делать выводы: если нулевая гипотеза отклоняется, то отклоняется она обычно в пользу альтернативной, но есть же третий вариант, что среднее может быть и меньше нуля, а не больше. Как конкретно сделать вывод, что отклонение среднего больше нуля?",
"Нулевую отвергли, что среднее равно нулю, но как больше стат значимой инфы получить - среднее превышает ноль или наоборот ниже нуля?",
"Коллеги, подскажите плиз как правильно перевести задачу ранжирования в плоскость ML.
Допустим есть 1 000 запросов и 100 000 документов.
Для каждого из 1000 запросов есть только данные о документах релевантных этому запросу (класс1). При том не факт что о всех, соответственно среди остальных документов тоже могут быть релевантные, просто они не размечены.
Задача построить систему которая по каждому запросу будет в топе выдавать релевантные документы.
Формируем датасет, где `X` фичи запроса, документа и пары запрос-документ, a `y` это - релевантен/не релевантен
Тренируем модель, подбираем гиперпараметры.
Для оценки качества используем допустим NDCG, нам важно корректное ранжирование только внутри каждого запроса.
Правильный путь?
Основной вопрос - что брать в качестве класса 0? Рандомный сэмпл из документов не размеченых как класс1? 
Как это решается в поисковиках/рекомендательных системах?",
"Про критерий Стьюдента и проверку гипотез. На  самом деле не всегда нулевая - это то, что жаждем отвергнуть (см. гипотезу о нормальности распределения, например), чаще всего нулевая про то, что легко проверять: в случае со средним проще сравнить с одним числом (ноль, например), чем со всеми остальными. В случае с двусторонним критерием все очевидно: либо ноль, либо все остальное. Фокусы начинаются, когда работаем с односторонними критериями,потому что тут на самом деле нулевая гипотеза уже не совсем про равенство (если альтернатива &gt;0, то нулевая фактически про &lt;=0, не про =0).",
"Для гуманитариев ряды или для тех, кто от вида разностного уравнения в обморок не падает?",
"freshmorning: Спасибо большое! Про другую историю можно пару слов - это в каком направлении копать? Для диплома пойдет и сравнение средних, а это так для себя.",
"Ребята, хотел обратиться к вам с просьбой. Мы хотим открыть доступ к нашему внутреннему инструменту по разметке данных для машинного обучения. И для того, чтобы понять насколько это актуально, и какой функционал требуется, мы узнаём экспертное мнение. Опрос по использованию размеченных данных не займёт у вас больше 2-3 минут. Буду вам крайне благодарен если вы пройдёте опрос или скинете ссылку на него вашим друзьям! Пройти опрос вы можете по ссылке: <https://goo.gl/forms/8QKv5GjtVrOByq6i2>
Надеюсь на ваше понимание.",
"Ребята, хотел обратиться к вам с просьбой. Мы хотим открыть доступ к нашему внутреннему инструменту по разметке данных для машинного обучения. И для того, чтобы понять насколько это актуально, и какой функционал требуется, мы узнаём экспертное мнение. Опрос по использованию размеченных данных не займёт у вас больше 2-3 минут. Буду вам крайне благодарен если вы пройдёте опрос или скинете ссылку на него вашим друзьям! Пройти опрос вы можете по ссылке: <https://goo.gl/forms/8QKv5GjtVrOByq6i2>
Надеюсь на ваше понимание.",
"..и еще подумать, нет ли такого, что падать доходности начинают не из-за утечек, а из-за того, что и до отчетности по косвенным признакам видно, что хорошего не жди (хотя тут я совсем не специалист, фин.рынки - не мое, как там информация распространяется и что может влиять на котировки. только на уровне среднего обывателя с нормальным экономическим образованием могу).",
"Но насчёт красного -- будет, как раз есть красный алюкобонд для моего компа",
"arseni: показывает 13-14$ в сутки, но только тестово запустили, никакой статистики ещё нет. Дома не поставишь -- серверный БП ревёт как бешеный",
"Ну да, как минимум халявное тепло (даже если будет отбивать только электричество), как максимум -- неплохая подработка",
"Это похоже на обучение VAE на данных, после чего (или одновременно) тренировка предсказания таргета по скрытому представлению. Я делал такое для картинок, работало хуже, чем просто по картинке предсказывать таргет. Но тема интересная, тоже было бы интересно узнать, у кого был такой опыт",
"какие метрики для скоринга как бинарной классификации имеет смысл юзать, если разметка есть только дефолтнулся/не дефолтнулся, кроме аука/лифта?",
"я вот с амазонами и гуглами не очень, но вот мне понадобилась машина помощнее, чем моя. С памятью минимум 16 Гб RAM. Я так понимаю, нужно обращаться к Amazon WS или Google. А есть ли между ними разница? Если я правильно понимаю, Google дает сейчас 300$ на счет, чтобы пользовались их сервисом. Моя цель - как можно быстрее запустить свой jupyter notebook на достаточно мощной машине. Как выбрать?",
iorana: а почему не хетцнер ? <https://www.hetzner.de/ot/hosting/produkte_rootserver/ex51ssd-gpu>,
"iorana: Попробуй EMR с autoscaling’ом на spot-instance’aх. Когда будешь что-то долгое считать он будем тебе сам добавлять машины в кластер, в моменты простоя количество машин будет уменьшаться -&gt; меньше платить.",
"вот писал тьюториал, как арендовать AWS спотовый инстанс и jupyter поставить <https://habrahabr.ru/post/280562/>",
"Народ, а кто какие SSD-шники использует?
Я тут потестировал SSD-диск со скоростью чтения 550 Mb/s, форматированный в btrfs...на нем просто навалом лежат картинки и я их python скриптом читаю батчами и по таймингам получается, что с этого SSD-шника чтение происходит с +/- той-же скоростью, что и с обычного HDD",
"я рад это слышать, но какой же у вас курс все-таки интересный! А vowpal wabbit я и вовсе не знаю. Надеюсь уже потом посмотреть!",
"Тогда оно зависло в памяти ещё в тот момент, когда ты её на ссд клал :slightly_smiling_face: И тест показывает скорость парсинга картинки из оперативки )",
а какие бенчмарки можно гонять? Я только рукописный код гонял,
посмотри всякие iotop'ы насколько  вообще при таком чтении диски загружены (хз как это делается в линуксе),
"А кто как использует AWS?
Просто по надобности поднимаете инстанс? А хранище?
Я пока не пользовался, но в голове была такая схема: два сервера один дешевый, второй мощный и один диск.

Дешевый сервер для скачки данных, парсинга, и тп, а быстрый для расчетов, в конце он убивается а хранилище остается.",
"Плюс хдд даст 100Мбайт/сек только на всяком линейном чтении, крупными блоками. А ссд можно подавать кашу из чтения/записи в каком угодно порядке всякими кусками и оно будет отлично работать(*)
*всё сложнее",
"кста, у амазона+гита есть студенческий пак, где накидывают 150$ на амазоне",
"Ребят, а нет ли хорошего актуального обзора по character-level репрезентациям для классификации текстов? Со времен Character-level Convolutional Networks for Text Classification (2015) воды, наверно, подутекло. Хочется понимать и какие есть способы репрезентации получать, и каким алгоритмам принято их скармливать для наилучшего результата. Сориентируйте, пожалуйста, где начать просвещаться.",
"можно посмотреть как я это рассказываю <https://www.youtube.com/watch?v=Zq-cw7YyHKA&amp;t=26600s>
но у нас оставалось мало времени, поэтому очень сумбурно вышло",
"<@U3172NP4Y> А вот еще два вопроса: 1. Имеет смысл сделать так: сначала создать t2.micro инстанс, он, я так понимаю, бесплатный. Все установить. Потом поменять его тип на, скажем, c3.4xlarge. Останутся ли устрановленные библиотеки? 
2. Как создавать образ (на локальном компе или на инстансе) и как его пересоздавать - это я совсем не знаю..
ps. (ок, вижу вот это 
<http://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html>)",
"документации слишком много. Ну и я не программист все-таки, а пользователь.
 yorko: ох =))) вот для этого и нужно было курс продолжать.. с докером раньше не работал, очень плохо себе представляю, как все это делать.

спасибо! разбираюсь дальше..",
"Сделал такое
```x = 2
y = 3
a = tf.add(x,y)
b = tf.multiply(x,y)
c = tf.pow(b,a)
with tf.Session() as sess:
    writer = tf.summary.FileWriter('logs', sess.graph)
    print(sess.run(c))
    writer.close()```
поясните за стремный граф. почему там какие-то непонятные массивы, а не по одному add, mul и pow?",
"tf.reset_default_graph() первой строкой в ячейке, где строится  граф",
"<@U53E5MFNU> а что они засунули то предыдущее поколение nvidia:(( Так ещё и обманывают с процессом, там нужно брать i7, так как в ксионе буст на одно ядро",
"Не могу нагуглить фоток как он выглядит с больше, чем одной видюхой. Похоже, майнеры предпочитают варик Ангарск энидшн Сереги",
"Просто забавно, как одни и те же палки из алюминия в одном случае ""какой :scream:» а в другом :shutupandtensorflow: ",
"Это в лучшем случае, так как такой профиль обычно в магазе не купишь: там будут только уголки и Т образные",
"и вот эти фитинги надо добыть достаточно плотные и прочные, чтобы оно всё держалось. Тоже задача ой какая нетривиальная",
"<@U07V1URT9> чот я не очень представляю, как это собирать на ""треугольном профиле и саморезах""",
"Но для такой простой штуки, как комп думаю можно всё в одном месте купить, ведь по сути это ящик из профиля, ничего сложного :)",
"про него вопросов ноль. Не так красиво, как с витринным профилем, но дёшево и эффективно",
"я думал про треугольный. и ломал голову, как там стыковать его",
Надо на комп примерно 3-4 метра (если на риг как грелка),
"Ещё есть ner-штучка, которая умеет приводить имена в нормальные формы; 
<https://github.com/bureaucratic-labs/natasha>

там кажц была даже ссылка на статью, как это делается, может, можно покопать и в ту сторону.",
Меня заинтересовала работа artwork1 -&gt; artwork2. Не подскажешь где можно поискатьстатьи или хоть слайды с этой конференции?,
"И вот еще вопрос: я хотел бы инстансом пользоваться с разных IP адресов, а как это правильно сделать? Можно ли добавлять IP адрес к уже созданном инстансу? Такое ощущение, что нет...",
"Для всех кто заинтересован в улучшении или изучении нашего сообщества, если у вас будет свободное время, заезжайте на наш первый ODS Introspect hackathon - NLP and Chatbots. Мы будем по адресу: Москва, проспект мира 26, “Райский Пирожок“, сегодня с 19.00 - 24.00, завтра с 9.00 - 24.00 и в воскресенье с 9.00 до 18.00. Если кто то хочет заехать ночью, мы будем находиться на другом этаже этого здания в кафе “Пироговая” которое работате круглые сутки. По любым вопросам пишите мне в слак, почту, или на тел: +79104150297.

Я уверен люди с любым опытом могут чем то помочь нам, или чему то научиться.

Также будет проведено несколько коротких презентаций про популярные NLP алгоритмы и задачи: применительно к online сообществам, и демонстрация того как быстро создавать простых telegram и slack чатботов.",
"Как так может быть: создал c3.4xlarge, должно быть 30 Гб рам. Запускаю код, в какой-то момент получаю Memory Error. Причем на компе с 16 гб RAM вычисления шли.",
"<@U0LSE3J2E> спасибо. Видел это уже. Думал, там что-то новое. Они не рассказывали, как это делали?",
"<!here|@here> повтор сообщения <@U04URBM8V> 

Для всех кто заинтересован в улучшении или изучении нашего сообщества, если у вас будет свободное время, заезжайте на наш первый ODS Introspect hackathon - NLP and Chatbots. Мы будем по адресу: Москва, проспект мира 26, “Райский Пирожок“, сегодня с 19.00 - 24.00, завтра с 9.00 - 24.00 и в воскресенье с 9.00 до 18.00. Если кто то хочет заехать ночью, мы будем находиться на другом этаже этого здания в кафе “Пироговая” которое работате круглые сутки. По любым вопросам пишите <@U04URBM8V> в слак, почту, или на тел: +79104150297.",
"natekin: а те, кто не приедет, окажутся на ODS Introvert hackathon?",
"Продолжение истории про Амазон AWS. Спасибо всем, кто помогал, я, кажется, разобрался, как это сделать. Но вот теперь такая задача. Хотелось бы иметь основные библиотеки (pandas, skicit..). Для этого можно Анаконду устновить, это у меня уже получается.

Но еще хотелось бы туда lightGBM. Так просто, скопировав из git, это сделать не получится, потому что нужно также помучиться с установкой gcc 7 версии. 

Мне уже несколько раз указали на Docker, но я не понимаю, как этим пользоваться, и не понимаю, что это.
-- можно ли взять докер контейнер из lightGBM, каким-то образом установить на aws и запустить с кондой?
-- нужно ли самому собрать уже установленные на локальном компе библиотеки (lightGBM уже есть на локальном) и каким-то образом отправить это на aws. 

Docker пока просто магия какая-то..",
"У меня оочень тупой вопрос про гит: я залил файл версии 0.1. Затем у себя сделал изменения, не закоммитив их. (Локально у меня файл версии 0.1_а). Другой человек в это время запулил файл версии 0.1 с гита, сделал свои изменения и закоммитил их (в гите файл версии 0.1_b). Теперь я не могу ни запулить, ни запушить. Как по науке это поправить?",
"<@U04URBM8V> <@U040HKJE7> а программа имеется? интересует в частности в какой момент будет вот это “Также будет проведено несколько коротких презентаций про популярные NLP алгоритмы и задачи: применительно к online сообществам, и демонстрация того как быстро создавать простых telegram и slack чатботов.”",
"<@U0BLA6ZSR> Завтра будет, когда точно, незивестно.",
"Чет все бп с мощностью 1500+ Вт раскупили. Где еще принято заказывать комплектующие, крмое computeruniverse?",
"Но тогда ты скажешь ""у тебя все советы сводятся, чтобы заказать хуй знает где на непонятных условиях""",
"Да, сейчас перепайка в Москве стоит бешенных денег. И без каких либо гарантий. Бери любой с амазона через посредника. Дорого будет, но делать нечего",
"<@U1CF22N7J> там нужно пересоединить провода так, чтобы был формат для pci. На ютюбе можно посмотреть какие там сопли делают)",
"Нафиг мне к вам ехать, когда у вас электричество дорогое и дерут бабло за всё :slightly_smiling_face:",
"А не подскажет ли кто датасет с фитнес-биометрией? Данные с гироскопов (бег/шаги/прочее), возможно пульс? Разметка пока особо не важна, скорее нужен просто массив данных.",
"По сабжу: затюнил все, что только можно было затюнить. Получил спидап 15-30х относительно последовательной версии и когда объем чтения перевалил за 10К 360р картинок в секунду, HDD начал заметно отставать. До этого, похоже клиентская сторона не достаточно грузила random access.",
"Много информации теряется в jpeg, это как в препроцессинге сделать линейную регрессию не на то, что ты тюнишь. Но в среднем, это конечно копейки в метрике",
"У google была забавная статья, где они работали с частями слов - “от” “дель” “но” “сти” - такая комбинация char и токен-левел",
Вот в последнем конкурсе бриташки как раз жпег давали и он был :meh: по качеству,
"jgc128: У меня знакомый занимается машинным переводом для эстонского активно, и у них state of the art пока как раз такой token-level, где токены находятся просто применив Byte-Pair-Encoding алгоритм к корпусу.

Конкретно в случае с эстонским (и особенно для финского) это имеет смысл, т.к. слова активно модифицируются суффиксами и свободно склеиваются друг с другом.

epäjärjestelmällistyttämättömyydellänsäkään, вот это всё.",
Вот тут пример как используют word2vec для spell чекинга <https://www.kaggle.com/cpmpml/spell-checker-using-word2vec>,
"Есть набор точек на плоскости, параметр d &gt; 0. Подскажите, пожалуйста, как называется следующий алгоритм? Возможно, не очень точно, но смысл: нужно расположить горизонтальную полосу высотой d так, чтобы в нее попало как можно больше точек.",
С точки зрения логики разницы быть не должно просто веса будут больше. Но как на практике точно не знаю.,
"Привет всем. Я правильно понимаю, что ""прогнозирование временных рядов"" - это когда у нас есть большой датасет одного временного ряда, и задача состоит в его продолжении? Но что делать, если мы имеем большое количество сущностей (более 300к), у каждой есть небольшой временной ряд (дата и некоторые данные на эту дату), и для каждой из этих сущностей нужно уметь предсказывать следующую дату?",
"никто не сталкивался с ситуацией, когда lspci/lshw вешают баш? ничего не выводят, при этом обычные хоткеи не помогают, приходится перезапускать ssh сессию. сервак новый, из девайсов 2 видюхи и рейд - подозреваю, админы при установке могли что-то запороть, но что-то не гуглится как побольше узнать что не так",
alxndrkalinin: у меня было чтото похожее когда один девайс на pci-e умирал,
"Спасибо, теперь хотя бы знаю, в какую сторону копать. До этого о панельных данных ничего не слышал.",
"Для русского получается тоже имеет смысл? так как у нас слова модифицируются суффиксами, приставками и окончаниями",
"Может кому интересно. Покопал библиотеку natasha на предмет лемматизации имен и фамилий. В целом намного лучше, чем голый pymorphy2. Если фамилия с именем, то каких-то проблем не нашел. Все приводит к лемме фамилии корректно основываясь на имени. Если фамилия одна, то уже хуже, но все же лучше чем просто использовать normal_form от pymorhpy2. Почему-то в правилах фамилии должны быть всегда с большой буквы. Впрочем можно создать свои правила без этого ограничения. Какие проблемы: фамилии заканчивающиеся на “а” лемматизирует не правильно: понять женская или мужская фамилия часто можно по окружающим словам, но такой анализ будет скорее всего достаточно сложен. Некоторые фамилии просто обрезает: Сафонов -&gt; Сафон. Или просто не находит: Томаров.",
"не совсем понятно, вам нужно предсказывать не значение величины на след дату, а произойдет ли след событие и когда оно произойдет?",
