{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from aioes import Elasticsearch\n",
    "from elasticsearch import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from chunk import TimeDistance\n",
    "from chunk import Chunker\n",
    "from slack_data_loader import SlackLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(['localhost:9200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_synonyms():\n",
    "    \"\"\"\n",
    "    Generate some synonyms in a file. All words separated by comma are treated as equal\n",
    "    \"\"\"\n",
    "    with open(\"help_data/synonyms.txt\", \"w\") as syns:\n",
    "        syns.write(\"xboost, эксгебуст, эксбуст, иксгебуст, xgboost\\n\")\n",
    "        syns.write(\"пыха, пыху, пых, php\\n\")\n",
    "        syns.write(\"lol, лол\\n\")\n",
    "        syns.write(\"питон, python\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_name = \"ods-slack-index\"\n",
    "mapping_name = \"thread\"\n",
    "message_mapping = \"message\"\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "          \"filter\": {\n",
    "            \"russian_stop\": {\n",
    "              \"type\":       \"stop\",\n",
    "              \"stopwords\":  \"_russian_\" \n",
    "            },\n",
    "            \"russian_stemmer\": {\n",
    "              \"type\":       \"stemmer\",\n",
    "              \"language\":   \"russian\"\n",
    "            },\n",
    "            \"synonyms_expand\": {\n",
    "              \"type\": \"synonym\", \n",
    "              # path to synonym file.\n",
    "              # for ES to be able to read it, security policy should be set as described here:\n",
    "              # https://stackoverflow.com/questions/35401917/reading-a-file-in-an-elasticsearch-plugin\n",
    "              \"synonyms_path\": \"/usr/share/config_data/synonyms.txt\"\n",
    "            }\n",
    "          },\n",
    "          \"analyzer\": {\n",
    "            \"russian_syn\": {\n",
    "              \"tokenizer\":  \"standard\",\n",
    "              \"filter\": [\n",
    "                \"lowercase\",\n",
    "                \"russian_stop\",\n",
    "                \"russian_stemmer\",\n",
    "                \"synonyms_expand\"\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\":{  \n",
    "        mapping_name:{\n",
    "          \"properties\":{\n",
    "            \"channel\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\"type\":\"string\", \"analyzer\":\"russian_syn\"},\n",
    "            \"ts\": {\"type\": \"date\"},\n",
    "            \"messages\" : {\n",
    "                \"properties\":{\n",
    "                    \"text\": {\"type\":\"string\", \"analyzer\":\"russian_syn\"},\n",
    "                    \"user_id\": {\"type\": \"keyword\"},\n",
    "                    \"user_real_name\": {\"type\":\"string\"},\n",
    "                    \"ts\": {\"type\": \"date\"}\n",
    "                }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        message_mapping:{\n",
    "            \"properties\":{\n",
    "                \"text\": {\"type\":\"string\", \"analyzer\":\"russian_syn\"},\n",
    "                \"user_id\": {\"type\": \"keyword\"},\n",
    "                \"user_real_name\": {\"type\":\"string\"},\n",
    "                \"ts\": {\"type\": \"date\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "async def create_index():\n",
    "    return await es.indices.create(\n",
    "        index=index_name,\n",
    "        body=index_body\n",
    "    )\n",
    "    \n",
    "async def check_index_exists():\n",
    "    return await es.indices.exists(index=index_name)\n",
    "\n",
    "\n",
    "async def delete_index():\n",
    "    return await es.delete(index=index_name)\n",
    "\n",
    "async def openclose():\n",
    "    \"\"\"\n",
    "    Closing and opening index again reloads synomyms file\n",
    "    \"\"\"\n",
    "    await es.indices.close(index=index_name)\n",
    "    await es.indices.open(index=index_name)\n",
    "    \n",
    "async def populate_index(channel, messages):\n",
    "    await es.index(\n",
    "        index=index_name,\n",
    "        doc_type=mapping_name,\n",
    "        body={\n",
    "            \"channel\": channel,\n",
    "            \"title\": messages[0]['text'],\n",
    "            \"ts\": messages[0]['ts'] * 1000,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "    )\n",
    "    \"\"\"\n",
    "    for message in messages: # make bulk upload here\n",
    "        await es.index(\n",
    "            index=index_name,\n",
    "            doc_type=message_mapping,\n",
    "            body=message\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "async def query_index(query):\n",
    "    return await es.search(\n",
    "        index=index_name,\n",
    "        doc_type=mapping_name,\n",
    "        body={\n",
    "            \"query\":{\n",
    "                \"multi_match\" : {\n",
    "                  \"fields\" : [ \"title^3\", \"messages.text\" ],\n",
    "                  \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "async def search_user(username):\n",
    "    return await es.search(\n",
    "        index=index_name,\n",
    "        doc_type=mapping_name,\n",
    "        body={\n",
    "            \"query\":{\n",
    "                \"multi_match\" : {\n",
    "                  \"fields\" : [ \"messages.user_real_name\" ],\n",
    "                  \"query\": username\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True}\n",
      "{'acknowledged': True, 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "if loop.run_until_complete(check_index_exists()):\n",
    "    print(loop.run_until_complete(delete_index()))\n",
    "    \n",
    "print(loop.run_until_complete(create_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload synonims without recreating the whole database\n",
    "gen_synonyms()\n",
    "loop.run_until_complete(openclose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def index_channel(channel = \"nlp\"):\n",
    "    data = SlackLoader(PATH_TO_DATA, only_channels=[channel])\n",
    "    chunker = Chunker()\n",
    "    groups  = chunker.get_groups(data)\n",
    "    \n",
    "    print(\"Indexing: \" + channel)\n",
    "\n",
    "    workers = []\n",
    "    for group in groups:\n",
    "        users = data.users\n",
    "        for msg in group:\n",
    "            if msg['user'] in users:\n",
    "                msg['user_real_name'] = users[msg['user']]['name']\n",
    "            if 'dt' in msg:\n",
    "                del msg['dt']\n",
    "            msg['timestamp'] = str(msg['ts'])\n",
    "            msg['ts'] = int(msg['ts'])\n",
    "            if \"attachments\" in msg:\n",
    "                for attach in msg[\"attachments\"]:\n",
    "                    if 'ts' in attach:\n",
    "                        attach['ts'] = float(attach['ts'])\n",
    "        workers.append(\n",
    "            asyncio.ensure_future(populate_index(channel, group))\n",
    "        )\n",
    "    return await asyncio.gather(*workers)\n",
    "\n",
    "async def index_channels(channels):\n",
    "    await asyncio.gather(\n",
    "        *[asyncio.ensure_future(index_channel(channel)) for channel in channels]\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing: nlp\n",
      "Indexing: deep_learning\n",
      "Indexing: datasets\n",
      "Indexing: sequences_series\n",
      "Indexing: bayesian\n",
      "Indexing: _meetings\n",
      "Indexing: edu_academy\n",
      "Indexing: edu_books\n",
      "Indexing: visualization\n",
      "Indexing: hardware\n",
      "Indexing: reinforcement_learnin\n",
      "Indexing: theory_and_practice\n"
     ]
    }
   ],
   "source": [
    "useful_channels = [\"nlp\", \"deep_learning\", \"datasets\",\n",
    "                  \"sequences_series\", \"bayesian\", \"_meetings\", \"edu_academy\",\n",
    "                  \"edu_books\", \"visualization\", \"hardware\",\n",
    "                  \"reinforcement_learnin\", \"theory_and_practice\"]\n",
    "\n",
    "loop.run_until_complete(index_channels(useful_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loop.run_until_complete(query_index(\"как использовать xgboost в python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------(nlp)--------------\n",
      "elwis: <https://gist.github.com/Kein1945/9111512>\n",
      "elwis: если надо просто отрезать окончания, то вот это подойдет\n",
      "stajilov: проверял недавно, не умеет нормально, лемматизацию может делать типо но нужен русский wordnet нормальный\n",
      "\n",
      "------------(nlp)--------------\n",
      "elwis: Коллега сделала интересную сравнительную таблицу чатботов, может кому-то пригодится: <https://medium.com/@datamonsters/25-chatbot-platforms-a-comparative-table-aeefc932eaff>\n",
      "alexantonov: elwis: Отличная статья. А совершенно случайно нет на русском языке?\n",
      "elwis: <@U32506X36> К сожалению нет\n",
      "\n",
      "------------(nlp)--------------\n",
      "elwis: Если tf-idf вектора нормализованные, можно вместо косинусной близости считать скалярное произведение\n",
      "\n",
      "------------(nlp)--------------\n",
      "rvnikita: Ребята, привет. Что почиттать чтобы быстро разобраться в основнах NLP? <https://www.amazon.co.uk/d/Books/Natural-Language-Processing-Python-Steven/0596516495> хорошая книжка или есть что-то другое более признаное?\n",
      "octocat: rvnikita: хорошая, но заточена под NLTK.\n",
      "elwis: это главная книга по nltk, есть в электронном виде: <http://www.nltk.org/book/>\n",
      "aledovsky: Одной из лучших книг по nlp на мой взгляд является Martin Jurafsky - Speеch and Language Processing. Это большая книжка, но из неё можно независимо читать главы.  Я бы предложил почитать несколько вводных плюс главы про прикладные задачи. Есть второе издание, которое нетрудно нагуглить в pdf и драфт третьего, который на сайте автора <http://web.stanford.edu/~jurafsky/slp3/>. Третье издание похоже в базовых главах и сильно отличается в описании прикладных задач.\n",
      "buzzword_miner: Обработка неструктурированны текстов \n",
      "Поиск. Организация и манипулирование\n",
      "buzzword_miner: <https://ozon-st.cdn.ngenix.net/multimedia/1010991212.jpg>\n",
      "\n",
      "------------(nlp)--------------\n",
      "elwis: была похожая проблема с bigartm, решил установив --threads 1. А что такое -j не подскажете? это то же самое?\n",
      "khansuleyman: -j - то же, что и --jobs. Одновременное выполнение указанного количества команд\n",
      "ryazanoff: Кто тыкал уже? Там проблемы с 3 питоном\n",
      "angriff07: так там и не заявляется работа с питоном 3... в readme написано, что python2.7\n",
      "\n",
      "------------(nlp)--------------\n",
      "elwis: это главная книга по nltk, есть в электронном виде: <http://www.nltk.org/book/>\n",
      "dimakarp1996: =\n",
      "i: <https://github.com/giacbrd/ShallowLearn>\n",
      "i: m.yurushkin: тоже вае на текстах хочу. а у тебя какой датасет?\n",
      "\n",
      "------------(nlp)--------------\n",
      "0x1337: Корректно ли считать схожесть текстов косинусным расстоянием, если вектора – это не OHE представление, а tf-idf веса?\n",
      "ololo: да, так обычно и делают, если я правильно вопрос понял\n",
      "alex.ozerin: Да, косинус между суммами ohe будет глупым булевым поиском. Tfidf -- разумный вариант взвешивания\n",
      "mrukhlov: а что за ohe?\n",
      "alex.ozerin: One hot encoding\n",
      "mrukhlov: спасибо\n",
      "elwis: Если tf-idf вектора нормализованные, можно вместо косинусной близости считать скалярное произведение\n",
      "amir: а зачем вообще использовать tf-idf, если есть w2v и даже более совершенные модели эмбеддингов?\n",
      "ololo: потому что tf-idf в некоторых случаях лучше работает, например для IR\n",
      "0x1337: у меня линейный свм порвал как тузик грелку Xgboost на ворд2век + bigARTM. \n",
      "amir: Не знал, что такое тоже может быть. А какие тексты используются?\n",
      "0x1337: новостные статьи на русском. \n",
      "evgeny: <@U4E1EF5CZ> а какие более совершенные модели эмбеддингов ты имеешь в виду?\n",
      "elwis: <@U3PETUSSE> а как ты соединил ворд2век и BigARTM если не секрет?\n",
      "0x1337: <@U443HBJ8L> Для документов считаешь распределение топиков, вот и новые фичи. \n",
      "elwis: ясно, а я сначала подумал что ты вектора слов как-то в BigARTM сумел запихнуть как токены)\n",
      "amir: <@U0D8KLBFV> google swivel, fasttext\n",
      "\n",
      "------------(nlp)--------------\n",
      "wingrime: Господа, nltk умеет стиминг русский?\n",
      "novitoll: Судя по тому, что я скачал все данные от nltk `import nltk;nltk.download('all')`, то тут только для en-US. Но можно проверить в директорий, куда все данные скачались. На Linux - это по дефолту хранится в `/home/user/nltk_data/stemmers/`. Тут есть только `porter_test` для инглиша. \n",
      "Думаю, для русского языка можно использовать `pymorphy2`\n",
      "dselivanov: когда я послдений раз смотрел стемминг делался через такую жопу, что после этого я вообще nltk перестал воспринимать за библиотеку. Он транслитерировал русский в английский, потом делал стемминг, затем конвертировал обратно\n",
      "dselivanov: такой вот пиздец\n",
      "gleberof: С русским языком у pymorphy2 тоже не все идеально. \"Открытие банка\" -&gt; \"открыть\", \"банка\". Понятно конечно почему. Но пока pymorphy2 лучшее что есть для стемминга\n",
      "wingrime: Спсб\n",
      "wingrime: А что умеет сейчас нормализацию?\n",
      "alexeyev: <@U50GC05J7> лемматизацию, в смысле?\n",
      "wingrime: В смысле аналогично заменам We'll -&gt; we will\n",
      "wingrime: Раскрытие сокращений\n",
      "wingrime: Синонимов\n",
      "elwis: <https://gist.github.com/Kein1945/9111512>\n",
      "elwis: если надо просто отрезать окончания, то вот это подойдет\n",
      "stajilov: проверял недавно, не умеет нормально, лемматизацию может делать типо но нужен русский wordnet нормальный\n",
      "windj007: если лицензия позволяет, то можно ещё\n",
      "<https://pypi.python.org/pypi/pymystem3/0.1.1>\n",
      "0x1337: +1, mystem пушка.\n"
     ]
    }
   ],
   "source": [
    "res = loop.run_until_complete(search_user(\"generall\"))['hits']['hits']\n",
    "for hit in res:\n",
    "    print(\"\\n------------({})--------------\".format(hit['_source']['channel']))\n",
    "    for msg in hit['_source']['messages']:\n",
    "        print(\"{}: {}\".format(msg['user_real_name'], msg['text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
